# Agent Checklist for SCoVA Project Implementation

## I. Project Setup and Data Acquisition

### A. Project Initialization

- **Continuous Documentation:** Maintain thorough and continuous documentation of the thought process, decisions, and challenges encountered throughout the project. Use a dedicated platform or document to record progress, even during periods of inactivity. This detailed log will be invaluable for tracking project evolution and understanding the rationale behind each step.

- **Preparation for In-Depth Discussions:** Before engaging in in-depth discussions, ensure familiarity with the CVSM-15 research paper and its associated code repository. This preparation should include understanding the relationship between the code and the experimental parameters (learning rate of 0.001 and batch size of 32). Critically, understand the performance evaluation metrics used in the paper: Jensen's Alpha and the Sharpe Ratio, calculated with transaction costs and using the 3-month Swedish Krona Short Term Rate (SWESTR) as the risk-free rate.

### B. Data Acquisition and Preprocessing

This section details the acquisition, preprocessing, and storage of financial data used for training and evaluating the trading agent. Consistent data handling is crucial for aligning the training and trade execution environments.

- **Data Sources and Instruments:** Acquire Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance for stocks listed in both the OMXS All-Share and the S&P 500, covering the period from 2016 to 2023. Adjust close prices for stock splits, but exclude dividends from these adjustments. Additionally, acquire equally weighted index price data for the following indices: OMXS All-Share, OMXS Large Cap, OMXS Mid Cap, OMXS Small Cap, First North All-Share (from Nasdaq), and the S&P 500 (from Yahoo Finance).

- **Return Label Calculation:** Calculate the return label using the following formula: `Return(t+h) = (Close(t+h) - Open(t+1)) / Open(t+1)`. `t` represents the last date included in the chart image data, and `h` represents the holding period, a hyperparameter ranging from 1 to 5 days. This formula calculates the return based on the difference between the closing price at `t+h` and the opening price at `t+1`, normalized by the opening price at `t+1`.

- **Data Storage and 5-Day Windowing:** Store downloaded stock data (both training and testing) in CSV files within `stock_data/train/` and `stock_data/test/`, respectively. The project utilizes a 5-day windowing approach for both input features and the target variable. Further details on this windowing approach and its implementation will be provided in subsequent sections.

- **Filename Convention for Holding Period:** Modify graph filenames to include the holding period (`h`). Incorporate `h` directly into the filename using a consistent format, such as `YYYY-MM-DD__YYYY-MM-DD__3.25__h=3__ABB.ST.png`. Alternatively, store the holding period information in a separate CSV file mapping each graph filename to its corresponding `h` value. This ensures traceability and facilitates analysis across different holding periods.

## I. Project Setup and Data Acquisition

### B. Data Acquisition and Preprocessing

- **Data Source:** Yahoo Finance will be the primary data source for obtaining Open, High, Low, Close, Volume (OHLCV) data for various stocks. This readily available resource offers historical price information crucial for training and evaluating our models.

## II. Model Development and Training

### A. Model Architecture

- **Model Input:** The CNN model receives a 5-day candlestick chart image as input. Generated using `matplotlib`, this chart visually represents the OHLC prices over the 5-day period. The model is trained to predict future returns based solely on this visual representation, without direct numerical price data input. The input data format must rigorously represent the OHLC prices within the 5-day window. The size _n_ of data points used to generate the candlestick chart images will be documented, with special attention paid to edge cases involving the last _n_ data points to ensure proper handling.

- **CNN Output:** The CNN's output will be a single numerical value representing the predicted return over the subsequent 5-day period. This prediction is generated from the visual analysis of the input candlestick chart image. A detailed explanation of how the CNN transforms the image input into this numerical output will be documented.

- **Model Architectures:** A Convolutional Neural Network (CNN) will be trained to predict 5-day future returns based on candlestick chart images. To enhance the model's ability to capture temporal dependencies, we will explore combining CNNs with Long Short-Term Memory networks (LSTMs) or Transformer models. Furthermore, to provide a more holistic market understanding, we will investigate incorporating fundamental and macroeconomic data alongside the candlestick chart images.

### B. Data Preprocessing and Training

- **Data Preparation:** Candlestick chart images will be generated from 5 consecutive trading days of OHLCV data. Each image will visually represent candlestick patterns, trading volume, and a moving average calculated from the OHLC data. Corresponding 5-day future return labels will be calculated as the percentage change in price from the open of the day following the 5-day window (t+1) to the close of the fifth day after the window (t+5): `((Close(t+5) - Open(t+1)) / Open(t+1)) * 100`.

- **Return Label Calculation (Trading):** The return label, consistently calculated across training and trading/backtesting, is determined using the formula `(Close(t+h) - Open(t+1)) / Open(t+1)`. Here, `t` represents the last day of the input window, `h` is the 5-day holding period, `Close(t+h)` is the closing price on day `t+h`, and `Open(t+1)` is the opening price on day `t+1`. During trading/backtesting, `trade.py` pulls future price data from Yahoo Finance for this calculation. This methodology ensures a clear separation between input and target data, preventing data leakage. The trade execution logic uses actual prices from t+1 to t+5 to evaluate model performance, reflecting real-world conditions. This data is not available during the prediction phase.

- **Training Data Scale-Up:** The CNN model will be trained on a large dataset (thousands of candlestick chart image/return label pairs) to generalize effectively. A pipeline will be established to prepare this data from historical market data.

- **Model Evaluation Metric:** Model performance will be assessed using appropriate regression metrics such as Mean Squared Error (MSE) and Root Mean Squared Error (RMSE). Other relevant metrics may also be considered.

- **Data Exclusions for CNN:** The CNN model will be trained exclusively on visual patterns within the candlestick chart images. It will not directly process dates, real price values (only scaled visual representations), or ticker symbols. Care will be taken to prevent this information from leaking into the training process.

## III. Testing and Evaluation

### A. Backtesting

- **Rolling Walk-Forward Validation:** Rolling walk-forward validation will be employed to rigorously evaluate out-of-sample performance and adaptability to changing market conditions.

### B. Performance Evaluation

- **Performance Metrics:** Standard performance metrics, including Jensen's Alpha and Sharpe Ratio, will be calculated to quantitatively assess the profitability and risk-adjusted return of the trading strategy. These metrics will provide a benchmark for evaluating the effectiveness of the model.

## A. Model Architecture

This section details the architecture of the Convolutional Neural Network (CNN) model, including the rationale behind key design choices like input window size and the potential benefits of a soft label approach.

- **Input & Output Window:** The model employs a 5-day input window, consistent with the approach of Jiang et al. (2023), providing a consistent timeframe for analyzing price action. While the model also predicts over a 5-day future period (output window), further investigation is needed to determine the optimal output window length and explore the potential benefits of varying output window sizes. The choice of a 5-day input window, supported by Jiang et al. (2023), balances incorporating sufficient historical information while remaining responsive to recent market movements.

- **From Hard to Soft Labels:** The current model uses hard labels (discrete buy/sell signals) for training. A soft label approach, where the CNN outputs a probability distribution over a range of potential returns, will be investigated. This approach is hypothesized to improve trading accuracy, particularly during validation, by allowing the model to express uncertainty. Research will focus on generating and utilizing soft labels effectively and comparing performance against the hard label baseline. This analysis will include assessing the impact of hard labels on validation performance metrics such as accuracy and precision.

## D. Model Enhancement and Refinement

This section details refinements to enhance the model's predictive capabilities and trading performance. Enhancements include a confidence-based trading strategy, hyperparameter optimization, and architectural adjustments.

- **Confidence-Based Trading with Historical Prediction Error Profiling (HPEP):** A hybrid confidence-ranking approach will be implemented. Trades will be filtered based on a minimum predicted return magnitude (or other suitable confidence metric) and then ranked. This focuses capital allocation on the most promising opportunities. HPEP will further refine this process. A post-training confidence profile will be created by binning validation set predictions based on predicted return and calculating the accuracy for each bin. During backtesting, trades will be filtered based on the historical accuracy of their corresponding bin in the HPEP map, using a tunable `confidence_threshold` hyperparameter. This threshold represents the minimum required accuracy for a trade to be executed and will be optimized via grid search or Bayesian optimization to maximize performance metrics like the Sharpe Ratio. User assistance has been requested to integrate HPEP into the `test_model.py` and `trade.py` scripts. A working prototype with dummy data is recommended for initial testing and simulation.

- **Separate CNNs for Varying Holding Periods:** Separate CNNs will be trained for each holding period (1 to 5 days) to create specialized models. Performance will be rigorously compared across these models using metrics such as validation loss, Sharpe Ratio, Alpha, and Mean Squared Error (MSE) to determine the most effective model for each time horizon.

- **Literature Review: Input Window Justification:** A thorough literature review will justify the use of a 5-day input window, providing a theoretical basis and potentially revealing further optimization avenues.

## Soft Label Implementation Details

This section outlines the technical implementation of the soft label approach for the CNN.

1. **Discretization of Return Space:** The continuous range of potential returns will be discretized into bins. For example, a range from -5% to +5% could be divided into 0.5% increments. This transforms the prediction task into a classification problem.

2. **Model Output Layer Modification:** The CNN's output layer will be modified…

---

## II. Model Development and Training

### A. Model Architecture (Specialization)

This section details the architecture of the models required for the SCoVA project, focusing on predicting the magnitude and timing of price movements, not just direction.

- **Output Encoding for Trading Signals:** Instead of predicting a single scalar value, the models will output a probability distribution over predefined return bins. This allows for a more nuanced representation of predicted returns and associated confidence levels.

### D. Model Enhancement and Refinement (Specialization)

This section details enhancements to the Convolutional Neural Network (CNN) model to improve prediction accuracy and trading performance.

- **Soft Labeling:** Transitioning from hard labels (single scalar return values) to soft labels using a Gaussian kernel. This creates a probability distribution centered around the observed return, enabling the model to learn from the proximity of returns to bin boundaries and potentially improving generalization. (Related to: _Prototype with Probabilistic Trading_, _Backtesting with Soft Labels_)

- **Probabilistic Output:** The CNN will be modified to output a probability distribution over the predefined return bins. The existing output layer will be replaced with a fully connected layer followed by a softmax activation function, ensuring output probabilities sum to one.

- **Loss Function:** The loss function will be changed from Mean Squared Error (MSE) to a more appropriate metric for comparing probability distributions, such as Kullback-Leibler (KL) Divergence or Cross-Entropy. This ensures effective model training with soft labels.

## Trading Strategy and Performance Analysis

Initial backtesting results show a mixed picture, revealing both potential and areas requiring improvement.

A positive alpha of +8.89% annually (after transaction costs) was achieved when applying the CNN model to the First North All-Share (small-cap) index using the OMXS All-Share model. This represents a substantial outperformance of +37.57% compared to the benchmark's -27.88% annual return, highlighting the model's potential in specific market segments.

However, despite 6 out of 8 portfolios achieving positive alpha _before_ transaction costs, the majority experienced negative or marginally positive alpha _after_ costs. This suggests inefficiencies in the trading implementation, primarily:

- **High Turnover:** Trading every 5 days leads to excessive transaction costs.
- **Uniform Weights:** Allocating equal weight to all trades fails to leverage prediction confidence.
- **Lack of Smart Trade Filtering:** Including low-confidence predictions increases risk.

The following enhancements are proposed:

1. **Alpha Breakdown (Long vs. Short):** Analyze the contribution of long and short positions to identify imbalances and optimization opportunities.

2. **Per-Index Sharpe Ratios (Before/After Cost):** Calculate Sharpe ratios for each index to understand the cost impact on risk-adjusted returns.

3. **Portfolio Construction Logic Modifications:** Revise the portfolio construction logic to address high turnover, uniform weighting, and low-confidence predictions. This includes exploring:
   - **Variable Holding Periods:** Holding positions longer based on prediction confidence.
   - **Risk-Based Weighting:** Allocating capital based on predicted probability of success.
   - **Confidence-Based Trade Filtering:** Excluding trades below a confidence threshold.

Further investigation is required regarding:

- **Impact of Short-Selling Constraints:** Potential limitations on short positions, particularly affecting small-cap stocks, require quantification and mitigation strategies.

- **Handling of Unsuccessful Trades:** Clarify the current handling of unsuccessful trades (contradicting model predictions) in the absence of a stop-loss mechanism, focusing on the impact of adverse price movements and informing risk management strategies.

### B. Monitoring (Enforcer)

This section details monitoring procedures for the live trading environment, focusing on risk management, handling unsuccessful trades, and adapting to market conditions.

- **Stop-Loss Mechanism:** Implement a stop-loss to mitigate potential losses. This will automatically exit trades when they move against the predicted direction by a predefined percentage (e.g., a -2.5% stop-loss for a predicted +3.0% return within 2 days).

- **Prediction Confidence Thresholding:** Implement a filter to execute trades only when the predicted magnitude or historical prediction accuracy (using Historical Prediction Error Profiling - HPEP data) exceeds a defined threshold.

- **Risk-Based Weighting:** Implement a weighting scheme considering prediction confidence, inverse historical volatility, and the signal-to-noise ratio of predicted versus actual returns during validation.

- **Prototype Stop-Loss Module:** Develop and backtest a prototype stop-loss module to refine its implementation and parameters under various market conditions.

- **Prototype Dynamic Trade Filtering Layer:** Develop and backtest a prototype dynamic trade filtering layer leveraging predicted return volatility or HPEP data to selectively allow trades based on predicted risk and potential return. This dynamic approach enables adaptation to changing market dynamics.

## II. Model Development and Training

This section details the model architecture and training process, incorporating "rally time" prediction. This involves predicting not only the magnitude of returns but also the time it takes to achieve them.

### A. Model Architecture

The model architecture will be enhanced to predict both return and rally time. Two primary approaches will be explored:

- **Multi-Head Neural Network:** A Convolutional Neural Network (CNN) with an EfficientNet backbone will be trained using two output heads. One head will predict the scalar return (reward) via regression, while the other will predict the rally time, defined as the number of candlestick periods (k) it takes for the closing price to reach or exceed the target price (Close[t] + predicted_return). The loss function will be a weighted sum of Mean Squared Error (MSE) for each head, balancing the importance of return and rally time prediction. Linear layers will follow the EfficientNet backbone for each head.

- **Survival Analysis Modeling:** Alternatively, survival analysis models (e.g., DeepSurv, DeepHit, Weibull Time-To-Event models) will be considered. These models predict the probability distribution of time until the target price is reached, naturally handling censored data (where the target price isn't reached within the observation window) and expressing uncertainty in rally time prediction.

### B. Training and Validation

Training and validation procedures will be adapted to incorporate rally time prediction.

- **Label Generation for Rally Time:** For each candlestick chart ending at time _t_, a lookahead window from _t+1_ to _t+N_ will be used. The label generation process will identify the smallest _k_ within this window such that Close[t+k] ≥ Close[t] + predicted*return. This \_k* value represents the rally time. If the target price isn't reached within the _N_-period lookahead window, _k_ will be set to _N+1_.

### C. Dynamic Plane Implementation and Rally Time

The impact of rally time prediction on the dynamic plane implementation requires further investigation. Specifically, whether rotation and transformations should incorporate temporal aspects related to rally time needs to be determined.

### D. Model Enhancement and Refinement

Model enhancements will focus on integrating predicted rally time into the trading logic.

- **Trade Decision Logic Enhancement:** The trading logic will be enhanced to incorporate predicted rally time, enabling more sophisticated strategies. This includes dynamic capital allocation based on predicted time premium and early exit strategies if the price fails to rally within the predicted timeframe, promoting more adaptive trading behavior.

- **Research on Rally Time Prediction:** Further research will explore the feasibility and credibility of accurate rally time prediction in stock market algorithms. This will involve investigating methods and experiments to improve the reliability of these predictions and enhance overall trading performance.

## III. Code Modifications and Dataset Generation

This section outlines code modifications and dataset generation required for incorporating rally time into the model. It also addresses research into advanced exit strategies and learning from unsuccessful trades.

- **Dataset Generation for Rally Time:** A new dataset will be created from existing stock data, labeling the duration of price rallies. This dataset will train the model to predict both price movement direction and the predicted trend's duration.

- **Code Modification for Multi-Head Model:** The codebase will be modified to accommodate the multi-head model, with one head predicting return and the other predicting rally time. Thorough testing will ensure both heads train effectively.

- **Metrics for Rally Time Prediction Accuracy:** Appropriate metrics (e.g., mean absolute error, root mean squared error, or a custom metric) will be defined to evaluate rally time prediction accuracy, guiding model training and evaluation.

- **Research Algorithmic Exit Strategies:** Research will explore alternative exit strategies beyond the current hardcoded stop-loss, including algorithms that dynamically adjust exit points based on market conditions, volatility, and other relevant factors, aiming to optimize exit timing and maximize returns.

- **Learning from Unsuccessful Trades:** A system will be developed to capture and integrate lessons from losing trades. This involves analyzing losing trade characteristics, adjusting model parameters, refining decision rules, or incorporating new data points to avoid similar future losses, continuously improving model performance.

## A. Model Architecture (Specialization)

This section details the model architectures explored and implemented for predicting stock market behavior based on candlestick chart images. Several approaches incorporating temporal context will be investigated and their effectiveness compared.

**Benchmark Comparison Experiment:** A benchmark comparison experiment will evaluate three architectural approaches: single static images, image pairs, and image sequences. This will determine the most effective representation and utilization of temporal information from candlestick charts.

**2-Image Paired Input Dataset Generator:** A dedicated data generator will create paired image inputs for the "picture-pair" benchmark approach. This generator will concatenate or stack two consecutive candlestick images (Picture B followed by Picture A), representing the temporal transition between them.

**CNN + LSTM Hybrid Architecture:** A hybrid architecture combining a Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM) network will be implemented. The CNN will extract features from candlestick images, and the LSTM will model the temporal transitions between candlestick patterns, leveraging the strengths of both networks.

**Vision Transformer (ViT) for Sequential Candlestick Data:** A Vision Transformer (ViT) model will analyze sequences of three consecutive 5-day candlestick chart images to predict market behavior. Two implementation options will be explored:

1. **EfficientNet Encoding:** Each image will be encoded with an EfficientNet model, and the resulting feature vectors will be fed into the transformer.
2. **Patch Embedding:** ViT-style patch embedding will be used, dividing each image into patches, flattening them, and adding positional encoding before inputting them to the transformer.

**Image Subtraction vs. Feature Subtraction:** The performance difference between image subtraction and feature subtraction for generating delta features within the ViT architecture will be evaluated to determine the more effective method for capturing changes between consecutive candlestick images.

## B. Training and Validation (Enforcer)

This section details the training and validation procedures, focusing on creating sample datasets, integrating delta features into the ViT architecture, developing a dual-input CNN model for evaluating frame-to-frame changes, and defining prediction targets.

- **Build Sample Dataset for 3-Image Sequences:** A sample dataset comprising sequences of three candlestick images will be created. This dataset will be used for training and evaluating the models designed to handle temporal dependencies.

- **Exit Logic:** Robust exit strategies are crucial for managing risk and maximizing profitability. The following exit logic will be implemented:

  - **Volatility-Aware Exit Thresholds:** Dynamic exit thresholds based on recent market volatility using the Average True Range (ATR). For example, the exit threshold could be 1.5 times the current ATR, allowing the agent to adapt to changing market conditions.

  - **Time-Based Confidence Decay:** Trades will be exited if the predicted reward does not materialize within the anticipated timeframe, preventing prolonged holding of losing positions based on outdated predictions.

  - **Prediction Divergence:** If retrained model predictions diverge significantly (e.g., 3-4%) from previous predictions, the trade will be exited, allowing the agent to react to shifting market dynamics.

  - **Portfolio Contextual Exit:** Trades underperforming their prediction group's average return over a specified period will be exited, prioritizing more promising trades within the portfolio.

- **Learning From Mistakes:** Several strategies will improve the model's ability to learn from its mistakes, particularly high-loss trades and difficult-to-predict scenarios:

  - **Sample Re-weighting During Retraining:** Cost-sensitive learning will be employed, assigning higher loss weights to misclassified high-loss trades during retraining.

  - **Bootstrapping of Hard Cases:** "Hard cases" (samples with the highest loss values) will be identified after each training epoch and added to a dedicated dataset for focused retraining.

  - **Meta-Model for Trade Review:** A meta-learner model will predict the likelihood of a trade failing _before_ execution, based on the primary model's prediction, chart features, and historical trade outcomes.

- **Reward Logic Enhancement:** The reward logic will be enhanced to incorporate temporal dependencies. Instead of assigning rewards based on individual images, the sequence of preceding images will be considered, allowing for a more nuanced reward calculation.

- **Model Input Adjustment: Handle Sequences:** The model's input layer will be adapted to handle image sequences rather than single images, supporting the architectural changes for incorporating temporal dependencies and enabling trend analysis.

_(The original document chunk included a partial, unfinished bullet point. This has been removed as it was incomplete and lacked context.)_

## II. Model Development and Training

### A. Model Architecture

This section details the architecture of the Vision Transformer (ViT) and Convolutional Neural Network (CNN) models, including data preparation, feature engineering, and dataset design. A key innovation is the focus on predicting future candlestick images rather than directly predicting returns.

**Input Data and Feature Engineering:**

The ViT model will process sequences of candlestick chart images, providing temporal context. Several key aspects of the input pipeline and data representation are addressed:

- **Sequential Candlestick Windows:** The ViT will receive sequences of _N_ candlestick charts as input, representing a series of consecutive trading days. The optimal length _N_ will be determined experimentally. This contrasts with previous experiments using a fixed sequence length of 3, which needs further investigation regarding whether it is a hard constraint of the ViT training layer.

- **Delta Features:** Delta features, capturing changes between consecutive candlestick windows, will be incorporated to highlight trends and volatility. Two methods will be explored:

  - **Image Subtraction:** Directly subtracting pixel values between consecutive candlestick chart images.
  - **Feature Subtraction:** Subtracting pre-calculated features from consecutive candlestick windows.

- **Positional Embeddings:** Positional embeddings are crucial for the ViT to interpret the sequential order of input images correctly. These will be implemented, either by training from scratch or leveraging a pre-trained model with positional extension.

- **Masking and Padding:** To handle variable-length sequences, a maximum sequence length (e.g., 5 charts) will be defined. Shorter sequences will be padded with blank images, and a mask will be used to inform the ViT which parts of the sequence are padding.

- **Memory Management:** Three distinct memory management strategies (Options A, B, and C) for handling stored data will be rigorously evaluated through a structured experiment using a consistent dataset. This experiment will analyze trade-offs between flexibility, performance, and training efficiency, considering the impact of added temporal context and exploring the extension of ViT capabilities to handle real-time, variable-length financial data. Evaluation metrics will include Sharpe Ratio, Directional Accuracy, Mean Squared Error (MSE), and Rally-Time Prediction Accuracy.

**Dataset Design and Generation:**

A dataset of image sequences will be generated using a sliding window approach.

- **Sequence Composition:** Each sequence will consist of charts representing t-2, t-1, and t, creating a three-day historical context. This structure might be adjusted based on the results of variable-length input experiments.

- **Labels:** Corresponding labels for each sequence will include:

  - **Return Value:** The return achieved over a defined holding period (e.g., 5 days).
  - **Rally Time:** This measure, requiring further definition, presumably quantifies the duration of an upward trend.
  - **Signal Class:** Categorical labels (BUY/SELL/HOLD).

- **Automated Generation:** A configurable tool will be developed to automate the generation of these image sequences from stock data using the defined sliding window approach.

**Model Architectures:**

Two primary model architectures will be developed and evaluated:

- **Vision Transformer (ViT) with Delta Integration:** The ViT architecture will integrate delta embeddings, representing changes between consecutive candlestick images within the input sequence.

- **Dual-Input Convolutional Neural Network (CNN):** This CNN will receive two inputs: the current candlestick chart image and a delta frame representing the difference between the current and previous frames.

**Prediction Targets:**

The models will be trained to predict one or a combination of the following:

- **Scalar Reward Value:** A numerical representation of the potential profit.
- **Categorical Label (BUY/SELL/HOLD):** A trading signal.
- **Multi-Head Output:** A combined prediction of reward value and rally time.

**Candlestick Image Prediction:**

The core innovation is the prediction of future candlestick images instead of directly predicting returns. Returns will then be derived from these predicted images. This approach reframes the problem as a visual prediction task, potentially capturing richer information about market dynamics.

### B. Backtesting

- **Backtesting Framework Development:** A comprehensive backtesting framework will be developed to evaluate model performance under realistic market conditions. This framework will simulate trading operations, calculate performance metrics (e.g., Sharpe Ratio), and facilitate analysis of model behavior in various market scenarios.

## Financial Forecasting using Candlestick Images

This section details the architecture, data model, and experimental design for financial forecasting using a candlestick image-based approach. The core concept involves generating future candlestick images and extracting predictable information for trading decisions.

**Architecture Considerations:**

This approach introduces specific risks and drawbacks:

- **Indirect Evaluation:** Evaluating model performance is less straightforward than with direct numerical return prediction.
- **Compounding Errors:** Errors in image generation can amplify inaccuracies when translated back to financial metrics.
- **Indirect Reward Supervision:** The intermediary step of extracting trading signals from images may weaken the learning process compared to direct profit/loss supervision.
- **Ambiguous Financial Implications:** Robust methods are required to translate visual patterns into concrete trading decisions.

Comparing this image-based approach to established financial modeling techniques is crucial:

- **Scalar Regression Models:** These predict numerical values directly, offering simplicity and interpretability.
- **Probabilistic Return Models:** These predict probability distributions of returns, providing a measure of uncertainty.

The comparison should encompass output type, supervisory signal, connection to trading strategies, richness of learned structure, interpretability, ambiguity risk, data requirements, and modeling complexity.

**Data Model:**

Extracting meaningful financial data from generated candlestick images is crucial. Two potential approaches are:

- **Pixel Location Analysis:** Analyzing pixel locations to determine open, high, low, and close (OHLC) prices.
- **Rendering Predicted Image Data:** Rendering image data into a format allowing direct OHLC extraction, potentially requiring specific encoding within the generated images.

**Tasks:**

The following tasks are essential for developing and validating this approach:

- **Prototype Design:** Two primary candidates for the image-to-image forecaster are:

  - **CNN Decoder:** Provides a strong foundation for image generation.
  - **Transformer-Based Image Generator:** Offers potential performance advantages, particularly in capturing long-range dependencies.

- **Experimental Design:** This involves:
  - **Training Architecture:** Training a ViT (Vision Transformer) or U-Net architecture on historical candlestick data.
  - **Post-Processing:** Developing a method to extract predicted returns from generated images.
  - **Baseline Comparison:** Comparing extracted return accuracy with baseline models (e.g., scalar regression, probabilistic return models).
  - **Performance Metrics:** Utilizing appropriate metrics, including RMSE, SSIM/LPIPS, and backtested profit performance.

### A. Model Architecture (Specialization)

This section details the model architecture, focusing on generating visually plausible candlestick sequences and translating them into actionable trading decisions.

- **Model Training Goal:** The model's primary objective is to accurately predict visual candlestick sequences. While financial returns are the ultimate goal, prioritizing visual pattern recognition aims to leverage the information embedded in candlestick charts, assuming accurate visual prediction correlates with profitable opportunities.

- **Model Evaluation Metrics:** Evaluating the model solely on financial returns is insufficient. Visual accuracy, assessed using metrics like MSE or RMSE comparing predicted and actual candlestick patterns, is crucial. This complements financial performance evaluation and provides insights into the model's understanding of market dynamics.

- **Trading Decisions:** The model must determine the necessity of a trade based on identified patterns. This decision-making component translates visual predictions into practical trading strategies.

## IV. Model Accuracy vs. Financial Performance

A critical task is to investigate the relationship between the model's accuracy in predicting visual patterns and its actual financial performance. This analysis will determine the effectiveness of using visual pattern recognition as a proxy for profitability and inform further model refinements. The model must provide clear, actionable trading decisions. Even with accurate visual predictions, the financial implications might be ambiguous. Therefore, the model's output should be evaluated for its ability to inform trading decisions, focusing on practical utility rather than solely on visual accuracy. This requires a mechanism to translate predicted candlestick sequences into concrete buy/sell/hold signals with associated confidence levels.

## Markdown Usage within the SCoVA Project

This section details how principles of causal grounding, distinguishing realism from trading value, and robust evaluation apply to using Markdown within the SCoVA project. These core concepts are crucial for any representational format.

**Causal Grounding in Markdown Documentation:** Markdown documentation should explain the rationale behind design choices and implementation details, mirroring the need for a predictive model to understand _why_ certain patterns occur. This includes documenting the thought process, justifying the use of specific libraries or functions, and explaining the logic behind code structures. This ensures maintainability and allows for future modifications based on a clear understanding of the system.

**Realism vs. Trading Value in Markdown Examples:** Markdown code examples and simulated trading scenarios should be realistic and reflect potential use cases. However, their primary value lies in demonstrating functionality and clarifying usage, not providing trading advice. Just as a realistic chart may not have trading value, a realistic Markdown example might not represent a profitable trading strategy. It's crucial to distinguish between illustrative examples and actionable trading recommendations.

**Robust Evaluation of Markdown Documentation:** Effective Markdown documentation is clear, complete, and facilitates understanding. Evaluation can involve peer reviews, user testing, and automated checks for consistency and adherence to style guidelines. Similar to evaluating generated images for trading relevance, evaluating documentation requires careful consideration of its purpose and target audience.

**Dual-Module Approach:** Markdown acts as one module, providing explanations and context, while the codebase forms the second. This parallels the Dual-Module Framework (Chart Generator + Trade Evaluator), with Markdown clarifying the code's function and purpose.

**Experimental Protocol for Markdown Utility:** An experimental protocol can assess the effectiveness of different documentation strategies, similar to comparing scalar vs. image-based prediction. This could involve comparing different levels of detail, presentation styles, or Markdown features to determine the optimal approach for conveying information.

## V. Dissertation and Documentation

### A. Dissertation Writing

- **Structure and Write Dissertation:** After finalizing the codebase, the dissertation must be structured and written with academic rigor, adhering to established standards for style, grammar, and citation. This includes clearly outlining the research question, methodology, results, and conclusions.

### B. Codebase Finalization

- **Provide Code Elements Prompt:** Before code development, a detailed prompt outlining the necessary code elements should be created. This prompt will serve as the blueprint for development, ensuring all essential components are addressed.
- **Develop Code:** Following the prompt, the codebase will be developed, including writing, testing, and debugging. Continuous discussion and review are crucial throughout this process.
- **Refine Dissertation Title:** The current title, "Automating Technical Analysis in Intraday Trading by using image snapshots of price action and making predictions using computer vision: a case study of Indian equity markets," requires refinement to be more concise and impactful while retaining the core concepts of "snapshot" and "computer vision."
- **Build Benchmark Model:** A benchmark model using image prediction as a latent feature is needed to assess the value of image-based predictions for risk management. Its performance will validate the primary model's efficacy and provide a comparative analysis.

### C. Codebase Documentation and Submission

- **Provide Codebase and Prompt:** For transparency and reproducibility, the codebase and its generating prompt must be provided.
- **Finalize Codebase:** The entire codebase requires thorough review and finalization before inclusion in the dissertation, ensuring consistency and correctness.
- **Provide Detailed Code Modules:** Well-documented code modules are essential. Each module should include comprehensive documentation explaining its purpose, functionality, and interaction with other system components.
- **Provide Experimental Logging Templates:** Structured templates for recording experimental results are required to ensure consistent and comparable data across experiments, facilitating interpretation and validation.
- **Provide Performance Charts and Tables:** Comparative performance charts and tables are necessary to visually represent experimental results, clearly demonstrating the performance characteristics of the proposed models and algorithms.

## II. Model Development and Training

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic 2D plane as a replacement for the fixed 3D Cartesian coordinate system. This dynamic plane, with rotational axes and a dynamic origin, adapts to market movements, similar to a Frenet frame or tangent plane. The goal is to explore how rotations of these axes influence the projection of market data, potentially revealing hidden patterns and improving predictive capabilities.

#### Rationale for Shifting from 3D to 2D

The project initially utilized a 3D model for representing market dynamics, which presented challenges. This 2D approach aims to simplify the representation of complex movements, such as parabolic trajectories, by rotating the axes and dynamically adjusting the origin point instead of relying on fixed axes and an absolute origin.

#### Key Research and Implementation Tasks

Implementing this dynamic plane involves the following:

- **Replacing the 3D Frame:** The existing 3D Cartesian frame will be replaced by a dynamic 2D plane. This plane's axes and origin will shift in response to market movements, providing a relative frame of reference, conceptually similar to a Frenet frame or a tangent plane. A key research area is how axis rotation within this dynamic plane affects data projection and potentially reveals new insights.

- **Mathematical Foundations:** A rigorous mathematical framework underpinning the dynamic 2D plane is required. This involves exploring coordinate transformations, manifolds, and the concept of "bending" space. Specifically, the dynamic plane will be analyzed as a moving frame on a 1-dimensional differentiable manifold. This approach attaches an orthonormal frame at each point on the manifold, rotating it along the path traced by market data.

- **Encoding Higher-Dimensional Data:** A critical task is understanding how a curve, specifically a parabola (typically represented along the z-axis in a 3D frame), translates into the dynamic 2D chart. The goal is to determine how 2D plane axis rotations effectively represent curves traditionally requiring a higher dimension.

- **Information Balance and Degrees of Freedom:** Careful consideration must be given to the balance between information encoded in position and orientation within the 2D representation. Using two coordinates (u,v) for position and three Euler angles (or a rotation matrix) for frame orientation, understanding the degrees of freedom is crucial for model stability and avoiding overfitting.

### C.i. Dynamic Plane Mathematical Formulation

This section details the mathematical underpinnings of the dynamic plane and its connections to established formalisms.

#### Algebraic Summary

A concise algebraic representation of the dynamic 2D plane will be provided here. _(Further details and equations will be added during implementation.)_

#### Connections to Established Formalisms

This subsection explores connections between the dynamic 2D plane and related mathematical and physical concepts:

- **Parallel Transport:** The relationship between vector movement within the dynamic plane and parallel transport along a curve.
- **Affine Connections:** The potential for describing transformations within the dynamic plane using affine connections and the implications.
- **Frenet-Serret Frames:** The relationship between the dynamic frame and the Frenet-Serret frame, used to describe the local geometry of curves in 3D space.
- **The SE(3) Group:** The representation of dynamic plane transformations within the special Euclidean group SE(3) and its implications.
- **Local Inertial Frames in General Relativity:** Analogies between the dynamic frame and local inertial frames in general relativity, particularly concerning local transformations and curvature.

#### Practical Uses and Limitations

This subsection analyzes the potential applications and limitations of the dynamic 2D plane representation.

- **Potential Uses:** Exploration of applications in fields like robotics, specifically how the dynamic frame could represent the movement and orientation of robotic arms or mobile robots. Further potential applications will be explored as the research progresses.
- **Limitations:** _(This section will be populated as limitations are identified during the research and implementation process.)_

```markdown
## I. Project Setup and Data Acquisition

No relevant items from the provided checklist apply to this section.

## II. Model Development and Training

### A. Model Architecture (Specialization)

This section details the architectural considerations for abstracting 3D spatial data, particularly within the context of the three-body problem and its potential application to financial markets, into a 2D representation using a dynamic frame. This involves defining a 2D plane whose orientation changes dynamically in response to the system's evolution.

- **Abstraction of 3D Space into 2D:** The core architectural challenge lies in effectively reducing the dimensionality of the problem from 3D to 2D while preserving essential information. Several mathematical and conceptual approaches will be investigated:

  - **Coordinate Charts:** Exploring the use of coordinate charts to map portions of 3D space onto a 2D plane. This requires careful selection of chart boundaries and smooth transitions between charts.
  - **Frenet-Serret Frame:** Utilizing the Frenet-Serret frame, a moving frame along a curve, to represent 3D curves within a 2D chart. This involves tracking the curve's tangent, normal, and binormal vectors.
  - **Stereographic Projections:** Investigating the application of stereographic projections to map points from a sphere (potentially representing the "shape space" of the three-body system) onto a plane.
  - **Shape Space:** Analyzing the system's dynamics within the context of shape space, which abstracts away the overall size and rotation of the system, focusing on the relative positions of the bodies.
  - **Dynamic Frames:** Implementing a general dynamic frame approach that adapts to the system's dynamics, providing a localized 2D perspective. This includes addressing how the frame's orientation is updated.
  - **Information Completeness and Orientation:** Addressing the trade-off between reducing dimensionality and retaining complete information about the system's orientation. Supplementary orientation parameters might be necessary to fully reconstruct the 3D state from the 2D representation.

- **Dynamic Frame Implementation:** A specific implementation of a dynamic frame will be developed to represent 3D data in a 2D chart. This representation will potentially involve:

  - **(u, v) Coordinates:** Storing two real numbers (u, v) to represent the location of a point within the current 2D chart.
  - **Orientation Parameters:** Maintaining parameters to define the chart's position and rotation within the 3D space. Simplifications, like fixing the center of mass, will be considered.

- **Mapping to Reduced Dimensions:** The mapping of the three-body problem to the reduced dimensions of the 2D dynamic frame requires detailed explanation, focusing on:

  - **Orientation and Scaling:** Describing how orientation and scaling are handled within this mapping. Orientation represents the rotational aspect of the system, while scaling relates to its overall size.
  - **Chaos and Complexity:** Addressing the inherent challenges of chaos and complexity in the three-body problem and how these are reflected in the 2D representation.
  - **New Periodic Orbits:** Discussing how recently discovered periodic orbits in the three-body problem can be incorporated and visualized within this 2D framework.

- **Computer Graphics, Navigation, and Data Compression:** Exploring the potential applications of the dynamic frame in computer graphics, navigation systems, and data compression. This includes investigating its use in animation, image manipulation, 3D modeling, adaptable frames of reference, and efficient data representation.
- **Limitations:** Addressing the limitations of the dynamic frame approach, including:
  - **Path Dependence:** Analyzing how the path taken within the dynamic plane influences the final state and its implications for predictability.
  - **Singularities:** Identifying any singularities or points where the dynamic frame breaks down or becomes ill-defined.
  - **Computational Overhead:** Evaluating the computational cost associated with implementing and using the dynamic plane and exploring optimization strategies.
- **Next Steps:** Outlining the next steps for developing the dynamic plane concept, including:
  - **Explicit Rotation Law:** Defining a precise mathematical rule governing the dynamic plane's rotation based on the underlying data or process.
  - **Curvature Invariants:** Exploring invariants related to the curvature induced by the dynamic transformations to gain insights into the underlying geometry.
  - **Generalization to Surfaces:** Extending the dynamic plane concept beyond 2D to more general surfaces and investigating implications for representing complex geometries.

### B. Training and Validation (Enforcer)

No relevant items from the provided checklist apply to this section.

### C. Dynamic Plane Implementation (Specialization)

No relevant items from the provided checklist apply to this section.

### D. Model Enhancement and Refinement (Specialization)

No relevant items from the provided checklist apply to this section.

## III. Testing and Evaluation

No relevant items from the provided checklist apply to this section.

## IV. Deployment and Monitoring

### A. Deployment (Facilitator)

- **Demonstrator Implementation (Optional):** A demonstrator application may be implemented to visualize and experiment with the application of the 2D dynamic frame to the three-body problem. This demonstrator could include:

  - **Jacobi Coordinates:** Using Jacobi coordinates, a common coordinate system for the three-body problem.
  - **Shape Sphere Projection:** Projecting the system's configuration onto the shape sphere.
  - **Reduced Hamiltonian Integration:** Integrating the reduced Hamiltonian, which governs the system's dynamics in the reduced dimensions.
  - **Trajectory Reconstruction:** Reconstructing the full 3D trajectories from the 2D representation.
  - **Shape Great-Circle Ansatz and R(t) Periodicity:** Incorporating advanced concepts like the shape great-circle ansatz and R(t) periodicity to explore and search for new choreographies (periodic solutions).

### B. Monitoring (Enforcer)

No relevant items from the provided checklist apply to this section.

## V. Dissertation and Documentation

No relevant items from the provided checklist apply to this section.
```

## II. Model Development and Training

This section details the architecture and enhancements made to the predictive models, incorporating principles of egocentric and allocentric frames of reference inspired by biological navigation systems. A key challenge is representing 3D market dynamics within a 2D framework while maintaining numerical stability, particularly with moving frame coordinates.

### A. Model Architecture (Specialization)

This subsection outlines the theoretical foundations and implementation details of the model architecture, designed to process sequential market data analogous to biological navigation.

- **Egocentric and Allocentric Mapping:** Inspired by O'Keefe's work on place and grid cells, the model incorporates both egocentric (self-centered) and allocentric (world-centered) frames of reference, mirroring human navigation strategies. This biologically-inspired framework provides a nuanced understanding of how the model perceives and interacts with the market environment. The representation of 3D market dynamics (e.g., price, volume, volatility) within a 2D chart, incorporating orientation as state information, and reconstructing market paths are key challenges addressed here. Analogous use cases, such as robot arm trajectories (sequential price movements), VR headset paths (market trend navigation), N-body dynamics (interacting market participants), and computer graphics (visualizing market data) will be explored. The impact of translational and rotational symmetries in market data and the necessity of tracking orientation (e.g., market trend direction) will be investigated.

- **Singularity and Chaos Investigation:** The potential for singularities or points of instability in the model's behavior, particularly with continuous market data streams, will be thoroughly investigated. The model's predictive behavior near these points and the potential for unpredictable outcomes will be analyzed. Furthermore, the model's sensitivity to chaotic market behavior, analogous to the three-body problem, and the role of any internal representations of market state will be examined.

- **3D Traversal Feasibility:** The model's ability to fully explore potential market states, at least in the forward direction of time (predicting future prices), will be verified. This ensures the model can capture a wide range of market scenarios.

### C. Dynamic Plane Implementation (Specialization)

This subsection details the implementation and stability analysis of the dynamic plane representation, crucial for capturing the agent's movement and orientation within the market environment.

- **Numerical Conditioning of Moving Frame Coordinates:** The numerical conditioning of using moving frame coordinates, analogous to body-fixed frames in robotics and UAV trajectory optimization, is a critical concern. The stability characteristics of these coordinates will be compared to global (x, y, z) coordinates, focusing on the benefits of body-fixed frames for enhanced numerical stability, particularly during rapid rotations.

- **Stability during Rapid Rotations:** Maintaining model stability during rapid market shifts is paramount. The model's performance under these conditions will be evaluated using Angular-momentum conserving integrators (Lie-group methods) with moving frame coordinates. Simpler integration approaches like Euler and RK4 will serve as benchmarks for comparison.

### D. Model Enhancement and Refinement (Specialization)

This subsection outlines potential model enhancements and refinements based on the dynamic plane implementation and insights from biological navigation.

- **Egocentric and Allocentric Mapping in Biological Systems:** Further research will investigate the neurophysiological evidence for egocentric and allocentric reference frames in the brain, focusing on the parietal/premotor cortex and hippocampal formation, respectively. Understanding the roles of head-direction cells and parallax in biological spatial understanding can inform model refinements, improving its navigational capabilities within the market environment. This research will explore how these biological mechanisms can be translated into computational models to enhance the model's internal representation of market dynamics and improve its predictive accuracy.

## Visualization, Data Storage, and Technical Constraints

This section details the implementation of data visualization and storage strategies related to the project's dynamic plane and coordinate transformations. Adhering to specific technical constraints, this section outlines the steps required to visualize the 3D helix, its corresponding 2D planar trace, and effectively store the underlying data.

**Data Storage and Logging:** Efficient storage of the planar coordinates (u, v) and the orientation of the moving frame is crucial. Planar coordinates will be tracked using an arc length parameter. Orientation will be stored using quaternions or rotation vectors, enabling reconstruction of the spiral path. This storage mechanism must respect memory and storage limitations while maintaining compatibility with other project requirements, such as visualization and rendering.

**Visualization:** Two key visualizations will be created as separate, standalone figures (subplots are prohibited):

- **3D Helix Visualization:** A 3D helix plot will be generated using Matplotlib with the following parameters: radius = 1.0, pitch = 0.5, and number of turns = 4. The plot will include X, Y, and Z axis labels and the title "3D Helix in Laboratory Coordinates."

- **2D Planar Trace Visualization:** The 2D planar trace visualization will be enhanced for clarity. Improvements may include a more intuitive visual representation of the trace, incorporating tooltips or annotations to explain its meaning, or providing supplementary documentation (e.g., a help section or tutorial) for a detailed conceptual explanation.

**Technical Constraints:** The following constraints govern the visualization implementation:

- **Matplotlib and NumPy:** `matplotlib` will be used for all plotting, alongside `numpy` for numerical computations. The use of `seaborn` is explicitly excluded to ensure consistency and avoid potential dependency conflicts.

- **Single Chart per Function Call:** Each function call to generate a chart must produce a single, self-contained chart. This impacts the combined visualization of the 3D helix and its 2D unfolded representation. Potential solutions include generating separate 3D and 2D charts or exploring animation with frame snapshots. However, the single chart requirement introduces complexity.

- **Performance Considerations for Animation:** While animation is a viable visualization option, potential performance bottlenecks must be carefully considered. If performance becomes critical, static visualizations (separate 3D and 2D charts) should be prioritized.

**UI/UX Considerations:**

- **Visualizing the Straight Line Trace:** The primary UI/UX goal is to clearly demonstrate that a straight line trace on the 3D helix corresponds to a straight line in the 2D unfolded representation, aligned with the moving frame of reference. Achieving clarity may require separate charts or a carefully designed animation.

- **Future Consideration: Orientation vs. Time:** Visualizing the evolution of orientation over time, potentially using rotation matrices or quaternion frames in a separate plot, is a desirable future enhancement. This is deferred to maintain focus on core visualization requirements.

## Enhanced Visualization of Moving Frame and Helix

This section details the implementation of visualizations designed to communicate the concept of a moving frame of reference along a helical path. The visualizations will clarify the relationship between the 3D helix and its 2D straight-line representation.

- **2D Unfolded Helix:** A 2D chart will illustrate the unfolded arc of the helix, demonstrating how the 3D helical path simplifies to a straight line when viewed from the moving coordinate frame.

- **3D Helix with Moving Frames:** A 3D visualization of the helix will incorporate moving coordinate frames, represented by tangent, normal, and binormal vectors, at various points along the curve. This will dynamically showcase the changing orientation of the moving frame.

- **Improved Visual Explanation of 3D Movement:** The visualization will clearly illustrate the upward movement along the z-axis of the helix, moving beyond a simple depiction of circular motion. Detailed graphics or visual aids will clarify how the 2D chart represents the 3D helix.

- **Synchronized 3D Trajectory and Orientation Visualization:** Two synchronized plots will illustrate the path and orientation of a point moving along the helix. One plot will show the 3D helix (global trajectory), while the other will display the orientation of the moving frame as an arrow. Note: Animation is not currently feasible.

- **3D Helix with Tangent Vectors:** A 3D plot of the helix will include tangent vectors at several points along the curve, representing the orientation of the moving frame. This visualization will demonstrate how movement along the tangent vector contributes to both horizontal and vertical displacement along the helix.

## 2D Rotational Plane Framework (Implementation and Analysis)

This section details the implementation and analysis of a 2D rotational plane framework as an alternative…

## Dynamic Plane Implementation

This section details the implementation of a dynamic plane using Principal Component Analysis (PCA) to enhance the model's ability to capture relevant price movements by focusing on relative changes rather than absolute values. This dynamic adjustment, applied before each prediction, ensures the model operates within a locally optimized frame of reference.

**Dynamic PCA Preprocessing Pipeline:**

Before each prediction, a dedicated preprocessing pipeline will dynamically adjust the input data using PCA. This pipeline consists of the following steps:

1. **Window Selection:** A sliding window of recent data (e.g., the preceding 5 days) will be selected for PCA calculation. This window aligns with the existing 5-day input window used by the model.

2. **Feature Preparation:** Relevant features (e.g., Open, High, Low, Close, Volume (OHLCV) data, and potentially derived technical indicators) will be prepared for PCA input. The specific features used will be documented and justified based on their relevance to price movement prediction.

3. **PCA Computation:** PCA will be applied to the selected features within the window to identify the principal components.

4. **Principal Component Selection:** The primary principal components capturing the most significant variance will be selected. The number of components retained will be determined experimentally to balance dimensionality reduction with information preservation.

5. **Feature Space Rotation and Re-centering:** The feature space will be rotated and re-centered based on the selected principal components. This transformation effectively aligns the data along the axes of greatest variance, emphasizing relative price changes.

6. **Model Input Preparation:** The transformed data will be formatted as input for the prediction model (Vision Transformer or CNN). This ensures the model receives data optimized for recognizing patterns in relative price movements.

**Integration with Vision Transformer/CNN Pipeline:**

The dynamic PCA transformation will be seamlessly integrated into the Vision Transformer (ViT) or CNN pipeline. At each time step, before a prediction is generated, the input candlestick data will be processed through the dynamic PCA pipeline. The transformed data will then be fed to the ViT or CNN for prediction. This ensures the model consistently operates on data optimized for recognizing local patterns in price movements, irrespective of the broader market trend. This approach is hypothesized to improve the model's ability to generalize across different market conditions and price ranges.

## I. Project Setup and Data Acquisition

### A. Project Initialization

- **Documentation:** Continuously document the project's progress, including challenges, decisions, and rationale. This encompasses:
  - Image type selection (e.g., candlestick, Heiken Ashi) and justification.
  - Development and refinement of the rotational axis paradigm, including behavior at each refocus point.
  - Addressing current uncertainties and tracking implementation evolution.
  - Potential risks and challenges of dynamic PCA (e.g., overfitting, computational overhead, integration complexity, loss of absolute reference frame, instability, model dependency, explainability, edge cases, baseline fairness).
  - Chosen baseline and comparison strategy to quantify the benefit of dynamic PCA refocusing (including comparisons against no PCA, static PCA, simple transforms, and alternative data-driven focus methods, along with an ablation study of PCA without re-centering and vice-versa).
- **Publication Preparation:** Ensure the paper and repository clearly reflect:
  - The image-based input data.
  - Implementation details of the rotational axis paradigm.
  - Discussions about the computational cost of PCA and potential optimizations.
  - Any chosen dimensionality reduction techniques.
  - The defined baseline and comparison strategy for evaluating dynamic PCA.

### B. Data Acquisition and Preprocessing

- **Data Source:** Acquire OHLCV data from Yahoo Finance.
- **Image Generation:** Generate the chosen chart images (e.g., candlestick, Heiken Ashi) according to the image input requirements. Ensure alignment with the rotational axis paradigm and consider potential performance implications related to PCA's computational cost. Include volume and moving average data in the chart images as needed.

## II.C. Dynamic Plane Implementation

This section details the implementation of the dynamic plane, crucial for generating rotated candlestick images. The dynamic plane rotates the coordinate system based on price action, providing a unique perspective on market movements.

**Concept:** The dynamic plane dynamically refocuses the coordinate system. The origin shifts with each new data point (e.g., candlestick), allowing the model to focus on relative price changes rather than absolute values, aligning the visual representation with the analysis of local price fluctuations. This is achieved through dynamic origin shifting and dynamic rotation.

**Dynamic Rotation:** At each time step (e.g., new candlestick or every few candlesticks), the coordinate frame is rotated to align the x-axis with the local movement vector. This transformation, applied to a small window of recent price data (e.g., the last 5-10 candlesticks), creates a rotated and centered snapshot of recent price action.

**Local Movement Vector Calculation:** The local movement vector (v) is calculated as:

```
v = P_current - P_previous
```

where `P_current` and `P_previous` represent the price and time (X,Y) coordinates of the latest and preceding data points, respectively.

**Rotation Matrix Calculation:** The rotation angle (θ) is calculated as:

```
θ = arctan(v_y / v_x)
```

This angle is used to construct the rotation matrix `R(θ)`:

```
R(θ) = [[cos(θ), sin(θ)], [-sin(θ), cos(θ)]]
```

This matrix rotates the data points around the origin.

**Dynamic Origin Shift:** After rotation, the origin is shifted to the current price point, centering it at (0,0) in the rotated coordinate system. This highlights oscillations around the current price trend, represented as deviations from 0 along the Y'-axis, focusing the model on relative price fluctuations.

## Dynamic Plane Implementation

This section details the design and implementation of a dynamic plane generator, which transforms candlestick data into a 2D representation suitable for input into a Vision Transformer (ViT) or Convolutional Neural Network (CNN). The core concept involves rotating the plane based on local price movement patterns, effectively capturing the dominant trend within a localized timeframe.

The generator takes candlestick data (time, open, high, low, close, and optionally volume) as input and produces a sequence of 2D image snapshots. Each snapshot represents the candlestick data within a dynamically rotated and re-centered coordinate system. This rotation aims to align the dominant price trend with the horizontal axis, potentially highlighting subtle patterns for the ViT or CNN to learn.

The following steps outline the dynamic plane generation process:

1. **Window Selection:** A sliding window defines the timeframe for local movement analysis. This window encompasses a fixed number of previous candlesticks, allowing the generator to adapt to recent market dynamics.

2. **Movement Vector Calculation:** Within the defined window, a movement vector is calculated, representing the dominant price trend. This vector is derived from the changes in price (e.g., close price differences) or potentially incorporates volume information.

3. **Dynamic Frame Construction:** Principal Component Analysis (PCA) is applied to the candlestick data within the sliding window. The top two principal components define the axes of the dynamically rotated 2D plane. This aligns the x-axis with the direction of greatest variance, effectively capturing the primary price movement.

4. **Rotation and Re-centering:** The 2D plane is rotated so that the dominant movement vector (or the first principal component) aligns with the horizontal axis. The plane is then re-centered on the most recent candlestick data point. This ensures that the focus remains on the latest market activity.

5. **Snapshot Rendering:** After rotation and re-centering, a 2D image snapshot of the transformed candlestick data is generated. These snapshots serve as input for the ViT or CNN. Interpolation techniques, such as anti-aliasing, are employed during rendering to mitigate potential rotation artifacts.

To address potential challenges arising from volatile price movements:

- **Volatility Jump Handling:** Smoothing or limiting the change in rotation angle between consecutive frames will dampen the effect of abrupt price fluctuations, ensuring smoother transitions and preventing erratic rotations.
- **Consistent Axis Scaling:** Maintaining a consistent scale (units per percentage move) on both axes across all generated frames is crucial for preventing bias and ensuring the model learns accurate relationships within the data.

A dedicated Python module, `RotatingSnapshotGenerator`, will encapsulate this logic. This module will utilize libraries like NumPy, Matplotlib, and PIL for numerical processing, visualization, and image manipulation. It will be optimized for batch operations to efficiently process large datasets. A detailed pseudocode implementation will be provided to ensure clarity and facilitate future development and collaboration. Finally, a conceptual diagram will visually illustrate the dynamic rotation and coordinate transformations within the 2D plane.

## Dynamic Plane Implementation and Visualization

This section details the implementation and visualization of the dynamic plane, a novel approach for transforming candlestick data into a format suitable for CNN and ViT models. The dynamic plane generator rotates the coordinate system based on price movement, creating a dynamic representation of market trends.

**Core Functionality:**

- **Dynamic Plane Generation:** The generator calculates the dynamic plane's rotation based on recent price movements. A configurable window size parameter controls the number of past data points considered, allowing adjustment of the "locality" of the dynamic plane. Smaller windows emphasize recent movements, while larger windows capture broader trends. Trading volume can optionally be incorporated as a third dimension, potentially enriching the input data.
- **Snapshot Generation for ViT/CNN:** The generator produces snapshots of the dynamic plane formatted for ViT and CNN input. These snapshots can be rendered as images, capturing localized price movements and volatility in a visually informative way, or as numerical features extracted from the 2D plane representation. Design considerations for image-based snapshots include color schemes, volume encoding, and representation of price fluctuations within the rotated frame.
- **Minimum Data Points:** The generator requires at least two data points to compute the dynamic plane, as a vector representing price movement necessitates two points.

**Visualization and UI/UX:**

- **Dynamic Plane Animation:** An animation will visualize the evolution of the dynamic plane as new data points are added, dynamically updating the displayed plane. This will aid in understanding the plane's behavior and its response to incoming data. This animation must handle edge cases gracefully, ensuring proper initialization even with no or only one initial data point.
- **Smoothing Techniques:** The visualization will incorporate smoothing techniques, such as Heikin-Ashi, to reduce noise and improve visual clarity, making it easier to discern trends and patterns.
- **Illustrative Examples:** Example images of both raw candlestick input and the transformed output after dynamic plane rotation will be provided for comparison and clarity.

**Model Integration and Evaluation:**

- **Training Pipeline Integration:** The generated dynamic snapshots will be integrated into both CNN and ViT training pipelines, ensuring compatibility between the output format and the respective model input requirements.
- **Performance Comparison (Static vs. Dynamic):** Experiments will compare the performance of models trained on traditional static candlestick frames against models trained on dynamic plane snapshots. This will provide empirical evidence for the dynamic plane's effectiveness.
- **Data Generation for Training:** A batch generation process will create datasets based on the dynamic plane principle, formatted for training CNNs and ViTs. This process will ensure sufficient data volume for effective model training. The simulation capabilities will also be extended to handle longer sequences of data for more robust evaluation and training.

## II. Model Development and Training

### C. Dynamic Plane Implementation

This section details the implementation of the dynamic plane, crucial for visualizing and interpreting the model's understanding of market dynamics. The following requirements and tasks ensure a robust and informative visualization.

- **Minimum Points for Rotation:** The dynamic plane rotation will commence only after at least three stable data points are available. This mitigates potential Principal Component Analysis (PCA) instability associated with fewer points, ensuring more robust rotation calculations. (Source: Chat1.json message #96)

- **Smooth Rotation Matrices:** Rotation matrices will be smoothed or stabilized to prevent dimensional blowups and ensure smooth transitions in the dynamic plane animation, resulting in a visually coherent and interpretable animation. (Source: Chat1.json message #96)

- **Standalone Animation Simulator:** A separate animation simulator will be developed to visualize the dynamic plane. This simulator will incorporate the delayed rotation (after 3+ points), smoothing of early plane formation, and provide a step-by-step visualization of the process, enabling in-depth analysis and debugging of the dynamic plane behavior. (Source: Chat1.json message #96)

- **Dynamic Point Movement:** The animation will clearly display the dynamic movement of individual data points within the rotating plane, visualizing how individual points contribute to the overall market dynamics captured by the plane's movement. (Source: Chat1.json message #96)

- **Live Frame Rotation and Recentering:** The animation will depict the frame rotating and recentering in real-time, providing insights into how the plane dynamically adapts to changing market conditions. (Source: Chat1.json message #96)

## Example Heiken Ashi Candlestick Images

This section outlines the requirements for generating example images of Heiken Ashi candlestick data, both before and after transformation. Animation of the transformation process is explicitly excluded from this stage.

- **Raw Heiken Ashi Input:** Provide example images of raw Heiken Ashi candlestick input (`ed9c0b71-94b0-42d1-ab5f-8a2839c7dba0`). These images should be representative of typical input data and serve as a baseline for comparison, demonstrating the initial data format fed into the transformation pipeline.

- **Transformed Heiken Ashi Input:** Provide example images of Heiken Ashi candlestick data after transformation (`f99ed645-72ca-49b1-8480-b9e06325cfed`). This includes refocusing the origin and mapping the static 3D axes to the principal 2D rotating axes. These images should clearly illustrate the effect of the transformation.

**Focus on Static Examples:** The current deliverable focuses specifically on static image examples, foregoing animation of the transformation (`00a76568-80fb-4415-bf45-8223723dfcac`). This allows for targeted analysis of the data transformation and simplifies the immediate task.

**Future Functional Requirement (Visualization):** While animation is currently excluded, visualizing the local parabolic trajectory within the reconstructed plane remains a future functional requirement (`7eb1aa09-62d8-4d00-9073-164b98ee274b`).

**Task: Data and Pipeline Preparation:** The current priority is preparing the real dataset, constructing the batch generator for candlestick images, and conducting small validation tests on the core pipeline (`66b911a1-1b3d-4581-bd2b-554cb15b603f`). Generating these example images directly supports this task by providing visual confirmation of the data transformation process.

## Heiken-Ashi Data Transformation and Visualization

This section details the implementation of Heiken-Ashi candles, including their generation, visualization, dynamic rotation, and saving the resulting charts.

**Heiken-Ashi Candle Generation:**

A function will generate Heiken-Ashi candles from standard OHLC (Open, High, Low, Close) data. This function will accept an array of standard candle data, where each element represents a single candlestick with its associated time, open, high, low, and close values. The function will calculate the Heiken-Ashi values (open_HA, high_HA, low_HA, close_HA) using the following formulas:

- `Close_HA = (Open + High + Low + Close) / 4`
- `Open_HA = (Previous Open_HA + Previous Close_HA) / 2`
- `High_HA = Max(High, Open_HA, Close_HA)`
- `Low_HA = Min(Low, Open_HA, Close_HA)`

The output will be an array of Heiken-Ashi candles, mirroring the input structure but containing the calculated Heiken-Ashi values.

**Heiken-Ashi Plotting:**

A plotting function will visualize the Heiken-Ashi candlesticks. Green candles (close_HA >= open_HA) will represent upward price movement, while red candles (close_HA < open_HA) will represent downward movement.

**Dynamic Rotation and Recentering:**

A function will implement dynamic rotation and recentering of the Heiken-Ashi data. This function will calculate the midpoint between the open and close values for each candle and use these midpoints to perform the rotation and recentering transformation.

**Rotated Heiken-Ashi Plotting:**

A separate plotting function will visualize the dynamically rotated and recentered Heiken-Ashi data on a 2D plane.

**Saving Generated Charts:**

Functions will be implemented to save both the standard and rotated Heiken-Ashi charts.

## II. Model Development and Training

### C. Dynamic Plane Implementation

This section details the implementation and testing of the dynamic plane concept under various simulated market conditions, including complex and chaotic scenarios. This involves generating synthetic data, creating standard and dynamic Heiken-Ashi charts for visualization, and applying the dynamic rotation and re-centering logic.

**Synthetic Data Generation and Visualization:**

- **Complex Price Pattern:** A price simulation will be implemented to reflect a realistic market scenario, including a rally, a subsequent drop, and a recovery phase, incorporating more complex dynamics than simple price patterns. This data will be used to evaluate the dynamic plane's performance under more realistic market fluctuations.
- **Choppy/Chaotic Market Conditions:** To test robustness under extreme volatility and rapid price fluctuations, a simulation of a chaotic, choppy sideways market will be implemented using the `generate_choppy_candlesticks(n=30)` function. This function produces candlestick data (time, open, high, low, close) with small, random price fluctuations around a base price. Standard and rotated dynamic plane Heiken-Ashi charts will be generated from this data using the `generate_heiken_ashi`, `plot_heiken_ashi_candlestick`, and `dynamic_rotate_recenter_heiken` functions and saved to `/mnt/data/standard_heiken_ashi_choppy.png` and `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`, respectively.
- **Strong Linear Uptrend/Sharp V-Shaped Recovery:** A third market regime, simulating a strong linear uptrend or a sharp V-shaped recovery, will be generated. This will provide a more comprehensive evaluation of the dynamic plane's performance across diverse market conditions. Corresponding standard and rotated dynamic plane Heiken-Ashi charts will be created and saved.

### D. Model Enhancement and Refinement

This section details the analysis and visualization of market regimes within the dynamic plane framework to refine the model and provide insights for the dissertation.

**Market Regime Analysis and Visualization:**

- **Regime Behavior Analysis:** The behavior of trend, reversal, and sideways market regimes within the dynamic plane will be analyzed. The implications of these behaviors for model learning will be summarized, focusing on how the dynamic plane transformation impacts the model's ability to recognize and predict these regimes. This analysis will be detailed enough for inclusion in dissertation chapter drafts.
- **Comparative Visualization:** To illustrate the effect of the dynamic plane transformation, standard Heiken-Ashi charts and rotated dynamic plane charts will be compared side-by-side for the three market regimes: Trend-Reversal-Recovery, Choppy Sideways, and Strong Linear Uptrend/Sharp V-Shaped Recovery. These visualizations will be combined into a single, comprehensive panel for clarity and ease of comparison, showcasing the impact of the transformation across diverse market conditions. Charts will be plotted on a shared subplot grid to avoid matplotlib figure stacking issues.

### A. Model Architecture

This section details the architecture considerations specific to handling the dynamic nature of the input data transformed through Principal Component Analysis (PCA). The model is designed to learn underlying relationships within the dynamic coordinate system created by PCA, rather than relying on fixed interpretations of price, time, and volume.

- **Relational and Geometric Focus:** The model will prioritize learning relationships and geometric patterns within the PCA-transformed space. This focus on emergent geometric patterns is crucial because traditional technical indicators become less relevant in a dynamically rotating coordinate system.
- **PCA's Impact on Feature Representation:** The impact of PCA on the representation of input features (Price, Time, Volume) will be investigated and documented. Understanding how these representations vary with different input patterns is essential for interpreting the model's behavior.

### C. Dynamic Plane Implementation (Continued)

- **Window Size and Smoothing:** Careful consideration will be given to the PCA window size to capture meaningful patterns while mitigating noise. Smoothing techniques or stability thresholds will be implemented to reduce the risk of noise-driven rotations in the PCA space, which could lead to overfitting.

### B. Training and Validation

This section requires further development and will address training and validation using PCA-transformed images. Content from the universal checklist relating to this topic will be incorporated here.

## IV. Deployment and Monitor

---

## II. Model Development and Training

### A. Model Architecture (Specialization)

This section details refinements to the model architecture, incorporating advanced concepts related to dynamic space representation and learning.

- **Dynamic PCA Rotation Stabilization:** Implement techniques to stabilize the dynamic PCA axes, mitigating the impact of noise or small window sizes and improving the robustness of the dynamic space representation.

- **Loss Function for Dynamic Space:** Design and implement a loss function that encourages the model to focus on meaningful movement patterns within the dynamic PCA space, guiding it to learn relevant relationships between price movements and the dynamic coordinate system.

- **Relational Feature Learning:** Shift the model's learning from absolute feature perception ("If price rises over time") to relational perception ("If the dominant movement along the local principal axis shows rising oscillations"), enabling the capture of more nuanced and context-dependent patterns.

- **Market Movement Algorithm Error Signal:** Incorporate an "Error Signal" component within the market movement algorithm, analogous to biological error correction mechanisms, to account for increasing uncertainty and delayed feedback in longer-term market predictions. This will improve adaptability to changing market conditions.

### B. Dynamic Plane Implementation (Specialization)

This section details the implementation of the dynamic plane, incorporating error signal feedback and addressing potential issues related to lagging rotation and PCA calculations.

- **Error-Signal Integration:** Integrate prediction errors into the model training process through a lightweight mechanism, such as auxiliary loss functions or frame stability monitoring, prioritizing effectiveness and computational efficiency.

- **Peripersonal vs. Extrapersonal Gap Simulation:** Develop a method to simulate the "peripersonal vs. extrapersonal" gap within backtesting, differentiating between near-term, predictable price movements (peripersonal) and longer-term, less predictable movements (extrapersonal).

- **Removal of Static Error Value in PCA:** Remove the static, scalar float value currently used in PCA calculations to represent prediction error memory, as it introduces inaccuracies and negatively impacts performance.

- **Lagging Rotation Deactivation Strategy:** Define a strategy to deactivate the lagging rotation mechanism and revert to standard dynamic plane behavior when appropriate, preventing unwanted distortions in the coordinate system.

### C. Rolling Frame Correction and Error Mitigation

This section details the implementation of a rolling frame correction algorithm inspired by biological wound healing, addressing issues like plateaus in dual records and frame coincidence. Gradual corrections and a return to normal behavior prevent over-correction and enable adaptation to changing market conditions.

**Algorithm Components:**

The rolling frame correction algorithm comprises the following key components: (Details to be added here - as they were not provided in the original chunk)

### D. Model Enhancement and Refinement (Specialization)

- **Market Movement Algorithm Error Signal Implementation:** This enhancement introduces an error signal feedback loop, allowing the model to adjust its internal representation of market structure based on the difference between predicted and actual market movements. Specific strategies include:

  - **Frame Confidence Correction:** Post-prediction comparisons between actual and predicted movements will trigger adjustments to rotational frame assumptions, refining future rotations based on prediction errors.

  - **Prediction Error Memory:** A rolling memory of recent prediction errors across dynamic frames will inform adjustments to rotation weighting, making dynamic plane stability adaptive to its success rate.

  - **Feedback-Driven Frame Smoothing:** During periods of high prediction error, dynamic plane rotation will be temporarily slowed to increase smoothing, introducing a conservative approach during volatile conditions.

  - **Dual-Frame Estimation:** Two overlapping local frames—"optimistic" (immediate rotation) and "stable" (slower smoothing)—will be maintained. Predictions will be dynamically weighted between these frames based on observed market consistency, combining rapid updates with cautious corrections.

### E. Functional Requirements

- **Access Chat Contents from Another Chat:** Investigate accessing the "Thought Experiment Discussion" chat within the "SCoVA" project, considering API calls or database access. Prioritize permissions and access control for data security. This functionality is essential for leveraging insights from related research.

### F. Dissertation and Documentation

- **Interpretability Projection:** Address the challenge of reduced economic interpretability from dynamic axis rotation using PCA. Propose and detail a mechanism to project the model's focus back into the original Time-Price-Volume space for human understanding and analysis of trading strategies. This likely involves mapping learned PCA features back to original features, revealing which drive predictions.

### G. Deployment (Facilitator) and H. Monitoring (Enforcer)

Content related to deployment and monitoring of the dynamic plane will be integrated from the universal checklist.

## Frame Correction and Healing

This section details the implementation of a frame correction and healing mechanism within the Dynamic Plane Generator to address potential instability and drift in the principal component analysis (PCA) frame.

**Frame Correction Action:**

When the error trend detector signals a correction, small adjustments are applied to the PCA frame. This involves reweighting the principal axes away from the pure PCA output towards a more stable configuration, potentially by discounting minor eigenvectors. The magnitude of these adjustments requires careful tuning to balance stability with responsiveness to genuine market shifts.

**Healing Phase Logic:**

Once the error magnitude returns within acceptable bounds, a healing phase begins. During this phase, the frame adjustments are gradually reduced, likely using an exponential decay function. The decay rate, a configurable parameter, controls the transition back to relying primarily on the PCA output.

**Dynamic Plane Generator Integration:**

The entire rolling frame correction algorithm, including both the correction action and healing phase, will be integrated into the `DynamicPlaneGenerator` class as a modular function to maintain code clarity and facilitate future modifications.

**Error Visualization and Performance Monitoring:**

To aid in understanding and debugging, a visualization will illustrate the "error spike → correction → healing decay" cycle using simulated market data. Furthermore, a "frame intervention" metric will quantify the frequency and magnitude of frame adjustments over a typical trading year (e.g., 252 trading days). This metric helps assess system stability and the sensitivity of the error detection mechanism, aiding in parameter optimization.

## Error Trend Detection and Refinement

This section details the implementation and refinement of the pseudocode and error trend detection mechanisms, focusing on robust code development and a dynamic, two-dimensional error analysis.

**Pseudocode Implementation:**

The existing pseudocode for the dynamic plane and related components will be translated into robust, production-ready code, carefully considering data structures, algorithms, and edge cases.

**Advanced Error Trend Detection:**

The error trend detection will be significantly enhanced to analyze trends within the dynamic 2D frame, considering price, time, and volume interrelationships. This goes beyond basic statistical measures, incorporating distance _and_ angular error for a more comprehensive view of prediction deviations.

**Deviation Vector and Angular Error Tracking:**

Deviation vector monitoring will be implemented, calculating the vector difference between predicted and actual movement in the 2D plane. Angular error, representing the directional difference between these vectors, will be calculated using the arccosine of the normalized dot product. This provides a precise measure of directional accuracy within the rotated plane.

## I. Project Setup and Data Acquisition

_(This section remains empty as no relevant checklist items were provided.)_

```
## PCA Frame Management

This section details the implementation and rationale behind maintaining a consistent frame of reference derived from Principal Component Analysis (PCA) for accurate comparison of predicted and realized movement vectors. This consistent frame is crucial for robust error calculation and model refinement. Two primary methods, "Freeze Frame" and "Reproject Realization," address potential shifts in the PCA plane due to market dynamics. These methods offer improvements over traditional distance and angular error calculations that assume a static PCA plane.

### Clarification on Rotations and Vectors

The implementation utilizes two rotational angles and two distance vectors. These correspond to: 1) the global frame transformation based on PCA, and 2) the local vector representing predicted movement within that transformed frame.  This clarifies a point raised in user feedback (Chat1.json message #117).

### Error Correction Calculation

Error correction within the dynamic 2D plane relies on calculating two distinct error values (Chat1.json message #118):

* **Distance Error:** The magnitude difference between the predicted and realized displacement vectors, quantifying the error in the predicted magnitude of movement.
* **Angle Error:** The orientation difference between the predicted and realized direction vectors, quantifying the error in the predicted direction of movement.

The initial frame creation rotation (global PCA transformation) is *not* considered in the error correction calculation. The focus is on the local misalignment within the dynamically rotated frame.

### Freeze Frame

For short-term predictions (1-5 candlesticks), the model maintains a consistent frame of reference by "freezing" the PCA frame calculated at time 't' for predicting and evaluating movements over this short future horizon.  This assumes minimal structural market drift during this interval, simplifying prediction evaluation and reducing computational complexity.  Realized movement at 't+1' is then compared directly to the prediction made within the frozen 't' frame.  (See "Reproject Realization" below for handling longer time horizons.)

### Reproject Realization

For scenarios where the "Freeze Frame" assumption doesn't hold (e.g., longer prediction horizons), the realized movement at 't+1' is reprojected back into the PCA frame calculated at 't'. This utilizes the original PCA basis (rotation matrix) from 't', enabling direct comparison within the initial prediction frame and mitigating discrepancies that might arise from recalculating PCA at 't+1'.  This approach ensures a consistent evaluation of the model's predictive capabilities across varying market conditions.

### Data Structures and Visualizations

* **Data Structure for PCA Basis:** A lightweight data structure will store the PCA basis (rotation matrix) for each time window to facilitate efficient reprojection.
* **Visualizations:**  Visualizations within the application will illustrate the "Freeze Frame" and "Reproject Realization" processes, clarifying their distinction from traditional error calculations based on a static PCA plane and demonstrating their function within the system.  A diagram (Chat1.json message #118) illustrating the two layers of rotation (global PCA transformation and local vector misalignment) and how the calculated errors relate to these rotations will further enhance understanding. Pseudocode for the "Freeze Frame" and "Reproject Realization" processes will be provided upon development.


### Model Enhancements using PCA Frame Management

The following enhancements leverage the described PCA frame management techniques:

* **Error Correction Pseudocode Prototype:**  A working prototype will validate the error correction logic, serving as a foundation for integration into the main model.
* **Lightweight Frame Adjustment:**  Small frame adjustments will correct accumulating angular errors when exceeding a predefined threshold, maintaining frame consistency while minimizing overhead.
* **Clarify PCA Plane Consistency:** Investigation will ensure comparability between the predicted and realized movement matrices, even with potentially differing PCA axes due to varying price and volume data. This may involve a transformation or alignment step.


These enhancements aim to refine the model's accuracy and robustness by addressing potential inconsistencies in the dynamic frame of reference and incorporating a robust error correction mechanism.
## Robust PCA Frame Management and Error Calculation

This section details the implementation of robust Principal Component Analysis (PCA) frame management and associated error calculations for accurate model evaluation in a dynamic market.  Addressing potential shifts in market data distribution is crucial for preventing spurious results and ensuring consistent comparisons between predicted and realized data points.

**Robust PCA Frame Management:**

Two primary approaches, "Freeze Frame" and "Reproject Realization," manage potential shifts in market data distribution and maintain relational consistency between predicted and realized data points.

* **Freeze Frame:** This method "freezes" the PCA rotation matrix, *R*, at the time of prediction. Both predicted and realized data points are then projected using this same *R* matrix, ensuring consistent comparisons even if the underlying data distribution and the PCA plane shift.

* **Reproject Realization:** This method reprojects the realized movement vector onto the original PCA plane defined at the time of prediction. This achieves a similar effect to the "Freeze Frame" method by ensuring comparisons within the original prediction frame.

These robust methods mitigate the risk of spurious results caused by minor market fluctuations, ensuring model reliability even with evolving market conditions.

**Error Calculation within a Dynamic PCA Frame:**

The error checking mechanism accounts for the dynamic nature of the PCA planes (PCA1 and PCA2). Instead of assuming a static plane, deviations between the predicted and realized values for *both* PCA1 and PCA2 are calculated. This approach accurately reflects the error within the context of the shifting PCA frame.

**Total Error Calculation:**

The total error encompasses both the vector deviation error within the dynamic local frame and the frame shift error (change between PCA axes):

**Total Error = Vector Deviation Error + Frame Shift Error**

**Frame Drift Error Measurement:**

Frame drift, the difference between two sets of basis vectors (frames), is measured using principal angles between the two subspaces.  In this 2D context, this involves computing the angle between PCA1<sub>*t*</sub> and PCA1<sub>*t+1*</sub>, and between PCA2<sub>*t*</sub> and PCA2<sub>*t+1*</sub>.  The frame error is a weighted sum:

**Frame Error = *α* * Angle(PCA1<sub>*t*</sub>, PCA1<sub>*t+1*</sub>) + *β* * Angle(PCA2<sub>*t*</sub>, PCA2<sub>*t+1*</sub>)**

where *α* and *β* are tunable weights, adjusting the relative importance of each PCA axis in the frame drift calculation.

**Weighted Error Components and Normalization:**

A comprehensive weighted error calculation incorporates vector deviation, angular error, PCA1 angle error, and PCA2 angle error:

* **Vector Error = *α₁*⋅d<sub>vec</sub> + *α₂*⋅θ<sub>vec</sub>**  (d<sub>vec</sub> = distance, θ<sub>vec</sub> = angle)
* **Frame Shift Error = *β₁*⋅θ<sub>PCA1</sub> + *β₂*⋅θ<sub>PCA2</sub>**
* **Total Error = *γ₁*⋅Vector Error + *γ₂*⋅Frame Shift Error**

where:

* *α₁*, *α₂* control the trade-off between distance and angle within the frame.
* *β₁*, *β₂* control the trade-off between PCA1 and PCA2 drift.
* *γ₁*, *γ₂* control the overall trade-off between prediction error and frame instability.


Normalization or another suitable adjustment will be applied to distance and angular errors before combining them to ensure a meaningful aggregate error.

**Frame Drift as Confidence Indicator:**

The potential of Frame Drift Error as a confidence indicator will be explored.  A larger drift might suggest lower confidence in the prediction and inform decisions about trading or holding, potentially through thresholds or integration into a risk management strategy.


**Illustrative Example and Pseudocode:**

A small-scale simulation will demonstrate vector deviation and PCA frame drift.  Pseudocode for the "Freeze and Correct" module, encompassing both "Freeze Frame" and "Reproject Realization" methods, and the total error calculation, will be provided to guide implementation and ensure reproducibility. This will include numerical examples and visualizations of predicted and realized paths in both the frozen and shifted frames.
## Error Handling and Dynamic Adjustment

This section details the mechanisms for handling errors and dynamically adjusting the model's behavior based on observed error trends to ensure robustness and adaptability to changing market conditions.

**Error Calculation:**

1. **Normalization:** Angle units (e.g., degrees) will be normalized to the range [0, 1] to simplify error computation and combination with other error components.
2. **Pseudocode:** Formal pseudocode will be developed for the multi-weight error computation to ensure accurate and consistent implementation.  This pseudocode will incorporate the α, β, and γ weights.
3. **Default Weights:** Initial default values for the α, β, and γ weights will be proposed based on trading dynamics and physics principles. A numerical example demonstrating the calculation of the total error from individual atomic error components will be provided.

**Dynamic Adjustment (Healing Phase):**

An Error Trend Detector will monitor a rolling window of calculated errors. If errors accumulate too rapidly, a "Healing Phase" is triggered, applying a correction factor to the PCA frame construction.  This correction factor decays as errors decrease. If errors spike again during the Healing Phase, the system re-enters "Correction Mode," restarting the corrective process.


## System State Management and Rolling Error

This section details the implementation of a rolling error buffer and the logic managing system state transitions between normal operation, "Wound" (significant accumulated error), and "Healing" (error recovery).

**Rolling Error Buffer:**

A circular buffer will store the total error over the last N steps (e.g., 5-10 windows).  With each new error, the mean and standard deviation of the errors within the buffer will be recalculated.

**State Transitions:**

State transitions are based on thresholds related to the rolling error statistics:

1. **Wound Threshold:**  The system enters the "Wound" phase when the rolling mean error exceeds k times the rolling standard deviation (recommended k = 2).
2. **Healing Threshold:** The system transitions to the "Healing" phase when the mean error drops below a threshold between 1 and 1.5 times the rolling standard deviation.

**Correction Factor:**

In the "Wound" phase (Correction Mode), a correction factor is applied to the PCA rotation or smoothing during frame construction.  The specific implementation of this factor (e.g., scaling the rotation matrix, adjusting smoothing parameters) will be detailed in the implementation documentation.


## Error Detection and Healing System Implementation

This section details the implementation of the error detection and healing system, including its modular design, correction factor decay, and dynamic re-entry into correction mode.

**Module Design:**  A dedicated module will encapsulate the error detection, correction, and healing logic.

**Healing Phase Decay Mechanism:**  An exponential decay (λ = 0.95 initially) will gradually reduce the correction factor's influence during the "Healing Phase," allowing a smooth transition back to the primary prediction model.  This value is subject to tuning.

**Dynamic Correction Re-entry:** If errors re-emerge during the healing phase, exceeding pre-defined thresholds, the system dynamically re-enters "Correction Mode."

**Tuning and Visualization:**  Initial tuning values for thresholds and decay rates will be based on common market regimes (e.g., high volatility, low volatility). Visualizations will depict the healing process, showing transitions between modes and the decay of the correction factor over sample market data.


## Performance-Based Healing and Decay Rate Adjustment

This section details a dynamic healing mechanism based on predictive performance, replacing a time-based approach.

**Tracking Prediction Performance:**

The mechanism tracks:

1. **True Prediction Values:** The raw, unaltered prediction values from the model.
2. **Prediction Correctness:**  Whether the predicted outcome was achieved.  Specific criteria for determining correctness will be defined based on the nature of the predictions.  This information will drive adjustments to the decay rate of the correction factor, optimizing the model's recovery based on its actual performance.
## II. Model Development and Training

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of a dynamic plane for data representation, addressing the challenges of scaling time and value axes appropriately.

* **Robust Scaling for Price and Volume:** To prevent extreme price and volume values from distorting visualizations, robust scaling will be employed. This approach ensures outlier values do not disproportionately influence the scaling of the axes, preserving a meaningful visual representation of the data, even with unseen extremes. This directly impacts the UI/UX of the visualizations.

* **Non-linear Time Representation:**  Linear time representation is insufficient for effective normalization.  Instead, a non-linear representation will be used, preserving chronological order while providing a meaningful scalar value for axis representation within the dynamic plane. This addresses the core data model for time within the visualization.

* **Live LTP Feed for Time Tracking (Evaluation):**  The feasibility of using a live feed of Last Traded Prices (LTPs) for tracking elapsed time between trades will be evaluated.  Potential downsides, such as the loss of historical context, will be considered.

---

## Healing-by-Correctness System

This section details the implementation of a "Healing-by-Correctness" system. This system dynamically adjusts corrective measures based on the model's predictive accuracy, minimizing unnecessary corrections during periods of high performance and maintaining or increasing corrections when performance deteriorates.  The system relies on evaluating the correctness of direction and magnitude predictions, regardless of frame-related issues.  A simple scoring system assigns +1 for correct predictions and 0 otherwise.

**Prediction Correctness Tracking:**  A rolling buffer of *N* timesteps will store the prediction correctness scores (1 or 0). The mean of this buffer will provide a measure of recent predictive accuracy.

**Dynamic Decay Rate Adjustment:** The correction factor's decay rate will be dynamically adjusted based on the mean prediction correctness.  A higher mean correctness (indicating better predictive performance) will accelerate the decay of the correction factor, reducing its influence.  Conversely, lower correctness will slow or halt decay, maintaining or increasing the correction factor’s influence.  This performance-based approach provides a more responsive healing mechanism than arbitrary exponential decay.

**`dynamic_decay_rate` Function:** This function, `dynamic_decay_rate(mean_correctness)`, calculates the decay rate of the correction factor. A suggested implementation is: `Decay Rate = 1 - (mean_correctness - healing_threshold)`. This formula ensures accelerated decay as correctness surpasses the `healing_threshold`.  For example, decay is slow when correctness is just above the threshold, but rapid when correctness is high (e.g., 95-100%).

**Healing Logic Update:** The existing healing logic will be modified to incorporate the `dynamic_decay_rate` function.  The correction factor will be scaled inversely with the rolling prediction correctness over the last *N* steps.  High correctness (e.g., ≥ 80%) should proportionally reduce the correction factor, while lower correctness should hold it steady or even increase it.

**Formal Pseudocode and Simulation:** Formal, modular pseudocode for the complete Healing-by-Correctness system will be provided, incorporating both the prediction correctness tracking and the `dynamic_decay_rate` function. A toy example simulating a wound, correction, and true healing (via regained predictive accuracy) will validate the logic and identify potential issues.

**Initial Healing Thresholds:**  Initial healing thresholds, such as 75-80% directional correctness, will be proposed based on relevant trading context. This threshold, used in the `dynamic_decay_rate` function, influences how aggressively the correction factor is reduced.


---

## Data Preprocessing for Principal Component Analysis (PCA)

Before applying PCA to the price (P), time (T), and volume (V) data, careful preprocessing is crucial. These steps ensure meaningful PCA results by addressing the different scales and distributions of these variables.

**1. Normalization:** Price (P), Time (T), and Volume (V) are normalized to a uniform scale before PCA to prevent features with larger magnitudes from disproportionately influencing the principal components. This normalization occurs within each rolling window.

**2. Z-score Normalization:** Within each rolling window of *N* data points, each feature (P, T, and V) is independently centered and scaled using z-score normalization. This involves subtracting the mean ($μ_t$, $μ_p$, $μ_v$) and dividing by the standard deviation ($σ_t$, $σ_p$, $σ_v$) for each feature within the window:

```

X_scaled[i] = [(t_i - μ_t) / σ_t, (p_i - μ_p) / σ_p, (v_i - μ_v) / σ_v]

```
Where `i` represents the data point within the window.


**3. Time Handling:** Two approaches are available for handling timestamps:

* **Relative Time Index:** For windows consistently containing the last *N* trading candles, a simple, z-score normalized sequence of integers (1, 2, ..., *N*) can represent the relative time within each window.
* **Absolute Clock Time:**  Calculate normalized time deltas to incorporate diurnal patterns:

```

Δt_i = (timestamp_i - μ_t) / σ_t

```
Note that large gaps in timestamps can inflate $σ_t$ and reduce the influence of time in the PCA.

**4. Volume Transformation:** To address the often heavy-tailed distribution of volume data, either a log transformation:

```

v_i' = log(1 + v_i)

````

or robust scaling (using the median and interquartile range (IQR)) can be applied.  Robust scaling involves subtracting the median and dividing by the IQR, making it less sensitive to extreme outliers.

**5. PCA Implementation:**  After preprocessing, PCA is performed on the scaled data matrix $X_{scaled}$ using Singular Value Decomposition (SVD):

```python
u, s, vh = np.linalg.svd(X_scaled, full_matrices=False)
axes = vh[:2]   # Two principal directions in T-P-V space
````

This code example selects the first two principal components.

## Data Transformation and Preparation

This section details the transformations applied to price and volume data before model input. These steps ensure consistent data representation and mitigate the impact of outliers.

**1. Volume Transformation:**

A three-step process transforms the volume data:

- **Log Transformation:** A logarithmic transformation (`np.log1p(volume)`) normalizes the often-skewed volume distribution.
- **Percentile Clipping:** Values outside the 5th and 95th percentiles are clipped to remove extreme outliers that could unduly influence the model.
- **Min-Max Scaling:** The clipped, log-transformed volume is then min-max scaled to the range [-1, 1], ensuring consistency with other features.

**2. Price Transformation:**

- **Log Return Calculation:** Price data is transformed into log returns.
- **Percentile Clipping:** Similar to volume, extreme outliers in the log returns are clipped at the 5th and 95th percentiles. This mitigates the impact of extreme price movements.
- **Min-Max Scaling:** The clipped log returns are min-max scaled to the range [-1, 1] for consistency.

**3. Time Representation:**

Time within each window is represented as fractional elapsed time, normalized to the range [0, 1]. This accounts for irregular time intervals and provides a consistent representation of time across different windows. This normalized time can be further scaled to [-1, 1] if required by the model.

**4. Data Assembly and PCA (Optional):**

The transformed time, price, and volume data are combined into a single matrix. Optionally, Principal Component Analysis (PCA) can be applied to this matrix for dimensionality reduction and feature extraction. If PCA is used, mean-centering the data beforehand is recommended. The choice of using PCA will depend on the specific characteristics of the data and model requirements.

This preprocessing pipeline ensures the data is appropriately scaled and robust to outliers, improving the performance and stability of downstream models.

## I. Project Setup and Data Acquisition

### B. Data Acquisition and Preprocessing

This section details the acquisition, preprocessing, and preparation of financial data, including the generation of candlestick charts and calculation of return labels. This data will be used for both visualization and as input to the CNN model.

- **Candlestick Chart Generation:** Generate candlestick charts with volume data for five distinct market patterns: uptrend with rising volume, downtrend with volume spikes, reversal (down then up), sideways chop, and breakout spike then stabilize. These charts will serve as visual aids for analysis and as input data for the CNN model. The inclusion of volume data is essential for capturing the relationship between price action and trading volume. Generating examples of these five patterns ensures the model is trained on a diverse range of market behaviors.

- **Data Preprocessing:** After acquiring the price and volume data, the following preprocessing steps are applied:

  - **Log Return Calculation:** Calculate the log return of the price data. This transformation helps to stabilize the variance and make the data more suitable for modeling.
  - **Percentile Clipping:** Clip extreme values in the log returns using a 99th percentile threshold for both positive and negative returns. This mitigates the impact of outliers.
  - **Min-Max Scaling:** Apply min-max scaling to the log return of price and the log-transformed volume, mapping them to the [-1, +1] range. The `time_frac` feature (representing the fraction of the trading day) is also scaled to this range. Using a consistent scaling range for all features is crucial for algorithms like PCA. The minimum and maximum values for scaling are calculated across the entire dataset to ensure consistent transformation.

## II. Model Development and Training

### C. Dynamic Plane Implementation

This section details the implementation of the dynamic plane representation, transforming candlestick data into a format suitable for CNN input. This involves data normalization, PCA rotation, and image generation.

**Transformation Method:** Candlestick and volume data are transformed using a combination of log returns, PCA rotation, and normalization to generate dynamic plane snapshots for CNN input. Time (`time_frac`), log return, and log volume are normalized to the range [-1, 1] before applying PCA rotation. This transformation aims to capture the essential price and volume dynamics in a format easily processed by the CNN.

**PCA Rotation:** Principal Component Analysis (PCA) is applied to the normalized time, log return, and log volume data. This reduces the dimensionality of the data while preserving the most important variance, creating a 2D representation suitable for visualization and CNN input. Optionally, the data matrix can be centered (mean subtracted from each column) before applying PCA, although this may be redundant given the min-max scaling.

**Image Generation and Visualization:** Five pairs of images will be generated, each pair illustrating the transformation process for a specific market pattern (uptrend with rising volume, downtrend with volume spikes, reversal, sideways chop, and breakout spike then stabilize). Each pair will consist of:

1.  A candlestick chart visualizing the raw price and volume data.
2.  A candlestick chart visualizing the normalized data (scaled to [-1, 1]) after applying the log transformation, percentile clipping, and min-max scaling. This represents the data _before_ PCA rotation.

Each candlestick in these charts represents a 10-minute interval within a single trading day. The generated image pairs will be displayed individually to facilitate clear visual comparison and analysis of the transformation's impact. A further visualization will show the 2D dynamic plane snapshot after PCA rotation, demonstrating the final input to the CNN. The necessity of explicitly including volume bars in this final visualization will be investigated.

## Data Representation and Analysis

This section explores the effectiveness of the current data transformation, which includes log-volume scaled to the [-1, 1] range, and investigates whether a separate visual representation of volume bars enhances the model's input. We analyze the impact of volume on Principal Component Analysis (PCA) patterns derived from a "Breakout, Spike, then Stabilize" price pattern.

### PCA and Volume Analysis for "Breakout, Spike, then Stabilize" Pattern

To understand the relationship between volume and PCA patterns, we will:

1. **Generate five distinct PCA examples:** These examples will be based on the "Breakout, Spike, then Stabilize" pattern, each with varying volume profiles. This variation allows us to assess how volume influences the resulting PCA patterns, both visually and analytically.

2. **Visualize scenarios:** For each of the five examples, we will generate two visualizations:

   - **Original Candlestick + Volume Chart (10-minute intervals):** This chart provides a baseline representation of the raw market data.
   - **Transformed Dynamic Plane Projection:** This chart displays the transformed data used as model input, including scaled time, log-return, and log-volume projected onto the first two principal components (PC1 and PC2). This visualization helps illustrate how the transformations affect the data representation.

3. **Analyze PCA Patterns:** We will compare the five PCA patterns, focusing on how volume magnitude and timing affect the trajectory shape in the transformed space. Key observations will include:
   - **Pre-spike clustering:** Analysis of data point clustering before the spike.
   - **Spike representation:** How the spike is represented in the PC2 component, particularly its relationship to volume.
   - **Post-spike stabilization:** Analysis of data behavior after the spike, focusing on cluster formation and trajectory dynamics.

## Data Smoothing and Interpolation

This section evaluates different interpolation techniques for potential use in processing price-time-volume data for feature extraction within a Convolutional Neural Network (CNN) or PCA context.

### Interpolation Techniques Evaluation

Several methods were considered:

- **Bivariate Spline Interpolation:** While effective for smoothing 2D data, this method was deemed unsuitable due to the potential distortion of crucial signals (sharp features and spikes) in the discrete price-time-volume data, which are essential for CNN and PCA analysis.

- **3D Interpolation:** Alternative 3D smoothing approaches, including extensions of bivariate spline interpolation, were explored.

- **Linear Interpolation:** This simpler method was evaluated as a potential alternative, given the core approach's reliance on capturing discrete data changes.

### Selected Approach

Based on the evaluation, bivariate spline interpolation and other complex smoothing methods were rejected due to the risk of obscuring critical features. Preserving the discrete changes in the data is paramount for effective feature extraction by CNN or PCA. The document will further detail the chosen approach, which prioritizes maintaining these discrete changes.

## Unrelated Task: Swaha.md Processing

The `Swaha.md` file, crucial for the bot's understanding of its function and persona, requires processing. Details of this task are outside the scope of this section.

## Agent Checklist for SCoVA Project Implementation (Excerpt)

This excerpt from the project checklist highlights the sections relevant to the preceding discussions.

```markdown
# Agent Checklist for SCoVA Project Implementation

## II. Model Development and Training

### C. Dynamic Plane Implementation (Specialization)

- _(Details related to data transformation and visualization discussed above)_

## III. Testing and Evaluation

_No relevant items from the provided checklist chunk._

## IV. Deployment and Monitoring

### A. Deployment (Facilitator)

- _(Details related to deployable system development and monitoring dashboard discussed below)_

## V. Dissertation and Documentation

_No relevant items from the provided checklist chunk._
```

## System Deployment and Monitoring

This section focuses on translating the research and experimental design into a functional algorithmic trading system.

Key elements include:

- **Deployable System Development:** Creating a complete, deployable algorithmic trading bot system encompassing data acquisition, preprocessing, model training, prediction, and trade execution.

- **Monitoring Dashboard:** Developing a real-time monitoring dashboard to track bot performance, key metrics, and trading activity, enabling ongoing monitoring and timely adjustments.

- **Model Training:** Training robust predictive models on the prepared dataset of candlestick images and potentially other relevant data sources.

- **DynamicPlaneGenerator Module Implementation:** Implementing this module to transform market data into a suitable format for the predictive models. This includes:
  - **Data Normalization:** Normalizing time and price (using log-returns). The specifics of volume normalization (e.g., log-volume) and scaling to the [-1, 1] range are addressed in the preceding Data Representation and Analysis section.
  - _(Remaining steps of the DynamicPlaneGenerator implementation)_

````
## II. Model Development and Training

### A. Model Architecture

This section details the architecture of the predictive model, emphasizing its multi-scale temporal design and the integration of various timeframes.  The model will leverage a dynamic plane representation of market data, processed across different timescales, and incorporate a hierarchical attention mechanism.

### B. Dynamic Plane Implementation

This subsection details the implementation of the dynamic plane for transforming market data into a 2D representation across different timeframes. This approach aims to capture the relational flow of market dynamics by focusing on the most influential movement axes.

1. **Data Normalization and Transformation:** Time, Price (using log-returns), and Volume (using log-transformation) are normalized to a uniform range of [-1, +1]. This ensures consistent scaling across features and prevents dominance by any single dimension while preserving chronological order.

2. **Dynamic Frame Construction:**  Within a recent time window for each timeframe (e.g., intraday, daily, weekly, monthly), Principal Component Analysis (PCA) is applied to the normalized data. This identifies the two principal components (PC1 and PC2) representing the axes of greatest correlated movement. These components define the dynamic 2D plane for that timeframe.

3. **Projection and Re-centering:**  Recent market history is projected onto the derived 2D plane (PC1 and PC2). The viewpoint is then re-centered, placing the most recent data point at the origin (0,0). This emphasizes the latest market movements relative to the dynamic axes.

4. **Image Generation:** The transformed 2D representation is rendered as an image, potentially using candlestick or Heiken-Ashi visualizations.  This image serves as input to the predictive model.

### C. Multi-Scale Temporal Model

This subsection describes the multi-scale temporal model and its integration with the dynamic plane representations.

1. **Non-Hierarchical Attention Mechanism:** A hierarchical attention mechanism is employed to weigh and combine the information from different timeframes. This allows the model to dynamically learn the relative importance of each timeframe’s data for the prediction task.

2. **Prediction Targets:** The model aims to predict a 2D movement vector (Δx', Δy') within the dynamic plane of the *primary* timeframe (e.g., intraday). This vector represents the anticipated future trajectory of market movement.  Additionally, the model predicts "Rally Time," estimating the time required for the predicted movement to materialize.

3. **Self-Correction and Adaptation:** A performance-driven feedback loop enhances adaptability. A Total Error signal, composed of Vector Deviation Error (distance and angular error between predicted and actual movement) and Frame Shift Error (discrepancy between predicted and actual PC axes), drives this feedback. "Wound Detection" identifies significant prediction errors, triggering a "Healing Phase" that gradually restores the full dynamism of the plane's rotation and scaling based on recovering predictive accuracy.

### D. Data Acquisition and Preprocessing

The data pipeline incorporates multiple timeframes (intraday, daily, weekly, monthly), necessitating a robust and efficient design. Separate datasets are prepared for each timeframe using the Dynamic Rotating Plane method. Careful optimization of data handling procedures mitigates potential performance bottlenecks during training and prediction.  The increased complexity of the data pipeline due to multiple timeframes is addressed through a modular and scalable design.
## II. Model Development and Training

### A. Model Architecture

This section details the architecture of the predictive model, focusing on the Vision Transformer (ViT), its self-correcting mechanism, and data normalization strategies.

**1. Vision Transformer (ViT):**  The core prediction engine is a Vision Transformer (ViT).  The ViT processes the dynamic planes generated from each timescale as a sequence of tokens, effectively capturing evolving market dynamics by learning dependencies and relationships between these different timescales.

**2. Self-Correcting Mechanism:**  A performance-driven feedback loop enhances model robustness and adaptability. This self-correcting mechanism uses a "Total Error" signal composed of:

* **Vector Deviation Error:**  Quantifies the deviation between the predicted and actual movement vectors.
* **Frame Shift Error:** Measures the error introduced by the dynamic rotation and shifting of the coordinate system in the dynamic plane.

When the Total Error exceeds a predefined threshold, the mechanism enters a "correction mode," dampening the dynamic plane's rotation to reduce reactivity to potentially spurious market fluctuations. This dampening gradually diminishes as prediction accuracy improves, restoring full responsiveness.

**3. Data Normalization:** Input data undergoes normalization to ensure consistent processing by the ViT, transforming Time, Price, and Volume data to a uniform range of [-1, +1]:

* **Time:** Normalized as fractional elapsed time.
* **Price:** Normalized using log-returns to capture percentage change and stabilize variance.
* **Volume:** Normalized using a log transformation to address skewness and robust scaling to mitigate outlier impact.  This ensures all features contribute equally to the model's learning process.


## III. Application Development

This section details the technical implementation choices for the SCoVA project, including backend and frontend technologies, application functionality, and user interface design.

**Backend Technology:** Python will be used for backend development, leveraging its extensive libraries for data analysis, machine learning, and web development.

**Frontend Technology:** A Progressive Web App (PWA) will be used for the frontend, offering cross-platform compatibility, offline functionality, and a native app-like experience.

**Application Functionality:** The Swaha trading application will support:

* **Initial Training:** Model training with the prepared dataset.
* **Backtesting:**  Model evaluation on historical data.
* **Live Trade Execution:** Real-time trade execution based on model predictions.
* **Trade Error Monitoring:**  Monitoring and reporting of trade execution errors.
* **Model Revision:**  Tools for model updates based on performance and market conditions.
* **Trade Ledger Reporting:**  Access to comprehensive trade reports.

**UI Design (iPad - Portrait):** The Swaha trading app's user interface, designed for a portrait iPad, will use tab-based or sidebar navigation for easy access to sections dedicated to Training, Backtesting, Live Trading, Error Monitoring, Model Revision, and Reporting.

**Main Dashboard UI:** The main dashboard provides a central hub for monitoring system performance and status:

* **Top Bar:** Displays Swaha Status (color-coded system status), Market Status, and Current P&L.
* **Main Content Area:** Displays a Live Portfolio Snapshot, Performance Chart, Last 5 Trades, Model Confidence & Error (Frame Stability, Prediction Correctness), and System Logs.


## IV. UI/UX Considerations

This section details the User Interface (UI) and User Experience (UX) considerations for various SCoVA modules.  A consistent and informative UI is crucial for effective system interaction.

**Training & Backtesting Module Color Scheme:** For clear visual distinction, equity curves will be solid blue, benchmark lines dashed grey, and loss curves differentiated using distinct colors (e.g., blue and purple) or varied line styles.
## UI/UX Design and Enhancements

This section outlines the user interface (UI) and user experience (UX) design for the SCoVA project's main dashboard, targeting a portrait iPad layout. The design emphasizes clear information hierarchy, accessibility, and intuitive status representation using a card-based system.

### Main Dashboard Layout (Portrait iPad)

The dashboard follows a top-down information architecture optimized for portrait iPad orientation.  A card-based design compartmentalizes information for improved visual clarity.  The layout comprises:

* **Top Bar:** Provides at-a-glance access to critical system status information.
* **Key Performance Visuals:** Displays graphical representations of key performance metrics, situated below the top bar.
* **Granular Logs (System Log Stream):** An auto-scrolling log stream at the bottom, providing detailed system insights and the rationale behind actions.


### Top Bar Elements

The top bar displays crucial real-time system information:

* **Swaha Status Indicator:** A circular icon with a pulsing glow for active states (Live Trading, Backtesting, Training) and static display for Idle or Error. Tapping the icon reveals details: model version (Live Trading), backtest experiment name (Backtesting), training progress (Training), and system logs (Error). Icons and colors are used for clear status identification: Live Trading (blue play icon), Backtesting (yellow fast-forward icon), Training (purple gears/brain icon), Idle (grey pause icon), and Error (orange warning icon).
* **Market Status:** A simple text display indicating the current market status (e.g., NSE: OPEN, NSE: CLOSED).
* **Current P&L (Today):** Displays the current day's profit and loss, showing both absolute value (e.g., +₹12,450.75) and percentage return (e.g., +1.25%). Blue indicates profit, orange indicates loss, and grey indicates no change.

### Key Performance Visuals (Widget Grid)

The following widgets provide a detailed overview of performance and model behavior:

* **Live Portfolio Snapshot:** A scrollable list of open positions. Each entry displays ticker, direction (LONG/SHORT), quantity, average price, last traded price (LTP), and unrealized P&L. Tapping a row expands it to show a mini sparkline of the stock's price movement since position opening.  LONG positions will be tagged with blue indicators and SHORT with orange.

* **Model Cognitive State - Frame Stability:** A horizontal bar or seismograph-style line chart visualizing the real-time stability of the model's cognitive frame using Total Error.  The line's color dynamically shifts from blue (stable) to yellow and then orange (increasing instability).

* **Model Cognitive State - Prediction Correctness:** A radial gauge visualizing the rolling accuracy of directional predictions over the last N trades. A blue-to-orange gradient represents the accuracy spectrum (blue for high accuracy, transitioning to orange for low accuracy).

* **Today's Performance:** A line chart comparing the portfolio's cumulative percentage return against the NIFTY 50's return. A scrubbing feature allows users to pinpoint exact return values at any time during the trading day.

* **Recent Trades:** A reverse-chronological list of the last five closed trades.  Each entry includes ticker, action (buy/sell), P&L, and trade duration. Profitable trades will be highlighted in blue, and losses in orange.

### Color Accessibility

The UI utilizes a colorblind-friendly palette. Blue replaces green, and orange replaces red for indicators and status displays, ensuring accessibility for users with red-green colorblindness.  Supporting icons and text labels further enhance clarity.
## UI/UX Considerations

This section details the UI/UX design specifications for the SCoVA project, emphasizing visual clarity and intuitive user interaction.

**Today's Performance Chart:**

The "Today's Performance" chart benchmarks "My Portfolio" against NIFTY 50.  A solid blue line represents "My Portfolio," while a dashed grey line represents NIFTY 50, enabling quick visual performance comparison.

**Live Trade Execution Module:**

This module uses a consistent color scheme: blue for profits/gains and orange for losses.  This applies to tables and LONG/SHORT tags for immediate trade outcome identification.  SHORT tags will use an orange background.  Unrealized P&L text will be blue for positive values and orange for negative values.


**Training & Backtesting Module:**

Optimized for iPad portrait orientation, this module features a two-panel layout: a left "Control Panel" and a right "Monitoring Display."

* **Control Panel:** This panel provides organized, collapsible sections for experiment configuration:
    * Data & Timeframe
    * Dynamic Plane Configuration
    * Model & Learning Architecture
    * Error & Healing Mechanism

  Input fields are provided for the experiment name, a Training/Backtesting toggle switch, and configuration options for data, timeframe, model parameters, and the self-correction system.  Intuitive input mechanisms include dropdown menus, interactive calendars, sliders, steppers, checkboxes, and toggles. Clear typography and logical element grouping ensure ease of use.


* **Monitoring Display:** This panel presents real-time visualizations and performance metrics. During training, a line chart displays training and validation loss. During backtesting, it displays an equity curve chart, benchmark comparison, and key performance metrics (Sharpe Ratio, Max Drawdown, Win Rate, and Total P&L), providing a comprehensive performance overview.



## II. Model Development and Training

### A. Model Architecture (Specialization)

This section details the model architecture, including configurations for the dynamic plane implementation and data handling. User-configurable options facilitate experimentation and optimization.

* **Data & Timeframe Configuration:**  The model architecture supports various asset universes (e.g., NIFTY 50, NIFTY 500, custom watchlists) and date ranges.  The system automatically splits the date range into training and validation sets for robust testing across different market conditions and asset groups.

* **Dynamic Plane Configuration:** The model's "perception system" interprets market data.  User-configurable options include:
    * **Candlestick Type:** Options beyond standard candlesticks (e.g., Heiken-Ashi) influence how the model perceives price action.
    * **Local Window Size:**  This parameter controls the model's sensitivity to short-term vs. long-term price movements.
    * **Feature Inclusion:** Users select features (price, time, volume) for dynamic plane calculations, controlling the information fed to the model.

* **Model & Learning Architecture:** This section offers extensive control over:
    * **Model Selection:** Choose from various models (e.g., CNN, ViT, hybrid architectures).
    * **Hyperparameter Tuning:** Adjust learning rates, batch sizes, optimization algorithms, and loss functions.  ViT-specific parameters (patch size, embedding dimensions, transformer layers, attention heads, dropout rate) are also configurable.
    * **Multi-Scale Context Fusion Methods:** Experiment with different fusion techniques (e.g., attention-based, concatenation, weighted average).
    * **Optimizer Selection:** Choose from various optimizers (e.g., Adam, SGD, AdamW) with configurable parameters.
    * **Loss Function Selection:** Select loss functions tailored to the prediction target (e.g., trend direction, reward magnitude).

* **Self-Correction System Configuration:**  This mechanism includes user-configurable options for:
    * **Wound Detection Threshold:** Sets the sensitivity for detecting prediction instability.
    * **Healing Trigger:** Defines the prediction accuracy threshold for triggering a "healing" process (e.g., retraining or parameter adjustments).

* **Live Visualization of Perception:** For enhanced understanding and debugging, the architecture provides a live, side-by-side visualization of the standard Heiken-Ashi chart and the dynamically rotated and re-centered 2D plane image input to the model, updating with each new candlestick.

## Enhancements and Monitoring

This section outlines UI/UX and monitoring improvements for the SCoVA project, enhancing user control, flexibility, and insight into model training and backtesting.

**Performance Monitoring:** Real-time visualizations of training/validation loss, equity curves, benchmark comparisons, and other KPIs are provided during training and backtesting, enabling continuous performance monitoring and timely adjustments.

**Input Validation:** The Control Panel implements input validation to prevent logical errors and ensure data integrity (e.g., preventing an end date before the start date).

**Model & Learning Architecture Enhancements:**  This area will receive further enhancements to expand user control and flexibility.  Further investigation will determine the specific options to add.  The current UI for the Model & Learning Architecture also requires improvements to clearly display information and enhance usability.
## II. Model Development and Training

This section details enhancements to the model's architecture, training process, and data requirements.

### E. Multi-Scale Context and Healing Mechanisms

This feature enhances the model's robustness and adaptability by incorporating information from different time scales and implementing healing mechanisms to address performance degradation.

**1. Multi-Scale Context Fusion:** This allows the model to learn from various time horizons, enriching the context for prediction.  Users can select from three fusion methods:

* **a) Attention-Based (ViT):**  Utilizes the attention mechanism of the Vision Transformer (ViT) architecture to weigh and combine information from different time scales.
* **b) Concatenation:** Directly concatenates features extracted from different time scales before inputting them to the model.
* **c) Weighted Average:**  Calculates a weighted average of features from different time scales, offering flexible control over each scale's contribution.

**2. Total Error Metric:**  A comprehensive error metric evaluates the effectiveness of multi-scale context fusion.  It comprises two weighted components:

* **a) Vector Error:** Measures the discrepancy between predicted and actual price movements.
* **b) Frame Shift Error:** Quantifies temporal misalignment between predictions and actual market events.  Users can adjust the weights of these components.

**3. Healing Mechanisms:**  Two triggers activate healing mechanisms to counteract performance decline:

* **a) Performance-Based Trigger:** Initiates healing when prediction accuracy falls below a user-defined threshold.
* **b) Time-Based Trigger:** Activates healing if the Total Error metric remains above a specified threshold for a defined duration.


### F. Data Requirements

**1. Expanded Data Download:** Intraday data for all listed stocks in both India and the US will be downloaded across all available intervals. This comprehensive dataset will facilitate the development and testing of multi-scale context fusion and other model enhancements.

## III. Technical Requirements

This section details the technical requirements for data handling, model input features, and dynamic system behavior.

**Library for Intraday Data:**  A suitable library will be selected for managing extensive long-term intraday data. This library must efficiently handle large datasets and provide robust functionalities for data manipulation, storage, and retrieval. The choice of library is crucial for the performance and scalability of the data processing pipeline.

**Categorical Model Input:** Model input candlesticks will be categorized to improve learning and avoid biases:

* **Market Capitalization (Caps):** Categorization by market capitalization (e.g., small-cap, mid-cap, large-cap) allows the model to discern patterns specific to different company sizes.
* **Sectors:**  Using sector-specific candlestick patterns allows the model to capture industry-specific market behavior.
* **Share Price Bins:** Grouping stocks by price ranges helps the model learn patterns relevant to different price levels, mitigating potential biases towards high-priced stocks.

**Rally Time Calculation Based on Categories:** Rally times will be calculated based on the aforementioned categories (Market Capitalization, Sectors, and Share Price Bins) to ensure consistent and relevant analysis across different market segments.
## II. Model Development and Training (Continued)

### E. Advanced Model Configurations

This section details advanced configurations and experimental setups designed to enhance model performance and adaptability. These configurations focus on context awareness, transfer learning, and efficient hyperparameter optimization.

**1. Dynamic Input Sizing:**

The model will dynamically determine the optimal number of candlesticks per frame and the total number of frames used as input, adapting to varying market conditions and individual stock characteristics.  Further research will define the mechanism for determining these optimal values.

**2. Cross-Market Transfer Learning:**

We will investigate the effectiveness of transfer learning by training models on one market (e.g., US) and applying them to another (e.g., India).  Separate training and evaluation phases for US-US, US-India, and India-India market pairs will assess the generalizability of learned patterns across different market environments.

**3. Context-Aware Weighted Predictions:**

Predictions will be weighted based on multiple periodicities (daily, weekly, monthly, quarterly, and yearly).  Weights will be dynamically determined, considering factors such as input size, stock characteristics (market capitalization, sector, share price), and potentially other relevant factors.  This allows the model to adapt to various time scales and stock categories. For example, long-term trends might be weighted more heavily for large-cap stocks, while short-term fluctuations could be more influential for small-cap stocks.  Optimal configurations will be determined through experimentation.

**4. PCA-Based Feature Representation:**

The model development and training process will be replicated using a dynamic plane constructed from the first two principal components (PCA axes) derived from the input data. This explores an alternative, potentially more informative, representation of price and volume data.

**5. Baseline Hyperparameter Permutation Testing:**

A comprehensive initial hyperparameter search will be conducted by testing all permutations of pre-defined hyperparameter values for a single epoch within specific training and testing date ranges. This establishes baseline performance levels for each permutation and informs subsequent, more focused hyperparameter optimization strategies.


## UI/UX Development

This section outlines the user interface and user experience (UI/UX) components essential for interacting with the SCoVA system, managing data, and monitoring performance.

* **Data Management:**  The UI will provide comprehensive data management features, including downloading data from various sources (e.g., Yahoo Finance), preprocessing it into the required format (e.g., OHLCV, candlestick charts), and managing the stored data.

* **Training Data Management and Visualization:**  Users can upload and visualize training data through the UI. Visualization tools will enable exploration and understanding of the data through various formats like charts, tables, and summary statistics, facilitating data validation and analysis.

* **Backtesting Interface:**  A user-friendly backtesting interface will allow users to define parameters, run simulations, and visualize results, including key performance metrics (e.g., Sharpe Ratio, Jensen's Alpha) for strategy assessment.

* **Real-time Monitoring Dashboards:**  Real-time monitoring dashboards will display key performance indicators (KPIs) and relevant information about the deployed trading bot's activity, providing a clear overview of its status and performance.


## UI/UX Enhancements for Advanced Functionality

This section details UI/UX enhancements designed to streamline model development, experimentation, and analysis within the SCoVA platform.

* **Dynamic Capital Allocation Control:**  The UI will enable users to define and adjust dynamic capital allocation strategies based on model predictions, market conditions, or other factors, integrating seamlessly with the trading simulation and backtesting components.

* **Streamlined Hyperparameter Testing:**  Intuitive UI elements will facilitate rapid testing of multiple hyperparameters simultaneously. Features like grid search visualization, parameter sliders, and real-time performance feedback will optimize the tuning process.

* **Experiment Builder Interface:**  An Experiment Builder interface will empower users to define complex experiment permutations, specifying model configurations, data preprocessing steps, and evaluation metrics. This facilitates systematic exploration of the parameter space and efficient comparison of different approaches.

* **Comparative Analytics Visualization:**  Visualization tools will enable clear comparison of results across multiple experiments. Charts, graphs, and tables will display key performance indicators, highlighting trends and patterns for informed decision-making.

* **Experiment Management System (EMS):**  A robust Experiment Management System (EMS) will organize and manage experiments, tracking parameters, results, and code versions in a centralized repository.  This ensures reproducibility and facilitates collaboration.
## UI/UX Enhancements

This section details the planned UI/UX enhancements to create a more streamlined and efficient research workflow centered around experiment management, incorporating Sanskrit terminology to reflect the project's philosophical underpinnings.

### Experiment-Driven Workflow Overhaul

The UI will be completely overhauled to prioritize an Experiment Management System (EMS) tailored to the project's complex research workflow.  This EMS will feature robust support for automated permutation testing and detailed comparative analytics, enabling rapid iteration and insightful analysis of experiment results.

### Data Management (Sanskrit name pending)

A dedicated Data Management module will centralize control over all project data.  This module will include functionality for:

* **Managing Data Sources:** Adding, removing, and updating experiment data sources.
* **Asset Universe Management:** Defining and managing the sets of assets used for testing and analysis.
* **Data Status Tracking:** Monitoring data source status and availability to ensure data integrity and facilitate troubleshooting.

### Gyaan Shala (Experiment Designer)

The "Gyaan Shala" (House of Wisdom) module, embodying the principle of *Viveka* (discrimination), will allow users to design experiment templates.  A visual canvas and drag-and-drop block interface will simplify the creation and modification of complex experimental setups, enabling users to fine-tune their trading strategies by adjusting hyperparameters, enabling/disabling features, and systematically testing different configurations.

### Campaign Runner (Sanskrit name pending)

A Campaign Runner module will execute saved experiment templates.  Key functionalities include:

* **"Try All Permutations":** Automates running experiments with all possible parameter combinations.
* **Multiple Execution Modes:** Supports various execution modes to cater to different experimental needs.

### Manana & Nididhyasana Dashboard (Results & Analytics)

The "Manana & Nididhyasana" Dashboard facilitates reflection and contemplation on trading results through interactive visualizations. Key features include:

* **Pattern Sangam Visualizer:** Displays archetypal dynamic plane images associated with profitable and costly trades, enabling users to identify recurring visual patterns.
* **Vairagya Score:** Measures model performance stability by assessing the consistency of gains versus reliance on outlier trades, promoting a detached perspective on results.


## I. Project Setup and Data Acquisition

This section details the initial project setup, incorporating core architectural principles guided by the Bhagavad Gita and establishing data acquisition pipelines.

### A. Architectural Alignment with Bhagavad Gita's Four Paths to Liberation

The entire application (bot and UI) will be restructured based on the Bhagavad Gita's four paths to liberation: Gyaan (Gyaan Yoga - knowledge), Bhakt (Bhakti Yoga - devotion), Karam (Karma Yoga - action), and Raaj (Raja Yoga - meditation/control).  These philosophical principles will be deeply integrated into the program's architecture, features, and implementation. For example:

* **Gyaan (Knowledge):**  Robust data analysis, insightful visualizations, and transparent access to information within the UI will facilitate understanding of market dynamics.
* **Bhakt (Devotion):**  A commitment to rigorous testing, validation, and continuous improvement will be reflected in the UI through clear progress indicators and performance metrics.
* **Karam (Action):** Responsible trading practices, avoiding exploitation, and prioritizing long-term stability will be promoted through UI features encouraging mindful trading decisions.
* **Raaj (Control):**  Risk management features, stop-loss mechanisms, and adherence to predefined trading rules will be implemented and accessible through UI tools that promote discipline and risk management.

### B. Dharmic Mandate Implementation

Inviolable ethical rules based on Satya (truthfulness), Shaucha (purity), and Santosha (contentment) will be hard-coded into the trading bot. These principles will guide the bot's actions and prevent unethical trading practices.  Examples include:

* **Satya (Truthfulness):** Maintaining immutable logs and ensuring transaction transparency. Using rigorously cleaned and validated data.
* **Shaucha (Purity):**  Implementing robust error handling and data validation for data integrity.
* **Santosha (Contentment):** Balancing profit maximization with long-term stability and avoiding excessive risk.

### C. Data Acquisition and Preprocessing

* **Data Source: Zerodha KiteConnect API:**  The Zerodha KiteConnect API will be used for downloading historical data and acquiring live training data via a websocket connection.
* **Websocket Connection for Live Data:** A websocket connection via the Zerodha KiteConnect API will stream live market data for training.
* **Order and Portfolio Management:** The Zerodha KiteConnect API will manage order generation, execution, and portfolio tracking (placing orders, monitoring positions, and overall portfolio management).


## II. Live Trading and Monitoring (Proposed Section Title - Sanskrit name pending)


### A. Live Trading Dashboard

The Live Trading Dashboard will prioritize displaying the "Dharma Adherence Score," ... (Continue with the rest of the content regarding Kurukshetra)


### B. Pranayama Gauge


A "Pranayama Gauge" on the dashboard will visualize the dominant timeframe influencing the bot's decisions. This gauge will display the Vision Transformer's (ViT) attention weights across different timescales (intraday, daily, weekly), providing insight into the "breath" or rhythm of the bot's trading strategy.
## I. Project Setup and Data Acquisition

This section outlines the initial setup within the idx.google development environment and the process of acquiring the necessary financial data for the SCoVA project.  This structured approach addresses the project management requirement for clear documentation and lays the groundwork for subsequent development phases, ultimately targeting deployment to Firebase via VS Code.

### A. Project Initialization

This stage focuses on initial planning and preparation to ensure a well-defined project blueprint, including detailed screen specifications and the broader project structure required for review.

1. **Document Thought Process:**  Maintain a comprehensive record of the project's evolution, including design decisions, rationale, and any challenges encountered.  This documentation, crucial for clarity and future reference, will reside within the idx.google environment and contribute to the overall project blueprint.

2. **Prepare for In-Depth Discussion:**  Prepare the project blueprint (the "paper") containing detailed technical specifications for each screen, along with the idx.google-based code repository.  This ensures a structured, review-ready format.

### B. Data Acquisition and Preprocessing

This subsection details the acquisition and preparation of financial data from Yahoo Finance.  The data is processed and structured for compatibility with the model training process.

1. **Data Source:**  Financial data (OHLCV - Open, High, Low, Close, Volume) is acquired from Yahoo Finance.

2. **Data Acquisition and Preprocessing:**  Acquire OHLC and relevant index data. Adjust historical prices for splits and dividends to ensure data accuracy.

3. **Data Storage:**  Processed data is stored as CSV files within the project directory, specifically in `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** Implement a 5-day rolling window for both input features (historical data) and output labels (future returns).

5. **Return Label Calculation:** Calculate the 5-day future return using the following formula: `(Close(t+5) - Open(t+1)) / Open(t+1)`.  This represents the percentage change in price from the open of the next day to the close five days later.

6. **Candlestick Chart Generation:** Generate candlestick charts incorporating volume and a moving average (specify the period of the moving average if determined). These charts will serve as visual inputs for the model.

7. **Return Label Integration:** Integrate the calculated 5-day future return labels with the corresponding candlestick chart images.  This ensures data synchronization for training.

8. **Filename Convention:**  Implement a clear and consistent filename convention that encodes the holding period (5 days in this case) within the filenames or a separate CSV file to maintain data organization.


## II. Model Development and Training

*(Content omitted)*

## III. Testing and Evaluation

*(Content omitted)*

## IV. Deployment and Monitoring

### A. Deployment

1. **Connect Code to Experiments:** Clearly link the codebase to the corresponding experiments and results.

2. **Reproduce Models (Optional):**  Provide a guide for reproducing the models and experiments, if applicable.

3. **Focus on Specific Repository/Model Aspects (Optional):**  Focus the analysis on specific aspects of the repository or model, if requested.

### B. Monitoring

*(Content omitted)*

## V. Dissertation and Documentation

*(Content omitted)*


# UI/UX Enhancements

This section details UI/UX enhancements designed to create a more intuitive and insightful user experience, grounded in principles of clarity and trust.

* **Darshan (Main Dashboard):**  The main dashboard, renamed "Darshan" (vision or view), provides a comprehensive overview of the bot's current status and performance, serving as the primary user interface.

* **Karma Ledger:**  The trade ledger, renamed "Karma Ledger," tags each trade with a "Karma Type" (Sattvic, Rajasic, or Tamasic) reflecting the ethical alignment of the trade and prevailing market conditions. This qualitative classification promotes mindful trading practices.

* **Shraddha Gauge (Faith/Trust Gauge):**  The "Shraddha Gauge" visualizes the bot's historical reliability and performance consistency, providing a broader perspective and fostering trust in the data-driven process.

* **Sankalpa (Intention/Will) and Trust Log:** The manual override, reframed as "Sankalpa" (intention or will), is accompanied by a "Trust Log" displaying the historical impact of manual interventions. This encourages trust in the system's recommendations (Ishvara Pranidhana).

* **Swaha's Chants (System Log Stream):**  The system log stream, renamed "Swaha's Chants," provides a continuous stream of the bot's operational thoughts and actions, offering deeper insight into its decision-making process.
## I. Project Setup and Data Acquisition

This section details the setup process and data acquisition procedures for the SCoVA project. A robust and efficient data pipeline is crucial, even though model development and training form the core of this project.  The following technical considerations facilitate experimentation and effective research management.

### A. Project Initialization

This subsection outlines the initial steps for setting up the experiment infrastructure, including the UI/UX elements for designing and managing experiments and the backend services for orchestration and execution.

* **Experiment Designer Canvas UI:** A Flutter-based user interface will be built to allow users to visually design and manage experiments.

### B. Data Acquisition and Management

This subsection details how market data is acquired, validated, and managed within the SCoVA project.  A robust data pipeline is essential for reliable backtesting and live trading.

* **Data Sources:** The primary data source will be the Zerodha Kite Connect API. This API provides access to real-time and historical market data, which is essential for training, backtesting, and live trading operations.
* **Data Validation:**  A rigorous validation process will be implemented to ensure data integrity and accuracy. This includes checks for data consistency, completeness, and accuracy.  Automated alerts will be triggered if any data anomalies are detected.
* **Data Storage:** Data will be stored and managed securely using Firebase.  This provides a scalable and reliable solution for storing and retrieving large datasets.  Data backup and recovery procedures will be implemented to ensure data availability.


## II. Model Development and Training

This section details the development, training, and refinement of the models used within the SCoVA project.

### A. Model Architecture (Specialization)

*(Existing content from the original outline remains here)*

### B. Training and Validation (Enforcer)

*(Existing content from the original outline remains here)*

### C. Dynamic Plane Implementation (Specialization)

*(Existing content from the original outline remains here)*

### D. Model Enhancement and Refinement (Specialization)

*(Existing content from the original outline remains here)*

### E. Experiment Design and Execution (Experiment Designer)

This subsection focuses on structuring and running experiments for model development and backtesting using reusable templates.

* **Experiment Templates:** Reusable 'Experiment Templates' will be created, saved, and managed. These templates will define the data flow and configuration for various experiments using a visual, node-based or block-based canvas, enabling a flexible and organized approach to experimentation and streamlining the testing of different model architectures, hyperparameters, and data preprocessing techniques.


### F. Campaign Running and Backtesting (Campaign Runner)

This subsection describes the execution of automated backtesting campaigns based on defined experiment templates.

* **Campaign Runner:**  An automated system will execute large-scale backtesting campaigns based on saved experiment templates. This automated approach allows for the efficient evaluation of different strategies across various market conditions. Leveraging the experiment templates, multiple backtests can be run with different parameters and configurations, facilitating rapid exploration of the parameter space.


### G. Live Trading Dashboard (Live Trading Dashboard)

This subsection outlines the requirements for a live trading dashboard.

* **Live Trading Dashboard:** A real-time dashboard will provide an overview of live trading activities, internal state, and performance. This dashboard is crucial for monitoring the model's performance in a live trading environment and will provide insights into current positions, trading signals, portfolio performance, and relevant internal metrics, enabling effective real-time monitoring and risk management.


## VI. Technical Specifications and Design Principles

This section outlines the technical specifications and guiding design principles for the Swaha project. These principles are fundamental to the project's development and influence all aspects of its design and implementation.

### A. Technical Specifications

The Swaha project will be developed using VS Code with Gemini Code Assist Agent, with a Python backend and a Progressive Web App (PWA) frontend. Deployment and management will be handled through Firebase.  This combination of technologies provides a robust and scalable environment for development, deployment, and ongoing maintenance.

### B. Design Principles

Swaha's core design principles are rooted in the four paths of Yoga as described in the Bhagavad Gita:

* **Gyaan Yoga (The Path of Knowledge):**  This principle emphasizes data purity and rigorous experiment design, reflected in the Gyaan Shala module (detailed below). It promotes a deep understanding of the market and its influencing factors.
* **Bhakti Yoga (The Path of Devotion):** This principle guides Swaha's development as a tool dedicated to executing its coded strategy with unwavering discipline and focus, emphasizing commitment to the chosen path and adherence to established rules.
* **Karma Yoga (The Path of Action):** This principle encourages skillful action in the market, driven by a clear understanding of the system and its dynamics, promoting a proactive trading approach while maintaining detachment from outcomes.
* **Raja Yoga (The Path of Meditation):** This principle emphasizes awareness and detachment from profit or loss, promoting a calm and balanced trading approach and minimizing emotional reactions to market fluctuations.

These principles will be integrated into every aspect of the design, from the overall architecture to specific UI details.

### C. Guiding Philosophy

Swaha is designed as more than just a tool for financial gain; it is envisioned as an instrument for practicing wisdom, discipline, and detachment within the financial market. Its primary purpose is to execute its Dharma (coded strategy) skillfully and mindfully, without attachment to profit or loss. This aligns with the principles of the four paths of Yoga and fosters a mindful approach to trading.

### D. Module Definitions

* **Module 1: The Gyaan Shala (The House of Wisdom) - Data & Experiment Design:** This foundational module combines data management and experiment design, focusing on data purity and creating structured blueprints for discovering market truths.

    * **1.1 Data Management:**  Data management is paramount within the Gyaan Shala. This involves sourcing, validating, storing, and managing all market data required for training, backtesting, and live trading. The Zerodha Kite Connect API will be used for accessing and retrieving this data, ensuring the reliability and integrity of the information used by Swaha.



## Technical Documentation (SCoVA Project)

This section provides comprehensive technical documentation for the SCoVA project, focusing on exhaustive technical detail and practical implementation guidance to minimize the need for supplemental prompts. Philosophical discussions are excluded. The goal is to provide a comprehensive, self-sufficient resource and reduce reliance on external queries, thereby streamlining the implementation process and addressing concerns about increased cloud costs due to excessive prompting.  The level of detail provided aims to leave no room for ambiguity.
## I. Project Setup and Data Acquisition

### A. Project Initialization

* **Cost-Effective Infrastructure:**  Given the computationally intensive nature of image generation and processing, a cost-effective infrastructure is crucial. The initial server-side approach is unsustainable for the anticipated volume.  We will investigate offloading this workload to a more suitable environment, exploring options like offline processing and client-side computation (detailed below). This requires a thorough evaluation of available solutions and a clear implementation plan.
* **Simplified User Interface:**  A user-friendly interface for model training and retraining will be maintained within the application. This UI will abstract the underlying infrastructure complexities, streamlining the user experience.

### B. Data Acquisition and Preprocessing

* **Data Sources and Storage:**  Market data (OHLCV) will be acquired from Yahoo Finance.  Historical data will be stored in Parquet format in Google Cloud Storage, while metadata, configuration, and real-time state will reside in Google Cloud Firestore.
* **Data Preprocessing:** Acquired OHLC and index data will be preprocessed, including price adjustments. A 5-day windowing logic will be implemented for input and output data.  Return labels will be calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`. The holding period will be encoded in filenames or a separate CSV file.
* **Kite Connect API Rate Limiting:** The data acquisition process will respect the Kite Connect API's rate limit of 10 requests per second by incorporating appropriate delays (e.g., `time.sleep(0.1)` in Python) between API calls. This ensures reliable data acquisition, especially for potential real-time or near real-time requirements.
* **Offline Image Generation:** Candlestick chart image generation will be moved offline to a more cost-effective infrastructure. This mitigates the performance impact on the main application server.  Potential solutions for efficient offline generation and storage will be explored.
* **Image Optimization:**  Candlestick chart image generation will be optimized for efficiency.  Strategies include caching processed images, exploring alternative image formats, and utilizing a more lightweight image generation library.  This optimization will reduce computational overhead and storage costs.
* **Client-Side Processing (Exploration):** The feasibility of offloading image generation and potentially some model training to iPads will be investigated. This could significantly reduce server load.  Considerations include client-side memory constraints, data access, and potential use of federated learning or a hybrid approach.



## II. Federated Learning Implementation (Exploration)

This section details the potential implementation of a federated learning system using iPads for on-device image generation and model training.

**Architecture:**

The system will employ a client-server architecture, with iPads acting as clients and a central server coordinating the learning process.  iPads will perform:

* **On-Device Image Generation:** Leveraging iPad hardware for efficient image processing.
* **Local Model Training:** Utilizing TensorFlow.js and iPad GPUs for accelerated on-device training.

The server's responsibilities include:

* **Model Distribution:** Distributing the initial model and updates to participating iPads.
* **Aggregation of Updates:** Collecting and aggregating model updates from iPads to improve the global model.
* **Data Management:** Managing the overall data used in the project.

**Workflow:**

1. **Image Generation (iPad):**  The iPad generates the required images.
2. **Local Training (iPad):**  The iPad trains a local model using TensorFlow.js, the generated images, and the distributed model.
3. **Update Transmission (iPad):**  The iPad sends model updates to the server.
4. **Aggregation (Server):**  The server aggregates updates from multiple iPads to refine the global model.
## Frontend and Backend Implementation Details

This section details the frontend (iPad) and backend (server) implementation of the SCoVA project. This distributed architecture enables client-side model training and efficient server-side orchestration.

**Frontend (iPad):**

The iPad is responsible for fetching raw market data, generating candlestick chart images, training the model locally, and sending model updates to the server. This significantly offloads the computational burden from the server.

* **Technologies:**
    * **TensorFlow.js:** Enables on-device model training, facilitating efficient local training of the CNN and potentially other models like ViT.
    * **Canvas API/JS Graphics Library:**  Used for consistent candlestick chart image generation from raw market data across devices.
    * **Web Workers:**  Handles background processing of data fetching, image generation, and model training, ensuring a smooth, responsive user experience.


**Backend (Server):**

The server stores and serves raw market data, maintains the global master model, and coordinates the training process across multiple iPads. It sends instructions and the current global model to connected iPads and aggregates received model updates.

* **Functionality:**
    * **API Endpoints:**  Facilitate seamless communication by serving data, providing the global model, and receiving model updates.
    * **Federated Averaging:** Combines model updates from the iPads to update the global master model, enabling collaborative training while preserving data privacy.


**UI/UX Adaptations:**

The Campaign Runner and Experiment Designer UI will be updated to reflect the client-side processing workflow.

* **Enhancements:**
    * **Status Indicators:** Provide real-time feedback on data downloading, image generation, local model training, and model uploading.
    * **Resource Monitoring:** Displays CPU usage, memory consumption, and other relevant metrics, allowing users to understand the training process's impact on iPad resources.


## PWA Suitability and Alternatives for GPU-Intensive Tasks

This section analyzes the feasibility of using a Progressive Web App (PWA) for the computationally demanding task of Vision Transformer training, given its GPU-intensive nature.

**PWA Suitability Investigation:**

A key architectural decision is the suitability of PWAs for GPU-intensive tasks, especially on iPads.  The investigation will focus on:

* **General PWA Suitability for GPU Tasks:**  Researching potential drawbacks regarding browser stability and resource management when using PWAs for computationally demanding GPU operations.
* **PWA Crash Probability under GPU Load:**  Determining the likelihood of browser crashes on iPads given the expected GPU load during Vision Transformer training.
* **Resource Limitations in PWAs:**  Analyzing the limitations, stability, and performance concerns of running resource-intensive Vision Transformer training within a PWA, specifically focusing on long-term operation on an iPad.
* **Technical Challenges of GPU-Intensive Training in PWAs:** Researching the capabilities of TensorFlow.js and WebGPU for training large models within a browser to determine architectural feasibility.

**Alternative Solutions:**

Given the potential challenges, exploring alternatives to PWAs for GPU-intensive tasks is crucial.

* **Alternatives if PWAs are Unsuitable:** If PWAs prove unsuitable, we will evaluate alternatives such as native application development and cloud-based solutions to ensure the required performance and stability.


## PWA Considerations

This section addresses the feasibility and performance concerns of deploying SCoVA as a PWA, particularly on iPadOS.

**Feasibility and Architecture:**

* **PWA Project Feasibility:**  A thorough feasibility analysis is crucial, considering the resource-intensive nature of Vision Transformer training on an iPad.  Resource limitations, cross-platform inconsistencies, and potential instability require careful evaluation, and optimization methods or alternative approaches will be explored.  This includes investigating the stability of Vision Transformer training within the PWA environment.
* **Hybrid Training Architecture:** To mitigate iPad resource constraints, a hybrid approach is proposed. Initial resource-intensive model training will be offloaded to a server, while the iPad handles fine-tuning with smaller batches using Web Workers to reduce browser crashes and address memory limitations.

**Performance and Stability:**

* **Validating PWA Performance Concerns:** User concerns regarding GPU-intensive tasks within the PWA on iPadOS will be validated through testing and analysis.  This will involve gathering…
## I. Project Setup and Data Acquisition

This section details the initial steps for setting up the SCoVA project and acquiring the necessary data within the context of a native iOS Swift application.  The transition to a native application necessitates careful consideration of data handling, processing, and storage within the iOS environment.  This decision, documented in (Chat2.json message #32), was made to address limitations encountered with PWAs, specifically concerning memory limits, long-running task stability, and GPU context access. We anticipate improvements in performance, stability, and hardware access for computationally intensive tasks like image generation and model training with this native approach.


### A. Project Initialization

1. **Documentation:**  The rationale behind the shift from a PWA to a native iOS Swift frontend is documented, including the limitations encountered with PWAs (memory constraints, stability issues with long-running tasks, and GPU access challenges) and the expected benefits of using a native iOS application. This documentation will be continuously updated to reflect project progress and evolving considerations.
2. **Preparation for In-Depth Discussion:**  All documentation and repository changes reflect the transition to a native iOS Swift app using SwiftUI.  Materials explaining this architectural shift (Chat2.json message #32) have been prepared, outlining the reasons for choosing this approach and detailing its impact on project architecture (Chat2.json message #31).  These materials will facilitate a thorough discussion of the new architecture and its compatibility with project requirements.

### B. Data Acquisition and Preprocessing

The shift to a native iOS Swift frontend requires evaluating the feasibility of performing computationally intensive tasks client-side (Chat2.json message #32), such as image generation and model training.  This evaluation will consider memory limits, long-running task stability, and GPU context access within the iOS environment.

1. **Data Source:** Yahoo Finance will continue to be the data source for OHLCV data. However, the specific methods for acquiring and handling this data within the iOS Swift environment require careful consideration.
2. **Data Preprocessing:** Data acquisition and preprocessing steps (acquiring OHLC and index data, adjusting prices) will be evaluated for feasibility within the iOS Swift environment.  This evaluation must account for the client-side image generation requirement (Chat2.json message #32), leveraging the device's GPU for the DynamicPlaneGenerator component's PCA calculations, rotations, and rendering of numerical data into image data or tensors. Compatibility between data preprocessing and client-side image generation requirements will be ensured.
3. **Data Storage:** The data storage location (previously CSV files in `stock_data/train/` and `stock_data/test/`) will be reassessed, considering how data will be accessed and managed within the native iOS app context.
4. **5-Day Windowing:** The implementation of the 5-day windowing logic for input and output will be maintained but adapted for compatibility with the iOS Swift environment.
5. **Return Label Calculation:** The formula for calculating the return label, `(Close(t+h) - Open(t+1)) / Open(t+1)`, remains unchanged. However, its implementation will be adapted for the iOS Swift environment.
```markdown
## I. Project Setup and Data Acquisition

This section details the initial steps for setting up the iOS project, focusing on client-side model training and data management using Apple technologies and frameworks.

### A. Project Initialization

This section outlines the initial project setup for iOS development.

1. **Document Thought Process:** Continuously document the rationale and decision-making process for choosing specific technologies and approaches for client-side model training, especially regarding Core ML, Metal, and local data storage solutions.

2. **Prepare for In-Depth Discussion:** Prepare documentation and code repositories specifically addressing the client-side implementation. This will facilitate focused discussion and review of the iOS-specific elements.


### B. Data Acquisition and Preprocessing

This section outlines client-side data handling, including local storage and preprocessing for Core ML model training.

1. **Data Source:** Yahoo Finance will be the primary data source.  Implement efficient data transfer and synchronization between the server and the client application.

2. **Data Preprocessing:** Client-side preprocessing should be optimized for mobile performance, leveraging hardware acceleration where possible. This includes acquiring OHLC and index data, and adjusting prices.

3. **Data Storage:** While CSV files might be used initially, transition to Core Data or Realm for optimized local storage on the iOS device. Document the chosen database and the rationale behind the selection.

4. **5-Day Windowing:** Implement 5-day windowing logic in Swift, ensuring compatibility with Core ML model input requirements.

5. **Return Label Calculation:** Implement the return calculation `(Close(t+h) - Open(t+1)) / Open(t+1)` efficiently in Swift.

6. **Candlestick Chart Generation:** Implement client-side candlestick chart generation with volume and moving averages. Leverage the Metal framework for GPU-accelerated rendering and optimization.

7. **5-Day Future Return Calculation:** Integrate the 5-day future return calculation with the client-side data preprocessing pipeline.

8. **Integration of Return Labels and Charts:** Ensure smooth data flow between chart generation and return label calculation within the Swift implementation.

9. **Filename Convention:** Maintain consistent filename conventions across both server-side and client-side data storage.


## Backend Architecture

The backend for the SCoVA project is a minimal orchestrator implemented in Python.  It manages data, model versions, and API connections, while offloading computationally intensive tasks to the iOS app.  This architecture leverages the client's processing power for greater efficiency.  The backend acts as a "traffic controller," directing information and resources between system components.

Its primary responsibilities are:

* **Data Serving:** Manages the master database of raw numerical data (OHLCV, index data, etc.) and serves it to the iOS app.
* **Authentication and API Key Management:** Handles secure connection and authentication with external APIs, specifically Zerodha's Kite Connect API, including managing API keys.
* **Model Versioning:** Stores and serves different versions of trained models.
* **Aggregation of Model Updates:** Aggregates model updates from client-side training.

This architecture shifts the workload to the client, which now handles on-device image generation, model training, backtesting, and live inference. The backend supports these client-side operations.


## Backend and Client Responsibilities

This section outlines the responsibilities of the backend (Python) and client (native iOS app) components.

### Backend Responsibilities

The Python backend is the central hub for model management, experiment orchestration, and data aggregation. Its primary responsibilities include:

* **Model Hub:** Stores and serves the master versions of trained Core ML models.
* **Orchestration & Aggregation:** Receives and applies lightweight model updates from the iOS app to the global model.
* **Experiment Management:** Manages experiment templates and high-level results from research campaigns.

### Client Responsibilities

The iOS app handles computationally intensive tasks, leveraging on-device processing power, particularly GPU utilization:

* **Heavy Lifting:** Performs on-device image generation, model training/fine-tuning using Core ML, backtesting, and live inference.

### Future Frontend Portability

After model fine-tuning and the experimental setup, the remaining functionality will be ported to other frontend platforms (web and Android), assuming limited image processing requirements for daily predictions and occasional re-tuning.


## IV. Deployment and Monitoring

### A. Deployment

This section outlines the deployment process…
````

## I. Project Setup and Data Acquisition

### A. Project Initialization

This section outlines the initial project setup, including technology choices and architectural decisions. A key decision point is the selection of Flutter for frontend development, requiring a thorough feasibility assessment.

**Frontend Technology:** We will investigate using Flutter for frontend development, considering its potential advantages in cross-platform development and performance. The following factors are crucial for evaluation:

- **Flutter/Python Integration:** A critical aspect is seamless integration between the Flutter frontend and the existing Python backend. This involves researching and documenting suitable data transfer mechanisms (e.g., REST APIs, WebSockets), authentication protocols (e.g., OAuth, JWT), and API design best practices.
- **Native Device Access:** The application requires access to specific device resources (e.g., camera, storage). We must verify that Flutter can effectively access these resources and manage the necessary permissions.
- **Performance Considerations:** Given the computationally intensive nature of the application, particularly client-side processing, Flutter's performance is a primary concern. A thorough performance evaluation is required.
- **Replacing Swift:** We will assess the feasibility of replacing the existing Swift frontend with Flutter. This requires a detailed comparison of Flutter's capabilities against the current Swift implementation, particularly concerning:
  - **On-Device Machine Learning:** Evaluate TensorFlow Lite integration within Flutter for on-device ML and compare its performance to the existing Swift implementation.
  - **Dynamic Plane Generation:** Assess Flutter's graphics capabilities, specifically its suitability for implementing the `DynamicPlaneGenerator`, including custom GPU graphics rendering and compare its performance to Swift.
  - **On-Device Training:** Determine Flutter's support for and performance of on-device training.

**Backend Technology:** We confirm the continued use of Python for the backend, leveraging its existing infrastructure and libraries.

## UI Implementation

This section details the implementation of the user interface, exploring the chosen UI framework and demonstrating how UI/UX principles are applied. The original checklist focused on Flutter; this version considers a broader approach and uses Markdown for illustrative purposes, particularly relevant for documentation and static site generation. This example showcases how UI/UX principles apply even in non-traditional UI contexts.

### Markdown as a UI Analog

Markdown, like declarative UI frameworks such as Flutter, describes _what_ the content should be, not _how_ to render it. This declarative approach simplifies UI development. The following analogies illustrate how Markdown mirrors Flutter's core UI concepts:

- **Declarative Structure:** Markdown elements (headers, lists, tables) define the structure, analogous to Flutter's widgets. The rendering engine handles the display, similar to Flutter's rendering pipeline.
- **Styling and Theming:** While Flutter uses pre-built design systems (Material Design, Cupertino), Markdown can be styled with CSS frameworks (e.g., Bootstrap, Tailwind) or custom CSS. This allows for consistent theming and layout, mirroring Flutter's styling mechanisms.
- **Custom Components:** For complex UI elements, Markdown can incorporate raw HTML, providing flexibility similar to custom Flutter widgets.

### Applying UI/UX Principles to Documentation

Using Markdown for SCoVA project documentation demonstrates how these principles translate:

- **Structure and Navigation (analogous to Flutter UI implementation):** Markdown headers, lists, and tables create a clear and navigable structure, mirroring a well-designed application UI.
- **Rendering Consistency (analogous to assessing Flutter's graphics capabilities):** Ensuring consistent rendering across different environments (e.g., GitHub, static site generators) is crucial, similar to evaluating Flutter's graphics performance.
- **Documenting ML Integration (analogous to investigating Flutter's ML capabilities):** Markdown documents the ML aspects (models, training, integration), mirroring how the Flutter UI would interact with the ML components.
- **Comparing Documentation Formats (analogous to comparing Flutter with Swift):** Evaluating Markdown against other formats (e.g., reStructuredText, LaTeX) is comparable to comparing UI frameworks.

This approach treats documentation as a user interface, ensuring it is well-structured, easy to navigate, and effectively communicates project details, similar to a user-friendly application.

## ML Integration and Data Management

This section details the implementation of core machine learning components and data management strategies within the application. It also addresses potential technical challenges related to TensorFlow Lite integration.

### ML Integration and Optimization

- **TensorFlow Lite Integration:** Integrate `tflite_flutter` for on-device inference on iOS and Android. This involves converting the trained model to the `.tflite` format for optimized mobile deployment and enabling real-time predictions.
- **Cross-Platform Considerations:** Explore a hybrid approach using TensorFlow Lite for broad compatibility and Core ML on iOS for potential performance gains. Investigate conditional compilation or runtime checks to dynamically select the appropriate inference engine.
- **Bug Mitigation:** Document and investigate any TensorFlow Lite integration bugs, including gathering logs, reproducing issues, and reporting them to the TFLite maintainers.

### Data Handling and Image Generation

- **Dynamic Plane Generation:** Implement `DynamicPlaneGenerator` in Dart using libraries like `ml_linalg` for normalization, PCA, and rotation. Integrate Flutter's `CustomPainter` for rendering the generated dynamic plane images within the application.
- **Local Storage:** Utilize local storage (e.g., SQLite, Hive) to manage and store raw market data on the device for faster access and offline functionality.

## Model Development and Training

### Model Architecture

This section details the model architecture, including platform-specific implementations and the communication layer between the application and the native machine learning code.

- **Platform-Specific Implementations:** The project uses Core ML on iOS for performance and TensorFlow Lite on other platforms for broader compatibility.
- **Interoperability:** Shared Dart code maintains a consistent UI and business logic. Platform Channels facilitate communication between Dart and native implementations (Swift for iOS, Kotlin/Java for Android) for invoking ML models and retrieving predictions.
- **Model Management:** The `.mlmodel` file (Core ML) is integrated into the Xcode project. TensorFlow Lite models are managed according to platform best practices.
- **Architectural Decision Logic:** The following pseudocode outlines the platform-specific model execution:

```
function predict(imageData) {
  if (platform == iOS) {
    // Use Core ML
    result = CoreMLHandler.predict(imageData);
  } else {
    // Use TensorFlow Lite
    result = TensorFlowLiteHandler.predict(imageData);
  }
  return result;
}
```

## Mobile Application Implementation

This section details the implementation of the mobile application components, addressing model management and portability.

The mobile application will follow a cross-platform architecture using Flutter and platform-specific code for model interaction. Dart will handle the UI, business logic, and data management, ensuring a consistent user experience across platforms. Platform channels will facilitate communication between the Dart code and native code for model execution.

- **Android (TensorFlow Lite):** A Kotlin class (`TFLiteHandler.kt`) within the Android project will preprocess image data, interact with the TensorFlow Lite model (stored in the assets directory), and return predictions.

- **iOS (Core ML):** A Swift class (`CoreMLHandler.swift`) within the iOS project will perform the same function as the Android handler, but using Core ML and interacting with the `.mlmodel` file.

- **Cross-Platform (Dart):**
  - The `DynamicPlaneGenerator`, along with all UI elements, data management, and business logic, will reside in the shared Dart codebase.
  - A `MethodChannel` will bridge the Dart code with the native platform code, enabling the Dart application to call platform-specific methods (e.g., `runPrediction`) for interacting with both TensorFlow Lite and Core ML.

## Model Management and Portability

This section outlines the strategy for managing and ensuring the portability of machine learning models.

- **Model Management:** A defined strategy will govern how the `.mlmodel` (iOS) and `.tflite` (Android) files are stored, updated, and accessed within the mobile application. This strategy will address version control, model updates, and potential on-device training or transfer learning.

- **Model Portability:** To address potential Core ML portability limitations, the following strategies will be explored:

  - **Model Conversion:** Investigate tools and techniques to convert the `.mlmodel` format to a more portable format like ONNX (Open Neural Network Exchange) for use with other machine learning frameworks.

  - **Cross-Platform Training Framework:** Evaluate using a framework like TensorFlow or PyTorch, which supports both iOS and Android natively, eliminating the need for model conversion and simplifying deployment while maintaining consistency.

## Model Architecture

This section details the model architecture, emphasizing a single, universal source while enabling cross-platform deployment and on-device training.

- **Universal Source Model (Python):** A framework-agnostic model (using TensorFlow or PyTorch) will serve as the single source of truth for the model's weights and architecture, ensuring consistency across deployments.

- **Conversion and Deployment:** A robust pipeline will convert the Universal Source Model into platform-specific formats: `.mlmodel` for iOS (Core ML), `.tflite` for Android, and potentially TensorFlow.js for web.

- **Training Framework Selection:** The project will evaluate the trade-offs between training exclusively with Core ML on Apple devices versus using a cross-platform framework, considering performance, development effort, and maintenance.

- **On-Device Training:** On-device training and fine-tuning will be supported using the platform-specific model formats (e.g., `.mlmodel` with Core ML on iOS).

- **Synchronization:** A mechanism will synchronize updated model weights (or weight deltas) between client devices and the central Python backend. After on-device training, clients will send updates to the server for potential aggregation and incorporation into the Universal Source Model, enabling continuous learning.

## Backend and Security Considerations

This section details the backend infrastructure and security measures.

- **Backend (Python):** A Python backend will manage the Universal Source Model, handle model conversion to platform-specific formats, and facilitate synchronization with client devices.

- **Model Update and Distribution:** A defined process will update the Universal Source Model on the backend, incorporating client updates, and automatically re-convert the model to platform-specific formats for distribution.

- **Security (Single User):** Focusing on single-user security, the following measures will be implemented:

  - **App and Backend Security:** Appropriate security measures will be implemented for both the application and backend, considering potential threats in a single-user environment.

  - **Single-User Authentication:** A secure authentication mechanism (e.g., API keys, local authentication) will verify the user's identity without the complexity of a full public login system.

## Security Implementation

This section further details the security implementation for the SCoVA project, focusing on device-bound authentication.

- **Device-Bound Authentication:** Robust device-bound authentication will be implemented using iOS/Flutter mechanisms. This will further enhance security by tying authentication to the specific device.

```markdown
## Security Considerations

This section outlines the security measures implemented for both the backend infrastructure and the client-side application to protect sensitive user data and maintain the integrity of the project.

### Backend Security

- **Serverless Infrastructure and IAM:** The backend utilizes serverless technologies like Cloud Run and Cloud Functions for enhanced scalability and reduced operational overhead. The principle of least privilege is enforced through Identity and Access Management (IAM) roles, limiting each microservice's access to only necessary resources.

- **API Gateway Protection:** All backend services are accessed through Google Cloud API Gateway, providing a central point for API key validation, ensuring only authorized clients can interact with the backend. API Gateway also enables logging of API activity for auditing and monitoring, and facilitates rate limiting to mitigate denial-of-service attacks.

- **Secure API Access with JWT (ID Token):** Backend APIs are secured using JWT (ID Token) validation with the Firebase Admin SDK in the Python backend. Multiple layers of security are employed, including rate limiting to prevent abuse, input validation to sanitize incoming data, and secure communication protocols (HTTPS) to encrypt data in transit. Access restrictions are enforced by verifying the JWT, allowing access only to authorized Google Cloud Run/Functions.

- **Secure Data at Rest and In Transit:** Data security is critical. HTTPS encrypts all data in transit. Firebase's default encryption at rest provides a foundational layer of protection. Additional Firebase security rules and IAM policies are implemented to further enhance data protection.

- **User API Request Verification and Authorization:** Authorization is enforced by verifying each user API request. The backend validates JWTs provided by Firebase Auth, utilizing the Firebase Admin SDK. The user's Firebase UID is securely stored as an environment variable, accessible only to authorized services, ensuring proper access control.

### Client-Side Security

- **Secure Authentication:** Firebase Authentication with OAuth providers (Apple and Google) is implemented for user login. The Refresh Token is securely stored in the device's hardware-backed keystore to protect user credentials. This is the primary authentication method for accessing the system.

- **Secure Token Storage and Backend Verification:** Secure token storage within the client application is paramount. Robust backend verification of these tokens is implemented using the Firebase Admin SDK.

- **Certificate Pinning:** Certificate pinning is implemented in the native mobile application (Flutter/Dart and Swift) to prevent man-in-the-middle (MITM) attacks, ensuring the app only communicates with the legitimate server.

- **Code Obfuscation and Runtime Checks:** Dart code (for the Flutter application) is obfuscated to make reverse engineering more difficult. Runtime checks are implemented in both Flutter (Dart) and Swift to detect tampering or jailbroken devices.

- **Biometric Authentication:** The app requires Face ID or Touch ID authentication to open, adding an additional layer of security.

## Remote Wipe Functionality

This section details the design and implementation of a remote wipe protocol for the application to address critical security scenarios. The process involves a secure trigger mechanism, backend actions, and client-side responses.

### Implementation Details

- **Activation Mechanism:** A dedicated Progressive Web App (PWA) serves as the secure access point for triggering the remote wipe.

- **Secure Trigger (PWA):** The PWA, hosted on Firebase, implements robust security measures, including multi-factor authentication utilizing phone number OTP and a duress passphrase.

- **Data Wiped During Remote Operation:** A remote wipe clears the following: _[Specify the exact application data and cached content to be cleared. Examples: User authentication tokens, cached market data, user preferences, trading history.]_

- **Remote Wipe Triggers:** The remote wipe can be triggered under the following circumstances:
  - Device theft (locked or unlocked state).
  - Compromised Gmail security (indicating potential unauthorized account access).
```

## Security and Recovery Procedures

This section details the implementation of security measures and recovery procedures, focusing on a remote wipe and lockdown functionality in case of a security breach.

### Emergency Lockdown Procedure

The application will implement an emergency lockdown procedure triggered by a "kill switch" flag. This procedure permanently disables the application, preventing unauthorized access even if an attacker manages to re-authenticate.

- **Kill Switch Flag (`isLockedOut`):** A boolean flag named `isLockedOut` will be stored in a protected Firestore document at `/app_config/security`. This flag serves as the kill switch. When set to `true`, it triggers the application lockdown.

- **Firestore Listener for `isLockedOut` Flag:** The client application will implement a real-time Firestore listener that continuously monitors the `/app_config/security` document for changes to the `isLockedOut` flag. If the flag is set to `true`, the listener immediately initiates the wipe and lockdown sequence.

- **UI for Triggering Lockdown:** A dedicated Progressive Web App (PWA) provides a user interface for manually triggering the emergency lockdown. The UI will be stark and minimalist, featuring a single, large, colorblind-safe button labeled "INITIATE EMERGENCY LOCKDOWN" to minimize the risk of accidental activation while ensuring accessibility.

- **Backend Execution Logic (Cloud Function `initiateEmergencyProtocol`):** Upon activation of the remote wipe trigger within the PWA, a Google Cloud Function named `initiateEmergencyProtocol` handles the backend logic. This function:

  1. Revokes the Zerodha Kite Connect Access Token, preventing further access to the user's trading account.
  2. Revokes Firebase Authentication Refresh Tokens, invalidating the user's current session and preventing re-authentication.
  3. Sets the persistent `isLockedOut` flag to `true` in the Firestore document at `/app_config/security`.

- **Client-Side Lockdown Sequence:** Upon detecting the `isLockedOut` flag set to `true`, the Swaha application performs the following actions:
  1. Forces Logout: The user is immediately logged out.
  2. Wipes Local Database: Any sensitive data stored locally is securely erased.
  3. Clears Caches: Cached data is cleared to remove any residual information.
  4. Enters Inert Mode: The application enters an inert state, displaying a message indicating it has been disabled and preventing further interaction.

### Recovery Process

Restoring application functionality after an emergency lockdown requires manual intervention and re-authentication:

1. **Reset `isLockedOut` Flag:** The `isLockedOut` flag in the `/app_config/security` Firestore document must be manually reset to `false` using the designated recovery process described below.

2. **Re-authenticate:** After the `isLockedOut` flag is reset, the user must re-authenticate with both Firebase and Zerodha to regain access.

## Restoration Protocol

This section details the restoration protocol for recovering from a service lockdown, prioritizing security and ease of recovery.

### Secure Recovery Endpoint

A dedicated Progressive Web App (PWA) hosted at a separate Firebase Hosting site (e.g., 'Swaha-recovery.web.app') serves as the Secure Recovery Endpoint. This PWA provides a secure interface for initiating the restoration process, featuring a "DEACTIVATE LOCKDOWN & RESTORE SERVICE" button. Access to this endpoint is secured via multi-factor authentication (MFA) independent of the user's primary Google account, including Firebase Phone Number Authentication (OTP) and a pre-set recovery passphrase.

### Restoration Cloud Function (`resetEmergencyProtocol`)

The `resetEmergencyProtocol` Cloud Function handles the backend logic triggered by the Secure Recovery Endpoint. Upon activation, this function:

1. **Resets Lockdown Flag:** Sets the `isLockedOut` flag in the `/app_config/security` Firestore document to `false`.
2. **Logs Restoration Event:** Logs the restoration event in the `security_events` collection, including the originating IP address and a timestamp.
3. **Confirmation Notification (Optional):** Optionally sends a confirmation notification via email or SMS.

## Security Considerations for Lockdown and Recovery

Robust security measures are essential for the lockdown and recovery procedures.

### Recovery Process Security

The system restoration process, initiated by the `resetEmergencyProtocol` function, requires independent MFA distinct from the user's primary Google account:

- **One-Time Passwords (OTP):** Sent to a pre-registered phone number.
- **Recovery Passphrase:** A pre-set passphrase.

### Logging Restoration Events

Each successful restoration logs an immutable entry in the `security_events` collection, including the originating IP address and a timestamp.

### Optional Restoration Confirmation

The system can optionally send an email or SMS confirmation upon successful restoration.

### PWA Security

The lockdown and recovery PWAs require robust MFA, implemented through a combination of factors, including device verification against a registered recovery device. Further investigation into alternative authentication methods is necessary, given user concerns regarding the usability of the current secret phrase approach.

## Security Enhancements for Swaha PWAs

This section details the planned security enhancements for Swaha Progressive Web Apps (PWAs), focusing on multi-factor authentication (MFA) using Time-based One-Time Passwords (TOTP) and physical security keys. These measures aim to secure PWA access, especially from untrusted environments, by adopting a zero-trust security model.

### Secure PWA Access from Untrusted Environments

The current security model for locking down and restoring PWAs relies on device ownership, which is not always feasible. To address this, we are implementing a zero-trust model, eliminating the reliance on inherent device trust. This approach enhances security when accessing PWAs from web terminals on untrusted devices.

### Multi-Factor Authentication (MFA) Implementation

MFA using TOTP and a physical security key (FIDO2/WebAuthn) will be implemented for PWA access. The authentication flow will be as follows:

1. **Initial Authentication:** Users authenticate with their existing Google/Apple account.
2. **TOTP Verification:** Users enter a TOTP code generated by their authenticator app (e.g., Google Authenticator, Microsoft Authenticator, Authy).
3. **Physical Security Key Authentication:** Users utilize their registered physical security key for final authentication.

This three-step approach mitigates risks from keyloggers, phishing attempts, and compromised devices.

### Technical Implementation Details

- **TOTP Integration:** During initial setup from a trusted device, users will link their Swaha account to their TOTP app by scanning a QR code. This app will then generate rotating 6-digit codes every 30 seconds, serving as the second authentication factor.

- **Physical Security Key Integration (FIDO2/WebAuthn):** Users will register their FIDO2/WebAuthn compliant physical security key (e.g., YubiKey) with the Swaha system during initial setup. This key will provide strong cryptographic challenge-response authentication as the third authentication factor.

### UI/UX Changes

The PWA user interface will be updated to seamlessly integrate the new MFA flow, prompting users for their TOTP code and guiding them through the physical security key authentication process. The UI will prioritize intuitiveness and user-friendliness for a smooth authentication experience.

## Offline Access and Social Recovery

This section details provisions for offline access and account recovery for the Swaha PWAs.

### Offline TOTP Support

To address scenarios where users may be offline, the following solutions for TOTP code generation will be implemented:

- **Backup Codes:** A set of one-time use backup codes will be generated during initial TOTP setup.
- **Recovery Mechanisms:** A secure recovery mechanism will be developed, potentially involving email verification or another trusted channel.
- **Exploration of Alternative Offline Authentication Methods:** We will continue to explore alternative offline authentication methods that balance security and usability.

### Social Recovery with Trusted Guardians

A social recovery system using trusted guardians will be implemented for both lockdown and restoration PWAs. This system functions as follows:

- **Guardian Registration:** Users pre-register trusted individuals as "Guardians."
- **Lockdown/Recovery Initiation:** Initiating lockdown or recovery requires approval from a quorum of registered guardians. This bypasses the need for device access or complex memorization, simplifying recovery during critical situations.

### Guardian Registration Process

- **Email-Based Registration:** Users enter the email addresses of their chosen guardians.
- **Secure Authorization Link:** Guardians receive a secure, time-limited authorization link via email. Clicking this link registers them in the backend system _without_ granting access to the user's account. Their permission is limited to approving or denying emergency lockdown/recovery requests.

### Quorum Configuration

Users can define the number of guardian approvals required to initiate a lockdown or recovery action (e.g., "3 out of 5 guardians"). This provides granular control over the recovery process.

## Project Setup and Data Acquisition

This section outlines the initial steps for project setup and data acquisition, establishing a foundation for subsequent model development and testing.

### Project Initialization

- **Document Thought Process (Continuous Updates):** Maintain a comprehensive, continuously updated record of the project's evolution, including design decisions, challenges, and potential solutions.
- **Prepare for In-depth Discussion:** Compile all relevant materials, including research papers, code repositories, and design documents, to facilitate productive discussions.

### Data Acquisition and Preprocessing

- **Data Source:** Yahoo Finance will be used to acquire OHLCV (Open, High, Low, Close, Volume) data.

_(The document chunk ended abruptly here, so this section remains incomplete.)_

## Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data used for training and testing the CNN model. This includes acquiring OHLCV (Open, High, Low, Close, Volume) and index data, adjusting historical prices, and storing the data for easy access.

1. **Data Acquisition:** Acquire the necessary OHLCV data for the selected financial instruments along with relevant index data.

2. **Price Adjustment:** Adjust historical prices for splits and dividends to ensure data accuracy and consistency.

3. **Data Storage:** Store the acquired and preprocessed data in CSV format within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

## Data Preparation and Feature Engineering

This section outlines the steps involved in preparing the acquired data for the CNN model, including windowing, label calculation, and candlestick chart generation.

1. **5-Day Windowing:** Implement a 5-day sliding window approach to create sequential data segments, each containing five consecutive days of OHLCV data for model input.

2. **Return Label Calculation:** Calculate the return label using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` represents the current time step and `h` represents the holding period. This label represents the percentage return over the holding period. For this project, calculate the 5-day future return (h=5).

3. **Candlestick Chart Generation:** Generate candlestick chart images from the 5-day windowed data, incorporating volume information and a moving average indicator. These images will serve as input for the CNN model.

4. **Data Synchronization:** Integrate the return label calculation with the candlestick chart image generation to ensure synchronized data and image creation, streamlining the data preparation pipeline.

5. **Filename Convention:** Implement a consistent filename convention that encodes the holding period within the filenames or utilizes a separate CSV file to store this information. This ensures easy identification and retrieval of data for specific holding periods.

## Minimum Password Length for Uncrackability

This section defines the requirements for password security, focusing on determining a minimum alphanumeric password length considered "uncrackable" against brute-force attacks, considering current and near-future computational capabilities.

**Definition of "Uncrackable":** "Uncrackable" in this context refers to a password requiring a cracking time exceeding practical limits (e.g., decades or centuries), considering the computational resources available to an attacker (CPUs, GPUs, specialized hardware) and aligning with industry-standard recommendations (e.g., NIST).

**Analysis:**

The following analyses will determine the minimum alphanumeric password length:

- **Brute-Force Cracking Speeds:** Research current brute-force cracking speeds using data on high-end hardware to establish a baseline for estimating cracking times.
- **NIST Guidelines:** Review NIST guidelines on password length and complexity to ensure alignment with best practices.
- **Password Length vs. Cracking Time:** Analyze the relationship between password length, the alphanumeric character set, and cracking time, considering current brute-force capabilities and strong hashing algorithms (bcrypt, Argon2). This analysis aims to establish a minimum length where cracking becomes prohibitively time-consuming.

## Password Management and Hardware Wallet Integration

This section details secure password management for the lockdown and restoration PWAs and the integration of hardware wallets, prioritizing self-sovereignty given the user's rejection of social recovery methods.

### PWA Password Authentication

Password authentication for the PWAs (hosted on Google Cloud and managed by Firebase Authentication) will implement standard practices: secure storage, password complexity requirements, and account recovery mechanisms (excluding social recovery). A practical minimum password length recommendation, informed by security best practices and cracking time research, will be provided, supported by a table illustrating cracking times for various password lengths, character sets, and hardware.

### Decentralized Recovery with Hardware Wallet

A decentralized recovery system using hardware wallets will enhance security and self-sovereignty. The process involves:

1. **One-Time Setup:** The user links their account to a hardware wallet-generated public address, stored as the sole authorization for emergency protocols. The private key remains secure within the hardware wallet.

2. **Untrusted Terminal Access:** When accessing the PWA from an untrusted device, the user connects their hardware wallet. The PWA displays a challenge on the wallet, which the user signs. The backend verifies the signature, granting access if valid.

**Advantages:**

- **Compromised Device Immunity:** Private keys remain secure even on compromised devices.
- **Self-Sovereignty:** Users retain complete account control.
- **Phishing Resistance:** Signing challenges displayed on the hardware wallet mitigates phishing.
- **No Passphrases:** Eliminates the risk of lost access due to forgotten passphrases.

### Hardware Wallet Integration Details

Integration requires:

- **Communication Protocols:** Implementing WalletConnect, WebHID, or WebUSB for PWA-hardware wallet interaction.
- **Challenge/Signature Handling:** Secure challenge generation and robust signature verification on the backend.
- **User Interface:** Incorporating UI elements like a "Connect Wallet" button.

## Implement Emergency Lockdown/Recovery via Disguised Terminal

This section details the implementation of a disguised web terminal for triggering emergency lockdown and recovery procedures. The terminal will appear as an innocuous webpage, such as a contact form for a fictitious business (e.g., a juicery), masking its true function. Lockdown/recovery protocols are activated by entering a pre-defined trusted phrase into a designated field within this disguised interface.

### User Interface and User Experience

The user interface will be meticulously crafted to resemble a legitimate website, such as a juicery's contact form. This disguise must be convincing to avoid suspicion while remaining functional. Key considerations include:

- **Visual Design:** The chosen disguise (e.g., juicery contact form) will inform the overall visual design, including layout, color scheme, and branding. The goal is a believable replica of a common web form.
- **Trusted Phrase Input Field:** A specific text field within the form will be designated for the trusted phrase. Its placement and labeling should integrate seamlessly with the overall design.
- **User Feedback:** Upon entering the trusted phrase and submitting the form, the system will provide feedback, either visually or through a subtle change in page content, indicating successful activation.

### Secure Handling of the Trusted Phrase

The trusted phrase's sensitivity and critical function necessitate robust security measures:

- **Secure Transmission:** The trusted phrase will be transmitted securely from the user's browser to the backend system using HTTPS encryption.
- **Secure Storage:** The trusted phrase will not be stored in plain text. Secure storage mechanisms, such as hashing or encryption, will be employed.
- **Access Control:** Strict access control will restrict access to the stored trusted phrase and related systems. Only authorized personnel will have access.

### Security Evaluation and Enhancements

The disguised terminal's security requires thorough evaluation and mitigation of potential vulnerabilities:

- **Discovery Risk Minimization:** The risk of unauthorized discovery will be assessed and minimized by ensuring a convincing disguise and eliminating obvious clues to its true purpose.
- **Enhanced Authentication:** Multi-factor authentication, such as combining a hardware wallet signature with the trusted phrase, will be considered to strengthen security.
- **Keylogger and Phishing Mitigation:** The system will be designed to mitigate keyloggers and phishing attacks. Security awareness training for authorized users will be provided.
- **Redundancy and Failover:** The system architecture will incorporate redundancy and failover mechanisms to prevent single points of failure and ensure continued operation.

## Hardware Wallet Integration and Additional Security Measures

This section details the implementation of hardware wallet integration and additional security measures related to the lockdown/recovery protocol activated via the disguised terminal.

**Trusted Phrase Recognition and Protocol Trigger:**

A backend function will recognize the pre-set trusted phrase submitted through the disguised contact form. Upon recognition, this function will trigger the corresponding lockdown or recovery protocol.

**Hardware Wallet Signature (2FA):**

After trusted phrase submission and recognition, a second authentication factor will be required using a hardware wallet signature. This ensures only authorized individuals with the hardware wallet can proceed.

**Hardware Key Loss Mitigation (Low Priority):**

Mitigation strategies for hardware key loss without associated device loss will be investigated as a low-priority item.

**Facial Recognition as 2FA (Exploration):**

The feasibility of facial recognition via webcam as a secondary authentication factor will be explored, including security and privacy implications. A fallback mechanism will be designed for users unable or unwilling to use facial recognition.

## I. Project Setup and Data Acquisition

### A. Project Initialization

This section details the initial setup required for the project, focusing on cost-effective strategies for development and testing. This includes generating mock data for local testing and a simplified model for initial deployment verification.

- **Mock Data Creation:** Create mock data in CSV or JSON format, incorporating edge cases such as price spikes, flat periods, and data gaps. This allows for thorough testing of components like the `DynamicPlaneGenerator`, data normalization functions (`time_frac`, `log_return`, `volume_scaling`), and the data loader without incurring the cost of using real-world data.
- **Dummy Model for Smoke Tests:** Develop a small, computationally inexpensive "dummy" neural network mimicking the input/output structure of the intended ViT model. This facilitates rapid on-device smoke tests using limited real data to verify Core ML training setup, loss calculation, backpropagation, and weight updates.
- **Documentation:** Document the thought process behind cost optimization strategies, including the rationale for using minimal date ranges and single-epoch training sessions on iOS devices. Prepare this documentation for in-depth discussion and review within the project repository.

### B. Data Acquisition and Preprocessing

This phase focuses on acquiring and preparing financial data from Yahoo Finance while minimizing costs and maintaining data integrity.

- **Data Source:** Utilize the Yahoo Finance API to acquire OHLCV data for minimal date ranges to reduce computational and storage costs. Document the chosen date ranges and the rationale for their selection within the project setup documentation. Ensure secure API access, documenting any necessary API keys or authentication mechanisms.
- **Preprocessing:** Implement data preprocessing steps optimized for minimal computational load. Document any adjustments made to the pipeline for cost optimization. Include robust data validation checks to ensure data integrity and prevent potential vulnerabilities related to malformed or manipulated data.
- **Data Storage:** Store the preprocessed data efficiently in CSV format within `stock_data/train/` and `stock_data/test/`. Document the chosen storage strategy and its rationale for cost-effectiveness.

## II. Model Development and Training

_(Content from original outline for Model Development and Training sections would be placed here)_

## III. Testing and Evaluation

_(Content from original outline for Testing and Evaluation sections would be placed here)_

## IV. Security Considerations

Given the project's focus on agent-based modeling and its potential deployment, integrating security measures is crucial. The following authentication methods are proposed:

- **Master Password:** Implement a master password using a strong hashing algorithm like scrypt (e.g., via Firebase Authentication with Identity Platform) for secure storage and protection against brute-force attacks.
- **Security Questions:** Implement a challenge-response system using security questions as a password recovery mechanism. Randomly select 5 questions from a user-defined set and require 3 correct answers for authentication. This mitigates guessing and social engineering attacks.
- **Visual Cryptographic Key:** Enhance security against brute-forcing, keylogging, and shoulder-surfing with a visual cryptographic key. Combine a user-defined visual pattern on a randomized grid with a password. Store a polynomial hash of the pattern coordinates and password on the backend. Present a new randomized grid each time for pattern entry, increasing complexity for attackers while maintaining user-friendliness.

These measures, documented and refined throughout the project lifecycle, will protect the trading agent, its data, and any future deployment environment.

## V. Efficient Development and Testing Strategies

This section details key strategies for cost-effective and rapid development and testing:

- **On-Device Training:** Conduct training directly on the iOS device to eliminate initial server costs, enabling full testing and debugging cycles locally.
- **Short Training Runs:** Utilize short, 1-epoch training sessions with minimal data for frequent, budget-friendly testing during development.
- **Unit/Integration Tests:** Employ mock data with edge cases to thoroughly test individual modules and their integration locally.
- **Dry Run Mode:** Implement a "Dry Run" mode in the Experiment Designer UI to simulate the entire workflow (excluding actual training) for end-to-end validation.
- **On-Device Smoke Tests:** Use the "dummy" model with limited real data for quick verification of Core ML functionality on the device.

```markdown
## III. Testing and Evaluation

This section details the testing and evaluation procedures for the SCoVA project, focusing on a robust three-stage strategy. This strategy ensures thorough validation of individual components, the entire pipeline, and finally, on-device performance. This tiered approach facilitates early identification and resolution of issues, minimizing debugging time and maximizing the reliability of the final product.

### A. Three-Stage Testing Strategy

The testing methodology employs a tiered approach encompassing unit and integration tests, simulated pipeline execution, and on-device smoke testing. This layered approach provides comprehensive coverage and facilitates efficient debugging.

**1. Unit & Integration Testing with Mock Data:**

This initial stage focuses on verifying the functionality of individual code units and their interactions using mock data, including edge cases. Isolating dependencies allows for targeted testing and easier identification of bugs within specific components. This involves:

- **Unit Testing:** Isolating individual functions or modules and verifying their correct behavior in various scenarios.
- **Integration Testing:** Testing the combined functionality of multiple units to ensure they interact correctly and data flows as expected.

**2. End-to-End Pipeline Simulation ("Dry Run" Mode):**

Once unit and integration tests are successful, the entire application pipeline is simulated in a "dry run" mode. This mimics the real-world execution flow without interacting with external services or devices. A "Dry Run" toggle in the Experiment Designer UI enables this mode. This crucial step allows us to:

- Validate the complete data flow and transformation process.
- Identify any potential bottlenecks or performance issues.
- Verify the correct integration of all components within the pipeline.

**3. On-Device Smoke Test with a Dummy Model:**

The final testing stage involves deploying the application to the target device and performing a "smoke test" using a pre-trained dummy model. This lightweight test verifies basic functionality on the actual hardware, focusing on connectivity and data flow, and confirms that:

- The application runs successfully on the device.
- The application can access the dummy model weights.
- Small, simulated weight updates can be sent back to the backend and processed, updating the "universal model." This verifies the communication channel and data synchronization between the application and the backend. The purpose of this test is to validate the entire training and update pipeline, including data loading from the `DynamicPlaneGenerator`, execution of the training loop (forward pass, loss calculation, and backpropagation), and successful extraction of updated weights. This approach offers a low-cost method to confirm pipeline functionality before full-scale training. The necessity of performing a single training epoch with the dummy model will be evaluated, considering the potential performance gains against the increased resource utilization (memory, processing, battery life).

### B. Performance Evaluation

This section will detail the performance evaluation metrics and methodology for assessing the efficiency and effectiveness of the SCoVA application. (Existing items from outline will be placed here)

## IV. Deployment and Monitoring

(Existing items from outline will be placed here)

## V. Dissertation and Documentation

(Existing items from outline will be placed here)

## Real-Time Event Detection with Computer Vision

This section details the exploration and implementation of real-time event detection using computer vision techniques, moving beyond the project's…
```

## Continual Threat Assessment Module (CTAM)

The Continual Threat Assessment Module (CTAM) proactively identifies and assesses potential market threats, enhancing the existing historical data analysis. The CTAM analyzes multiple real-time visual data sources, using specialized anomaly detection models and a fusion mechanism to generate a Systemic Threat Level (STL) score. This score informs various core trading system components, enabling a more dynamic and responsive approach to market fluctuations.

**Multi-Source Visual Inputs:** The CTAM ingests real-time visual data from the following sources, providing a comprehensive market overview:

- **Primary Equity Charts:** Real-time snapshots of equity price charts.
- **Corresponding Futures Charts:** Real-time snapshots of futures contracts related to the underlying equities.
- **Visualized Options Chain Data (Heatmap):** Heatmap visualizations of options chain data, highlighting unusual activity or significant changes.

**Specialized Anomaly Detection Models:** Specialized Convolutional Neural Networks (CNNs) act as "threat detectors" for different asset classes:

- **Equities Detector:** Identifies equity market anomalies, such as sudden price gaps and unusual volume spikes.
- **Derivatives Detector:** Analyzes visualized options chain heatmaps to detect unusual patterns or anomalies indicating potential market threats.

**Fusion and Systemic Threat Level (STL) Assessment:** The outputs from the equity and derivatives detectors are combined using a fusion model or weighted-average function to generate a single STL score. This score represents the overall perceived market threat level.

**Integration with Core Trading System:** The STL score influences several key system components:

- **Proactive Risk Management (Withdrawal):** High STL scores can trigger proactive risk mitigation strategies, such as reducing market exposure to minimize potential losses during periods of high threat.
- **Dynamic Plane Adjustment:** The STL score informs the Dynamic Plane, a component responsible for adapting to market changes. It adjusts the Dynamic Plane's smoothing factor, enabling a more effective response to market volatility based on the perceived threat level.
- **Enhanced Trade Prediction Context:** The STL score serves as a context token for final trade predictions, providing additional information and context for improved decision-making.

**Model Retraining Strategy:** The CTAM models are continuously retrained, differentiating between "flow" (gradual market trends) and "shock" (sudden, significant events). This continuous learning approach prevents the system from becoming desensitized to extreme market movements and ensures its responsiveness to both gradual and abrupt market changes.

## II. Model Development and Training

This section details the development and training of the dual-system model architecture, comprising the Flow Engine and the Shockwave Prediction Module (SPM). The Flow Engine handles normal market behavior, while the SPM is specialized for predicting short-term market movements following significant price shocks.

### A. Model Architecture

The current model architecture exhibits limitations in handling volatile market conditions. A dual-system approach is proposed to address these limitations and enhance performance.

**Current Limitations:**

- **Oversimplified Error Correction:** The current error-correction model, based on a "wound and healing" analogy, oversimplifies market dynamics and struggles with the complexities of real-world behavior. Its mean reversion bias smooths out potentially profitable "shock" events and extreme fluctuations. The resulting "zone of comfort" hinders responsiveness to sharp, volatile price movements.

**Proposed Dual-System Architecture:**

- **Flow Engine:** This component leverages the existing, refined error-correction mechanisms for stable market conditions, focusing on exploiting predictable behavior.
- **Shockwave Prediction Module (SPM):** Designed to detect and react to outlier "shock" events (flash crashes, earnings gaps, news-driven spikes, extreme volatility), the SPM acts as a contextual override, enabling the model to capitalize on these high-alpha opportunities.

**SPM Architecture:**

- **Shockwave-Specific Training:** The SPM will be trained exclusively on historical "shocker events."
- **Architecture Exploration:** Potential architectures include a reactive Convolutional Neural Network (CNN) or a transformer with a modified attention mechanism, optimized for predicting immediate post-shock behavior.
- **Short-Term Prediction Target:** The SPM predicts a very short-term movement vector (e.g., the next 1-3 candles) following a shock event.

## Adaptive Strategy and Shockwave Integration

This section details the adaptive strategy for integrating the Flow Engine and SPM predictions, aiming to maximize profit in both stable and volatile market conditions.

### Dynamic Weighting ("Seesaw" Mechanism)

A dynamic weighting mechanism, the "seesaw," adjusts the influence of Flow Engine and SPM predictions based on market volatility. During periods of increased volatility (higher Systemic Threat Level - STL), the SPM's influence increases, while the Flow Engine's decreases. Conversely, during stable periods (lower STL), the Flow Engine's influence predominates.

### Systemic Threat Level (STL)

The STL quantifies market volatility, ranging from 0 (low) to 1 (high). The following formula calculates the final prediction:

```
Final Prediction = (1 - STL) * FlowEngine_Prediction + (STL) * SPM_Prediction
```

This ensures a seamless transition between models as market conditions evolve.

### Opportunistic Threat Response

High market volatility is treated as an opportunity. The SPM, coupled with the adaptive weighting, allows the system to capitalize on rapid price fluctuations characteristic of volatile markets, leveraging the SPM's ability to predict short-term movements following shockwave events.

### Profit Maximization Objective

The core objective is profit maximization. All design decisions, including the dual-system architecture and adaptive weighting, are driven by this goal. The system's success will be evaluated based on its profitability, not just trading activity.

## Re-architecting Project Swaha

This section details the architectural overhaul of Project Swaha, guided by the four pillars: Continuity, Enforcement, Facilitation, and Specialization. This restructuring aims to address workload imbalances and streamline the project, going beyond superficial changes to redefine roles, data flow, and system topology.

**Key Architectural Changes:**

- **Continuity:** Expands to oversee all pre- and post-execution processes, ensuring clear prerequisites and expected outcomes for smooth transitions between project phases.
- **Enforcement:** Focuses on resource management, process monitoring, error detection, and prevention, enhancing system robustness and reliability.
- **Facilitation:** Manages all input/output operations, acting as the interface between internal processes and external systems, streamlining communication and data exchange.
- **Specialization:** Workload will be more appropriately balanced with other roles. This addresses the current imbalance where Specialists carry a disproportionate burden.
- **Overall Architecture:** The project's architecture is being fundamentally redesigned based on user feedback and the four pillars. This involves deconstructing and reevaluating existing components, redefining data flow, and redesigning system topology for improved efficiency, effectiveness, and maintainability.

This overhaul signifies a significant shift in Project Swaha's structure, improving project execution through redefined roles, redistributed workloads, and a focus on clear communication and process management.

## Architectural Considerations for the SCoVA Project

This section details the architectural decisions made in the SCoVA project, guided by the four pillars: Continuity, Enforcement, Facilitation, and Specialization. The architecture has been iteratively refined through deconstruction, restructuring, and reimagining to optimize alignment with these principles.

Initially, the framework was deconstructed and analyzed based on the four pillars. Heavy components were identified, and their roles reassessed within the user's vision. This involved breaking down components into atomic functions and evaluating their contributions to each pillar. The data flow and system topology were then redefined based on this analysis.

A comprehensive architectural reassessment followed, focusing on categorizing components according to the four pillars and ensuring alignment between data flow and component roles, emphasizing clear communication pathways.

Further refinement, driven by user critique, involved deconstructing and reassigning components based on the four pillars. Modules like the `DynamicPlaneGenerator` and `Multi-Scale Vision Transformer` were atomized, with their functions assigned to the appropriate pillar. Stringent communication protocols were established between Specialist components, routing data flows through designated Facilitators.

Finally, a deep refactoring solidified these changes. The `DynamicPlaneGenerator` and `Multi-Scale Vision Transformer` were dissected, their functions realigned to the defined pillars. Communication protocols were redesigned, mandating that Specialist components interact exclusively through Facilitators, adhering strictly to the new system principles. This rigorous process ensures a robust and well-defined architecture for the SCoVA project.

## SCoVA Project Services

This section details the services essential for robust and reliable operation of the SCoVA project, focusing on pre- and post-campaign analysis, real-time monitoring, and execution control. These services ensure data integrity, efficient resource utilization, and effective system management.

### Pre-flight Validation Service

Before operations like training or backtesting, this service verifies all necessary conditions are met:

- **Data Integrity:** Confirms the completeness, consistency, and correctness of input data, checking for missing values, correct data types, and adherence to expected ranges.
- **Environment Health:** Verifies the availability and functionality of computational resources, including disk space, memory, and software dependencies.
- **Template Schema Validity:** Ensures configuration files and templates adhere to the defined schema, preventing errors from inconsistencies or incorrect usage.

### Post-flight Analytics Service

After a campaign completes, this service processes results and generates insights:

- **Campaign Summaries:** Provides concise summaries of campaign performance, including key metrics, visualizations, and relevant statistics.
- **Model Registry Updates:** Updates the model registry with information about trained models, including performance metrics, hyperparameters, and training data.
- **Archetypal Pattern Identification:** Identifies recurring patterns and trends in results, providing insights for future model improvements and strategy adjustments, contributing to the system's "memory."

### Real-time Process Monitor

This service continuously tracks the health and resource consumption of active training and backtesting jobs:

- **Resource Utilization Monitoring:** Tracks CPU usage, memory consumption, disk I/O, and network activity to identify bottlenecks and optimize resource allocation.
- **Training Stagnation Detection:** (Description of this functionality is missing in the original text and should be added here.)

## Create Core Services

This section details the creation of several key services crucial for the SCoVA project, encompassing inter-module communication, data processing, and model inference.

**1. Broker Service (Facilitation)**

This service will manage communication between different modules within the SCoVA project. Acting as a mediator, it receives requests and routes them to the appropriate service. This decoupled architecture enhances modularity and maintainability. The broker service will handle request queuing, prioritization (if necessary), and error handling, ensuring robust and reliable inter-module communication.

**2. Normalization Service (Specialization)**

This service will normalize numerical arrays. Normalization is essential for consistent input data ranges for downstream processes, such as Principal Component Analysis (PCA) and model training. The specific normalization method (e.g., min-max scaling, standardization) will be determined based on project requirements and data characteristics.

**3. PCA Service (Specialization)**

This service will implement Principal Component Analysis (PCA) on numerical arrays. PCA is a dimensionality reduction technique used to identify the most important features in the data. This service will accept a numerical array as input and return the principal components and the explained variance ratio. This information can be leveraged for feature selection, noise reduction, and potentially improving model training efficiency.

**4. Coordinate Rotation Service (Specialization)**

This service will rotate numerical arrays based on specified basis vectors. This is crucial for coordinate transformations and potential data augmentation techniques. The service will accept a numerical array and the new basis vectors as input and return the rotated array.

**5. Model Inference Service (Specialization)**

This service will handle model inference on single image tensors. It will encapsulate model loading, preprocessing steps, and the inference call. This abstraction simplifies the inference process and promotes code reusability. The service will accept an image tensor as input and return the model's prediction.

## Architectural Considerations

This section outlines key architectural decisions for the SCoVA project, focusing on component interaction and responsibilities.

**Experiment Runner and Broker Interaction:**

An `ExperimentRunner` service will manage individual training and backtesting runs for each model permutation. It will orchestrate these runs by making calls to the `Broker` service, which directs requests to the appropriate specialized components.

**Communication Restrictions for Specialists:**

To maintain a clear separation of concerns and ensure modularity, Specialist services will adhere to strict communication restrictions. They will not directly interact with external systems (e.g., cloud storage, databases). All inputs and outputs will be mediated through designated Facilitator services. This separation allows Specialists to focus solely on their core functionality. Separate Facilitators may be employed for input and output operations, further enhancing modularity. Direct communication between Specialists or with external systems is prohibited.

**Enforcer's Domain - Data Storage Access:**

Access to internal data storage (databases and cloud storage) falls exclusively under the domain of Enforcer services. This restriction reinforces data security and integrity, ensuring controlled access. Enforcers will retrieve and store data as required by other components, acting as gatekeepers for data access.

**Function Naming Convention:**

A consistent function naming convention is crucial for code readability and maintainability. Two approaches are being considered:

1. **Suffixing with Type:** Append the functional type (e.g., `_continuity`, `_enforcer`) to each function name, clearly indicating its role within the four pillars framework.

2. **Deriving from Class Category:** Infer the function type from the class it belongs to, assuming the class adheres to the four pillars categorization. This reduces redundancy but requires careful class design.

The chosen approach will be documented upon finalization.

**Refactoring Functional Pillars:**

The definitions of the four functional pillars (Continuity, Enforcement, Facilitation, and Specialization) will be reviewed and refined to create a more robust and well-defined architectural foundation. Clearer definitions will improve code organization, component interaction, and overall system design.

## Architecture

This section outlines the architectural principles guiding the SCoVA project. Adhering to these principles will ensure a robust, maintainable, and scalable system.

**Key Architectural Principles:**

- **Isolation of Specialists:** Specialist components, responsible for specific tasks (e.g., candlestick chart generation, CNN training, dynamic plane transformations), will be completely isolated. Their interactions will be meticulously managed through well-defined interfaces. This isolation enhances modularity and reduces unintended side effects. For example, the CNN training component will only receive preprocessed candlestick images and return labels, without direct access to raw market data or the backtesting framework.

- **Asynchronous Communication:** Asynchronous communication using a publish/subscribe (pub/sub) model will be implemented between specialist components to mitigate latency and improve responsiveness. This allows components to operate independently and efficiently.

## I. Project Setup and Data Acquisition

This section details the initial steps of the SCoVA project, encompassing project initialization and the acquisition and preprocessing of financial data, laying the groundwork for subsequent model development and training.

### A. Project Initialization

This stage outlines the initial project setup. While not explicitly addressed in the checklist, this phase is crucial for establishing the context for applying the four-pillar architecture (Facilitators, Enforcers, Specialists, and Continuity services) to subsequent tasks. Further documentation and discussion will occur as the project progresses.

### B. Data Acquisition and Preprocessing (Continuity)

This stage describes acquiring, preprocessing, and preparing financial data for model training. The Continuity service pattern ensures a robust and reliable data pipeline.

- **Data Acquisition:** A dedicated `DataAcquisitionService` (inheriting from `ContinuityService`) will acquire OHLCV (Open, High, Low, Close, Volume) and Index data from Yahoo Finance, ensuring a continuous flow of data into the project.

- **Data Preprocessing and Storage:** A `DataPreprocessingService` (also inheriting from `ContinuityService`) will preprocess the acquired data (including price adjustments) and store it as CSV files in designated directories (`stock_data/train/` and `stock_data/test/`).

## II. Model Development and Training

This section details the core components of the model architecture, focusing on data preprocessing and single-epoch training. The services described below are designed with modularity and separation of concerns in mind to promote code reusability and maintainability.

### A. Model Architecture (Specialization)

- **NormalizeWindow (Specialist):** This service normalizes the raw numerical input array. It receives raw data and a configuration dictionary as input and returns the normalized array. It operates without external dependencies (e.g., `google-cloud-storage`, `google-cloud-firestore`, `requests`) to maintain a focused responsibility.

- **ComputePrincipalComponents (Specialist):** This service performs dimensionality reduction. It takes the normalized array from `NormalizeWindow` and returns the top two principal component vectors. It utilizes only `numpy` for computational efficiency and portability.

- **ProjectToPlane (Specialist):** This service projects the original data onto a 2D plane defined by the principal components calculated by `ComputePrincipalComponents`. It receives the original data and the basis vectors and returns the 2D projected data, simplifying data representation for further processing.

- **TrainOneEpoch (Specialist):** This core training service handles model training for a single epoch. It receives the model artifact (as bytes), training data tensors, and a configuration dictionary, returning the updated model artifact (as bytes). It remains agnostic to data origin and destination, emphasizing its focus on the training process itself.

- **UI_Gateway (Facilitator):** This service acts as the API gateway for the frontend application, handling requests like `/startCampaign`. It translates user interactions into internal workflows, potentially triggering model training and related processes. A detailed `/startCampaign` workflow will be provided in a subsequent section.

## Microservice Architecture

This project employs a microservice architecture based on four pillars: Facilitators, Enforcers, Specialists, and Continuity services.

- **Workflow_Broker (Facilitator):** This service orchestrates complex sequences of specialist calls (e.g., constructing the dynamic plane, executing an experiment). Triggered by tasks submitted to a Pub/Sub queue, it enables asynchronous operation and service decoupling.

- **State_Enforcer (Enforcer):** This Cloud Function manages all Firestore database read/write operations. Its dedicated service account has exclusive Firestore read/write permissions, ensuring data integrity and security. It utilizes a single function with conditional logic (e.g., if/else or match/case) to handle different request types.

- **Resource_Enforcer (Enforcer):** This Cloud Function manages Google Cloud Storage (GCS) object storage. Its dedicated service account has exclusive GCS read/write permissions. Like `State_Enforcer`, it uses a single function with conditional logic for various operations.

- **Live_Execution_Enforcer (Enforcer):** This service exclusively places, modifies, or cancels orders through the Zerodha API. A secure endpoint (e.g., `/executeOrder`) provides controlled interaction with other services.

- **Pre-flight_Validation_Service (Continuity):** Invoked by the `UI_Gateway`, this service performs pre-flight validation checks. Crucially, it adheres to the principle of least privilege by sending requests to the `Resource_Enforcer` instead of directly fetching data.

## I. Project Setup and Data Acquisition

This section outlines the initial steps for setting up the project and acquiring the necessary data, emphasizing a clear and documented process.

### A. Project Initialization

1. **Continuous Documentation:** Maintain comprehensive documentation throughout the project lifecycle, detailing design decisions, challenges, and solutions. This living document will ensure transparency and facilitate future development.

2. **Collaboration Setup:** Prepare a paper providing a comprehensive overview of the project's goals, methodology, and results, accompanied by a well-organized code and data repository. This setup will support in-depth discussions, collaboration, and reproducibility. It must also account for the updated inter-service communication requirements, enabling unrestricted communication between Facilitators, Enforcers, and Continuity services, all tracked by the distributed tracing system detailed in subsequent sections.

### B. Data Acquisition and Preprocessing (Continuity)

This subsection details the acquisition, preprocessing, and preparation of data for model training. Critically, the distributed tracing system will be integrated into this stage to monitor data flow, given the new flexibility in microservice communication.

---

## Windowing and Label Generation

A 5-day windowing logic will be implemented for both input and output data. Return labels will be calculated using the formula `(Close(t+h) - Open(t+1)) / Open(t+1)`. This consistent application of data transformations aligns with the `ContinuityService` principle.

### Candlestick Chart and Label Integration

Candlestick charts will be generated, incorporating volume and moving averages. The calculated return labels will be integrated with the candlestick chart image generation process. A dedicated `ChartGenerationService`, inheriting from `ContinuityService`, will manage this continuous data processing and transformation.

### Filename Convention

The filename convention will be updated to encode the holding period, either directly within filenames or through a separate CSV file. This ensures data management consistency, a core function of the `ContinuityService`, potentially handled by the `DataPreprocessingService`.

The four base classes, particularly `ContinuityService`, are fundamental to ensuring reliable and consistent data flow throughout this phase. Leveraging inheritance from `ContinuityService` allows each service responsible for data acquisition, preprocessing, and transformation (e.g., `DataAcquisitionService`, `DataPreprocessingService`, `ChartGenerationService`) to adhere to a standardized architecture, improving maintainability and scalability. Detailed technical specifications and pseudocode for these services, including `ContinuityService` itself, are essential for implementing this inheritance-based architecture.

---

## Microservice Architecture and Performance Considerations

This project will adopt a microservice architecture based on the four pillars principle: Continuity, Enforcement, Facilitation, and Specialization. Decomposing the system into microservices categorized by these pillars prioritizes extreme decoupling, modularity, and role purity. This approach offers significant advantages in terms of enhanced security, testability, and scalability. However, it introduces potential challenges related to latency and network overhead due to increased inter-service communication.

To mitigate these potential bottlenecks, the following strategies will be implemented:

- **Asynchronous Communication:** Google Cloud Pub/Sub will be used for appropriate inter-service communication. This asynchronous approach minimizes latency and improves responsiveness by decoupling services, particularly beneficial for non-critical requests.

- **Network Bottleneck Mitigation:** Several strategies will be employed:

  - **Service Discovery:** A robust service discovery mechanism will enable dynamic service location and communication.
  - **Load Balancing:** Load balancing will distribute traffic across service instances, preventing overload and ensuring high availability.
  - **Optimized API Design:** Efficient API design will minimize the size and number of requests for inter-service communication.

- **Continuous Performance Testing:** End-to-end performance tests will be integrated from the project's outset. This proactive approach allows for continuous latency monitoring, early bottleneck identification, and ensures real-time performance is maintained throughout development. These tests are crucial for validating the effectiveness of the mitigation strategies.

---

## System-wide Logging and Visualization

This section details the implementation of a system-wide logging and visualization system to track the performance and interactions of different services within the SCoVA project. This facilitates efficient debugging, performance analysis, and understanding of complex inter-service interactions, especially in asynchronous and parallel scenarios.

The core component is a **system-wide logbook** recording crucial service execution information:

- **Function Execution Time:** Track execution time for all service functions (Facilitators and Specialists), including time spent paused while awaiting responses.
- **Network Interactions:** Log all network interactions to provide insights into latency and potential bottlenecks.
- **API Calls:** Record all inter-service API calls, including parameters and results.

A **visualization component** will facilitate analysis of this logged data, specifically addressing asynchronous and parallel service challenges:

- **Branching Threads:** Visually represent thread branching during asynchronous calls.
- **Loopbacks:** Clearly depict loopbacks when Facilitators receive responses from Specialists, clarifying interaction flow and timing.

To leverage existing tools, the following will be investigated:

- **Existing IDE Functionality:** Assess the chosen IDE for existing logging capabilities.
- **Adaptable Tools:** Explore existing logging and visualization tools for potential adaptation.

A key use case for this logbook is **bottleneck identification and refactoring**. The logged data will pinpoint performance bottlenecks, guiding refactoring efforts, potentially by decomposing complex Facilitators.

This system will initially be implemented as a **co-located monolith** within a single codebase for simplified development and debugging, allowing for easier transition to a distributed architecture if needed later.

## Architectural Philosophy

The model analyzes market data within a dynamically rotating 2D plane. This plane continuously re-centers and rotates based on changes in time, price, and volume, replacing static candlestick image analysis. Market analysis is conducted using two distinct models: a "Flow Engine" for standard market behavior and a "Shockwave Prediction Model (SPM)" for chaotic market events. Predictions are synthesized using a weighted "seesaw" mechanism influenced by a calculated "Systemic Threat Level (STL)."

## System Architecture

This project adopts a distributed, client-side heavy architecture designed for efficiency and scalability, leveraging a lightweight backend orchestrator. The system adheres to a "Four Pillars" structure:

- **Specialists:** These independent components handle specific computational tasks. Examples include the CNN for trade signal generation, the ViT for sequential candlestick data processing, and modules for dynamic plane implementation (rotation, translation, snapshot generation).

- **Facilitators:** These components manage communication and data flow. The backend serves as the primary facilitator, brokering interactions between the client app, data sources (Yahoo Finance), model storage (Universal Model Hub), and backtesting services (Cloud Tasks).

- **Enforcers:** These components manage resources and ensure system integrity. The backend enforces data consistency within the Universal Model Hub, manages resource allocation for backtesting jobs, and handles security and access control.

- **Continuity:** These components manage pre- and post-flight processes. The backend handles data acquisition and preprocessing, ensuring data quality and consistency. It also manages model deployment, monitoring, and performance evaluation.

Computationally intensive tasks, such as image generation and model training, are performed on the client device (using the native app) to minimize server costs. The lightweight backend orchestrator, implemented using Python (Cloud Run/Functions with Firebase), handles data serving, model management, backtesting orchestration (via Cloud Tasks), and API interactions.

The backend also houses the Universal Model Hub, a central repository for the "universal source model." This model is converted into platform-specific formats for deployment to client devices via the native app (Swift/Core ML or Flutter/hybrid native ML). Weight updates from on-device training are sent back to the backend to update the universal source model, creating a continuous feedback loop. This architecture facilitates efficient model development and adaptation to changing market conditions.

## Enhanced Functionality and Architecture

This section details additional functional and architectural requirements.

**Functional Requirements:**

- **Advanced Error Signal:** Implement a Total Error signal composed of Vector Deviation Error and Frame Shift Error for a more nuanced understanding of model health. (Source: Chat2.json message #96)

- **Performance-Based Healing:** Tie the model "healing" process (adjustments or retraining) to prediction accuracy, triggering interventions based on performance degradation. (Source: Chat2.json message #96)

- **Multi-Scale Periodicity:** Incorporate intraday, daily, and weekly timeframes to improve the model's understanding of cyclical patterns. (Source: Chat2.json message #96)

- **"Rally Time" Prediction:** Predict the expected time (in candlesticks) for a predicted movement to occur. (Source: Chat2.json message #96)

**Architecture:**

- **Distributed Tracing:** Implement system-wide distributed tracing using OpenTelemetry to visualize workflows and identify performance bottlenecks. (Source: Chat2.json message #96)

## Markdown Usage

This document utilizes Markdown for formatting. No specific Markdown guidelines or concerns from the checklist data require addressing in this section.

## A. Backtesting (Continuity)

This section details the implementation of a robust backtesting framework.

1. **Robust Backtesting Engine:** Develop a high-fidelity backtesting engine that accurately reflects real-world market dynamics, incorporating:
   - **Market Impact:** Simulate the effect of large trades on asset prices.
   - **Latency:** Account for delays in order execution.
   - **Slippage:** Model the difference between expected and actual trade prices.
   - **Order Queues:** Simulate the order book and order queuing.
   - **Commissions:** (This was incomplete, needs further details from the source if available.)

```markdown
## Narrative Generation and Explainability

This section details the integration of explainability and narrative generation into the SCoVA project, ensuring clear justifications for trading decisions and meeting compliance requirements. This involves leveraging a feature store, implementing explanation AI techniques, and a dedicated narrative generation service.

The `Narrative_Generation_Service` generates human-readable explanations for each trade executed by the system. Triggered by the `Post-flight_Analytics_Service` after trade completion, these narratives are stored in the Karma Ledger for reporting and audit trails. The service incorporates the following components:

- **Feature Store Integration:** The `Narrative_Generation_Service` integrates with the Feature Store to retrieve versioned input features and system state information used for the trade decision, ensuring consistent and auditable explanations.

- **Attribution Methods:** To identify factors influencing model decisions, the service implements both model-agnostic and model-specific attribution methods:

  - **LIME (Local Interpretable Model-agnostic Explanations):** LIME explains individual predictions by locally approximating the model with a simpler, interpretable one.
  - **SHAP (SHapley Additive exPlanations):** SHAP values quantify each feature's contribution to a prediction, providing a comprehensive understanding of feature importance.
  - **Attention Maps:** For transformer models, attention maps visualize the input sections the model focused on during prediction.

- **LLM Integration:** A Large Language Model (LLM) synthesizes information from the feature store, LIME, SHAP, and attention maps into coherent narratives, transforming technical insights into human-readable explanations.

- **Explanation AI Integration:** The potential integration of a dedicated "explanation AI" will be explored to further enhance narrative depth and clarity, providing richer insights into the transformer model's predictions.

**Leveraging the Feature Store for Reporting and Compliance:**

The Feature Store serves as the central repository for all data justifying trade decisions, crucial for meeting reporting and compliance requirements. This addresses regulatory concerns regarding model predictions, ensuring transparency and avoiding a "black box" perception. This approach enables clear explanations for each trade, facilitating compliance and building trust.

## Narrative Generation, Karma Ledger Enhancement, and Paper Trading

This section details integrating a Large Language Model (LLM) for narrative generation, enhancing the Karma Ledger for improved auditability, defining narrative structure and data requirements, and developing a paper trading module.

**Narrative Generation and Integration:**

- **LLM Integration:** The `Narrative_Generation_Service` integrates an LLM (e.g., Gemini API) to generate narrative explanations based on technical attributions and system state. The service sends a detailed prompt to the LLM, including information from prior steps and a predefined template, enhancing explainability.

- **Karma Ledger Enhancement:** Narrative explanations are integrated into the Karma Ledger, providing a detailed, auditable record of each trade's decision-making process, improving transparency and enabling post-trade analysis.

**Narrative Structure and Data:**

- **Defined Narrative Structure:** Trade narratives follow a structured format with sections for: Market Perception, Flow Engine Analysis, Shockwave Analysis, Final Prediction, and Execution Decision. This ensures consistency and comprehensiveness.

- **Input Data for Narrative:** Data points required for narrative generation are collected from the Enforcers and the Feature Store, including: the input candlestick chart image, model state information, and overall system state (STL, seesaw weights, CorrectionFactor, risk exposure, and available capital).

**Paper Trading Module:**

- **Paper Trader Development:** A paper trading module simulates trades based on system predictions without risking real capital, enabling testing and refinement of trading strategies before live deployment.

## Paper Trading Implementation

This section details implementing a paper trading environment to mitigate risks before live trading. This involves creating a simulated brokerage service and integrating it with the existing live trading infrastructure.

**Risk Mitigation:** Paper trading allows for risk-free testing and refinement of trading strategies in a simulated market environment.

**Architecture:**

- **`Paper_Brokerage_Simulator` (Enforcer):** A new Enforcer service, `Paper_Brokerage_Simulator`, emulates the Zerodha Kite Connect API. It handles order execution and portfolio management within the paper trading context, maintaining its internal state in Firestore. It exposes the same API endpoints as the `Live_Execution_Enforcer` for seamless switching between modes. Fills are simulated based on real-time market data.

- **`Live_Execution_Enforcer` Modification:** The `Live_Execution_Enforcer` is modified to support `LIVE` and `PAPER` modes, controlled by a toggle in the Live Trading Dashboard. Trade requests are routed to either Zerodha Kite Connect API (`LIVE`) or the `Paper_Brokerage_Simulator` (`PAPER`).

**UI/UX:**

- **Live Trading Dashboard Toggle:** A "Paper Trading" mode toggle in the Live Trading Dashboard allows users to easily switch between live and simulated trading.
```

## Paper Brokerage Simulator and Order Book Feature Engineering

This section details the implementation of a realistic paper brokerage simulator and the derivation of order book features to enhance model input.

The `Paper_Brokerage_Simulator` will simulate realistic order fills based on live tick data from Zerodha, incorporating live bid/ask prices and volumes. Partial fills will be modeled to accurately represent real-world trading conditions. This realistic simulation is crucial for evaluating trading strategies under realistic market dynamics. The simulator will leverage live WebSocket tick data, including simulated network latency and realistic bid-ask spreads, bridging the gap between historical backtesting and live trading.

To further enhance realism and provide valuable input features, a new specialist service, `DeriveOrderBookFeatures`, will be implemented. This service will process raw market depth data and generate the following derived features:

- **Order Book Imbalance (OBI):** A measure of the relative buy and sell pressure in the order book.
- **Weighted Average Price (WAP):** The average price weighted by the volume available at each price level.
- **Bid-Ask Spread:** The difference between the highest bid price and the lowest ask price.

These derived features will be used as input to the `DynamicPlaneGenerator`, enriching the input data and potentially improving model performance. A comprehensive review of all current usages of market depth data received with live tick updates will be conducted to ensure consistent and efficient utilization of this information.

## Order Book Integration and Market Depth Analysis

This section details the integration of order book data and the development of a market depth anomaly detection system to provide the model with a more comprehensive understanding of market dynamics and potential risks.

The `Workflow_Broker` (Facilitator) will be modified to incorporate real-time order book information by calling the `DeriveOrderBookFeatures` specialist. The resulting features (OBI, WAP, and Bid-Ask Spread) will be added as new dimensions to the data window used by the `Normalization_Service` and `PCA_Service` before being passed to the `DynamicPlaneGenerator`. This integration allows the Dynamic Plane to consider supply and demand pressures, potentially improving its predictive capabilities.

A new `MarketDepthAnomalyDetector` specialist service will be implemented using a Convolutional Neural Network (CNN) trained to identify anomalies within a Market Depth Heatmap visualization. The output of this detector, `P(OrderBookShock)`, represents the probability of a sudden shift in market depth.

The Market Depth Heatmap, visualizing market depth data over time, will serve as input for the `MarketDepthAnomalyDetector`. The heatmap will use the Y-axis for price levels (bids and asks), the X-axis for time, and color intensity for the quantity available at each price level.

The `P(OrderBookShock)` score will be integrated into the Comprehensive Threat Assessment Model (CTAM) fusion model, alongside `P(PriceShock)` and `P(DerivativesShock)`, for a more comprehensive threat assessment.

The `Paper_Brokerage_Simulator` will also be enhanced to incorporate live market depth data for order filling, providing a more realistic simulation. For market orders, the simulator will emulate "walking the book" to model price slippage accurately. For limit orders, it will simulate order placement within the order book and execute fills based on the Last Traded Price (LTP) and available quantity.

## Market Depth Data Handling

Due to the limitations of Zerodha's market depth data (fixed ₹0.05 price increments between -₹0.25 and +₹0.25), the initially planned spread-based calculations have been discarded. The low information content of spreads derived from this data structure would be ineffective. Instead, the project will leverage the available information on order quantities and counts at each of the five fixed bid/ask levels.

Two new specialist services will facilitate this:

1. **`CalculateOrderBookImbalance`:** This service will calculate the Order Book Imbalance (OBI) from the market depth data. The output will be a normalized float between -1.0 and +1.0 (representing selling and buying pressure, respectively), and will be incorporated as a fourth dimension into the `DynamicPlaneGenerator`'s input data window (alongside Time, Price, and Volume).

2. **`GenerateDepthQuantityHeatmap`:** This service will generate a heatmap visualization of order book quantity changes over time. The heatmap will consist of 10 rows (5 bid and 5 ask levels) and N columns representing the last N time steps. Color intensity will correspond to the quantity at a specific price level and time. This heatmap will be input for the `MarketDepthAnomalyDetector` CNN.

## II. Model Development and Training

This section details the architecture, training process, and enhancements for the models used in the SCoVA project, focusing on incorporating market depth information and refining the visual analysis of candlestick data.

### A. Model Architecture (Specialization)

This subsection outlines the specific architectures and design choices for the models employed.

- **Market Depth Anomaly Detector:** A specialized Convolutional Neural Network (CNN), named `MarketDepthAnomalyDetector`, will be trained to analyze market depth data...

## Enhancements Related to Order Book Dynamics, Limit Order Behavior, and Brokerage Simulation

This section details enhancements related to order book dynamics, limit order behavior, and brokerage simulation. These enhancements aim to improve system resilience, predictive accuracy, and the realism of simulated trading.

**Order Book Volatility and Systemic Threat Level:**

- **Order Book Volatility Calculation:** A new specialist service, `CalculateOrderBookVolatility`, will be implemented. This service will calculate the standard deviation of Order Book Imbalance (OBI) values over a recent time window (e.g., the last few seconds). The resulting "Order Book Volatility" score will be provided to the Self-Correction & Healing Controller. This controller will use the volatility score to dynamically adjust the `CorrectionFactor` for the `DynamicPlaneGenerator` during periods of heightened market instability, enhancing system resilience.

- **Top-of-Book Pressure Gradient Monitoring:** The Cognitive Threat Analysis Module (CTAM) will continuously monitor the ratio of Level 1 Bid Quantity to Level 1 Ask Quantity. Significant and rapid changes in this ratio, indicative of potential market shocks or imbalances, will trigger a spike in the Systemic Threat Level (STL). This elevated STL will shift predictive authority to the Shockwave Prediction Model, optimized for reacting to sudden market movements.

**Limit Order Behavior and Brokerage Simulation Enhancements:**

- **Intraday Limit Order Behavior Analysis:** An investigation will be conducted into the exchange's handling of intraday limit orders, focusing on the "at or better" execution policy (e.g., 'X or lower' for buy orders, 'X or higher' for sell orders). This investigation will consider the prevalence of limit orders in intraday trading and ensure consistency between the exchange's behavior and the system's design. Necessary adjustments or clarifications within the system will be made based on the findings.

- **`Paper_Brokerage_Simulator` Price Improvement:** The `Paper_Brokerage_Simulator` will be enhanced to incorporate price improvement for limit orders, increasing the realism of paper trading results:

  - **Limit Buy Orders:** Filled at the live ask price if it's less than or equal to the limit price.
  - **Limit Sell Orders:** Filled at the live bid price if it's greater than or equal to the limit price.

- **Karma Ledger and Dharma Adherence Score Updates:** The Karma Ledger will be updated to include "Slippage" (negative difference between intended and actual execution price) and "Price Improvement" (positive difference) for each trade. The Dharma Adherence Score will incorporate these metrics to provide a more nuanced measure of trading effectiveness, separating P&L attributable to investment skill (Alpha) from P&L due to trade execution.

## Price Improvement Integration and Order Book Resilience Enhancement

This section details enhancements to incorporate price improvement data into the Vision Transformer model and enhance order book resilience analysis.

**Price Improvement Integration:**

- **`CalculatePriceImprovementRate` Service:** A new specialist service, `CalculatePriceImprovementRate`, will calculate the rolling average of price improvement received on trades over a defined time window (e.g., N minutes or hours). The resulting "Price Improvement Rate" will be used as a new feature.

- **Price Improvement Rate as Model Input:** The "Price Improvement Rate" will be integrated as a context token into the Vision Transformer predictive models. This allows the models to learn correlations between price improvement rates and subsequent market behavior, enhancing predictive accuracy and profitability tracking.

**Order Book Resilience Enhancement:**

- **Enhanced `ComputeOrderBookState` Feature:** The `ComputeOrderBookState` specialist service will be expanded. In addition to the existing OBI feature, a new "Book Resilience Score" will be calculated. This score, determined by the ratio of Level 1 quantity to the quantity at deeper levels (Levels 2-5), provides insights into order book depth and potential vulnerability to large orders. A low resilience score, combined with bullish patterns identified by the Flow Engine (if applicable and mentioned elsewhere in the document), could signal increased risk.

## Anxiety Model and DynamicPlane Enhancement

This section details the implementation and training of an "Anxiety Model" designed to enhance the performance and robustness of the DynamicPlane algorithm by predicting potential errors and dynamically adjusting its behavior.

The Anxiety Model is a meta-learning model that analyzes real-time market depth data and outputs an "Anxiety Level." This level serves as a crucial input for modulating the Error Detector and Weight Shifter components of the DynamicPlane system.

**Training the Anxiety Model:**

The Anxiety Model is trained using historical backtest data. The target variable is the Total Error (Vector Deviation + Frame Shift) calculated from the DynamicPlane algorithm's performance. The model learns to associate patterns in high-frequency market depth data with errors in the DynamicPlane algorithm.

Key input features derived from market depth data include:

- **Order-to-Quantity Ratio:** The ratio of the number of orders to the total quantity of shares being bid or offered.
- **Rate of Change of Order Book Imbalance (OBI):** How quickly the buy and sell order balance shifts.
- **Level 1 Dominance:** The proportion of the order book represented by the best bid and offer prices.
- **Order Book "Flicker" Rate:** The frequency of order book changes, reflecting market activity and potential instability.

**Post-Hoc Analysis and Feature Engineering:**

A post-hoc analysis correlates the DynamicPlane algorithm's actions, stored in a feature store, with historical order book data. This analysis generates the training data for the Anxiety Model, enabling it to learn the relationship between market conditions and the likelihood of errors. This analysis also informs the feature engineering process for the DynamicPlane algorithm itself. By incorporating the number of individual orders at each price level, rather than just the total quantity, the algorithm gains a more nuanced understanding of market depth and resilience. This refined input, coupled with the Anxiety Model's predictions, provides a stronger signal to the Vision Transformer, allowing for better differentiation between fleeting and sustained price movements.

**Dynamic Trading Modes:**

Two distinct trading modes—"flow" and "shock"—are implemented. The Anxiety Model determines the appropriate mode based on its real-time assessment of market conditions and the potential for errors in the DynamicPlane predictions. This dynamic switching enhances the system's adaptability and resilience to varying market dynamics.

**Execution Quality Feedback Loop:**

A feedback loop within the Self-Correction & Healing Controller continuously monitors rolling average execution quality. If increasing slippage and decreasing price improvement are detected, the `CorrectionFactor` increases, making the DynamicPlane perception more conservative and mitigating potential prediction errors. This dynamic adaptation further enhances the system's robustness.

## Project Setup and Data Acquisition

This section details the project setup and data acquisition process, focusing on the SCoVA (Snapshot Computer Vision Algorithm) and its integration within the larger agent architecture, including the Anxiety Model.

### Project Initialization

- **Documentation:** Comprehensive documentation will track design decisions, rationale, and evolving system understanding. This includes details about SCoVA's integration with other components, such as the Anxiety Model, Error Detector, and Weight Shifter, with a particular focus on integrating the Anxiety Level.

- **Repository and Collaboration:** A dedicated repository will house the project's code, data, and documentation, facilitating organized version control and collaborative development. Explanations of the interaction between SCoVA and the Anxiety Model, including the impact of the Anxiety Level on other modules, will be included.

### Data Acquisition and Preprocessing

- **Data Source and Preparation:** Historical order book data will be acquired from a suitable source based on availability and quality. This data will be preprocessed to train the Anxiety Model, focusing on patterns indicative of potential errors or the need for model adjustments.

- **Order Book Feature Engineering:** Features will be engineered from the order book data to capture both the quantity of shares and the number of individual orders at each price level. This is crucial for calculating the "Liquidity Gradient" metric, which assesses the concentration of supply and demand.

- **Anxiety Level Indicator Data Preparation:** Data will be processed to create input features for the Anxiety Model, potentially including historical market depth, algorithm performance metrics, and other relevant market indicators. This preprocessing will identify the specific features and transformations needed to represent market conditions accurately.

- **Data Storage:** Preprocessed data will be stored in a structured format (e.g., CSV, HDF5) within the project repository for efficient access and management of the training and testing datasets.

## II. Model Development and Training

This section details the architecture, training process, and enhancements for the SCoVA predictive models. Key considerations include asymmetric modeling and feature engineering to account for the different dynamics of bull and bear markets.

### A. Model Architecture (Specialization)

This subsection outlines the architecture of the SCoVA models, incorporating asymmetric elements and addressing cross-timeframe pattern recognition.

- **Asymmetric Prediction Models:** Separate prediction models, `Bull_Flow_Engine` and `Bear_Flow_Engine`, will be trained on historical data from uptrends and downtrends, respectively. A high-level regime-detection model will select the appropriate engine's prediction. This specialization allows for potentially improved performance in each market regime. (Source: Chat2.json message #120)

- **Graph-based Perceptual Model:** A graph-based model will represent different timeframes (intraday, daily, weekly, etc.) as nodes, with edges representing learned influence between them. This model, utilizing a Graph Neural Network (GNN) as the fusion mechanism, replaces the previous hierarchical multi-scale context model and offers a more flexible representation of timeframe relationships. This shift addresses the project's focus on snapshot computer vision for financial market prediction, using discrete visual snapshots of market data.

- **Exploration of "Non-Hierarchical Asymmetric" Implications:** An analysis of the terms "non-hierarchical asymmetric" and their relevance to the current algorithm design will be conducted. This exploration aims to identify potential future architectural improvements and enhance predictive power.

### B. Training and Validation (Enforcer)

This subsection discusses the training and validation procedures, including asymmetric risk management.

- **Asymmetric Risk Management:** The `Portfolio_Risk_Manager` will implement asymmetric risk management, potentially using different maximum drawdown limits for short versus long positions, reflecting the potential long bias in equity markets. (Source: Chat2.json message #120)

- **Asymmetric Loss Function:** A custom loss function will be developed that penalizes the underestimation of losses more heavily than overestimation or accurate prediction. Specifically, a penalty factor (e.g., 2.0) will be applied if the predicted return is greater than the actual return. A higher penalty (e.g., 3.0) will be applied if the actual loss is larger than predicted. This risk-averse approach prioritizes avoiding large losses.

### C. Dynamic Plane Implementation (Specialization)

This subsection describes the Dynamic Plane implementation, including asymmetric feature engineering.

- **Asymmetric Feature Engineering for DynamicPlaneGenerator:** The `CalculateAsymmetricFeatures` service will calculate `Upside Volatility` (standard deviation of positive returns) and `Downside Volatility` (standard deviation of negative returns). The `Workflow_Broker` will provide this output, along with other potential asymmetric features, as a context token to the Vision Transformer. This provides explicit information about the asymmetry of price movements. (Source: Chat2.json message #122)

- **Upside and Downside Volatility Features (Integration):** These calculated upside and downside volatility measures will be integrated as features into the model's input, providing it with direct information about the asymmetry of recent price fluctuations.

### D. Model Enhancement and Refinement (Specialization)

This subsection focuses on model enhancements and refinements, incorporating asymmetric self-correction mechanisms.

- **Asymmetric Self-Correction:** The `HealingController` will incorporate an asymmetric self-correction mechanism, reacting more aggressively to large, unexpected losses than to equivalent gains, reflecting the principles of prospect theory. (Source: Chat2.json message #120)

## V. Dissertation and Documentation

### A. Dissertation Writing (Continuity)

- **Document Computer Vision Aspect:** The dissertation will clearly document SCoVA's utilization of computer vision as its primary modality, highlighting its innovative application in financial modeling.

## II. Model Development and Training

### A. Model Architecture

This section details the architecture of the Vision Transformer (ViT) model, focusing on the integration of asymmetric features to enhance its understanding of market dynamics. This is achieved by using a calculated feature vector as a context token within the ViT architecture.

- **Asymmetric Feature Integration:** The `AsymmetricFeatureEngine` service computes a feature vector representing market asymmetry. This vector is passed to the `Model_Inference_Service` and used as a context token for the ViT, alongside the Dynamic Plane image tensor.

- **ViT Context Token Integration:** The Vision Transformer is modified to accept and process this context token. The self-attention mechanism within the ViT learns relationships between the visual patterns in the Dynamic Plane images and the context provided by the asymmetric feature vector.

### B. Asymmetric Feature Engineering

This section details the implementation of asymmetric features to capture the nuanced differences between market uptrends and downtrends, acknowledging the inherent bias towards upward movement due to the persistent nature of economic activity. These features are calculated by the dedicated `AsymmetricFeatureEngine` service and used as context for the Vision Transformer model.

- **Asymmetric Feature Engineering for Market Dynamics:** This process focuses on implementing asymmetric feature engineering techniques that differentiate between market rises (organic growth) and falls (corrective pullbacks), recognizing that markets tend to rise more often than they collapse.

- **`AsymmetricFeatureEngine` Service:** This service calculates a vector of asymmetric features from a given window of raw market data. This vector serves as a context token for the Vision Transformer, providing additional market context.

- **Price & Volatility Asymmetry Features:** The `AsymmetricFeatureEngine` service implements the following features:

  - **Upside vs. Downside Volatility:** Semi-deviation measures volatility separately for upward and downward price movements.
  - **Volatility Skewness:** Quantifies the asymmetry in the volatility distribution.
  - **Volatility Kurtosis:** Measures the "tailedness" of the volatility distribution, indicating the likelihood of extreme volatility events.

- **Volume & Participation Asymmetry Features:**

  - **Accumulation/Distribution Ratio:** Measures the relative volume flow during uptrends versus downtrends, providing insights into buying and selling pressure.
  - **Order-to-Quantity Asymmetry:** Compares the order-to-quantity ratios on the bid and ask sides of the order book, offering a perspective on the balance of buying and selling conviction.

- **Correlation Asymmetry Feature:**
  - **Price-Volume Correlation State:** Calculates the correlation between log-returns and log-volume separately for positive and negative return candles, helping to distinguish between periods of market fear and greed, acknowledging that price-volume relationships can behave differently during uptrends and downtrends.

### C. Asymmetric Regime Detection and Terrain-Based Input

This section details enhancements to the model architecture by incorporating asymmetric regime detection and exploring terrain-based input for the ViT.

- **Asymmetric Regime Detection:** An unsupervised clustering model (e.g., Gaussian Mixture Model (GMM) or Self-Organizing Map) identifies distinct market regimes based on asymmetric feature vectors. Trained offline using historical data, this model classifies the current market regime in real-time, providing a Regime ID (integer) used as a context token for the ViT, enabling context-aware predictions. This involves:

  - **Training the Unsupervised Clustering Model:** The chosen model (GMM or Self-Organizing Map) is trained offline using historical asymmetric feature vectors to identify 4-8 distinct market regimes.
  - **`IdentifyAsymmetricRegime` Service:** This specialized service encapsulates the regime detection logic. It accepts the asymmetric feature vector as input and returns the corresponding Regime ID.

- **Terrain-Based Input for ViT (Exploration):** This alternative approach categorizes environmental metrics (e.g., humidity, temperature, Air Quality Index (AQI)) into a predetermined number of bins, using the category as input to the ViT. This leverages domain expertise and may improve prediction accuracy by adding explanatory power. However, potential drawbacks require further investigation.

## I. Project Setup and Data Acquisition

This section outlines the project setup and acquisition of financial data for model training and evaluation.

### A. Project Initialization

- **Documentation:** Maintain comprehensive documentation of the project's development, including decisions, challenges, and evolving strategies. This will be crucial for understanding the project's trajectory and rationale.
- **Preparation for Review:** Prepare the research paper and code repository for comprehensive review and analysis. This includes ensuring well-documented code and a clear articulation of the project's methodology and findings in the paper.

### B. Data Acquisition and Preprocessing

- **Data Sources:** Obtain Open, High, Low, Close, and Volume (OHLCV) data for stocks listed in both the OMXS All-Share and the S&P 500 from Yahoo Finance, covering the period from 2016 to 2023. Acquire equally weighted index data for OMXS All-Share, OMXS Large Cap, OMXS Mid Cap, OMXS Small Cap, and First North All-Share from Nasdaq, and S&P 500 index data from Yahoo Finance.
- **Data Adjustment:** Adjust stock closing prices for splits, but not for dividends.
- **Data Storage:** Store the processed data in CSV files within `stock_data/train/` and `stock_data/test/` directories.
- **Data Windowing:** Implement a 5-day rolling window for both input features and output labels. Each data point will consist of five consecutive days of OHLCV data.
- **Return Label Calculation:** Calculate return labels using the formula `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 't' is the last day of the 5-day input window, and 'h' is the holding period.

## II. Model Development and Training

### A. Model Architecture

This section details the model architecture, enhanced to leverage asymmetric features and regime identification for improved performance. The core architecture is a Vision Transformer (ViT) augmented with a novel "Dual-Token Context Injection" approach.

- **Dual-Token Context Injection:** This hybrid approach balances the explainability of discrete regimes with the granularity of raw features by injecting two context tokens into the ViT:

  - **Regime ID Token (Categorical):** Represents the identified market regime based on asymmetric features, providing high-level context.
  - **Asymmetric Vector Token (Continuous):** Contains the raw vector of asymmetric features (e.g., upside/downside volatility, skewness), providing detailed information about the price distribution.

- **Component Enhancements:**

  - **Asymmetric Feature Engine:** The `AsymmetricFeatureEngine` will be modified to compute the raw asymmetric feature vector for the Asymmetric Vector Token.
  - **Regime Identification:** The `IdentifyAsymmetricRegime` module will be enhanced to analyze the asymmetric feature vector and output a discrete Regime ID for the Regime ID Token.
  - **Vision Transformer Modification:** The ViT will be adapted to process the Regime ID and Asymmetric Vector Tokens alongside the dynamic plane image. The self-attention mechanism will integrate information from all three inputs. This requires modifying the input embedding layer and potentially the positional encoding.

These modifications aim to improve the model's ability to capture and interpret complex market dynamics by providing both categorical and continuous contextual information related to market asymmetry.

## III. Narrative Generation Enhancement

This section details the implementation of contextual information into the model's input using a dual-token approach, enhancing the narrative generation service and providing more granular detail for decision-making.

**Dual-Token Context Injection**

Two tokens are injected into the ViT model's input:

1. **Regime ID:** Represents the current market regime, providing high-level context.
2. **Raw Asymmetric Feature Vector:** Represents a vector of raw asymmetric features, providing granular details about market conditions.

The ViT will be trained to utilize both tokens in conjunction with the candlestick image data. This approach offers several advantages:

- **Flexibility:** The model can learn the relative importance of each token.
- **Non-Destructive Context:** Contextual information is added without altering the original candlestick image data.
- **Granular Detail:** The feature vector provides more granular details about market conditions.

**Narrative Generation Service Enhancement**

The `Narrative_Generation_Service` will incorporate both the Regime ID and the raw asymmetric feature vector into the trade narrative explanations, providing richer context and deeper insight into the model's reasoning.

**Implementation Details**

The dual-token approach uses both the feature vector and regime ID as input. The system prioritizes flexibility. If the feature vector doesn't contribute meaningfully to performance, its calculation won't create a bottleneck and can be removed if deemed unnecessary.

## I. Project Setup and Data Acquisition

This section details the process of setting up the project and acquiring the necessary data for training and evaluating the CNN model. The project uses candlestick chart images as input, requiring a specific data acquisition and preprocessing pipeline.

### A. Project Initialization

This stage focuses on establishing a clear foundation for the project.

- **Document thought process:** Maintain comprehensive documentation throughout the project lifecycle, capturing evolving ideas, decisions, and rationale. This ensures clarity and traceability.
- **Prepare for in-depth discussion:** Prepare materials, including documentation and code repositories, to facilitate effective communication and collaboration regarding the research paper and associated GitHub repository.

### B. Data Acquisition and Preprocessing

This stage details how data is sourced, processed, and prepared for the CNN model. A key aspect is the creation of candlestick charts as visual input.

1. **Data Source:** Yahoo Finance will be used to acquire OHLCV (Open, High, Low, Close, Volume) data.

2. **Data Acquisition and Preprocessing:** The process involves acquiring OHLC and index data, with subsequent price adjustments as needed.

3. **Data Storage:** Acquired and preprocessed stock data will be stored in CSV files within `stock_data/train/` (training data) and `stock_data/test/` (testing data).

4. **5-Day Windowing:** Both input data (for candlestick chart generation) and output data (return labels) will be based on 5-day periods. The input will be a 5-day candlestick chart, and the output will be the subsequent 5-day return.

5. **Return Label Calculation:** The return label, representing the target variable, will be calculated as: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window, `h` is the 5-day holding period, `Close(t+h)` is the closing price on the last day of the holding period, and `Open(t+1)` is the opening price on the day after the input window.

6. **Candlestick Chart Generation:** Candlestick charts, incorporating volume and moving average indicators, will be generated.

7. **Candlestick Chart and Return Label Integration:** The candlestick chart generation process will be integrated with return label calculation. Each chart will have a corresponding return label, forming a complete data point for training.

8. **Filename Convention:** Chart image filenames will encode the holding period (`h`) using the format `YYYY-MM-DD__YYYY-MM-DD__3.25__h=5__ABB.ST.png` (example with h=5), or this information will be stored in a separate CSV file for traceability.

## I. Project Setup and Data Acquisition

This section details the initial steps of the project, encompassing project initialization, data acquisition, preprocessing, and preparation for model training.

### A. Project Initialization

This phase focuses on establishing a robust foundation for the project, including documentation and preparation for collaborative discussions.

1. **Document Thought Process:** A comprehensive record of the project's development will be maintained throughout its lifecycle. This documentation will track decisions, challenges, solutions, and evolving insights.

2. **Prepare for In-depth Discussion:** Project materials, including code and documentation within a version-controlled repository, will be organized to facilitate thorough discussions and collaboration.

### B. Data Acquisition and Preprocessing

This phase outlines the process of gathering, processing, and preparing the financial data for subsequent model training. The data will be used to generate candlestick chart images and corresponding return labels, which will serve as input and output for the CNN model. Note that dates, actual price values, and ticker symbols will not be directly used in the CNN model training.

1. **Data Source:** Financial data, including Open, High, Low, Close, and Volume (OHLCV), will be acquired from Yahoo Finance.

2. **Preprocessing:** The downloaded OHLCV data will be preprocessed, including the acquisition and integration of relevant index data (if needed) and any necessary price adjustments (e.g., for splits or dividends). Details of these preprocessing steps will be documented.

3. **Data Storage:** The preprocessed data will be stored as CSV files in designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** The data will be organized into 5-day windows. Each window will represent a single data point for the model and will be used to generate a candlestick chart image.

5. **Candlestick Chart Generation:** Candlestick charts will be generated from each 5-day window of OHLCV data using the `matplotlib` library. These charts will include volume information and a moving average indicator.

6. **Return Label Calculation:** For each 5-day window, a corresponding return label will be calculated. The target variable will represent the return over a holding period _h_, calculated as: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 't' represents the last day of the 5-day window.

7. **Integration and Filename Convention:** The return label calculation will be integrated with the candlestick chart generation process, ensuring each chart is paired with its corresponding label. The file naming convention will encode the holding period _h_ either within the filename itself or in a separate CSV file to maintain the association between images and target returns. The impact of the final _n_ data points on the training data (where _n_ is the size of the dataset), particularly related to windowing at the end of the dataset, will be addressed and documented.

8. **CNN Input Data Format:** The generated candlestick chart images will serve as the input data for the CNN model. How these images are transformed into numerical representations by the CNN, and the interpretation of those numerical representations, will be thoroughly investigated and documented.

## I. Project Setup and Data Acquisition

This phase establishes the project foundation and gathers the necessary financial data for training the CNN model. The core decision to use 5-day candlestick windows significantly influences the data acquisition and preprocessing steps.

### A. Project Initialization

This stage involves the preliminary steps to organize the project and prepare for subsequent development.

- **Documentation:** Meticulous documentation of the decision-making process and rationale behind each step is crucial for reproducibility and understanding the project's evolution. This includes justifying data sources, preprocessing techniques, and any deviations from the original research.
- **Preparation for Discussion:** All necessary materials, including relevant research papers and the code repository, will be prepared to facilitate in-depth discussions regarding the project's progress and challenges.

### B. Data Acquisition and Preprocessing

The following steps outline the acquisition and preparation of financial data, structured around the use of 5-day candlestick windows as model input:

1. **Data Source:** OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance. Relevant index data will also be acquired if necessary.
2. **Data Preprocessing:** Acquired OHLC data will be preprocessed to adjust for splits and dividends, ensuring data accuracy.
3. **Data Storage:** Preprocessed data will be stored in CSV format within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.
4. **5-Day Windowing:** A 5-day sliding window will be applied to the OHLCV data to create input features for the CNN. The rationale for this window size will be documented, supported by literature review.
5. **Return Label Calculation:** Return labels will be calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 'h' represents the holding period (1 to 5 days). Separate models will be trained for each holding period.
6. **Candlestick Chart Generation:** Candlestick chart images will be generated from each 5-day window, incorporating volume and potentially moving averages as additional visual features.
7. **Return Label Integration:** The calculation of return labels will be tightly integrated with the candlestick chart generation process, ensuring each image has a corresponding label.
8. **Filename Convention:** The filename convention will clearly indicate the holding period associated with each data point, either by encoding it in the filename or using separate CSV files for different holding periods.

### C. Model Training Strategy (Preview)

Separate CNN models will be trained for each holding period (1 to 5 days), allowing for specialized models targeting different trading horizons. Performance will be evaluated using metrics such as validation loss, Sharpe ratio, alpha, and Mean Squared Error (MSE).

## I. Project Setup and Data Acquisition

This section details the project setup and the acquisition of the necessary OHLCV data from Yahoo Finance. While the initial checklist items primarily focus on data manipulation and model training, incorporating practical trading considerations from the outset is crucial for developing a robust and realistic trading strategy. Therefore, this section not only outlines the data acquisition process but also addresses how these practical considerations will be integrated into the data preprocessing pipeline.

### A. Project Initialization

This subsection outlines the foundational steps for project organization and collaboration.

- **Documentation:** This project maintains comprehensive documentation throughout its lifecycle, capturing evolving thought processes, design decisions, and experimental results. This documentation will be continuously updated.
- **Repository:** The project code and data are managed within a dedicated repository to facilitate collaboration, version control, and reproducibility. Details of the repository structure and access will be provided separately.

### B. Data Acquisition and Preprocessing

This subsection details the acquisition of OHLCV data and the preprocessing steps necessary to prepare it for model training, incorporating the practical trading considerations discussed earlier.

1. **Data Source:** OHLCV data will be acquired from Yahoo Finance. This choice provides readily available historical data for a wide range of financial instruments.

2. **Data Acquisition and Preprocessing:** This stage involves downloading the raw OHLCV and index data, adjusting prices for splits and dividends, and incorporating key features to address practical trading constraints:

   - **Short-Selling Constraints:** A flag will be included in the dataset to indicate whether short-selling is permitted for each stock. This is particularly important for small-cap stocks, which are of interest due to their potential for higher returns but may face short-selling restrictions.

   - **Trading Strategy Analysis:** The data will be preprocessed to support analysis of different trading strategies. This includes facilitating flexible weighting schemes (beyond uniform weighting) and enabling filtering of trades based on model confidence. These mechanisms will be crucial for optimizing alpha generation and managing portfolio turnover.

   - **Risk Management:** The return label calculation will be carefully designed to reflect the potential losses of holding a position for the entire 5-day holding period. This is essential given the absence of a stop-loss mechanism in the initial strategy and is crucial for developing future risk mitigation strategies. Data will also be structured to analyze unsuccessful trades, comparing predicted and actual outcomes.

3. **Data Storage:** Preprocessed data will be stored as CSV files in designated directories (`stock_data/train/` and `stock_data/test/`).

4. **5-Day Windowing:** Both input features and target variables will be structured using a 5-day rolling window. The justification for this input window length is based on the effectiveness demonstrated in existing literature (Jiang et al., 2023) for financial time series data. While the 5-day _output_ (prediction) window is chosen for consistency, further investigation into its optimal length is warranted.

5. **Return Label Calculation:** The return label, representing the target variable, will be calculated as `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `h` is the holding period (5 days in this case). This formula captures the return from the first day after the input window closes until the end of the holding period.

6. **Candlestick Chart Generation:** Candlestick charts, incorporating volume and moving average indicators, will be generated from the OHLCV data.

7. **Return Label Integration:** The calculated return labels will be directly associated with the corresponding candlestick chart images.

8. **Filename Convention:** A consistent filename convention will be used to encode the holding period and other relevant information, ensuring clear identification and organization of data files.

This structured approach ensures a clear and organized foundation for subsequent model development and evaluation. By integrating practical trading considerations into the data acquisition and preprocessing pipeline from the outset, the project is set up to develop a more robust and realistic trading strategy.

## I. Project Setup and Data Acquisition

This section details the initial steps of setting up the project environment and acquiring the necessary financial data for model training and evaluation. This includes defining the data source, outlining preprocessing steps, establishing a structured storage system, and preparing the data through windowing, labeling, and visualization.

### A. Project Initialization

1. **Project Documentation:** A detailed log will be maintained throughout the project lifecycle, documenting decisions, rationale, challenges encountered, and progress. This log will ensure transparency, facilitate analysis, and support reproducibility.

2. **Project Repository:** A dedicated repository will house the project codebase and supporting documentation, including the foundational research paper. This will facilitate version control, collaboration, and clear communication of project goals, methodologies, and findings.

### B. Data Acquisition and Preprocessing

1. **Data Source:** Historical OHLCV (Open, High, Low, Close, Volume) data will be sourced from Yahoo Finance, a readily available and widely accepted source of financial data.

2. **Data Preprocessing:** Acquired OHLC and relevant index data will be preprocessed. This includes adjusting historical prices for splits and dividends to ensure data accuracy and consistency.

3. **Data Storage:** Processed data will be stored in CSV format within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. This structured approach facilitates efficient data management.

4. **Data Windowing:** Data will be organized into sequential 5-day windows for both model input features and output labels. This consistent window size will be maintained throughout the project.

5. **Return Label Calculation:** The target variable, representing the return over a 5-day holding period (h=5), will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`. This calculates the percentage change from the opening price on the day following the 5-day window (t+1) to the closing price 5 days later (t+h).

6. **Candlestick Chart Generation:** Candlestick charts, incorporating volume and a moving average indicator, will be generated for each 5-day window. These charts will serve as the primary input for the CNN model.

7. **Return Label Integration:** The return label calculation will be integrated with the candlestick chart generation process, ensuring each chart is paired with its corresponding 5-day future return.

8. **Filename Convention:** A consistent filename convention will be implemented, encoding the 5-day holding period either directly in the filenames or through the use of separate CSV files for different holding periods. This ensures clear data organization and identification.

## I. Project Setup and Data Acquisition

This section outlines the initial steps for setting up the project and acquiring the necessary financial data, emphasizing meticulous documentation and preparation for collaborative discussions. The focus is on preparing data suitable for identifying and predicting periods of rapid price increases ("rally time").

### A. Project Initialization

1. **Project Documentation:** Maintain comprehensive documentation throughout the project lifecycle. This includes recording decisions, rationale, and any changes in direction, especially regarding data sources, preprocessing steps, and the definition and identification of "rally time." This documentation will ensure reproducibility and support future analysis.

2. **Collaboration Materials:** Prepare materials for collaborative discussions and presentations. This includes a well-organized research paper draft outlining the project scope, methodology, and preliminary findings, and a structured code repository with clear documentation and comments to facilitate effective communication and collaboration.

### B. Data Acquisition and Preprocessing

The following steps detail acquiring, preprocessing, and organizing the financial data required for model development and training, with a particular emphasis on "rally time" identification.

1. **Data Source Identification and Selection:** Identify and select reliable data sources suitable for identifying and labeling "rally time" periods. Potential sources include high-frequency trading data, news sentiment data, or other relevant market indicators. Document the selection criteria clearly.

2. **"Rally Time" Data Preparation:** Detail the steps involved in acquiring and preprocessing the data to generate the "rally time" dataset. Define "rally time" precisely (e.g., a specific percentage price increase within a defined timeframe) and create corresponding labels. If relevant to the "rally time" definition, this includes:

   - **OHLCV Data Acquisition:** Acquire Open, High, Low, Close, and Volume (OHLCV) data from a reliable source such as Yahoo Finance, adjusting for splits and dividends to ensure accuracy and consistency.
   - **Index Data Acquisition:** If necessary, acquire relevant index data for comparative analysis and context.
   - **5-Day Windowing:** Implement a 5-day windowing approach, using a sequence of 5 consecutive days of OHLCV data as input.
   - **Return Calculation:** Calculate return labels using a clearly defined formula, such as `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 'h' represents the holding period. Specify the holding period used (e.g., 5-day future returns).
   - **Candlestick Chart Generation:** Generate candlestick charts incorporating volume and moving average indicators from the windowed data for use as input to CNN models.

3. **Data Storage and Organization:** Establish a clear and consistent data storage system. Store the acquired and preprocessed data (including OHLCV data, index data, labels, and metadata) in a structured manner, using designated directories (e.g., `stock_data/train/` and `stock_data/test/`) for training and testing data. Ensure the filename convention incorporates information about the holding period (e.g., through filename encoding or a separate CSV file).

4. **Data Quality Assurance:** Implement rigorous data quality assurance measures to ensure the accuracy and reliability of the data, including checks for missing values, outliers, and other potential errors.

This section focuses on project setup and data acquisition. Model architecture, training, and evaluation are addressed in later sections.

## Project Setup and Data Acquisition

This phase establishes the project foundation and prepares the necessary data for training and evaluating a model to predict "rally time." This involves acquiring financial data, preprocessing it into a suitable format for a sequential image-based model, and creating a robust data pipeline. The project will explore advanced model architectures, such as Vision Transformers (ViTs), and feature engineering techniques. This section focuses on data preparation to support these explorations, specifically addressing the requirement for sequential input data (sequences of candlestick chart images) for the model.

### Project Initialization

- **Documentation:** A comprehensive record of the project's progress, decisions, rationale, and challenges will be maintained throughout the development process.

- **Collaboration Preparation:** Relevant materials, including preliminary findings, code repositories, and research papers, will be organized to facilitate productive discussions and collaboration.

### Data Acquisition and Preprocessing

The data acquisition and preprocessing pipeline will be adapted to handle sequences of candlestick chart images, as required by the model architecture.

1. **Data Acquisition and Storage:** Historical OHLCV (Open, High, Low, Close, Volume) and relevant index data will be acquired from Yahoo Finance. Price data will be adjusted for splits and other corporate actions. Crucially, the data storage format will be revised to accommodate sequences of candlestick images. Options include storing sequences in a single file (e.g., using HDF5) or using a directory structure that groups related images.

2. **5-Day Window Sequencing:** The existing 5-day windowing logic will be extended to create sliding windows of 5-day candlestick charts. Each data point for the model will be a sequence of these 5-day windows, capturing market dynamics over time.

3. **Candlestick Chart Generation and Return Label Calculation:** Candlestick chart images will be generated, and corresponding return labels will be calculated for each image within the sequence, considering an appropriate holding period (e.g., 5-day future return).

4. **Data Structure for Sequences:** The data structure (filename convention or a more structured format) will be revised to unambiguously represent the sequences of candlestick charts and their associated return labels. This ensures the model correctly interprets the sequential input.

### Sample Dataset for ViT Model Development

To facilitate early experimentation with a Vision Transformer (ViT) architecture, a sample dataset of 3-image sequences will be created from stock chart data. This dataset will be used to evaluate the effectiveness of the ViT model with sequential candlestick data and to explore the integration of delta features, calculated via both image subtraction and feature subtraction. A comparative analysis of these two delta feature approaches will be conducted. This research will inform the final model architecture.

## I. Project Setup and Data Acquisition

This section details the initial project setup and the acquisition of the financial data required for training and evaluating the Vision Transformer (ViT) model. A robust data pipeline is crucial, especially given the project's focus on exploring varying ViT input sequence lengths. This section addresses the foundational aspects of data preparation, while acknowledging the need for flexibility to accommodate future experiments with different sequence lengths, masking, padding, and positional embeddings.

### A. Project Initialization

- **Documentation and Planning:** Meticulous documentation of the project's progress, design decisions, and evolving understanding of ViT input requirements is essential. This includes detailed tracking of experimental designs, rationale behind chosen approaches, and specific considerations regarding variable-length sequences, masking, and padding for the ViT model. The code repository should be organized to reflect the project's structure and accompanied by preliminary documentation outlining the overall approach.

- **Preparation for Discussion:** The codebase and documentation should be prepared to facilitate in-depth discussions about the project. This includes a well-structured repository and documentation that clearly outlines the approach and plans for handling variable-length ViT inputs.

### B. Data Acquisition and Preprocessing

The data acquisition and preprocessing steps are designed with flexibility in mind to support various input sequence lengths (N=3, 4, and 5) for the ViT model. The data structure should be adaptable to handle variable sequence lengths using techniques like masking and padding, accommodating the maximum sequence length while allowing for shorter sequences. While the specific implementation details for handling variable-length input in the ViT model are addressed in later sections, the data preparation process anticipates these future requirements.

- **Data Source:** Open, High, Low, Close, Volume (OHLCV) data, along with relevant index data, will be acquired from Yahoo Finance. Any necessary adjustments for stock splits and dividends will be performed.

- **Data Storage:** The acquired and preprocessed data will be stored in CSV files within designated training (`stock_data/train/`) and testing (`stock_data/test/`) directories. The CSV file format will be designed to facilitate the creation of varying input sequence lengths later in the project.

- **Data Preprocessing:** Subsequent steps involve creating candlestick chart images, calculating returns using the formula `(Close(t+h) - Open(t+1)) / Open(t+1)` (where 't' represents the last day of the 5-day window and 'h' represents the holding period), and integrating these elements. A 5-day sliding window will be used to create sequential input data, generating candlestick charts with volume and a moving average indicator for each window. These charts will serve as input for the CNN. Filename conventions and data organization will anticipate the need for future experimentation with different values of _N_ and the application of masking and positional embeddings during ViT model training. This proactive approach ensures a streamlined transition to later stages of model development.

### C. Memory Management and Evaluation

- **Memory Management Framework:** Before proceeding with large-scale data acquisition, a robust framework for evaluating different memory management strategies (Options A, B, and C) will be established. This framework will utilize the same dataset across all options, enabling a consistent comparison of their flexibility and efficiency.

## I. Project Setup and Data Acquisition

This phase establishes the project foundation and acquires the necessary financial data. While model development and training are central, robust data pipelines and careful setup are crucial for success. The project's core architectural decision to predict candlestick images instead of returns directly (d66925c7-57d3-4b58-b1ee-7e015a63dc9d) significantly influences data acquisition and preprocessing. This approach necessitates a focus on visual sequence forecasting, framing the problem as image-to-image prediction. Consequently, the project will generate and store candlestick chart image sequences rather than raw OHLCV data, requiring dedicated tools and libraries for image generation and storage during project setup.

To validate this image-based approach, a thorough conceptual soundness evaluation (607036c8-b9b9-4c09-b43b-bac7844fae06) is essential. This involves analyzing how human traders interpret candlestick patterns, providing a benchmark for model performance. The project setup will include documentation and analysis of existing literature on human candlestick pattern recognition.

Furthermore, a comprehensive analysis of the theoretical advantages (5a3ae327-335c-4c23-9a1c-aa7eae93cdc0) of this approach will be documented during project initialization. This analysis will consider factors such as the richness of visual representation, capacity for modeling uncertainty and causal reasoning, the type of training supervision required, interpretability of results, and the flexibility offered for generating synthetic data.

Finally, although not explicitly listed, the need for reusable components is acknowledged. Given the complexity of image data processing and potential model variations, reusable components for data loading, masking, and ViT wrappers (7646d8c7-14e4-47d0-851b-87497732ef96) will be developed early in the project setup phase to improve maintainability and extensibility.

### A. Project Initialization

1. **Document Thought Process (Continuously Updated):** This document will capture the rationale for choosing an image-based approach, outlining its advantages and disadvantages compared to traditional methods like scalar regression and probabilistic return models. The comparison will encompass output types, supervisory signals, relevance to trading, richness of learned structure, interpretability, risk of ambiguity, data requirements, and modeling complexity. The design and implementation process of the image-to-image candlestick forecaster prototype, including the chosen architecture (CNN decoder or transformer-based image generator), will also be documented and continuously updated as the project progresses.

2. **Prepare for In-Depth Discussion (Paper and Repository):** Materials for detailed project discussions will be prepared, including documentation within the code repository and initial drafts for the final dissertation. These materials will cover the chosen architecture, data acquisition process, and planned experiments.

### B. Data Acquisition and Preprocessing

Data acquisition and preprocessing are intertwined with the image-based representation. The following steps outline this integrated process:

1. **Data Source and Implications for Image Generation:** While Yahoo Finance will be the primary source for OHLCV data, the subsequent generation of candlestick images introduces specific requirements and challenges.

2. **Data Acquisition, Preprocessing, and Return Extraction:** OHLC and index data will be acquired and prices adjusted. Crucially, the process will consider the ultimate goal of extracting returns from the generated candlestick images. This will influence decisions regarding image resolution, data normalization, and the method for extracting open, close, high, and low values from these images. This extraction may involve pixel location analysis or rendering the predicted image data back into numerical form.

3. **Data Storage and Image Storage Considerations:** CSV files will be stored… (Continue detailing storage specifics)

## I. Project Setup and Data Acquisition

This section details the initial steps required to set up the project and acquire the financial data for training and evaluating the image-based prediction model. This involves defining the project's scope, gathering relevant data, preprocessing it into a suitable format (candlestick images and corresponding return labels), and organizing it for subsequent model development. While this section primarily addresses practical setup, it's crucial to consider the downstream implications of data acquisition and preprocessing on the model's eventual functionality and evaluation.

### A. Project Initialization

This subsection focuses on the initial steps for organizing the project and establishing best practices for documentation and collaboration.

1. **Continuous Documentation:** A detailed log will be maintained throughout the project lifecycle to document design decisions, challenges encountered, and solutions implemented. This ensures transparency and facilitates future analysis and improvements.

2. **Resource Preparation:** All relevant project materials, including research papers, code repositories, and data sources, will be organized and readily accessible for in-depth discussions and collaboration. This ensures a shared understanding among team members and enables effective communication.

### B. Data Acquisition and Preprocessing

This subsection outlines the processes for acquiring, preprocessing, and storing the financial data, specifically focusing on the creation of candlestick image sequences and their corresponding return labels.

1. **Data Source:** Historical financial data, including Open, High, Low, Close, and Volume (OHLCV) values, will be acquired from Yahoo Finance. This reliable source provides a comprehensive dataset for training and evaluation.

2. **Data Preprocessing and Adjustment:** The acquired OHLCV data will be preprocessed to ensure consistency and accuracy. This includes handling missing values, adjusting prices for corporate actions like splits and dividends, and incorporating relevant index data if necessary.

3. **Data Storage:** The preprocessed OHLCV data will be stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. A separate directory will be created to store the generated candlestick chart images, maintaining a clear separation between raw data and processed data.

4. **Candlestick Image Generation:** Candlestick charts will be generated from the OHLCV data, incorporating volume and a moving average (the specific period will be determined during experimentation). The image format (e.g., PNG) and resolution will be carefully chosen to balance visual clarity and computational efficiency, while ensuring the accurate extraction of price information later.

5. **5-Day Windowing (Image Sequence Generation):** 5-day windowing will be applied to the candlestick images, creating sequences of five consecutive candlestick images. These sequences will serve as the input to the prediction model.

6. **Return Label Calculation:** 5-day future returns will be calculated based on the original OHLCV data using the formula `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 'h' represents the holding period (5 days in this case). These calculated returns will serve as the ground truth labels for model training and evaluation.

7. **Return Extraction from Predicted Images (Methodology Development):** A robust method will be developed to extract open and close prices from the _predicted_ candlestick images. This is crucial for calculating returns based on the model's predictions and comparing them to the ground truth labels. This method will be a key focus of development and will likely involve image processing techniques.

8. **Data Integration and Filename Convention:** A system will be implemented to link each 5-day candlestick image sequence with its corresponding ground truth return label. This might involve a specific filename convention that encodes the sequence start date and holding period, or a separate CSV file mapping filenames to return labels. This ensures data integrity and facilitates efficient model training and evaluation.

This structured approach to project setup and data acquisition emphasizes the importance of aligning the data preparation process with the specific requirements of the image-based prediction model. The careful consideration of candlestick image generation, return label calculation, and the method for extracting price information from predicted images lays a strong foundation for subsequent model development and evaluation.

## I. Project Setup and Data Acquisition

This phase establishes the project's foundation and gathers the necessary data. It encompasses practical steps like data download and preprocessing, as well as crucial setup procedures to ensure a well-documented and reproducible project.

### A. Project Initialization

Before acquiring data, fundamental project setup is essential. This includes:

1. **Project Documentation:** Maintain a comprehensive record of the project's evolution. This involves continuously documenting the decision-making process, rationale behind chosen methods, encountered challenges, and their solutions. This documentation will be invaluable for tracking progress, understanding the project's trajectory, and ensuring reproducibility.

2. **Preparation for Review:** Organize project materials, including the code repository and any accompanying documentation (e.g., research paper), in a clear and accessible manner to facilitate effective communication and collaboration during project reviews.

### B. Data Acquisition and Preprocessing

This subsection details acquiring, cleaning, and transforming raw financial data into a format suitable for model training. This includes:

1. **Data Source:** Obtain Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance.

2. **Data Preprocessing:** Acquire the necessary OHLC and index data. Implement price adjustments for corporate actions, such as stock splits and dividends, to ensure data accuracy and consistency.

3. **Data Storage:** Store the preprocessed data in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **Windowing:** Implement a 5-day sliding window to create input sequences and corresponding output labels. This captures short-term price patterns for analysis.

5. **Return Label Calculation:** Calculate the return label, representing future return over a holding period (h), using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`. Here, 't' represents the current time step.

6. **Candlestick Chart Generation:** Generate candlestick charts from the windowed data, incorporating volume and moving averages to visualize price action.

7. **Integration of Return Labels and Charts:** Integrate the return label calculation with the candlestick chart generation process to ensure each image is associated with its corresponding label. For a 5-day holding period (h=5), these labels represent the 5-day future return.

8. **Filename Convention:** Implement a consistent filename convention that encodes the holding period. This can be achieved by embedding the holding period directly in the filenames or by maintaining a separate CSV file to track this information.

## I. Project Setup and Data Acquisition

This section outlines the initial project setup and data acquisition process, establishing a foundation for subsequent model development. While the current focus is practical implementation with financial data, the project maintains a connection to theoretical concepts related to dynamic coordinate systems and dimensionality reduction.

### A. Project Initialization

1. **Project Documentation:** A comprehensive log will document the project's evolution, including design decisions, challenges, and potential solutions. This ongoing documentation will ensure transparency and facilitate future refinements. This includes exploring connections to concepts like parallel transport, affine connections, and the SE(3) group, as they relate to the core concept of a dynamic 2D plane. Additionally, potential applications and limitations of this representation in fields like robotics and computer graphics will be considered. A concise algebraic summary of the dynamic 2D plane concept will be maintained, along with proposed next steps for theoretical exploration, including formulating an explicit rotation law and investigating curvature invariants.

2. **Discussion Materials:** A paper and code repository will be developed to facilitate in-depth discussions on the project's technical aspects. This includes documenting the transition from a fixed 3D frame to a dynamic 2D plane and the mathematical implications for representing curves and analyzing information balance and degrees of freedom. A key focus will be exploring the application of the 2D dynamic frame concept to the three-body problem, investigating representation challenges and potential solutions using relative coordinates, barycentric frames, or shape space representations. Recent advances in the three-body problem, such as machine learning approximations and new periodic solutions, will also be explored.

### B. Data Acquisition and Preprocessing

This subsection details the acquisition and preparation of financial data. The theoretical framework established in Project Initialization will inform the practical implementation described below.

1. **Data Source:** Open, High, Low, Close, and Volume (OHLCV) data will be acquired from Yahoo Finance.
2. **Data Preprocessing:** Acquired OHLC and index data will be preprocessed, including adjustments for stock splits and dividends.
3. **Data Storage:** Preprocessed data will be stored as CSV files in designated training (`stock_data/train/`) and testing (`stock_data/test/`) directories.
4. **Windowing:** A 5-day rolling window will be implemented for input and output data.
5. **Return Label Calculation:** Future returns will be calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 'h' represents the holding period.
6. **Candlestick Chart Generation:** Candlestick charts will be generated, incorporating volume and a moving average.
7. **Return Label Integration:** The return label calculation will be integrated with the candlestick chart generation process to ensure proper alignment.
8. **Filename Convention:** The filename convention will encode the holding period, or a separate CSV file will store this information.

## I. Project Setup and Data Acquisition

This section details the project setup and the acquisition and preprocessing of financial data from Yahoo Finance. This data will serve as an initial proxy for the more complex navigational data envisioned for the project's later stages.

### A. Project Initialization

1. **Project Documentation:** The project's progress, decisions, challenges, and conceptual understanding will be continuously documented within the project repository. This detailed record will mirror the investigative process common in research fields like egocentric and allocentric frames of reference.

2. **Dissemination Materials:** A formal paper and a well-structured project repository will be developed to facilitate in-depth discussion and external review. The repository will contain the code, data, and documentation, while the paper will present the research, findings, and implications. This approach reflects the importance of clear communication and reproducibility in research.

### B. Data Acquisition and Preprocessing

This subsection outlines the steps for acquiring and preparing the financial data for model training.

1. **Data Source:** Historical OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance.

2. **Data Acquisition and Preprocessing:** Along with OHLCV data, relevant index data will be acquired. Price adjustments for splits and dividends will be applied to ensure data accuracy.

3. **Data Storage:** The acquired and preprocessed data will be stored in CSV format within `stock_data/train/` (training data) and `stock_data/test/` (testing data).

4. **5-Day Windowing:** Input features and output labels will be organized into 5-day windows to capture short-term price movements.

5. **Return Label Calculation:** The target variable, representing the 5-day future return, will be calculated as `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the current time step and `h` is the 5-day holding period.

6. **Candlestick Chart Generation:** Candlestick charts, including volume and moving average indicators, will be generated from the OHLCV data to visualize price action.

7. **Return Label Integration:** The return label calculation will be integrated with the candlestick chart generation process to ensure alignment between visual input and target variables.

8. **Filename Convention:** The filename convention will encode the 5-day holding period to facilitate data management. This could involve incorporating the holding period directly in the filenames or using separate CSV files for different holding periods.

This structured approach ensures data quality and consistency for subsequent model development and evaluation. It draws inspiration from rigorous data handling practices in scientific research, ensuring data clarity and traceability.

## I. Project Setup and Data Acquisition

This section details the initial project setup and the acquisition of the necessary data, including the preparation of the project environment and the raw data required for subsequent analysis and visualization. While the project's core analysis revolves around a 2D rotational plane framework, the initial data acquisition and preprocessing will utilize a standard approach with OHLCV (Open, High, Low, Close, Volume) data before transitioning to the 2D representation. This two-stage process necessitates careful planning and documentation, especially regarding the visualization strategies for both the initial data exploration and the final 2D framework.

### A. Project Initialization

1. **Document thought process (Continuously update progress and thinking):** Meticulous documentation of the project's rationale and evolving understanding is crucial. This includes the motivation for using a 2D rotational plane framework, its conceptualization (including the dynamic origin and rotational axes), and the expected benefits for analyzing financial market movements. This documentation will be continuously updated throughout the project lifecycle, capturing design decisions, challenges encountered, and any deviations from the initial plan. The rationale for library selections (e.g., `matplotlib`) and discussions of visualization approaches (e.g., candlestick charts, 3D helix, 2D unfolded representation) will be included.

2. **Prepare for in-depth discussion (Paper and repository):** The chosen visualization strategies, their constraints (e.g., single chart preference, potential performance issues with animation), and the justification for using `matplotlib` while avoiding `seaborn` will be clearly documented in the project repository and prepared for discussion in the final paper.

### B. Data Acquisition and Preprocessing

This phase involves acquiring, preprocessing, and preparing the data for subsequent analysis and visualization. OHLCV data will be acquired from Yahoo Finance and stored as CSV files within dedicated directories (`stock_data/train/` and `stock_data/test/`). Preprocessing will include adjusting prices as needed and implementing a 5-day windowing logic for both input and output data.

**Data Visualization:** Visualizations will play a key role in understanding the data and communicating the project's findings. The following visualizations will be created using `matplotlib`:

- **Candlestick Charts:** 5-day candlestick charts, incorporating volume and moving average indicators, will provide a visual representation of the market data within each window. Return labels, calculated as `(Close(t+h) - Open(t+1)) / Open(t+1)` (where _h_ is the holding period from time t+1 to t+h), will be integrated into the candlestick chart generation process. Filenames or a separate CSV file will encode the holding period _h_.

- **Planar Trace Visualization:** The visualization of the planar trace in the moving chart (related to the 2D rotational framework) will be refined for clarity, potentially incorporating tooltips, annotations, or supplementary documentation.

- **3D Helix Visualization:** A separate 3D helix visualization will be generated, using labeled X, Y, and Z axes and the title "3D Helix in Laboratory Coordinates." Subplots will be avoided.

**Data Storage and Logging:** Data storage for planar coordinates (u, v), the orientation of the moving frame (potentially using quaternions or rotation vectors), and an arc length counter will be implemented. The storage mechanism will balance memory and storage limitations with visualization and rendering requirements. This stored data will serve as the source for several visualizations, especially those related to the 2D rotational plane framework.

## I. Project Setup and Data Acquisition

This section outlines the initial project setup and acquisition of financial data, emphasizing the foundational elements required for the subsequent dynamic data representation and modeling.

### A. Project Initialization

1. **Documentation:** Maintain comprehensive documentation of design decisions, including the rationale for the dynamic projection system, implementation specifics, anticipated impact on model learning, and explorations of alternative approaches. This documentation should be continuously updated throughout the project lifecycle.

2. **Communication & Reproducibility:** Prepare materials for in-depth discussions regarding the project, including presentations and documentation explaining the theoretical underpinnings of the 2D framework and the dynamic projection system. Establish a dedicated repository to house the code, data, and documentation, ensuring transparency and reproducibility.

### B. Data Acquisition and Preprocessing

This subsection details the acquisition and preprocessing of financial data, specifically OHLCV data from Yahoo Finance, for use within the 2D framework. The data will be structured into 5-day windows and transformed into candlestick charts for model input.

1. **Data Source:** Yahoo Finance.

2. **Data Acquisition and Preprocessing:** Acquire OHLCV and relevant index data. Adjust prices for splits and dividends to maintain data consistency. Preprocessing should also account for the requirements of the dynamic projection, potentially including normalization or other preparatory steps to ensure compatibility with subsequent transformations.

3. **Data Storage:** Store acquired data in CSV files within designated directories: `stock_data/train/` and `stock_data/test/`.

4. **5-Day Windowing:** Implement a 5-day rolling window mechanism for both input features and output labels. Carefully consider the interaction between the windowing logic and the dynamic projection system, especially the handling of transitions between consecutive windows.

5. **Return Label Calculation:** Calculate the return label using the formula `(Close(t+h) - Open(t+1)) / Open(t+1)`, where _h_ represents the holding period.

6. **Candlestick Chart Generation:** Generate candlestick charts from the 5-day window data, incorporating volume and moving average indicators. Consider the impact of the dynamic projection on chart generation and interpretation.

7. **Return Label Integration:** Seamlessly integrate return label calculation with candlestick chart generation, ensuring proper alignment between input and output data.

8. **Filename Convention:** Implement a clear filename convention that incorporates the holding period, or utilize separate CSV files to manage different holding periods. This is crucial for data integrity and clarity, especially given the dynamic nature of the data transformation.

9. **Dynamic Projection Considerations:** While the details of the dynamic projection (e.g., PCA-based rotation and re-centering) are addressed in later sections, its implications should be considered throughout the data acquisition and preprocessing stages. Document the rationale for integrating PCA into the network pipeline, whether applied to raw data, image embeddings, or as a network layer. Address technical challenges, such as backpropagation through dynamic transformations, ensuring proper gradient flow and exploring differentiable approximations if necessary. Prepare materials for in-depth discussions on these architectural choices and their implications for model training and performance.

## Project Setup and Data Acquisition

This section details the initial project setup, including data acquisition and the crucial preprocessing step of generating dynamically rotated candlestick images. This foundational work prepares the data for the model training phase.

### A. Project Initialization

This stage focuses on establishing a solid project foundation through documentation and planning.

1.  **Document Thought Process:** Continuously document progress and evolving ideas.
2.  **Prepare for In-Depth Discussion:** Prepare a research paper and a corresponding code repository.

### B. Data Acquisition and Preprocessing

This stage encompasses acquiring, preprocessing, and preparing financial data for generating dynamic candlestick images.

1.  **Data Source:** Acquire Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance.
2.  **Data Preprocessing:** Clean the OHLCV data, handling missing values, adjusting for stock splits and dividends, and incorporating relevant index data.
3.  **Data Storage:** Store the preprocessed data in CSV files within designated `stock_data/train/` and `stock_data/test/` directories.
4.  **5-Day Windowing:** Implement a 5-day rolling window for input and output data.
5.  **Return Label Calculation:** Calculate the return label using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 'h' represents the holding period.
6.  **Candlestick Chart Generation:** Generate candlestick charts from the windowed data, including volume and moving average indicators.
7.  **Return Label Integration:** Associate each candlestick chart image with its corresponding calculated 5-day future return.
8.  **Filename Convention:** Implement a clear file naming convention that encodes the holding period, either within the filename itself or using a separate CSV file.

### C. Dynamic Candlestick Rotation

This critical preprocessing step dynamically rotates candlestick data based on price movement, creating images that serve as model input.

1.  **Dynamic Snapshot Generation:** Rotate and re-center candlestick data according to the price movement vector, then redraw the data as an image with candlesticks positioned relative to the dynamically calculated X' and Y' axes. Time is not strictly horizontal in this representation.

2.  **Addressing Technical Challenges:** Implement solutions for the following challenges:

    - **Rotation Artifacts Handling:** Use anti-aliasing techniques to minimize distortions during image redrawing after rotation.
    - **Volatility Jump Handling:** Mitigate abrupt frame rotations from large price movements by using smoothing techniques or limiting the delta between rotation angles.
    - **Consistent Axis Scaling:** Maintain consistent axis scaling (units per percentage move) across all generated frames to avoid introducing biases.

3.  **Pseudocode Pipeline:** Develop a detailed pseudocode pipeline for the dynamic rotating snapshot generator, outlining the following steps:

    1.  Calculate the price movement vector.
    2.  Determine the rotation angle and matrix.
    3.  Re-center the data (dynamic origin shift).
    4.  Redraw the candlestick data as a rotated image.

## I. Project Setup and Data Acquisition

This phase establishes the project foundation and acquires the financial data necessary for model training and evaluation, specifically the candlestick data (time, price, and optionally volume) required for the dynamic plane generator. While this section focuses on standard data acquisition and preprocessing, it's crucial to consider the data's eventual transformation into a dynamic 2D plane, influencing decisions regarding data sources, storage formats, and initial preprocessing steps.

### A. Project Initialization

This stage involves setting up the project's foundational elements, including:

1. **Continuous Documentation:** Maintaining a detailed record of the project's evolution, including decisions, challenges, and rationale. This documentation will reside within the project repository and be continuously updated.
2. **Repository and Paper Preparation:** A dedicated repository will house the project code and documentation. A research paper draft will also be maintained and updated alongside the codebase to facilitate discussion and collaboration.

### B. Data Acquisition and Preprocessing

This phase details acquiring, preprocessing, and storing the financial data. This data will ultimately feed the dynamic plane generator, which transforms the 3D data (time, price, and volume) into a dynamic 2D representation. The following steps outline this process:

1. **Data Source Identification and Acquisition:** Identify and acquire the necessary OHLCV (Open, High, Low, Close, Volume) data from a reliable source like Yahoo Finance. Timestamps will also be acquired to represent the time dimension.

2. **Data Preprocessing and Formatting:** This step prepares the raw OHLCV and timestamp data for the dynamic 2D plane implementation. While the actual transformation occurs in a later stage, this phase ensures data compatibility. This includes:

   - **Data Cleaning and Validation:** Handling missing values, outliers, and ensuring data integrity.
   - **Data Structuring:** Organizing the data into a suitable format for efficient access and processing by the dynamic plane generator. This may involve creating specific data structures to facilitate the subsequent windowing and PCA calculations.
   - **Storage:** Storing the preprocessed data in a persistent and easily accessible format.

The subsequent dynamic plane generation, outlined below but implemented later, will involve projecting this data onto a 2D plane using dynamically rotating axes derived from PCA on local price movements. This context informs the current data acquisition and preprocessing steps.

- **Dynamic 2D Plane Generation (Future Implementation):** The acquired data will be used to generate a dynamic 2D plane. This involves:
  - **Local Frame Definition using PCA:** Defining a dynamic coordinate system using PCA applied to a local window of candlestick data.
  - **Dynamic Origin Refocusing:** Dynamically shifting the origin of the 2D plane with each market movement (change in price and time).
  - **Rotating Snapshot Generation:** Creating rotated snapshots of the 2D plane at each time step, capturing the dynamic representation of the market data. A module, potentially named `RotatingSnapshotGenerator`, will encapsulate this transformation. A conceptual diagram will be included to illustrate its operation.
  - **Angle Theta Re-evaluation:** The calculation of the rotation angle (theta) will be re-evaluated within the context of the dynamic origin and rotating axes.

This data acquisition and preprocessing phase lays the essential groundwork for the subsequent dynamic plane generation and analysis, ensuring the data is appropriately formatted and structured for this complex transformation.

## I. Project Setup and Data Acquisition

This section details the acquisition and preparation of financial data for training and evaluating the models. This includes generating candlestick chart images for input into Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).

### A. Project Initialization

- **Documentation:** Thorough documentation of decisions, rationale, and any changes in direction will be maintained throughout the project lifecycle. This includes documenting the data acquisition and preprocessing steps.

- **Dissemination:** A paper and corresponding code repository will be developed to facilitate in-depth analysis and discussion of the project's findings.

### B. Data Acquisition and Preprocessing

The following steps outline the data acquisition, preprocessing, and preparation process:

1. **Data Source:** Yahoo Finance will be the primary source for obtaining Open, High, Low, Close, and Volume (OHLCV) data for the selected financial instruments.

2. **Data Acquisition and Adjustment:** Acquire OHLC and relevant index data. Adjust prices for splits and dividends to ensure data accuracy.

3. **Data Storage:** Acquired and preprocessed data will be stored in CSV format within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** A 5-day sliding window approach will be used to prepare the data. Each data point will consist of information from the preceding five trading days.

5. **Return Label Calculation:** The return label will be calculated using the following formula, where 't' represents the current time step and 'h' is the holding period (h=5): `(Close(t+h) - Open(t+1)) / Open(t+1)`. This represents the return from the open of the next day (t+1) to the close of the final day in the holding period (t+h).

6. **Candlestick Chart Generation:** Candlestick chart images visualizing the 5-day window data will be generated. These charts will include volume information and a moving average (the specific type and period will be determined later).

7. **Return Label Synchronization:** The return label calculation will be synchronized with candlestick chart generation, ensuring each image has a corresponding return label.

8. **Filename Convention:** A clear filename convention will be established, either embedding the holding period within the filename or utilizing separate CSV files for different holding periods. This ensures easy identification and retrieval of data.

This process delivers the candlestick data (time, price, and volume) required for the dynamic plane generation described in a later section. These steps form the basis for subsequent model development and evaluation.

## I. Project Setup and Data Acquisition

This section details the initial project setup and the process of acquiring and preparing the necessary financial data. It covers project initialization, data sources, preprocessing steps, and data storage. While some aspects of data manipulation are relevant for later model training, this section focuses specifically on acquiring the raw data and performing basic preprocessing.

### A. Project Initialization

1. **Documentation:** Maintain comprehensive documentation throughout the project, including design decisions, challenges, and solutions. This will ensure transparency, reproducibility, and facilitate future development.
2. **Discussion Preparation:** Organize project documentation and the code repository in a clear and accessible manner to support effective communication and collaboration.

### B. Data Acquisition and Preprocessing

The following outlines the planned data pipeline. Specific implementation details will be documented as the project progresses.

1. **Data Source:** Yahoo Finance will be used to acquire OHLCV (Open, High, Low, Close, Volume) data.
2. **Data Acquisition and Preprocessing:** Standard preprocessing techniques will be applied to ensure data quality and consistency. This includes acquiring necessary index data and adjusting prices for corporate actions such as splits and dividends.
3. **Data Storage:** Acquired and preprocessed data will be stored as CSV files in designated training and testing directories (`stock_data/train/` and `stock_data/test/`, respectively).
4. **5-Day Windowing:** Data will be organized into 5-day windows for both model input and corresponding output labels.
5. **Return Label Calculation:** Return labels will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where _h_ represents the holding period. This calculates the return from the open of the day following the 5-day window.
6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day window data, incorporating volume and moving average indicators. These images will serve as input to the CNN/ViT models. Example images of these raw candlestick inputs will be provided.
7. **Return Label Integration:** Return label calculation will be integrated with the candlestick chart image generator to ensure synchronization between input images and target outputs.
8. **Filename Convention:** The filename convention will encode the holding period (h), either within the filename itself or through a separate CSV file.

## II. Dynamic Plane Transformations

After generating the initial candlestick chart images, they will be transformed using a Rotating Dynamic Plane Generator. This process will be documented and visualized as follows:

- **Transformed Candlestick Images:** Example images of the transformed candlestick data will be provided. These examples will illustrate the effect of the dynamic plane transformations (refocusing of origin and rotation).
- **Dynamic Redrawing Visualization:** An animation will visualize the step-by-step evolution of the plane as new data points are added, illustrating the refocusing of the origin and the rotation of the axes.

The Rotating Dynamic Plane Generator adheres to the following:

- **Image Rendering:** The generator will produce image representations suitable for CNN/ViT models.
- **Minimum Data Points:** The generator requires at least two data points for dynamic plane calculations.

## I. Project Setup and Data Acquisition

This section details the initial steps of the project, encompassing project initialization, data acquisition, and preprocessing. It outlines the process of gathering, preparing, and structuring financial data for subsequent model training and visualization. Addressing potential visualization challenges stemming from data sparsity in the early stages of data acquisition is also considered.

### A. Project Initialization

1. **Document Thought Process (Continuously update progress and thinking):** Maintain comprehensive documentation of the evolving project rationale, design decisions, and encountered challenges. This living document will be invaluable for tracking progress, facilitating collaboration, and ensuring reproducibility.

2. **Prepare for In-Depth Discussion (Paper and repository):** Organize project materials, including preliminary documentation and initial code repositories, to support in-depth discussions about project architecture, design, and data handling strategies. This preparation fosters effective communication and a shared understanding of the project scope.

### B. Data Acquisition and Preprocessing

This subsection details acquiring, preprocessing, and structuring financial data from Yahoo Finance. Considerations for handling early-stage data visualization challenges related to PCA and animation are also addressed.

1. **Data Source: Yahoo Finance (Acquire OHLCV data):** The primary data source will be Yahoo Finance, providing Open, High, Low, Close, and Volume (OHLCV) data for the selected financial instruments.

2. **Data Acquisition and Preprocessing (Acquire OHLC, Index data, adjust prices):** Acquire the necessary OHLCV data and any relevant index data. Implement necessary price adjustments, such as those for stock splits and dividends, to ensure data accuracy.

3. **Data Storage Location (CSV files in `stock_data/train/` and `stock_data/test/`):** Store the acquired and preprocessed data in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing (Implement 5-day window logic for input and output):** Implement a 5-day windowing strategy for both input features and output labels, defining the timeframe for analysis and prediction.

5. **Return Label Formula (Calculate return: `(Close(t+h) - Open(t+1)) / Open(t+1)`):** Calculate the return label using the specified formula, where `t` represents the current time step and `h` represents the 5-day holding period.

6. **Create Candlestick Chart Images (Generate candlestick charts with volume and moving average):** Generate candlestick chart images, incorporating volume information and potentially a moving average indicator, for use as model input.

7. **Create Return Labels (Calculate 5-day future returns):** Calculate the 5-day future returns to serve as labels during model training.

8. **Integrate Return Label Calculation with Candlestick Chart Image Generator:** Ensure synchronization between the return label calculation and candlestick chart image generation.

9. **Update Filename Convention (Encode holding period in filenames or separate CSV):** Employ a consistent filename convention, encoding the holding period in filenames or using separate CSV files for different holding periods to facilitate data management.

10. **Addressing Early-Stage Data Visualization Challenges:** The following considerations mitigate potential issues arising from limited data points in early-stage visualizations:

    - **PCA Instability:** Principal Component Analysis (PCA) calculations can be unstable with limited or collinear data, potentially affecting dynamic plane rotation. Delay PCA calculations until sufficient data points are available.
    - **Minimum Points for Animation:** The animation framework requires at least two data points. Handle single-point frames gracefully, potentially by displaying a placeholder or an empty canvas.
    - **Format Offsets:** Ensure correct formatting of offsets, especially with limited data, to prevent dimension mismatches and ensure accurate calculations and animation behavior.

By addressing these considerations during project setup and data acquisition, we establish a robust foundation for handling both early-stage data sparsity and later-stage data richness in visualizations and analysis.

## Data Preparation and Processing

This section details the preparation and processing of financial data, including generating candlestick charts, calculating returns, and structuring the data for model input. It covers both the preparation of real market data from Yahoo Finance and the generation of synthetic data for robust testing.

### I. Real Market Data Processing

This subsection outlines the steps to prepare data downloaded from Yahoo Finance for model training and evaluation.

1. **Data Storage (CSV Files):** Acquired and preprocessed data will be stored in CSV files within dedicated directories: `stock_data/train/` for the training dataset and `stock_data/test/` for the testing dataset.

2. **5-Day Windowing:** A 5-day sliding window approach will be implemented for both input features (candlestick data) and output labels (future returns). The model will receive five consecutive days of candlestick data as input and predict the returns over the subsequent five days. This aligns with the requirement for a batch generator for candlestick images.

3. **Return Label Calculation:** The return label will be calculated using the following formula: `(Close(t+5) - Open(t+1)) / Open(t+1)`. This calculates the percentage return over the 5-day holding period.

4. **Candlestick Chart Generation:** Candlestick chart images will be generated from the OHLCV data, incorporating volume and moving average indicators for additional context. While the project explores Heiken-Ashi candles, this step focuses on generating standard candlestick charts.

5. **Data Integration:** The processes of generating candlestick chart images and calculating corresponding return labels will be integrated to ensure data consistency.

6. **Filename Convention:** A consistent filename convention will be used for candlestick chart images and their corresponding return labels. The 5-day holding period will be encoded in the filenames or managed through separate CSV files for clear organization.

### II. Heiken-Ashi Data Processing and Visualization

This subsection details the generation, transformation, and visualization of Heiken-Ashi candlestick data, leveraging both real market data and synthetically generated data.

#### A. Heiken-Ashi Generation and Visualization (Real and Synthetic Data)

1. **Heiken-Ashi Candle Generation:** A function will generate Heiken-Ashi candles from standard OHLC data, accepting an array of standard candles and returning an array of Heiken-Ashi candles.

2. **Heiken-Ashi Plotting:** A function will visualize the generated Heiken-Ashi candlesticks, with green bodies for up moves (close >= open) and red bodies for down moves (close < open).

3. **Dynamic Rotation and Recentering:** A function will dynamically rotate and recenter the Heiken-Ashi data based on the midpoint between the open and close values for each candle.

4. **Rotated Heiken-Ashi Plotting:** A dedicated function will plot the dynamically rotated and recentered Heiken-Ashi data on a 2D plane.

5. **Chart Saving:** Both standard and rotated Heiken-Ashi charts will be saved as PNG files to designated paths (e.g., `/mnt/data/standard_heiken_ashi.png`, `/mnt/data/rotated_dynamic_plane_heiken_ashi.png`).

#### B. Synthetic Data Generation and Heiken-Ashi Visualization

1. **Synthetic Data Generation (Choppy Data):** The `generate_choppy_candlesticks(n=30)` function will create simulated choppy market data with _n_ data points, including times, opens, highs, lows, and closes. This data will be used to rigorously test the dynamic plane implementation.

2. **Heiken-Ashi Chart Generation (Standard and Rotated):** Both standard and rotated Heiken-Ashi charts will be generated from the synthetic choppy data using the previously defined functions. These charts will be saved as `/mnt/data/standard_heiken_ashi_choppy.png` and `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`, respectively. The rationale for using synthetic choppy data is to test the dynamic plane's robustness under challenging market conditions, such as sideways movements and high volatility. This will be documented in the project repository and the dissertation, including parameter choices, visualizations, and insights.

This dual approach, utilizing both real-world and synthetic data, provides a comprehensive evaluation of the Heiken-Ashi transformations and the dynamic plane's effectiveness under various market conditions.

## I. Project Setup and Data Acquisition

This section details the initial project setup and the acquisition of the necessary financial data. Although the project's core focuses on geometric pattern recognition within a dynamic coordinate system generated by Principal Component Analysis (PCA), this phase establishes the groundwork for that analysis by creating a robust data pipeline and defining the PCA parameters.

### A. Project Initialization

1. **Project Documentation:** A comprehensive log will track the project's evolution, including design choices, rationale for decisions, challenges encountered, and potential solutions. This continuous documentation ensures a clear and traceable development process.
2. **Project Resources:** A version-controlled repository will house the project code, data, and documentation. A preliminary paper outline will be drafted, outlining the key research questions, proposed methodology, and expected outcomes, serving as a basis for discussions and refinement.

### B. Data Acquisition and Preprocessing

The data acquisition and preprocessing phase is structured to meet the requirements of the subsequent PCA transformation and geometric pattern recognition.

1. **Data Acquisition:** Financial data, including price, time, and volume, will be acquired from a suitable source (e.g., Yahoo Finance). While the model aims to learn relationships between data points rather than relying on fixed meanings of these features, they are essential as the model will learn from patterns in the derived dynamic coordinate system. Specific tickers and timeframes will be determined based on the research objectives.
2. **Data Preprocessing:** The acquired data will be preprocessed for PCA. This includes cleaning, and potentially normalizing or standardizing, to ensure the PCA transformation is not skewed by differing scales or magnitudes between features. Crucially, this stage includes:
   - **Windowing and Smoothing:** A rolling window approach (e.g., 5-day) will be implemented. Smoothing techniques or stability thresholds will mitigate noise-driven rotations in the PCA space. The rationale for the chosen window size and smoothing method will be documented.
3. **Data Storage:** Preprocessed data will be stored in a structured format (e.g., CSV files in designated train/test directories) for efficient access and management during later project stages. The filename convention will clearly indicate the holding period associated with each data point (e.g., encoding it in the filename or using separate files). Target variables (returns) will be calculated using the formula `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window and `h` is the holding period. Candlestick charts, including volume and a moving average indicator, will be generated from the windowed data to serve as input for the CNN model. These charts will be aligned with their corresponding calculated return labels.

This structured approach to data acquisition and preprocessing establishes a solid foundation for subsequent model development and training, allowing the model to learn relationships within the dynamically rotated PCA space effectively.

## I. Project Setup and Data Acquisition

This phase establishes the project foundation and acquires the necessary data for model training and evaluation. It begins with defining the project scope and preparing for in-depth discussions, followed by establishing a robust data acquisition and preprocessing pipeline.

### A. Project Initialization

1. **Document Thought Process:** A detailed record of the project's conceptualization, including initial ideas, assumptions, and anticipated challenges, will be maintained and continuously updated. This "living document," residing within the project repository, will track the project's evolution and justify design decisions.

2. **Prepare for In-depth Discussion:** To facilitate collaboration, a preliminary paper outlining project goals, methodology, and expected outcomes will be prepared. The project repository will be organized and well-documented for easy navigation and contribution.

### B. Data Acquisition and Preprocessing

This subsection details the acquisition and preparation of financial data, creating a reproducible data pipeline.

1. **Data Source:** Financial data, specifically OHLCV (Open, High, Low, Close, Volume) data, will be acquired from Yahoo Finance. Specific tickers and the time period will be defined based on project requirements.

2. **Data Acquisition and Preprocessing:** Raw OHLCV data will be acquired and preprocessed. This includes acquiring relevant index data (if necessary) and adjusting prices for splits and dividends to ensure data consistency.

3. **Data Storage Location:** Processed data will be stored as CSV files in `stock_data/train/` (training data) and `stock_data/test/` (testing data).

4. **Windowing:** A 5-day sliding window will be used for both input features and output labels. The rationale and implications of this choice will be documented.

5. **Return Label Calculation:** The target variable, representing the return over the holding period (h), will be calculated as: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window. The rationale for this calculation will be documented.

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day window data, including volume and potentially a moving average indicator. If a moving average is used, its parameters will be specified.

7. **Label Integration:** The calculated return labels (5-day future returns) will be directly associated with their corresponding candlestick chart images.

8. **Filename Conventions:** The holding period (h) will be encoded in the filenames of the candlestick chart images or stored in a separate CSV file mapping images to their respective holding periods.

9. **Further Preprocessing:** Any additional data preprocessing steps, informed by advanced concepts, will be documented here.

## I. Project Setup and Data Acquisition

This section details the initial steps of setting up the project environment and acquiring the necessary financial data. These foundational elements are crucial prerequisites for subsequent model development and training.

### A. Project Initialization

1. **Continuous Documentation:** Maintain a comprehensive log of the project's evolution, including design decisions, rationale, challenges encountered, and potential solutions. This ongoing documentation will be invaluable for tracking progress, facilitating collaboration, and ensuring reproducibility.

2. **Preparation for Collaboration:** Organize project materials, including code repositories and documentation (e.g., research papers, reports), to facilitate in-depth discussions and collaboration. Implement clear version control and documentation practices within the repository.

### B. Data Acquisition and Preprocessing

This subsection outlines the data acquisition and preprocessing procedures.

1. **Data Source:** Financial data, including Open, High, Low, Close, and Volume (OHLCV) data, will be acquired from Yahoo Finance.

2. **Data Acquisition and Preprocessing:** The acquired OHLCV data will be preprocessed. This includes acquiring relevant index data and adjusting prices for corporate actions such as stock splits and dividends to ensure data consistency.

3. **Data Storage:** Processed data will be stored as CSV files in designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** A 5-day sliding window approach will be implemented for time series analysis. Input sequences will consist of five consecutive days of OHLCV data.

5. **Return Calculation:** The future return for a holding period _h_, denoted as _e_, will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where _t_ represents the current time step.

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day windowed data, incorporating volume information and moving averages. These charts will serve as visual representations of market trends and will be used as model input.

7. **Return Label Integration:** The calculated return labels (future returns over the holding period _h_) will be directly linked to the corresponding candlestick chart images, enabling the model to associate visual patterns with future price movements.

8. **Filename Convention:** The filenames of the candlestick chart images or a separate CSV file will encode the holding period _h_. This ensures a clear and organized data structure.

This structured approach to data acquisition and preprocessing lays the foundation for the subsequent model development and training phases, enabling efficient data handling and analysis. Further checklist items related to error handling and dynamic frame adjustments will be addressed in later stages of the project.

## I. Project Setup and Data Acquisition

This section details the initial steps of the project, encompassing project initialization, environment setup, and the acquisition and preprocessing of the necessary financial data. Thorough documentation of these steps is crucial for reproducibility and understanding the project's evolution.

### A. Project Initialization

This subsection focuses on the initial setup and organization of the project, laying the groundwork for subsequent stages.

- **1. Project Conceptualization and Documentation:** Maintain a comprehensive record of the project's development, including the initial motivations, underlying assumptions, anticipated challenges, and design decisions. This living document should be continuously updated as the project progresses and new insights emerge. It will serve as a valuable resource for reflecting on design choices, explaining the project's evolution, and ensuring alignment with core objectives. Crucially, consider the future implications of error trend detection and deviation vector monitoring on the overall project direction.

- **2. Stakeholder Communication and Collaboration:** Prepare all necessary materials, such as preliminary research papers and code repositories, for in-depth discussions with stakeholders. Clear and well-organized documentation, including preliminary documentation on data preprocessing methods and their rationale, will facilitate productive feedback and ensure alignment on project goals and methodologies. Anticipate potential questions regarding data suitability for error trend detection and deviation vector monitoring.

### B. Data Acquisition and Preprocessing

This subsection details the process of acquiring, cleaning, transforming, and storing the raw financial data into a format suitable for model training. The data acquisition and preprocessing pipeline is designed with future implementation of error analysis and vector calculations in mind, specifically the calculation of realized movement vectors and deviations from predicted vectors within a dynamic 2D plane.

- **1. Data Source:** Yahoo Finance will be the primary data source for Open, High, Low, Close, and Volume (OHLCV) data.

- **2. Data Acquisition:** Collect OHLCV and relevant index data. Apply necessary price adjustments for splits and dividends to maintain data consistency.

- **3. Data Storage:** Store processed data in CSV files within dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

- **4. Data Windowing:** Implement a 5-day rolling window approach to structure the data and capture short-term price movements.

- **5. Return Label Calculation:** Calculate the target variable (return over holding period _h_) using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`.

- **6. Candlestick Chart Generation:** Generate candlestick charts incorporating volume and a moving average indicator for visual representation of price action.

- **7. Return Label Integration:** Integrate the return label calculation into the candlestick chart generation process to link each chart with its corresponding return.

- **8. Filename Convention:** Update the filename convention for charts and data files to include the holding period information, either directly in the filename or through a separate CSV file.

### C. Forward-Looking Considerations: Data Representation and Error Analysis

While not implemented in this stage, the following considerations regarding error calculation and interpretation within the dynamic 2D plane representation are crucial for future development and are documented here for transparency and planning.

- **Clarification of Rotational Angles and Distance Vectors:** Verify and document the precise number of rotational angles and distance vectors used in the dynamic plane representation, ensuring consistency between implementation and documentation. This clarification is essential for accurate interpretation of data transformations within the dynamic plane.

- **Error Correction Calculation:** The specific methodology for error correction calculation within the dynamic plane representation will be detailed in a later section, once the implementation details are finalized. This will include a clear explanation of how deviations from predicted vectors are analyzed and used for error correction.

## I. Project Setup and Data Acquisition

This section details the initial steps of setting up the project and acquiring the necessary data. While this phase focuses on data preparation, it also lays the groundwork for consistent analysis and error management throughout the project lifecycle. Specifically, downstream considerations regarding model prediction evaluation and comparison against actual market movements inform the architectural decisions outlined below.

This phase includes preparing the data sources and formatting the data for the model. Although the dynamic frame calculations occur later, the data acquired and preprocessed here will be crucial for those calculations. Given the short-term prediction horizon (1-5 candlesticks), dynamic frame adjustments are not implemented in this initial phase. We assume structural market drifts within this timeframe are negligible compared to potential predictive errors, thus avoiding continuous recalculation of the dynamic frame unless dealing with high-frequency trading scenarios.

However, the data must be structured to allow for later implementation of dynamic frame adjustments. Therefore, this stage focuses on acquiring and preparing the raw data without applying dynamic transformations. Specific implementation details for dynamic frame adjustments, including pseudocode, visual simulations, data structure design for the PCA basis, and visualizations of freeze-frame and reprojection functionalities, are deferred to the Model Development and Implementation section.

### A. Project Initialization

This stage covers the initial project setup. Thorough documentation and preparation are key for a smooth transition to more technically demanding stages.

1. **Continuous Documentation:** Maintaining comprehensive documentation of the project's evolution, rationale, decisions, insights, and challenges is crucial for maintaining project understanding and facilitating communication. This documentation will serve as a living record of the project's development.

2. **Preparation for In-depth Discussion:** Preparing supporting materials, such as preliminary documentation and a structured code repository, is essential for productive discussions and collaboration. These resources will provide a shared context and facilitate a deeper understanding of the project's technical aspects.

### B. Data Acquisition and Preprocessing

This phase covers the acquisition and preprocessing of financial data for model training and evaluation. Specifics of data sources, storage, and preprocessing steps will be detailed in subsequent documentation. The following architectural considerations will influence how the acquired data is used and interpreted within the model:

- **PCA Plane Consistency:** Ensuring the 2D plane formed by Principal Component Analysis (PCA) remains consistent between predicted and realized price and volume matrices is crucial. This necessitates investigating whether a transformation or alignment step is necessary to maintain comparability and will affect how the price and volume data are preprocessed.

- **Freezing the Dynamic Frame for Prediction:** The dynamic PCA frame generated at time 't' will be frozen for predicting and evaluating market movements over the short prediction horizon. This assumes market structure remains relatively stable within this timeframe, simplifying error calculation by maintaining a consistent frame of reference. This requires saving the dynamic frame's state at each time 't'.

- **Reprojecting Realized Movement:** The realized movement at 't+1' will be reprojected back into the PCA frame established at 't' using the original PCA basis (rotation matrix). This allows direct comparison within the initial prediction frame, even if PCA recalculated at 't+1' yields different results. This requires storing the rotation matrix used at time 't' with the data.

The error correction mechanism, utilizing Distance and Angle Error metrics calculated between predicted and realized displacement and direction vectors (excluding the initial frame creation rotation), will be fully detailed in the Model Evaluation section, along with visualizations illustrating the relationship between rotations, the dynamic plane, and prediction errors.

## I. Project Setup and Data Acquisition

This section details the initial steps of the project, focusing on setting up the project environment and acquiring the necessary financial data for model training and evaluation. This includes data collection, preprocessing, and establishing a foundation for documenting the project's progress.

### A. Project Initialization

1. **Document Thought Process:** A detailed record of the project's development, including design decisions, challenges, and evolving strategies, will be maintained. This documentation will provide valuable context for understanding the project's direction and a comprehensive history of its progress.

2. **Prepare for Collaboration:** A central repository for code and documentation, along with a structured research paper, will be established to facilitate collaboration and ensure all project information is easily accessible.

### B. Data Acquisition and Preprocessing

The following steps outline the data acquisition and preprocessing pipeline:

1. **Data Source:** Historical stock price data, including Open, High, Low, Close, and Volume (OHLCV), will be acquired from Yahoo Finance.

2. **Data Acquisition and Preprocessing:** The acquired OHLCV data, along with relevant index data, will undergo preprocessing, including price adjustments. Details of these steps will be provided in subsequent sections.

3. **Data Storage:** Processed data will be stored as CSV files in the `stock_data/train/` and `stock_data/test/` directories. The organization and structure of these files will be further detailed.

4. **5-Day Windowing:** A 5-day sliding window will be implemented to prepare the data for input to the model and for calculating the target variable.

5. **Return Label Calculation:** 5-day future returns will serve as the target variable and will be calculated as `(Close(t+5) - Open(t+1)) / Open(t+1)`. A detailed explanation and implementation of this calculation will be provided.

6. **Candlestick Chart Generation:** Candlestick charts, incorporating volume and moving averages, will be generated from the processed data.

7. **Integration of Return Labels and Charts:** The calculated 5-day future returns (target variable) will be integrated with the candlestick chart generation process. The specifics of this integration will be detailed.

8. **Filename Convention:** A consistent filename convention will be used, potentially encoding the holding period within the filenames or using a separate CSV file for tracking. Details of this convention will be explained.

### C. Implications of Algorithmic Considerations on Data Preprocessing

While the specifics of algorithm development will be covered in later sections, certain considerations influence the data acquisition and preprocessing stages:

- **Robust PCA Frame Management:** The chosen method for PCA frame management (either Freeze Frame or Reproject Realization) will influence how the data is stored and processed to maintain consistency between predicted and realized data points, even with potential shifts in market data distribution. This may affect decisions regarding data frequency, normalization, and the timeframe for analysis.

- **Data Normalization:** Data normalization is anticipated to be crucial due to differing units and scales across various data elements. Appropriate normalization strategies will be incorporated during preprocessing.

- **Error Calculation:** The anticipated error calculation, involving vector deviation error, angular error, and PCA angle errors, informs the data preprocessing stage. Data will be structured to facilitate efficient calculation of these components.

- **Confidence Indicators:** Frame drift error will be calculated and stored during preprocessing for use as a confidence indicator.

## I. Project Setup and Data Acquisition

This section details the initial setup and the process of acquiring and preparing the necessary financial data for model training and evaluation.

### A. Project Initialization

This stage focuses on establishing a structured foundation for the project and fostering clear communication.

1. **Document Thought Process:** A detailed log of the project's development, including decisions, rationale, and challenges, will be maintained. This ensures transparency, reproducibility, and provides valuable context for future analysis.

2. **Prepare for In-Depth Discussion:** Relevant materials, such as research papers and the code repository, will be organized to facilitate effective communication and collaboration.

### B. Data Acquisition and Preprocessing

This phase outlines the acquisition, preprocessing, and preparation of financial data for model input.

1. **Data Source:** Financial data, including Open, High, Low, Close, and Volume (OHLCV), will be acquired from Yahoo Finance.

2. **Data Preprocessing:** Along with OHLCV data, relevant index data will be collected. Historical prices will be adjusted for splits and dividends to ensure accuracy.

3. **Data Storage:** Processed data will be stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** Data will be organized into 5-day sliding windows to capture short-term price movements and trends, serving as input for the model.

5. **Return Label Calculation:** The target variable, representing the future return, will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the current time and `h` is the holding period (5 days).

6. **Candlestick Chart Generation:** Candlestick charts will be generated for each 5-day window, incorporating volume and a moving average indicator. These charts will serve as visual representations of the data and potentially as input features for the model.

7. **Return Label Integration:** The calculated return labels will be directly associated with their corresponding candlestick chart images.

8. **Filename Convention:** A consistent filename convention will be implemented to encode the holding period, ensuring clear identification and efficient data management. This might involve incorporating the holding period directly into the filename or using a separate CSV file to map filenames to their respective holding periods and return labels.

## I. Project Setup and Data Acquisition

This section details the acquisition and preparation of the financial data required for training and evaluating the CNN model. This includes downloading, preprocessing, and storing the data in a suitable format. A key aspect of this stage is the creation of candlestick chart images and corresponding return labels.

### A. Project Initialization

1. **Project Documentation:** Meticulous documentation of the project's evolution, including design decisions, rationale, challenges encountered, and solutions implemented, will be maintained throughout the project lifecycle. This comprehensive record will be crucial for understanding the final model, reproducing results, and facilitating in-depth discussions.

2. **Preparation for Review:** All necessary materials, including the research paper and code repository, will be organized and prepared for review and discussion. This will ensure clear communication of the project's goals, methodology, and findings.

### B. Data Acquisition and Preprocessing

1. **Data Source and Acquisition:** Financial market data will be acquired from [Specify Data Source]. This data will include [Specify Data Points, e.g., open, high, low, close, volume] for [Specify Assets, e.g., specific stocks, indices].

2. **Data Preprocessing:** The acquired data will be preprocessed to create the input for the CNN model:

   a. **Candlestick Chart Generation:** Five-day candlestick charts will be generated, incorporating volume and moving average indicators.

   b. **Return Label Calculation:** Corresponding return labels will be calculated for each 5-day window, representing the future return over the holding period (h). The return will be calculated as `(Close(t+h) - Open(t+1)) / Open(t+1)`.

   c. **Data Alignment:** The return label calculation will be integrated with the candlestick chart generation process to ensure proper alignment between the visual input and the target variable.

3. **Data Storage and Management:** The generated candlestick chart images and corresponding return labels will be stored [Specify Storage Method and Location, e.g., in a dedicated directory within the project repository, in a cloud storage bucket]. The filename convention will clearly indicate the holding period (h) associated with each data point, either by encoding it within the filename or by storing data for different holding periods in separate directories or files. This will facilitate efficient data management and analysis.

## I. Project Setup and Data Acquisition

This section details the steps involved in setting up the project and acquiring the necessary data, including crucial data transformations for effective model input.

### A. Project Initialization

- **Documentation:** Continuously document the project's progress and rationale behind decisions, particularly concerning data transformations and their anticipated impact on model performance. This documentation should be a living document, updated as the project evolves.
- **Preparation for Discussion:** Prepare comprehensive materials for in-depth project discussions, including a dedicated section in the research paper and clear documentation within the project repository, specifically addressing the data transformation process.

### B. Data Acquisition and Preprocessing

This subsection outlines the acquisition of financial data from Yahoo Finance and its preprocessing for use in the model. Several key transformations enhance data representation and prepare it for model input, including specific preprocessing for Principal Component Analysis (PCA).

- **Data Source and Storage:** Financial data, including Open, High, Low, Close, and Volume (OHLCV), along with relevant index data, will be acquired from Yahoo Finance. The acquired data will be adjusted as needed and stored as CSV files in designated directories: `stock_data/train/` and `stock_data/test/`.

- **5-Day Windowing:** Data will be organized into 5-day windows, which will serve as individual inputs for the model.

- **Return Label Calculation:** The target variable, or return label, will be calculated as `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `h` represents the holding period. The filename convention will incorporate the holding period information, either embedded within the filenames or through a separate CSV file.

- **Candlestick Chart Generation:** Candlestick charts, incorporating volume and moving average indicators, will be generated and integrated with the calculated return labels.

- **Data Transformations:**

  - **Price Transformation:** Raw price values will be transformed into relative returns, calculated as either percentage change or log return relative to the first price in each window. This anchors the price to zero at the beginning of the window, mitigates the impact of extreme price fluctuations, and enhances model robustness against outliers.

  - **Time Transformation:** Time will be encoded as a fractional elapsed time within each window, represented as a value between 0 and 1. This preserves chronological order, addresses limitations of linear indexing with unevenly spaced intervals, and provides a meaningful scalar representation for normalization.

  - **Volume Transformation (for PCA):** Due to potential heavy-tailed distribution, volume data for PCA will be transformed using either a log transformation ($v_i' = \log(1 + v_i)$) or robust scaling (using median and Interquartile Range). This step is performed _before_ the z-score normalization described below.

  - **PCA Preprocessing:** Price (P), Time (T), and Volume (V) data used for PCA will undergo specific transformations within each rolling window of _N_ data points:

    - **Normalization:** All features (P, T, V) will be z-score normalized (zero mean and unit variance) to ensure equal contribution to the PCA. This involves calculating the mean ($\mu$) and standard deviation ($\sigma$) for each feature within the window and applying the following scaling formula: $X_{\text{scaled}}[,i,.,.] = \Bigl(\tfrac{t_i-\mu_t}{\sigma_t}, \tfrac{p_i-\mu_p}{\sigma_p}, \tfrac{v_i-\mu_v}{\sigma_v}\Bigr)$.

    - **Time Handling:** Time data will be represented using either relative time indices (integers from 1 to _N_) within each window or as z-score normalized time deltas ($\Delta t_i = \frac{\text{timestamp}_i - \mu_t}{\sigma_t}$). For the latter, precautions will be taken to address potential issues with large gaps in timestamps inflating $\sigma_t$.

    - **PCA Implementation:** After preprocessing, PCA will be performed on the scaled data matrix $X_{scaled}$ using Singular Value Decomposition (SVD) in `numpy`:
      ```python
      u, s, vh = np.linalg.svd(X_scaled, full_matrices=False)
      axes = vh[:2]   # two principal directions in T-P-V space
      ```

## I. Project Setup and Data Acquisition

This section details the initial steps of the project, including data preparation and transformation. It focuses on acquiring the necessary data, preprocessing it, and preparing it for model training.

### A. Project Initialization

- **Documentation:** Thorough documentation of the project's evolution is crucial, including decisions and rationale regarding data acquisition and preprocessing. This documentation will serve as a valuable record of the project's development.
- **Preparation for Discussion:** Prepare all necessary materials for a comprehensive discussion of the project, including a detailed paper outlining the project's goals, methodology, and findings, and a well-organized repository containing the project's code and data.

### B. Data Acquisition and Preprocessing

This stage focuses on preparing the data for model input. This involves several crucial transformations to normalize and standardize the input features.

1. **Data Normalization and Transformation:**

   - **Price (P):** Price will be represented as the log return from the opening value within each 5-day window. This helps stabilize the time series data and address potential issues with heteroskedasticity. The log returns will be normalized using min-max scaling to the range [-1, 1] based on the minimum and maximum values within each window.

   - **Volume (V):** Apply a logarithmic transformation (`np.log1p(volume)`) to handle the large range often observed in volume data. Following the log transformation, clip extreme outliers at the 5th and 95th percentiles to mitigate the impact of exceptionally high or low values. The clipped log volume will then be robustly scaled using median and interquartile range (IQR) to the range [-1, 1].

   - **Time (T):** Time within each 5-day window will be represented as fractional elapsed time, calculated by subtracting the minimum timestamp from each timestamp and dividing by the total elapsed time within the window. This fractional elapsed time will then be normalized to the range [-1, 1] using the transformation: `(2 * time_frac) - 1`.

2. **Principal Component Analysis (PCA):**

   - **Matrix Construction:** The transformed time, price, and volume data will be organized into a 3-dimensional matrix. Each row will represent a point in time within a 5-day window, and the columns will represent: (1) normalized fractional elapsed time, (2) normalized log return of price, and (3) robustly scaled and normalized volume.

   - **PCA Application:** PCA will be performed on this 3-dimensional matrix using Singular Value Decomposition (SVD). Optionally, each column can be mean-centered before applying PCA. The resulting principal components will capture the correlations between these three features, allowing for a more concise and informative representation of the data.

3. **Data Storage:** The transformed data, both before and after PCA, will be stored in CSV files within designated directories (`stock_data/train/` and `stock_data/test/`) for training and testing datasets. The filename convention will clearly identify the holding period (5 days in this case) associated with each data point.

4. **Target Variable (Return Label):** The target variable for the model will be the 5-day future return, calculated as `(Close(t+5) - Open(t+1)) / Open(t+1)`. This calculation will be synchronized with the data windowing and candlestick chart generation (described below).

5. **Candlestick Charts:** Candlestick chart images will be generated for each 5-day window, incorporating volume and moving averages. These images will potentially be used as model input, and their generation will be synchronized with the calculation of the 5-day future return (target variable).

This data acquisition and preprocessing strategy ensures that the data is properly formatted, scaled, and transformed for effective model training and evaluation.

## I. Project Setup and Data Acquisition

This section outlines the project setup and the acquisition and preprocessing of financial data for training Convolutional Neural Networks (CNNs). This involves obtaining raw market data, transforming it into a usable format, and generating visual representations for analysis.

### A. Project Initialization

- **Documentation:** The project's rationale, experimental design, and decision-making processes (including the selection of specific market patterns) will be thoroughly documented and continuously updated throughout the project lifecycle.
- **Dissemination:** A dedicated code repository and accompanying documentation will be created to facilitate in-depth discussion, collaboration, and reproducibility of results.

### B. Data Acquisition and Preprocessing

This subsection details the acquisition, preprocessing, and transformation of the financial data.

- **Data Source:** Open, High, Low, Close, and Volume (OHLCV) data will be acquired from Yahoo Finance, along with corresponding index data for price adjustments as needed.
- **Data Storage and Windowing:** The preprocessed data will be stored as CSV files in designated directories (`stock_data/train/` and `stock_data/test/`). A 5-day rolling window of OHLCV data will be used for both model input and output, ensuring consistency.
- **Return Label Calculation:** The 5-day future return, serving as the target variable, will be calculated as `(Close(t+5) - Open(t+1)) / Open(t+1)`.
- **Data Transformation and Normalization:** The OHLCV data will undergo the following transformations to create dynamic plane snapshots:
  - **Log Returns:** Calculate log returns for both price and volume. For volume, address potential issues caused by zero or low values that could skew normalization. Specifically, a small constant can be added to the volume before the log transformation is applied.
  - **Percentile Clipping:** Clip extreme outlier values in the log return of price and volume at the 5th and 95th percentiles to mitigate the influence of extreme fluctuations.
  - **Min-Max Scaling:** Scale the `time_frac`, log return of price, and the transformed volume data to the [-1, 1] range using the minimum and maximum values across the entire dataset to prevent data leakage between time steps.
- **Dimensionality Reduction:** Apply Principal Component Analysis (PCA) to the normalized time, log return, and log volume data. Centering the data before PCA may be considered, although this might be redundant given the min-max scaling. PCA reduces dimensionality while preserving crucial information, improving model efficiency and potentially preventing overfitting.
- **Candlestick Chart Generation:** Candlestick charts, incorporating volume data and moving averages, will be generated for both pre- and post-transformation data. Each 10-minute interval within the 5-day window will be represented as a single candlestick, resulting in a sequence of 5 images per training example. These images will be displayed individually for clear visual inspection.
- **Example Visualization:** Five distinct examples showcasing various market patterns (uptrend with rising volume, downtrend with volume spikes, reversal, sideways chop, and breakout followed by stabilization) will be visualized as both pre- and post-transformation candlestick charts to demonstrate the effects of data preprocessing.
- **Filename Convention:** A consistent filename convention, encoding the 5-day holding period, will be implemented to ensure data integrity and facilitate efficient data retrieval.

## I. Project Setup and Data Acquisition

This phase focuses on establishing the project environment, acquiring the necessary financial data from the Indian equities market, and preprocessing it for subsequent model development. A key aspect of this project involves generating candlestick chart images, particularly focusing on a "Breakout Spike then Stabilize" pattern, alongside exploring its representation in a transformed dynamic plane.

### A. Project Initialization

1. **Project Documentation:** Maintain comprehensive documentation throughout the project lifecycle. This includes design decisions, challenges encountered, solutions implemented, and the rationale behind key choices. This documentation will serve as a valuable resource for understanding the project's evolution and for future reference.

2. **Prepare for Review:** Organize all project materials, including documentation and the code repository, to facilitate thorough reviews and discussions. This ensures easy access to relevant information for team members and stakeholders.

### B. Data Acquisition and Preprocessing

1. **Data Source:** Obtain Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance, specifically targeting the Indian equities market.

2. **Data Preprocessing:** Acquire OHLC and relevant index data. Adjust prices as needed (e.g., for splits and dividends) to ensure data accuracy.

3. **Data Storage:** Store the acquired and preprocessed data in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** Implement a 5-day sliding window approach, where each input to the model consists of five consecutive days of OHLCV data.

5. **Return Label Calculation:** Calculate the return using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the 5-day window and `h` is the holding period.

6. **Candlestick Chart Generation:** Generate candlestick charts visualizing the 5-day window data, including volume and a moving average indicator.

7. **Return Label Integration:** Combine the candlestick chart generation with the calculation of the corresponding return labels, ensuring each image is paired with its target value.

8. **Filename Convention:** Implement a clear file naming convention that incorporates the holding period (`h`), either directly in the filenames or in a separate CSV file linked to the image data.

9. **Visualization and Analysis of Transformed Data:** To understand how volume influences the visual representation of a “Breakout Spike then Stabilize” pattern, generate five variations of this scenario, each with a different volume profile. Visualize each variation using both the original candlestick chart (10-minute intervals) and a transformed dynamic plane projection. The dynamic plane projection utilizes time, log-return, and log-volume, scaled to [-1, +1], and then reduced to two principal components (PC1 and PC2) using PCA. Analyze these visualizations to determine whether volume information is sufficiently captured by the transformed data points or if explicit volume bars are necessary in the dynamic plane snapshots. Further analysis will involve generating five examples of these PCA patterns with varying data volumes, focusing on the impact of volume magnitude and timing on the trajectory shape in the PCA space. Specifically, examine the clustering before the spike, the jump along PC2 during the spike, and cluster/trajectory formation during the stabilization period.

## I. Project Setup and Data Acquisition

This phase establishes the project foundation and acquires the necessary data for training and evaluating the algorithmic trading model. While the immediate focus is on acquiring and preprocessing daily data for candlestick chart generation, the project architecture anticipates the future incorporation of multi-timeframe data (e.g., weekly, monthly, quarterly, yearly) to capture broader market trends and cyclical patterns. This forward-thinking approach recognizes the potential influence of factors like after-market activity, which aren't readily apparent in isolated daily data. Future integration of these additional timeframes will require careful consideration of appropriate weighting schemes to maximize predictive accuracy. This section details the initial steps for daily data acquisition and preprocessing, while acknowledging the planned expansion to multi-timeframe data later in the project lifecycle.

### A. Project Initialization

1. **Documentation:** Thorough documentation of design choices, rationale, and progress will be maintained throughout the project. This includes addressing potential challenges, such as data pipeline complexities associated with integrating multi-timeframe data (10-minute, daily, weekly, monthly, etc.) using the Dynamic Rotating Plane method. This documentation will ensure clarity and reproducibility.
2. **Communication and Collaboration:** Preliminary documentation and repository structure will be prepared to facilitate focused discussions on architectural decisions. This includes exploring the choice between an ensemble approach with specialist models for each timeframe versus a unified multi-input model.

### B. Data Acquisition and Preprocessing

The following steps outline the data pipeline for acquiring, preprocessing, and preparing daily data for the creation of dynamic input images:

1. **Data Source:** Open, High, Low, Close, and Volume (OHLCV) data will be acquired from Yahoo Finance. This data forms the basis of the candlestick charts and subsequent dynamic transformations.
2. **Data Acquisition and Preprocessing:** Acquire the necessary OHLCV and relevant index data. Apply price adjustments (e.g., for splits and dividends) to ensure data accuracy.
3. **Data Storage:** The acquired and preprocessed data will be stored in CSV format within dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.
4. **5-Day Windowing:** A 5-day sliding window will be implemented to create sequential data inputs for the model.
5. **Return Label Calculation:** The target variable (return label) will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window and `h` is the holding period.
6. **Candlestick Chart Generation:** Candlestick charts will be generated from the OHLCV data, potentially incorporating volume and moving averages as additional visual features.
7. **Return Label Integration:** Synchronize return label calculation with candlestick chart generation to create corresponding input-output pairs for model training.
8. **Filename Convention:** Implement a clear file naming convention that encodes the holding period associated with each candlestick chart image (e.g., embedding the holding period in the filename or using separate CSV files).
9. **Future Multi-Timeframe Data Integration:** While not implemented in this stage, the data pipeline is designed to accommodate future integration of multi-timeframe data, as described in the introduction.

This data will ultimately be used to generate dynamic images based on Time, Price, and Volume, projected onto a 2D plane derived from Principal Component Analysis (PCA). The prediction goals involve forecasting movement vectors and rally time within this dynamic plane. This understanding informs the data acquisition and preprocessing decisions made in this stage.

## I. Project Setup and Data Acquisition

This section outlines the initial project setup and the acquisition of the necessary financial data, including defining the data source, preprocessing steps, and data storage strategy. While the core model architecture utilizes Vision Transformers (ViT) processing dynamic planes derived from normalized Time, Price, and Volume data, this initial phase focuses on preparing the foundational OHLCV data for training and evaluation.

### A. Project Initialization

This stage involves setting up the project and preparing for detailed discussions.

1. **Documentation:** Maintain comprehensive documentation throughout the project lifecycle, including design decisions, rationale, challenges encountered, and insights gained. This documentation will be crucial for tracking progress, understanding the project's evolution, and facilitating future analysis.
2. **Communication Materials:** Prepare materials, including relevant papers and the project repository, to facilitate in-depth discussions and presentations on the project's progress and technical details.

### B. Data Acquisition and Preprocessing

This stage encompasses the acquisition, preprocessing, and preparation of the data for training the Vision Transformer model.

1. **Data Source:** Yahoo Finance will be the primary data source, providing Open, High, Low, Close, and Volume (OHLCV) data, which will serve as the raw input for generating the dynamic plane representations used by the ViT.
2. **Data Acquisition and Preprocessing:** OHLCV and relevant index data will be acquired from Yahoo Finance. Price adjustments for splits and dividends will be handled during preprocessing to ensure data accuracy and consistency. Following acquisition, data will be preprocessed at multiple granularities (intraday, daily, weekly, and monthly) to support the multi-scale model using the Dynamic Rotating Plane method. This method normalizes Time, Price, and Volume data, creating a dynamic 2D plane re-centered on the latest data point for each prediction point. These dynamic planes will serve as context images for the ViT.
3. **Data Storage:** The acquired and preprocessed data will be stored as CSV files in designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.
4. **5-Day Windowing:** A 5-day sliding window will be implemented for both input and output data, defining the temporal context for the model's predictions.
5. **Return Label Calculation:** The return label, representing the return over the holding period (h) starting from the open of the day following the window, will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the current time step and `h` is the prediction horizon (e.g., h=5 for a 5-day prediction).
6. **Candlestick Chart Generation:** Candlestick charts, incorporating volume and moving average indicators, will be generated from the OHLCV data to provide visual representations of market activity.
7. **Return Label Integration:** Calculated return labels will be directly associated with their corresponding candlestick chart images during the generation process.
8. **Filename Convention:** The filename convention will encode the holding period (e.g., 5 days) and the timeframe (e.g., daily, weekly) to manage data associated with different prediction horizons and the multi-scale datasets generated by the Dynamic Rotating Plane method.

## I. Project Setup and Data Acquisition

This section details the project's initial setup, including data acquisition and preprocessing. It outlines the data sources, preprocessing steps, and the methodology for preparing the data for model training.

### A. Project Initialization

1. **Project Documentation:** Comprehensive documentation will be maintained throughout the project lifecycle. This documentation will track design decisions, challenges encountered, implemented solutions, and the evolving project rationale.
2. **Collaboration Resources:** A dedicated code repository and accompanying research paper will be established to facilitate collaboration, in-depth discussions, and transparent knowledge sharing.

### B. Data Acquisition and Preprocessing

1. **Data Source and Selection:** OHLCV (Open, High, Low, Close, Volume) data will be sourced from Yahoo Finance. Users will be able to define the asset universe (e.g., NIFTY 50, NIFTY 500, or a custom watchlist) and the desired date range. The specified date range will be automatically partitioned into training and validation sets.
2. **Data Preprocessing:** Acquired OHLC data, along with relevant index data, will undergo preprocessing. This includes adjustments for stock splits and dividends to ensure data accuracy.
3. **Data Storage:** Preprocessed data will be stored in CSV format within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.
4. **5-Day Windowing:** Data will be structured into 5-day rolling windows to create input features and corresponding output labels for the model.
5. **Return Label Calculation:** 5-day future returns will serve as labels for the predictive model. These returns will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` represents the initial time and `h` is the 5-day holding period.
6. **Candlestick Chart Generation:** Candlestick charts, visualizing the 5-day data window, will be generated for each data point. These charts will include volume and a moving average indicator. They will be directly linked to their corresponding calculated 5-day future returns and will ultimately serve as input for the Vision Transformer (ViT) model. The project will offer configurable chart settings.
7. **Data Normalization:** Before model training, Time, Price, and Volume data will be normalized to a uniform [-1, +1] range to ensure stable and efficient training of the ViT model. Time will be represented as the fractional elapsed time within the trading day. Price will be transformed using log-returns, and Volume will undergo a log transformation followed by robust scaling.

## I. Project Setup and Data Acquisition

This section details the initial steps of the project, focusing on establishing the project foundation and acquiring the necessary financial data for training and evaluating the trading agent. The project utilizes a 5-day lookahead period (holding period) for predictions, meaning the model is trained to predict the return over the five days following the initial 5-day input window. This lookahead period is a key parameter and can be adjusted in future implementations. The primary data source is Yahoo Finance, providing Open, High, Low, Close, and Volume (OHLCV) data. Future development may incorporate alternative data sources and candlestick types, such as Heiken-Ashi.

### A. Project Initialization

- **Project Documentation:** A comprehensive log of the project's development, including design decisions, challenges, solutions, and any changes in direction, will be maintained. This documentation will be crucial for understanding the project's evolution and rationale.

- **Collaboration Materials:** All project materials, including code, documentation, and the project proposal paper, will be organized and maintained in a readily accessible repository to facilitate collaboration and review.

### B. Data Acquisition and Preprocessing

This subsection outlines the process of acquiring and preparing the financial data.

1. **Data Source:** OHLCV data is acquired from Yahoo Finance.

2. **Data Acquisition and Preprocessing:** This process includes acquiring OHLC and relevant index data, adjusting prices for corporate actions (e.g., splits and dividends), and potentially calculating additional technical indicators.

3. **Data Storage:** Processed data is stored in CSV files within designated directories (`stock_data/train/` for training data and `stock_data/test/` for testing data).

4. **5-Day Windowing:** A 5-day sliding window is implemented for both input features and output labels. The model receives five days of historical OHLCV data as input and predicts the return over the subsequent five days.

5. **Return Calculation:** The return is calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` represents the last day of the 5-day input window, `h` represents the 5-day lookahead period (holding period), and `t+1` signifies the opening price on the day immediately following the input window. This calculates the percentage return from the open of the first day after the input window to the close of the final day of the lookahead period.

6. **Candlestick Chart Generation:** Candlestick charts are generated from the 5-day window data. These charts incorporate volume information and a moving average indicator (the specific period will be documented). These charts serve as visual inputs for the Convolutional Neural Network (CNN) model. Parameters such as candlestick type (e.g., standard, Heiken-Ashi), local window size, and feature inclusion (price, time, volume) will control the model's perception system.

7. **Return Label Generation:** Corresponding 5-day future return labels are calculated for each candlestick chart image.

8. **Data Synchronization:** The return label calculation is integrated with the candlestick chart image generator to ensure each chart is paired with its corresponding return label.

9. **Holding Period Encoding:** A consistent filename convention or a separate CSV file will be used to explicitly encode the holding period (currently 5 days) for each data point.

## I. Project Setup and Data Acquisition

This section details the initial setup of the project and the process of acquiring and preparing the necessary financial data. While this project encompasses advanced model development and analysis techniques, this section focuses on the foundational steps of establishing the project and securing the data required for subsequent stages.

### A. Project Management

1. **Documentation:** A comprehensive log of the project's progress, including design decisions, rationale, and encountered challenges, will be maintained. This living document will provide valuable context and inform future development.

2. **Communication Materials:** A project paper outlining the goals, methodology, and anticipated outcomes, along with a well-structured code repository, will be prepared to facilitate clear communication and collaboration among stakeholders.

### B. Data Acquisition and Preprocessing

1. **Data Source:** Historical Open-High-Low-Close-Volume (OHLCV) data will be acquired from Yahoo Finance. This publicly available source offers a comprehensive historical record of stock prices and trading volume.

2. **Data Preprocessing:** Along with OHLCV data, relevant index data will be acquired. Price adjustments for corporate actions such as stock splits and dividends will be applied to ensure data accuracy and consistency.

3. **Data Storage:** Acquired and preprocessed data will be stored in CSV format within dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **Data Windowing:** A 5-day rolling window will structure the data for time series analysis. Each input to the model will consist of OHLCV data for five consecutive trading days.

5. **Return Calculation:** The target variable, representing the return, will be calculated as: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 't' is the last day of the 5-day window and 'h' is the holding period (yet to be defined). This formula calculates the percentage change between the closing price at time `t+h` and the opening price at time `t+1`.

6. **Candlestick Chart Generation:** Candlestick charts, incorporating volume and a moving average indicator (type to be determined), will be generated from the 5-day windowed data. These charts will serve as visual representations of the data and as input to the CNN model.

7. **Return Label Generation:** Corresponding to each candlestick chart, a return label representing the return over the holding period 'h' will be calculated.

8. **Data Integration:** The candlestick chart generation and return label calculation will be integrated to ensure data consistency and efficient processing.

9. **Filename Convention:** A clear file naming convention will be implemented, encoding the holding period 'h' within the filenames or using separate CSV files for different holding periods.

This structured approach to data acquisition and preprocessing ensures data accuracy, consistency, and accessibility. The 5-day window and the specific return calculation are key elements influencing model training and performance. The undefined holding period 'h' will be addressed in a subsequent section.

## I. Project Setup and Data Acquisition

This section details the initial steps of the project, focusing on setting up the project environment and acquiring the necessary data for model training and evaluation. It encompasses preparing documentation and the code repository, as well as establishing a robust data pipeline. While UI/UX elements are important for later stages, this section concentrates solely on the foundational data processes required for the project's core functionality. The focus here is on backend data acquisition and preprocessing, not on user interaction with these processes. The functional requirements outlined previously directly inform the structure and content of the acquired data.

### A. Project Initialization

1. **Document Thought Process (Continuously Update Progress and Thinking):** Comprehensive documentation of the project's evolution is crucial. This involves continuously recording the rationale behind decisions, evolving ideas, and encountered challenges. This documentation will serve as a valuable resource for understanding the project's trajectory and informing future development.

2. **Prepare for In-Depth Discussion (Paper and Repository):** Thorough preparation is essential for productive project discussions. This includes preparing a research paper outlining the project's goals, methodology, and anticipated outcomes, and a well-organized code repository to facilitate technical discussions and collaboration.

### B. Data Acquisition and Preprocessing

This subsection outlines the steps involved in acquiring, preprocessing, and preparing the data for subsequent model training and evaluation. This stage is critical, as data quality and consistency directly impact model performance and reliability. The acquisition process must consider the previously mentioned functional requirements, including context awareness, transfer learning capabilities between different markets (US and India), context-aware periodicity, suitability for PCA analysis, and compatibility with the hyperparameter permutation testing methodology.

1. **Data Source (Yahoo Finance):** Acquire Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance for the specified US and Indian stock markets. Ensure data consistency between the two markets for effective transfer learning.

2. **Data Acquisition and Preprocessing:** Acquire raw OHLCV and relevant index data. Adjust prices for splits and dividends to ensure data accuracy and consistency. Include stock metadata (market cap, sector, etc.) to support context-aware periodicity and other analyses. Gather sufficient features for meaningful PCA analysis.

3. **Data Storage:** Store acquired and preprocessed data in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data, structured according to the defined training and testing date ranges required for hyperparameter permutation testing.

4. **Windowing:** Implement a 5-day rolling window for input data, aligning with the context awareness requirement for variable candle numbers per frame. Determine and document the optimal number of frames.

5. **Return Label Calculation:** Calculate the return label (target variable) using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 'h' represents the holding period.

6. **Candlestick Chart Generation:** Generate candlestick charts for each 5-day window, incorporating volume and a moving average indicator.

7. **Return Label Integration:** Integrate return label calculation with candlestick chart generation to maintain consistency between visual representation and target variable.

8. **Filename Convention:** Encode the holding period and other relevant information (e.g., market, stock symbol) in filenames or utilize a separate metadata CSV file for efficient data management and retrieval. This will support variable frame and candle requirements as well as facilitate the different transfer learning scenarios.

This meticulously designed data acquisition and preprocessing pipeline establishes a robust foundation for developing reliable predictive models.

## I. Project Setup and Data Acquisition

This phase establishes the project foundation and acquires the necessary data for subsequent model training. While the final deployment target is Firebase (using VS Code with Gemini Code Assist Agent for development), this section focuses on the data acquisition and preparation steps. A structured project blueprint will guide the overall project, with this section detailing the essential first steps.

### A. Project Initialization

1. **Project Documentation:** Maintaining comprehensive documentation throughout the project lifecycle is crucial. This includes recording design decisions, data source selections, preprocessing techniques, encountered challenges, and implemented solutions. This documentation ensures reproducibility and provides valuable insights into the project's evolution.

2. **Collaboration Preparation:** Given the project's complexity, thorough preparation for discussions and collaboration is essential. This involves organizing project materials, including the developing paper and the code repository, for clear communication and efficient teamwork. This includes establishing a clear project structure and version control system.

### B. Data Acquisition and Preprocessing

This subsection outlines the process of obtaining and preparing the data required for the project, ensuring data quality and consistency. The following steps will be implemented:

1. **Data Source:** Yahoo Finance will be used as the primary data source for OHLCV (Open, High, Low, Close, Volume) data.

2. **Data Acquisition and Adjustment:** OHLC and index data will be acquired from Yahoo Finance. Necessary adjustments will be made to the price data to ensure accuracy and consistency.

3. **Data Storage:** Acquired and preprocessed data will be stored as CSV files in designated directories within the project structure.

4. **Data Preprocessing:** This stage includes the following steps:
   - **Windowing:** A 5-day rolling window will be implemented for data analysis.
   - **Return Label Calculation:** Return labels will be calculated based on the price data.
   - **Candlestick Chart Generation:** Candlestick charts will be generated from the OHLC data.
   - **Return Label Integration:** Calculated return labels will be integrated with the generated candlestick charts.
   - **Filename Generation:** A consistent file naming convention will be used for all generated files.

## Project Setup and Data Acquisition

This section details the initial steps of establishing the project and acquiring the necessary financial data for the multi-scale, self-correcting, relational spacetime vision transformer for predictive intraday market analysis. While the four yogas framework isn't explicitly applied here, the principles of clarity and comprehensiveness are prioritized for developers. The following steps outline a structured approach to data management, reflecting the principles of the Gyaan Shala (The House of Wisdom) by emphasizing data purity and creating a robust framework.

### Project Initialization

This stage focuses on establishing a solid foundation for the project:

1. **Continuous Documentation:** Maintain comprehensive documentation throughout the project lifecycle. This includes recording design decisions, experimental results, and any changes in direction, ensuring transparency and reproducibility.

2. **Preparation for Review:** Prepare all necessary materials for in-depth discussions, including the project's research paper and a well-structured code repository hosted within Google's IDX IDE. This facilitates collaboration and ensures a shared understanding of the project.

### Data Acquisition and Preprocessing

This stage outlines the process of acquiring, validating, and preparing market data for subsequent model training:

1. **Data Source:** While the original checklist specified Zerodha Kite Connect API, this implementation uses Yahoo Finance to acquire historical Open, High, Low, Close, and Volume (OHLCV) market data.

2. **Data Acquisition and Preprocessing:** Acquire OHLC and relevant index data. Preprocess the data by adjusting prices for splits, dividends, and other corporate actions to maintain accuracy.

3. **Data Storage:** Store the processed data in CSV format within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** Implement a 5-day rolling window logic to prepare the data for time series analysis. This involves creating input sequences of 5 consecutive trading days.

5. **Return Label Calculation:** Calculate the return label, representing the percentage change in price, using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 'h' is the holding period.

6. **Candlestick Chart Generation:** Generate candlestick chart images visualizing the 5-day window data, including volume and moving averages as additional visual features.

7. **Return Label Integration:** Integrate the return label calculation with the candlestick chart image generator to ensure data consistency.

8. **Filename Convention:** Implement a consistent filename convention that incorporates the holding period (h). This can be achieved by encoding 'h' within the filenames or by using separate CSV files for different holding periods.

## I. Project Setup and Data Acquisition

This section details the initial setup of the project and the process of acquiring and preparing the necessary financial data. While the complete project encompasses a complex architecture, including backend, frontend, database, and deployment considerations, this stage focuses exclusively on establishing the foundation for data acquisition, preprocessing, and storage.

### A. Project Initialization

1. **Project Documentation:** Comprehensive documentation will be maintained throughout the project lifecycle. This documentation will serve as a living document, recording design choices, rationale for decisions, development progress, and any changes in project direction.

2. **Collaboration and Review Materials:** The project's codebase, architectural decisions, and progress will be readily available for review and discussion. A dedicated repository will be established containing well-documented code, data, and any other relevant artifacts to facilitate collaboration and ensure stakeholder alignment.

### B. Data Acquisition and Preprocessing

The following steps outline the data acquisition and preprocessing procedures, ensuring data quality and consistency for subsequent model training:

1. **Data Source:** Financial data, specifically Open, High, Low, Close, and Volume (OHLCV) data, will be acquired from Yahoo Finance using their API. The specific API calls, parameters, and any implemented error handling mechanisms will be documented. Any necessary data cleaning or validation steps performed at this stage will also be detailed.

2. **Data Acquisition and Preprocessing:** In addition to OHLCV data, relevant market index data will be acquired. The specific indices used and the rationale behind their selection will be documented. Acquired price data will be adjusted for splits, dividends, and other corporate actions. The formulas and code used for these adjustments will be explicitly provided.

3. **Data Storage:** The acquired and preprocessed data will be stored in CSV files within dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. The file naming conventions and the internal data organization within each file will be clearly defined.

4. **5-Day Windowing:** The data will be structured into 5-day windows to be used as input for the CNN model. The implementation details of the windowing logic, including handling of overlapping or non-overlapping windows and the specific start and end dates for each window, will be documented. Comprehensive code examples will be included.

5. **Return Label Calculation:** The return label for each 5-day window will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` represents the current time step and `h` represents the holding period in days. A detailed explanation of the formula, its implications, and examples for different values of `h` will be provided.

6. **Candlestick Chart Generation:** Candlestick charts will be generated for each 5-day window, incorporating volume data and a moving average indicator. The specific charting library used, the parameters for chart customization (e.g., moving average period), and the format and resolution of the generated images will be documented.

7. **Return Label Integration:** The calculation of the 5-day future return label will be integrated directly within the candlestick chart generation process. This ensures each generated image is paired with its corresponding return label. The implementation details of this integration, including how the labels are stored or associated with the images, will be thoroughly documented.

8. **Filename Convention for Holding Period:** The filename convention or a separate CSV file will be used to explicitly encode the holding period (h). The specific format used will be clearly defined to ensure easy identification and management of data for different holding periods.

## I. Project Setup and Data Acquisition

This section details the initial setup of the project and the acquisition of the necessary financial data. While the core project leverages federated learning with iPads performing image generation and model training, this initial stage focuses on centralized data acquisition and preprocessing before distribution to client devices. Given the potential computational costs associated with image generation, strategies for efficient resource utilization are paramount.

### A. Project Initialization

1. **Documentation:** Maintain comprehensive documentation of the project's development, including decisions made regarding data sources, preprocessing steps, and the rationale behind each approach. This ensures reproducibility and facilitates future analysis.

2. **Federated Learning Architecture:** Prepare documentation detailing the project's federated learning architecture, including client-server interactions, data transfer protocols, and the model aggregation process. This will be crucial for in-depth discussions and understanding of the system's design.

### B. Data Acquisition and Preprocessing

The following steps outline the centralized data preparation process:

1. **Data Source:** Acquire Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance.

2. **Data Preprocessing:** Acquire relevant index data alongside OHLCV data. Adjust historical prices for splits and dividends to ensure accuracy.

3. **Data Storage:** Store the preprocessed data in CSV files within designated `stock_data/train/` and `stock_data/test/` directories.

4. **5-Day Windowing:** Organize the OHLCV data into 5-day windows to provide context for the model. This windowing approach applies to both input features and output labels.

5. **Return Label Calculation:** Calculate the return label, representing the 5-day future return, using the following formula: `(Close(t+5) - Open(t+1)) / Open(t+1)`, where 't' represents the starting day of the window.

6. **Candlestick Chart Generation:** Generate candlestick charts for each 5-day window, incorporating volume and a moving average indicator.

7. **Data Integration:** Integrate the return label calculation with the candlestick chart generation process, ensuring each image is linked to its corresponding label.

8. **Filename Convention:** Implement a consistent filename convention that incorporates the 5-day holding period for easy identification and retrieval. Alternatively, store the labels in a separate CSV file clearly linked to the corresponding candlestick chart images.

### C. Computational Resource Optimization

To mitigate the computational demands of image generation and processing, the following strategies will be employed:

- **Offline Image Processing:** Perform the computationally intensive candlestick chart image generation offline to reduce the load on the application server and minimize cloud computing costs. The specifications of this offline processing system will be documented.

- **Image Generation Optimization:** Explore optimization strategies for image generation and computer vision algorithms. This may include caching processed images, using lightweight image formats, or other techniques to reduce computational overhead.

This prepared data will be distributed to the iPads for client-side processing as detailed in later sections. The central server will primarily orchestrate and aggregate client-side model updates, leveraging the iPads' processing capabilities within the federated learning framework.

## Project Setup and Data Acquisition

This phase establishes the project's foundation and acquires the necessary data for model training, considering both server-side and potential future client-side (iPad/PWA) components. The feasibility of using a Progressive Web App (PWA) on resource-constrained devices like iPads for computationally intensive tasks like Vision Transformer training requires careful evaluation. This evaluation informs the project's initial server-side setup and data handling strategies to ensure future compatibility and scalability.

### Project Initialization

This initial step involves documenting the project setup strategy and preparing for in-depth discussions. Key considerations include:

1. **PWA Feasibility Assessment:** Thoroughly investigate the feasibility of utilizing a PWA for Vision Transformer training on an iPad. Document research findings regarding PWA suitability for GPU-intensive tasks, including TensorFlow.js performance, WebGPU capabilities, potential limitations (browser crashes, resource constraints, 50MB cache limit), and long-term operational stability. This documentation should be continuously updated.

2. **Architectural Design and Documentation:** Prepare comprehensive documentation outlining the technical challenges of running GPU-intensive Vision Transformer training within a PWA, including the likelihood of browser crashes under heavy GPU load. Explore alternative solutions for GPU tasks if PWAs prove unsuitable, such as native applications or cloud-based solutions. This documentation should inform the architectural decisions for both the current server-side implementation and potential future client-side development.

### Data Acquisition and Preprocessing

Data acquisition and preprocessing strategies are designed with flexibility to accommodate both the current server-centric approach and a potential future hybrid approach leveraging both server and client resources. The following considerations are paramount:

- **Server-Side Capabilities:** The server will act as the central hub for data storage, initial model training, and potentially serving pre-processed data and model versions to client devices. The server's API will need to be designed to support both current and future client interaction.

- **Client-Side Considerations (Future PWA):** While client-side processing is not the immediate focus, data handling should be optimized for potential future implementation on iPads via a PWA. This includes considering data chunking and transfer strategies to address potential bandwidth and cache limitations. The feasibility study will inform whether client-side tasks will include image generation, model fine-tuning using TensorFlow.js, and uploading model updates. Technologies under consideration for client-side implementation include TensorFlow.js, the Canvas API or a suitable JavaScript graphics library for image generation, and Web Workers for background processing. UI adaptations for the Campaign Runner and Experiment Designer will be addressed if client-side processing is adopted.

- **Hybrid Architecture Preparedness:** A hybrid architecture, with initial training on the server and fine-tuning on the client, is being considered. Data should be structured and stored to facilitate efficient splitting and transfer between these environments. This approach addresses potential GPU constraints on iPads while leveraging their local processing capabilities for specific tasks.

## I. Project Setup and Data Acquisition

This section outlines the initial project setup and the process of acquiring and preprocessing the financial data required for training and evaluating the stock prediction models. The project's architecture emphasizes client-side processing for computationally intensive tasks, such as image generation and model training, with the backend primarily serving as an orchestrator for data and API access. This architectural decision is driven by the need for efficient utilization of device resources and enhanced performance, particularly for features like on-device image generation.

### A. Project Initialization

1. **Documentation:** Maintain comprehensive documentation throughout the project lifecycle, including design decisions, rationale for choices (e.g., data sources, preprocessing steps), challenges encountered, and implemented solutions. This living document will be a valuable resource for future reference and collaboration.

2. **Project Organization:** Organize project materials, including code repositories and documentation, to facilitate effective collaboration and discussion.

### B. Data Acquisition and Preprocessing

A robust and efficient data pipeline is crucial for client-side model training. The following steps detail the data acquisition and preprocessing procedures:

1. **Data Source and Storage:** Raw OHLCV (Open, High, Low, Close, Volume) data will be fetched from Yahoo Finance and stored locally on the device using a high-performance database like Core Data or Realm. This allows for fast, offline access during model training and other operations.

2. **Data Preprocessing:** The acquired OHLCV data, along with relevant index data, will undergo preprocessing, including price adjustments.

3. **Data Formatting and Windowing:** The preprocessed data will be formatted for the machine learning models. This involves creating 5-day windows of OHLCV data and calculating corresponding return labels. These windows serve as the primary input for the CNN model. The return label is calculated as: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `h` is the holding period.

4. **Candlestick Chart Generation:** Candlestick chart images will be generated from the 5-day windowed data for the CNN model. These charts will include volume and moving average indicators. The return label calculation will be integrated with the image generation process to ensure data consistency. The holding period will be encoded within the filenames of these chart images or in a separate accompanying CSV file.

This process will be implemented using Swift, leveraging Apple's Core ML framework for machine learning and Metal framework for GPU-accelerated image generation and rendering within the DynamicPlaneGenerator component. This component handles PCA, rotations, and rendering of numerical data into images or tensors suitable for Core ML. The transition to a native iOS Swift frontend using SwiftUI addresses previous limitations encountered with a Progressive Web App (PWA) architecture, specifically regarding memory management, long-running task stability, and GPU context access, which are critical for client-side image generation and model training. A feasibility study will be conducted to thoroughly assess the capabilities of this native approach before full implementation. The overall architecture will be reassessed to ensure seamless integration of the new frontend with the existing backend and data processing pipeline.

## I. Project Setup and Data Acquisition

This section details the initial project setup and data acquisition process. While the overall project encompasses model development, training, and potential frontend integrations (web and Android), this section focuses solely on the foundational steps. Meticulous documentation is emphasized throughout this phase to facilitate future discussions and ensure project clarity.

### A. Project Initialization

1. **Project Documentation:** Maintain comprehensive documentation of the project's evolution. This includes design decisions, challenges encountered, and their solutions. Regular updates are crucial to reflect the current project state. This could involve a research journal, detailed commit messages, and evolving design documents.

2. **Prepare for Collaboration and Review:** Organize the project to facilitate in-depth discussions and reviews. This involves a well-structured repository for code and data, along with preliminary documentation outlining project goals, methodologies, and expected outcomes. This centralized resource ensures all stakeholders have access to the latest information.

### B. Data Acquisition and Preprocessing

This subsection outlines the steps involved in acquiring and preparing the financial data for model training and evaluation.

1. **Data Source:** Financial data, including Open, High, Low, Close, and Volume (OHLCV), will be sourced from Yahoo Finance.

2. **Data Acquisition and Preprocessing:** The acquired OHLCV data will be preprocessed. This includes cleaning the data, handling missing values, and potentially adjusting prices for corporate actions (e.g., stock splits, dividends). The use of index data will be explored if relevant.

3. **Data Storage:** The preprocessed data will be stored as CSV files in designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** A 5-day windowing approach will be implemented for preparing the model input and output data. The model input will consist of a sequence of 5 days of OHLCV data.

5. **Return Label Calculation:** The return label, representing the target variable for the model, will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`.

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the OHLCV data, potentially incorporating volume and moving average indicators. These charts will serve as visual representations of the data and may be used as model input.

7. **Return Label Integration:** The calculation of 5-day future returns (return labels) will be integrated with the candlestick chart generation process to maintain data integrity and consistency.

8. **Filename Convention:** A consistent filename convention will be employed, potentially encoding the holding period within the filenames or using separate CSV files for different holding periods. This will facilitate efficient data management and retrieval.

## I. Project Setup and Data Acquisition

This section details the initial project setup and the acquisition and preparation of the financial data used for model training. A decoupled architecture, with a clear separation between backend data handling and frontend computation/visualization (targeting PWA and Android app deployment), informs the data processing pipeline. This architectural choice enhances portability and facilitates future development flexibility.

### A. Project Initialization

1. **Documentation:** A comprehensive project log, documenting the design decisions, rationale, encountered challenges, and ongoing progress, will be maintained throughout the project lifecycle. This will serve as a valuable resource for future analysis and refinement.

2. **Preparation for Dissemination:** All project materials, including the code repository, documentation, and research paper (in progress), are being meticulously organized for clear communication and reproducibility. The decoupled architecture will be emphasized, showcasing the separation between data handling, model training, and frontend components.

### B. Data Acquisition and Preprocessing

The following steps outline the data pipeline, emphasizing the preparation of data for subsequent model development:

1. **Data Source:** Financial data (OHLCV - Open, High, Low, Close, Volume) is acquired from Yahoo Finance.

2. **Data Acquisition and Preprocessing:** The acquired OHLCV data, along with relevant index data, is preprocessed. This includes adjustments for stock splits and dividends to ensure data accuracy and consistency.

3. **Data Storage:** Processed data is stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** A 5-day sliding window approach is implemented for both input features and output labels, creating sequential data segments.

5. **Return Label Calculation:** The return label is calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the current timestep and `h` represents the holding period (in this case, 5 days).

6. **Candlestick Chart Generation:** Candlestick charts are generated for each 5-day window, incorporating volume and a moving average indicator. These charts serve as visual input for the model.

7. **Return Label Integration:** The calculated 5-day future return labels are directly integrated with the corresponding candlestick chart images, ensuring data synchronization.

8. **Filename Convention:** A consistent filename convention encodes the holding period to facilitate efficient data management and retrieval.

While image processing for daily predictions and model re-tuning are essential for frontend integration and post-development stages, these aspects are addressed in later sections. This section focuses solely on establishing the foundational data pipeline.

## I. Project Setup and Data Acquisition

This section outlines the initial steps of the project, focusing on establishing a solid foundation and gathering the necessary data. While the provided checklist chunk primarily addresses mobile deployment and debugging (TensorFlow Lite, local storage, bug investigation), these considerations inform the broader project setup, particularly data preprocessing and output formats. This section details the initial setup and planned data acquisition, with the understanding that mobile-specific details will be integrated in later stages as the project progresses.

### A. Project Initialization

This stage involves establishing foundational elements, including documentation and preparing for technical discussions regarding the chosen mobile architecture.

- **Documenting the Decision-Making Process:** Maintaining a comprehensive record of the project's development, including the rationale behind key decisions, is crucial. This documentation will be continuously updated as the project evolves. Specifically, decisions regarding mobile platform selection (e.g., Flutter) will be documented, along with the evaluation of potential benefits, drawbacks, and alternative technologies. This will ensure transparency and facilitate informed future development.

- **Preparing for Technical Discussions:** To facilitate productive discussions about the mobile architecture and integration, relevant documentation, including research papers and code repositories, will be compiled and readily available.

### B. Data Acquisition and Preprocessing

This subsection outlines the plan for acquiring and preparing the financial data that will be used for model training and evaluation.

The project will leverage Yahoo Finance as the primary data source for Open, High, Low, Close, and Volume (OHLCV) data. The following steps will be undertaken:

1. **Data Acquisition:** OHLCV and relevant index data will be acquired from Yahoo Finance.
2. **Data Preprocessing:** Acquired data will be preprocessed, including adjustments for splits, dividends, and other corporate actions to ensure accuracy.
3. **Data Storage:** Processed data will be stored in CSV format within dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.
4. **Windowing:** A 5-day windowing approach will be implemented, segmenting the data into five consecutive trading days for model input.
5. **Return Label Calculation:** Return labels, representing the future return over a holding period 'h', will be calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`.
6. **Candlestick Chart Generation:** Candlestick charts, incorporating volume and moving averages, will be generated as visual inputs for certain machine learning models.
7. **Return Label Integration:** The calculated return labels will be associated with their corresponding candlestick chart images.
8. **Filename Convention:** The filename convention will be updated to reflect the holding period 'h', either embedded in the filename or through separate CSV files.

This structured approach ensures a robust data pipeline and prepares the data for subsequent model development and evaluation. Future iterations of this document will include more specific details regarding mobile platform integration as those decisions are finalized.

## I. Project Setup and Data Acquisition

This section details the initial steps for setting up the project and acquiring the necessary financial data for training and validating the machine learning models. This foundational phase ensures data quality and consistency, which are critical for the success of subsequent model development and mobile application integration.

### A. Project Initialization

1. **Document Thought Process:** Maintain comprehensive documentation of the project's development, including technical details, rationale behind decisions, challenges encountered, and potential solutions. This living document should be continuously updated to reflect the evolving understanding of the project and its goals, supporting reproducible research and providing a valuable resource for future reference.

2. **Prepare for In-Depth Discussion:** Establish a version-controlled repository to house the project code and data, ensuring proper organization. Concurrently, begin structuring a formal paper to document the project thoroughly, facilitating in-depth discussion and analysis of the project's progression and results.

### B. Data Acquisition and Preprocessing

1. **Data Source:** Acquire Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance.

2. **Data Refinement:** Acquire OHLC data, along with relevant index data (if applicable). Perform necessary price adjustments, such as adjusting for splits and dividends, to ensure data accuracy and consistency.

3. **Data Storage:** Store the acquired and preprocessed data in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** Implement a 5-day sliding window for input data. Each data point will consist of five consecutive days of OHLCV data, serving as input for the model.

5. **Return Label Calculation:** Define the return label as the percentage change between the closing price at time `t+h` (where `h` is the holding period) and the opening price at time `t+1`: `(Close(t+h) - Open(t+1)) / Open(t+1)`. This will be the target variable for the model.

6. **Candlestick Chart Generation:** Generate candlestick chart images from the 5-day windowed data, incorporating volume information and a moving average indicator.

7. **Return Label Association:** Calculate the 5-day future returns (holding period `h=5`) and associate these labels with their corresponding candlestick chart images.

8. **Filename Convention:** Implement a clear filename convention that encodes the holding period within the filenames of the candlestick images or utilize a separate CSV file to maintain this mapping, crucial for data organization and retrieval during training and evaluation.

This structured approach to data acquisition and preprocessing provides a consistent and reliable dataset for model development and training, ensuring the data is correctly formatted, cleaned, and organized for efficient use in subsequent stages.

## I. Project Setup and Data Acquisition

This section details the initial steps required to set up the project and acquire the necessary data for training and evaluating the SCoVA agent. It covers project initialization, data acquisition, preprocessing, and preparation for model input.

### A. Project Initialization

This stage focuses on laying the groundwork for the project, ensuring clear documentation and preparation for in-depth discussions.

1. **Project Documentation:** Maintain comprehensive documentation of the project's evolution, including design choices, rationale, challenges encountered, and solutions implemented. This living document will serve as a valuable resource for understanding the project's trajectory, justifying decisions, and ensuring maintainability.

2. **Project Materials:** Prepare all necessary materials for in-depth discussions and review, including the research paper and code repository. Ensure the repository is well-organized, documented, and easily navigable.

### B. Data Acquisition and Preprocessing

This stage covers the acquisition of financial data, its preprocessing, and preparation for model consumption.

1. **Data Source:** Obtain Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance. This data will serve as the foundation for the candlestick charts and subsequent model input.

2. **Data Preprocessing:** Acquire the necessary OHLC and relevant index data. Adjust prices for stock splits and dividends to ensure data accuracy and consistency.

3. **Data Storage:** Store the acquired and preprocessed data in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **Data Windowing:** Implement a 5-day sliding window approach to prepare data for the model. Each input will consist of five consecutive days of OHLCV data.

5. **Return Label Calculation:** Calculate the return label using the following formula, where `t` is the last day of the input window and `h` is the holding period (presumably 5 days, matching the window size): `(Close(t+h) - Open(t+1)) / Open(t+1)`.

6. **Candlestick Chart Generation:** Generate candlestick chart images from the 5-day windowed data. These images will be used as input for the CNN model. Include volume information and a moving average on the charts.

7. **Return Label Integration:** Synchronize the return label calculation with the candlestick chart image generation to ensure each image is paired with the correct return label.

8. **Filename Convention:** Implement a clear and consistent filename convention that encodes the holding period within the filenames or use a separate CSV file to store this information. This will be essential for organizing and retrieving data for different holding periods.

This structured approach to project setup and data acquisition sets the stage for efficient model development and rigorous testing. The documentation and organization outlined here are crucial for maintaining project clarity and ensuring reproducibility of results.

## I. Project Setup and Data Acquisition

This section outlines the initial steps for setting up the project and acquiring the necessary financial data for training and evaluating the trading agent. This includes project initialization, defining the data sources, preprocessing steps, and establishing a robust data pipeline.

### A. Project Initialization

This stage focuses on the foundational aspects of the project, ensuring proper documentation and preparation for collaborative discussions.

1. **Document Thought Process:** Maintain a detailed log of the project's evolution, including design decisions, rationale, challenges encountered, and potential solutions. This living document will serve as a valuable resource for future reference and analysis.

2. **Prepare for Discussion:** Organize project materials, including the code repository and relevant research papers, to facilitate thorough discussions and collaboration. This preparation ensures all stakeholders have access to the necessary information.

### B. Data Acquisition and Preprocessing

This stage details the process of acquiring, preprocessing, and preparing financial data for model input.

1. **Data Source:** Utilize Yahoo Finance as the primary data source to obtain Open, High, Low, Close, and Volume (OHLCV) data for the selected financial instruments.

2. **Data Preprocessing:** Download the necessary OHLCV data along with relevant index data. Adjust prices for splits and dividends to maintain data consistency and accuracy.

3. **Data Storage:** Store the acquired and preprocessed data in CSV format within dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** Implement a 5-day sliding window approach to prepare the data for model input, creating input sequences consisting of five consecutive days of OHLCV data.

5. **Return Label Calculation:** Calculate the return label using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` represents the last day of the input window and `h` represents the holding period (presumably 5 days).

6. **Candlestick Chart Generation:** Generate candlestick chart images from the 5-day windowed data, incorporating volume information and a defined moving average indicator (type and period to be documented).

7. **Return Label Integration:** Integrate the return label calculation directly within the candlestick chart image generation process to ensure synchronized data and image creation.

8. **Filename Convention:** Implement a clear and consistent file naming convention that encodes the holding period within the filenames or by using a separate CSV file for return labels with appropriate referencing.

## I. Project Setup and Data Acquisition

This phase establishes the project foundation and acquires the necessary financial data for model training and evaluation. It encompasses defining the project scope, gathering data, and preprocessing it into a usable format for subsequent model development.

### A. Project Initialization

This stage lays the groundwork for the project, including documentation and preparation for discussions.

1. **Document the thought process:** Maintain a comprehensive, continuously updated log of the project's evolution, including design decisions, rationale, challenges, and solutions. This living document, whether a project log or dedicated files, will provide valuable insights into the project's trajectory.

2. **Prepare for in-depth discussion:** Establish a dedicated repository to house the project's codebase and documentation. Compile necessary materials for thorough discussions, including a research paper outlining project goals and methodology, technical specifications, and progress reports.

### B. Data Acquisition and Preprocessing

This stage details the procedures for acquiring, preprocessing, and storing financial data.

1. **Data Source:** Obtain Open-High-Low-Close-Volume (OHLCV) data from Yahoo Finance.

2. **Data Acquisition and Preprocessing:** Download OHLCV and relevant index data (e.g., S&P 500). Adjust historical prices for splits and dividends to ensure data accuracy.

3. **Data Storage:** Store the acquired and preprocessed data in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** Implement a 5-day rolling window for both input features (OHLCV data) and output labels (returns).

5. **Return Label Calculation:** Calculate the holding period return (h = 5 days) using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` represents the current time. This calculates the percentage change between the closing price at t+h and the opening price at t+1, normalized by the opening price at t+1.

6. **Candlestick Chart Generation:** Create candlestick charts for each 5-day window, incorporating volume and moving average indicators for visual data representation.

7. **Return Label Integration:** Synchronize the return label calculation with the candlestick chart generation process, ensuring each chart is paired with its corresponding return label.

8. **Filename Convention:** Implement a clear file naming convention that readily identifies the holding period associated with each data file, either by encoding it in the filename or using separate CSV files for labels.

## I. Project Setup and Data Acquisition

This section outlines the initial steps for setting up the project and acquiring the necessary financial data. It focuses on establishing a robust foundation for subsequent model development and training.

### A. Project Initialization

1. **Project Documentation:** Maintaining comprehensive documentation of the project's evolution is crucial. This includes recording all decisions, rationale, explorations, and modifications throughout the development lifecycle. This continuous documentation will provide valuable insights into the project's trajectory and facilitate future analysis and refinement.

2. **Preparation for Dissemination:** Thorough preparation for discussions and presentations about the project is essential. This includes organizing all relevant materials, such as the research paper, code repository, and documentation, to ensure clear and productive communication. The paper will articulate the methodology, findings, and conclusions, while the repository will ensure reproducibility and transparency of the codebase.

### B. Data Acquisition and Preprocessing

This subsection details the procedures for acquiring, preprocessing, and preparing the financial data for model input. A rigorous approach to data handling is critical for ensuring the quality and reliability of subsequent analyses.

1. **Data Source:** Yahoo Finance will serve as the primary data source for this project, providing Open, High, Low, Close, and Volume (OHLCV) data for the selected financial instruments.

2. **Data Preprocessing:** The acquired OHLCV data will undergo preprocessing to ensure consistency and usability. This includes acquiring corresponding index data, adjusting prices for splits and dividends, and handling missing data.

3. **Data Storage:** The preprocessed data will be stored as CSV files within dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. This organized structure facilitates efficient data management and access, and promotes reproducibility.

4. **Data Windowing:** The data will be structured using a 5-day rolling window, meaning each data point will consist of the OHLCV values for the preceding five days. This window serves as input for the model.

5. **Return Label Calculation:** The target variable, or return label, representing the 5-day future return, will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the current time and `h` is the holding period (5 days). This formula represents the percentage return over the holding period.

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day window data. These charts will visually represent the price action and will include volume information and a moving average indicator to provide context. These charts, along with their corresponding return labels, will form the dataset for the model.

## Project Setup and Data Acquisition

This section details the initial setup of the SCoVA project and the process of acquiring and preparing the necessary financial data for model training and evaluation. It focuses on establishing a robust data pipeline and ensuring data integrity.

### A. Project Initialization

1. **Project Documentation:** Maintain a comprehensive log of the project's evolution, including design decisions, challenges encountered, and solutions implemented. This documentation will be crucial for tracking progress, understanding the rationale behind choices, and ensuring transparency throughout the project lifecycle.

2. **Collaboration Setup:** Establish a collaborative environment by setting up a version-controlled code repository and preparing any necessary documentation for discussions, such as a project proposal or initial findings report.

### B. Data Acquisition and Preprocessing

This subsection outlines the steps involved in acquiring, preprocessing, and preparing financial data for the SCoVA project.

1. **Data Source:** Acquire Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance using the `yfinance` library (or an equivalent method). Specify the chosen financial instruments.

2. **Data Refinement:** Acquire relevant index data. Adjust historical prices for splits and dividends to ensure data accuracy and consistency. Document the specific indices used and the adjustment methodology.

3. **Data Storage:** Store the acquired and preprocessed data in CSV files within dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. Describe the file naming conventions and directory structure.

4. **5-Day Windowing:** Implement a 5-day sliding window to create input features and corresponding output labels. Detail the specific implementation of this windowing logic.

5. **Return Label Calculation:** Calculate the return label using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the current time and `h` is the 5-day holding period.

6. **Candlestick Chart Generation:** Generate candlestick chart images for each 5-day window, including volume and a moving average indicator (if applicable). Specify the charting library used and relevant chart parameters.

7. **Data Association:** Integrate the return label calculation with the candlestick chart image generation process, ensuring each image is linked to its corresponding return label. Clearly document the association method, such as filename correspondence or a shared data structure.

8. **Filename Convention:** Implement a clear filename convention that encodes the holding period (5 days) in the filenames or utilize separate CSV files for different holding periods. This will facilitate data management and analysis.

This detailed process ensures a robust foundation for subsequent model development and training within the SCoVA project.

## I. Project Setup and Data Acquisition

This section details the initial steps for setting up the SCoVA project and acquiring the necessary financial data. It covers project initialization, data acquisition, preprocessing, and storage, with a focus on cost-optimization during the development phase.

### A. Project Initialization

1. **Documentation:** Detailed documentation will be maintained throughout the project lifecycle, including the rationale behind decisions, challenges encountered, and solutions implemented. A key focus will be tracking and justifying cost-optimization strategies.
2. **Preparation for Review:** The project setup will be thoroughly documented in both the research paper and the code repository, including a clear explanation of the cost-saving measures implemented.

### B. Data Acquisition and Preprocessing

The following steps will be executed with a focus on minimizing server usage and associated costs, particularly during initial development and debugging:

1. **Data Source:** Yahoo Finance will be used to acquire OHLCV data. Initial testing and debugging will utilize small date ranges to minimize data volume and processing requirements.
2. **Data Preprocessing:** Preprocessing steps will be optimized to reduce computational load. This includes adjusting prices for splits and dividends to ensure data accuracy.
3. **Data Storage:** Data will be stored locally in CSV files within `stock_data/train/` and `stock_data/test/` directories to reduce reliance on cloud storage during initial development.
4. **5-Day Windowing:** A 5-day windowing logic will be implemented for both input and output, carefully designed for efficient memory usage and minimal processing overhead.
5. **Return Label Calculation:** The return label will be calculated as `(Close(t+h) - Open(t+1)) / Open(t+1)`, optimized for computational efficiency. The holding period (h) will be encoded in the filename or a separate CSV file.
6. **Candlestick Chart Generation:** Candlestick charts will be generated, incorporating volume and moving average data. The generation process will be optimized to minimize computational and storage costs.
7. **Integration:** The return label calculation and candlestick chart image generation will be integrated to minimize overhead.

The goal of this phase is to establish a robust and cost-effective data pipeline. The strategy for minimizing server costs during app development involves conducting initial tests and debugging using minimal data ranges and reduced epoch training sessions on the iOS device itself, minimizing dependence on cloud computing resources.

## I. Project Setup and Data Acquisition

This section details the initial project setup and data acquisition process. Given the focus on cost-effective testing, initial model development and testing will utilize mock data. This approach allows for early identification and resolution of potential issues before incurring the expense of real-world data.

### A. Project Initialization

1. **Documentation:** Maintain comprehensive documentation throughout the project lifecycle. This includes documenting the rationale behind design decisions, data generation processes (including edge cases and data formats), and any challenges encountered. This documentation will be essential for reproducibility, troubleshooting, and future reference.

2. **Preparation for Discussion:** Prepare comprehensive project documentation and a well-organized code repository to facilitate discussion and collaboration. This should include clear explanations of the data acquisition and preprocessing pipeline, testing strategies, and cost-saving measures.

### B. Data Acquisition and Preprocessing

This subsection outlines the acquisition and preparation of data for the initial stages of model development and testing.

1. **Mock Data Generation:** Initially, mock data will be used. This data will be designed to include various market scenarios, such as:

   - **Spikes:** Sudden, significant price changes.
   - **Flat Periods:** Periods of low volatility and minimal price movement.
   - **Gaps:** Missing data points.

   This allows for robust testing and validation of the data pipeline and model training process.

2. **Data Format and Storage:** Mock data will be generated in a suitable format (e.g., CSV) and stored in a designated directory.

3. **Dummy Model Implementation:** A simplified "dummy" neural network will be created for initial smoke tests. This serves as a placeholder for the full model during early development, allowing for validation of the training pipeline and update mechanism.

4. **Smoke Test Clarification:** The initial single-epoch training run on the dummy model is **not** for inference. Its purpose is solely to validate the end-to-end training and update process within the Core ML framework. This includes verifying:

   - Data loading.
   - Training loop execution.
   - Weight extraction.

5. **Data Pipeline Verification:** Critically, confirm the correct data flow from the data generator to the Core ML training session on the target device.

6. **On-Device Training Loop Validation:** Execute a complete training step (forward pass, loss calculation, backpropagation) on the target device with the dummy model. This verifies the Core ML framework's functionality and stability for on-device training.

## I. Project Setup and Data Acquisition

This section details the initial steps of the project, encompassing the setup of the project environment and the acquisition of the necessary financial data for model training and evaluation. While the initial phase concentrates on establishing a robust foundation using historical equity data, the overall project aims to incorporate real-time event detection and computer vision analysis of various financial instruments, including futures, options, and derivatives.

### A. Project Initialization

1. **Document Thought Process (Continuous Updates):** A clear record of the project's evolution, including decisions, rationale, and any changes in direction, will be maintained and continuously updated throughout the project lifecycle. This living document will be crucial for tracking progress and ensuring transparency.

2. **Prepare for In-depth Discussion (Paper and Repository):** Materials for in-depth project discussions, including a documented progress report and a readily accessible repository containing the project's code and data, will be prepared to facilitate collaboration.

### B. Data Acquisition and Preprocessing

This subsection details the acquisition and preprocessing of equity data for training and testing the model. This structured approach ensures data integrity and consistency, laying the groundwork for future expansion to other financial instruments.

1. **Data Source:** Open, High, Low, Close, and Volume (OHLCV) data will be acquired from Yahoo Finance.

2. **Data Acquisition and Adjustment:** OHLCV data, along with relevant index data, will be collected. Prices will be adjusted for splits and dividends to maintain data integrity.

3. **Data Storage:** Acquired and preprocessed data will be stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** 5-day windows of OHLCV data will be created to serve as model input.

5. **Return Label Calculation:** The return for each 5-day window will be calculated using the following formula, where 'h' represents the holding period (in days): `(Close(t+h) - Open(t+1)) / Open(t+1)`.

6. **Candlestick Chart Generation:** Candlestick chart images, incorporating volume and a moving average indicator, will be generated from the 5-day windows.

7. **Return Label Integration:** The calculated 5-day future returns will serve as labels for the corresponding candlestick chart images, ensuring synchronization between image and label generation.

8. **Filename Convention:** A consistent filename convention, either embedding the holding period directly or using separate CSV files, will be implemented.

### C. "Shocker" Event Considerations

While not implemented in this initial phase, considerations for handling unexpected market events (“shocker events”) will inform project design choices regarding data processing and model training. These considerations ensure the system can handle unforeseen market volatility.

1. **Definition of “Shocker” Events:** "Shocker events" will be defined using quantifiable characteristics within time series data, such as volatility spikes, unusual trading volume, and rapid price changes. This definition will guide data preprocessing and model training.

2. **Cognitive Threat Analysis Module (CTAM):** A dedicated CTAM will be developed to detect and analyze “shocker events” using lightweight CNN models applied to financial charts. This module will operate independently of the Dynamic Plane Generator (described elsewhere).

3. **CTAM Integration and Framework Response:** The system's response to the CTAM's generated "Threat Level" score, including potential modifications to the Dynamic Plane Generator's parameters, will be defined.

4. **Visual CTAM Development:** The CTAM's CNN models will be designed to balance accurate detection with computational efficiency for real-time anomaly detection.

This foundational phase, focusing on historical equity data and a basic 5-day window approach, prepares the project for future exploration of real-time event detection using computer vision techniques applied to a broader range of financial instruments. The inclusion of “shocker event” considerations ensures the system is designed for robustness and responsiveness to market volatility.

## I. Project Setup and Data Acquisition

This section outlines the initial steps for setting up the project and acquiring the necessary data. While the core objective is profit maximization, this phase focuses on establishing the foundation for model development and training. This involves careful documentation of design decisions and preparing for in-depth discussions regarding the project's architecture, particularly concerning the dual-system approach ("Flow Engine" and "Threat Engine") and addressing potential biases like mean reversion.

### A. Project Initialization

1. **Document thought process (Continuously update):** Maintain a detailed record of the project's evolution, including design decisions, experimental results, the rationale behind architectural choices, and potential limitations of the current approach. Specifically, document the limitations of a simplistic "wound and healing" error-correction model and the need for a more nuanced framework. Explicitly acknowledge and track the potential for mean reversion bias. From the outset, document the planned dual-system approach ("Flow Engine" and "Threat Engine"), along with planned improvements.

2. **Prepare for in-depth discussion (Paper and repository):** Prepare the necessary documentation and a version-controlled code repository to facilitate in-depth discussions. This should include documenting the current model's limitations, the proposed dual-system solution, and a technical explanation of how mean reversion influences the model's behavior. This will ensure transparency and reproducibility, facilitating external review and future contributions.

### B. Data Acquisition and Preprocessing

This subsection outlines the strategy for acquiring and preprocessing data to support both the "Flow Engine" (focused on normal market behavior) and the "Threat Engine" (designed for "shock" events). This may involve capturing data points related to market volatility or other indicators of extreme market movements.

The initial focus will be on equity data from Yahoo Finance (OHLCV data) for the development and validation of core functionalities. While options and futures data are part of the long-term vision, their incorporation will be deferred to future iterations due to their complexity. This simplified approach allows for rapid iteration and experimentation, ultimately leading to a more robust and scalable final product.

The following steps will be implemented:

1. **Data Source:** Yahoo Finance (OHLCV data).

2. **Data Acquisition and Preprocessing:** Acquire OHLC data and apply any necessary price adjustments (e.g., for splits and dividends) to ensure data accuracy. Index data acquisition is deferred to later stages.

3. **Data Storage:** Store acquired data in CSV format within dedicated `train` and `test` directories (`stock_data/train/` and `stock_data/test/`).

4. **Windowing:** Apply a 5-day rolling window to the data for both input features and output labels.

5. **Return Label Calculation:** Calculate returns using the formula `(Close(t+h) - Open(t+1)) / Open(t+1)`, representing the percentage change from the opening of the next day (t+1) to the close of the day at the end of the holding period (t+h).

6. **Candlestick Chart Generation:** Generate candlestick charts for each 5-day window, incorporating volume and a moving average. These will serve as visual input for the CNN models.

7. **Return Label Integration:** Ensure calculated return labels are correctly associated with their corresponding candlestick chart images.

8. **Filename Convention:** Implement a clear and consistent file naming convention that encodes the holding period within the filenames or uses separate CSV files for different holding periods.

While the specific details of data sources and preprocessing steps are outlined above, the overall strategy is guided by the architectural considerations of the dual-system approach and the need to capture both "normal" and extreme market behaviors. This structured approach lays the groundwork for subsequent model development and training.

## I. Project Setup and Data Acquisition

This section details the initial steps of the project, focusing on establishing a robust foundation for subsequent development. Given the architectural overhaul of Project Swaha, this phase emphasizes the importance of the Continuity role in overseeing the processes and expected outcomes.

The Continuity role will be central to this phase, ensuring seamless transitions between data acquisition, preprocessing, and storage. This involves:

- **Verifying Prerequisites:** Confirming the availability of necessary resources and tools, including access to Yahoo Finance, functionality of data acquisition scripts, and sufficient storage space.
- **Defining Expected Outcomes:** Clearly defining the format of acquired OHLCV data, the structure of processed data (5-day windows and calculated return labels), the storage location and naming convention (`stock_data/train/` and `stock_data/test/` with encoded holding period), and the successful generation of candlestick charts with corresponding return labels.
- **Overseeing all Processes:** Actively monitoring the data acquisition and preprocessing pipeline, ensuring data integrity and adherence to predefined specifications, tracking progress, identifying potential issues, and implementing necessary corrective actions.

### A. Project Initialization

1. **Documentation:** Maintain comprehensive documentation of the project's evolution, including design decisions, challenges, and rationale for chosen solutions.
2. **Preparation for In-Depth Discussion:** Organize relevant materials, including research papers and the project's code repository, to facilitate productive discussions and collaboration.

### B. Data Acquisition and Preprocessing

1. **Data Source:** Acquire Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance.
2. **Data Acquisition and Preprocessing:** Acquire OHLC and relevant index data. Adjust prices for stock splits and dividends to ensure data accuracy.
3. **Data Storage:** Store the processed data in CSV format within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.
4. **5-Day Windowing:** Implement a 5-day windowing approach, using 5 consecutive days of data as input features for the model.
5. **Return Label Calculation:** Define the return label as `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 't' is the last day of the 5-day window and 'h' is the holding period.
6. **Candlestick Chart Generation:** Generate candlestick chart images for each 5-day window, including volume and a moving average indicator.
7. **Return Label Integration:** Associate the calculated return labels with their corresponding candlestick chart images.
8. **Filename Convention:** Implement a consistent filename convention that encodes the holding period in the filenames or utilize separate CSV files for different holding periods.

This structured approach ensures data is readily available and correctly formatted for subsequent model development and training. The focus is on creating a robust and reliable data pipeline to support the overarching goal of profit maximization.

## I. Project Setup and Data Acquisition

This phase establishes the project foundation, including defining the scope, gathering data, and outlining the architectural approach. This architecture, centered around Continuity, Enforcement, Facilitation, and Specialization, ensures a robust and scalable system. A key focus is the deconstruction of processes into atomic units and their subsequent realignment according to these four pillars, creating a well-defined structure for data flow and inter-module communication.

### A. Project Initialization

This stage lays the groundwork for the project's architecture based on the four pillars (Continuity, Enforcement, Facilitation, and Specialization). This groundwork precedes data acquisition to ensure a structured approach.

1. **Document the Design Process:** Maintain a comprehensive log of architectural decisions, rationale for component atomization, and pillar assignments. This living document ensures transparency and traceability throughout the project lifecycle.

2. **Prepare for Architectural Discussion:** Prepare diagrams and documentation to facilitate in-depth discussions about the architectural restructuring based on the four pillars. This includes deconstructing existing frameworks, atomizing components, and reimagining the system architecture.

### B. Data Acquisition and Preprocessing (Continuity)

This stage details the acquisition and preprocessing steps, emphasizing continuity for a smooth and consistent data flow. Each component involved in data handling will be categorized according to the four pillars, with particular attention paid to "Specialist" components, whose interactions will be mediated by "Facilitator" components to ensure a clear separation of concerns.

1. **Data Source (Yahoo Finance):** Acquire OHLCV (Open, High, Low, Close, Volume) data from Yahoo Finance, implementing this retrieval in accordance with the defined architecture and the four pillars.

2. **Data Acquisition and Preprocessing:** Acquire necessary index data and adjust stock prices accordingly. Deconstruct this process into atomic functions, each classified by its architectural role (e.g., data cleaning as "Enforcement," data retrieval as "Continuity").

3. **Data Storage:** Store the acquired and preprocessed data in CSV files within dedicated directories (`stock_data/train/` and `stock_data/test/`), managed by designated "Facilitator" components.

4. **Windowing:** Implement logic to create 5-day windows of data for both input and output, atomizing this step and reevaluating its position within the data pipeline based on the four pillars.

5. **Return Label Calculation:** Calculate the return using the formula `(Close(t+h) - Open(t+1)) / Open(t+1)`, encapsulating this calculation as a distinct function and defining its architectural role (likely "Continuity" or "Specialization").

6. **Candlestick Chart Generation:** Generate candlestick chart images, including volume and moving average data. Design the image generation module with clear communication pathways, particularly with potential "Specialist" modules.

7. **Future Return Label Calculation:** Calculate 5-day future returns to serve as labels for model training.

8. **Integration of Label Calculation and Image Generation:** Combine the label calculation and image generation processes, adhering to the defined communication protocols and data flow.

9. **Filename Convention:** Implement a filename convention that encodes the holding period, ensuring clear data organization and supporting the "Continuity" of the data flow.

## I. Project Setup and Data Acquisition

This section outlines the initial steps for setting up the project and acquiring the necessary financial data for model training and evaluation. It covers project initialization for collaborative development, data gathering, preprocessing, and preparation for subsequent model development stages. While this document addresses data storage and access, it does _not_ include explicit instructions for specialized services like normalization, principal component analysis, or model inference. These services will be addressed in later stages as needed.

### A. Project Initialization

1. **Documentation:** Maintain thorough documentation of the project's progress, including design decisions, challenges, and modifications to the initial plan. This documentation is crucial for tracking progress, understanding the rationale behind decisions, and facilitating collaboration.

2. **Collaboration Infrastructure:** Establish a shared code repository and a collaborative document (e.g., a shared paper) for in-depth discussions, design documentation, and experimental results. This central hub will enable seamless collaboration and knowledge sharing.

### B. Data Acquisition and Preprocessing

This subsection details the acquisition and preprocessing of financial data, adhering to the project's architectural principles.

1. **Data Source:** Acquire Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance.

2. **Data Preprocessing:** Download the OHLCV data and any relevant index data. Apply necessary price adjustments (e.g., for splits and dividends) to maintain data integrity.

3. **Data Storage:** Store the preprocessed data in CSV files within dedicated directories for training (`stock_data/train/`) and testing (`stock_data/test/`) data.

4. **5-Day Windowing:** Implement a mechanism to create 5-day windows of OHLCV data as input features for the model.

5. **Return Label Calculation:** Calculate the 5-day future return using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window and `h` is the holding period (5 days). This represents the percentage return from the open price on the day following the window (t+1) to the close price at the end of the holding period (t+h).

6. **Candlestick Chart Generation:** Generate candlestick chart images for each 5-day window, incorporating volume and a moving average indicator.

7. **Data Integration:** Combine the candlestick chart generation and return label calculation to create a streamlined data preparation pipeline, ensuring data consistency.

8. **Filename Convention:** Implement a clear filename convention that encodes the holding period, either within the filenames themselves or using separate CSV files for different holding periods.

9. **Architectural Considerations:**
   - **Data Access (Enforcement):** Define data storage access protocols, ensuring secure and controlled access. Document the chosen storage location (`stock_data/train/` and `stock_data/test/`) and how Specialists will interact with the data through Enforcers.
   - **Facilitated Data Acquisition:** Specialists retrieving data (e.g., from Yahoo Finance) must interact with external systems through a Facilitator. Document the process for requesting and receiving data via the Facilitator to maintain controlled data flow.
   - **Function Naming Convention:** Adhere to a consistent function naming convention (e.g., suffixing with type or deriving type from class) for all functions involved in data acquisition and preprocessing.
   - **Functional Pillar Alignment:** Ensure responsibilities within data acquisition and preprocessing align with the four functional pillars (Continuity, Enforcement, Facilitation, and Specialization). For example, data validation and quality checks fall under Enforcement, while data retrieval and transformation might fall under Specialization.

This detailed procedure ensures the data is properly formatted, organized, and prepared for subsequent model development.

## I. Project Setup and Data Acquisition

This phase establishes the project foundation and acquires the necessary data. Following the SCoVA project's architectural considerations, the following principles guide setup and data acquisition:

- **Modular Design with Defined Interfaces:** Data acquisition and preprocessing will be a self-contained module with clearly defined interfaces for interaction with other components, such as the model training module. This modularity ensures that changes within the data pipeline don't necessitate widespread system modifications. The module will output data in a standardized format consumable by different models without modification.

- **Asynchronous Operations:** Asynchronous communication will enhance efficiency. For example, after data preprocessing, a message queued to the model training component triggers its operation, preventing blocking and improving overall responsiveness.

- **Access Control and Data Integrity (Resource Enforcer):** A "Resource Enforcer" component will manage access to raw and processed data, ensuring data integrity and consistency through access control mechanisms, data validation checks, and logged access requests. Only authorized modules will have write access to designated data storage locations (`stock_data/train/` and `stock_data/test/`).

- **Workflow Orchestration (Facilitator):** A "Facilitator" component will oversee the entire data acquisition and preprocessing workflow, coordinating tasks like data download, preprocessing, and storage. This centralized orchestration simplifies management and monitoring.

- **Object-Oriented Design for Maintainability:** Class inheritance will structure the data acquisition and preprocessing services, promoting a clear separation of concerns and code reusability. A base class might define common data handling operations, while derived classes implement specific preprocessing steps, like calculating return labels or generating candlestick chart images.

This section details the steps involved in setting up the project and acquiring the necessary data, including gathering, processing, and preparing the data for subsequent model training.

### A. Project Initialization

1. **Continuous Documentation:** Throughout the project lifecycle, meticulous documentation will track the thought process, decisions, rationale, progress, and any evolving direction. This ensures clarity and traceability of the project's evolution.
2. **Preparation of Discussion Materials:** Comprehensive documentation and a well-organized code repository will be prepared to facilitate in-depth project discussions, covering design, implementation, and results.

### B. Data Acquisition and Preprocessing

1. **Data Source:** Historical OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance.
2. **Data Preprocessing:** Acquired OHLC data and relevant index data will be preprocessed, including adjusting prices for splits and dividends to ensure accuracy and consistency.
3. **Data Storage:** Preprocessed data will be stored in CSV files within `stock_data/train/` (training set) and `stock_data/test/` (testing set).
4. **5-Day Windowing:** Data will be organized into 5-day input windows for the model, with corresponding output representing the target variable (e.g., future return).
5. **Return Label Calculation:** The target variable (return label) will be calculated as `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window and `h` is the holding period (e.g., 5 days).
6. **Candlestick Chart Generation:** Candlestick charts with volume and moving average indicators will be generated from the 5-day window data for visual input to the CNN model.
7. **Synchronized Label and Chart Generation:** Return label calculation will be integrated with candlestick chart generation to ensure synchronized creation of input images and target variables.
8. **Filename Convention:** The filename convention for candlestick chart images and return labels will encode the holding period, either within the filenames themselves or in a separate CSV file.

Specialized services like `NormalizeWindow`, `ComputePrincipalComponents`, `ProjectToPlane`, `TrainOneEpoch`, and the facilitator service `UI_Gateway` are relevant to later stages of the project and will be addressed in subsequent sections. This section focuses specifically on data preparation required before model development and training. The Resource Enforcer, managing access to Google Cloud Storage (GCS), where project data will likely reside, is critical for data integrity and security. Its exclusive read/write permissions ensure controlled access throughout the data pipeline. Any service requiring data from GCS must interact through the Resource Enforcer.

## I. Project Setup and Data Acquisition

This section outlines the initial steps for setting up the SCoVA project and acquiring the necessary data. The initial architecture will be a co-located monolith, but with considerations for logging and performance monitoring to facilitate future scaling and debugging. This approach allows for rapid prototyping and development while laying the groundwork for a more distributed architecture if needed.

### A. Project Initialization

1. **Document Thought Process (Continuous Updates):** A comprehensive system-wide logbook will be maintained from the outset. This logbook will capture architectural decisions, including the rationale for the initial monolithic approach and any future transitions. It will also document potential performance impacts, mitigation strategies, and the reasoning behind chosen communication methods. This documentation will be continuously updated throughout the project lifecycle.

2. **Prepare for In-Depth Discussion (Documentation and Repository):** Documentation will include performance testing plans and potential scaling strategies. The repository will be structured to allow for a clean transition to a microservice architecture if necessary, with clear separation of concerns and well-defined interfaces.

### B. Data Acquisition and Preprocessing

The data acquisition and preprocessing steps will be implemented within the initial monolith. Performance considerations, especially regarding data handling and processing speed, are paramount.

1. **Data Source: Yahoo Finance (OHLCV Data):** Acquire Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance using a robust method designed to handle potential network interruptions.

2. **Data Acquisition and Preprocessing (OHLC, Index Data, Price Adjustments):** Acquire OHLCV and relevant index data, then perform any necessary price adjustments. Integrate performance testing into this process to identify and address potential bottlenecks early.

3. **Data Storage (CSV Files):** Store acquired and preprocessed data in CSV format within designated `stock_data/train/` and `stock_data/test/` directories. The storage solution should be optimized for access speed.

4. **5-Day Windowing:** Implement 5-day windowing logic for input and output data, considering its potential impact on processing speeds, especially with large datasets.

5. **Return Label Calculation:** Calculate return labels using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`. Optimize this calculation for performance within the preprocessing pipeline.

6. **Candlestick Chart Generation:** Generate candlestick charts including volume and moving average. Ensure this process is efficient and doesn't introduce unnecessary overhead.

7. **5-Day Future Return Calculation:** Calculate and store 5-day future return labels.

8. **Integration of Return Label Calculation and Chart Generation:** Combine these processes to streamline the data flow and minimize processing time.

9. **Filename Convention:** Implement a clear filename convention that encodes the holding period. This improves data management and retrieval efficiency.

By proactively addressing performance considerations and implementing robust data handling strategies, this phase establishes an efficient data pipeline for subsequent model development and training. Continuous performance testing and monitoring will be essential to ensure responsiveness and scalability.

## I. Project Setup and Data Acquisition

This section details the initial steps of the project, including setting up the project environment and acquiring the necessary data. A key aspect of this phase is meticulous documentation of the project's progress and rationale.

### A. Project Initialization

This stage focuses on laying the groundwork for the project, ensuring a clear understanding of the objectives and a structured approach to development.

1. **Document the thought process:** Maintain comprehensive documentation of the project's evolution, including design decisions, rationale, and any changes in approach. This documentation will be crucial for understanding the project's trajectory and for future reference. This includes documenting the rationale for choosing a co-located monolith architecture.

2. **Prepare for in-depth discussion:** Organize the project materials, including code repositories and relevant documentation, to facilitate in-depth discussions and analysis. This ensures all relevant information is readily available for review and collaboration. Establish clear documentation standards and version control practices. The project's logging capabilities, including any visualization tools, will be a key part of this preparation, enabling detailed analysis of system behavior and performance.

### B. Data Acquisition and Preprocessing

This stage details the acquisition and preparation of the financial data used for model training and evaluation. Performance monitoring and logging will be integrated throughout this process.

1. **Data Source:** Acquire Open-High-Low-Close-Volume (OHLCV) data from Yahoo Finance. The data acquisition process will be logged to monitor performance and identify potential issues.

2. **Data Preprocessing:** Acquire relevant index data alongside the OHLCV data. Perform necessary price adjustments, such as adjusting for splits and dividends, to ensure data accuracy. Log the execution time of each preprocessing step.

3. **Data Storage:** Store the acquired and preprocessed data in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. Log the time taken for data storage operations.

4. **5-Day Windowing:** Implement a 5-day windowing approach. Input data will consist of 5 consecutive days of data, with the corresponding output relating to a specific period following this window. Log the performance of this step.

5. **Return Label Calculation:** Calculate the return label, the model's target variable, using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window, `h` is the holding period, and `Close(t+h)` and `Open(t+1)` represent the closing price at time `t+h` and the opening price at time `t+1`, respectively. Log the execution time of this calculation.

6. **Candlestick Chart Generation:** Generate candlestick charts from the OHLCV data, incorporating volume information and a moving average indicator. Log the performance of this potentially computationally intensive process.

7. **5-Day Future Return Calculation:** Calculate return labels representing the 5-day future returns and associate them with the corresponding candlestick chart images. Log the execution time of this step.

8. **Integration of Return Label Calculation and Chart Generation:** Integrate the return label calculation with the candlestick chart image generator to ensure each generated image has its corresponding return label readily available. Log the performance of this integration.

9. **Filename Convention Update:** Update the filename convention to incorporate the holding period, either by encoding it directly in the filenames or by using separate CSV files for different holding periods. Log this final data preparation step.

This structured approach to data acquisition and preprocessing ensures the data is properly prepared and organized for subsequent model development and training. The clearly defined data sources, preprocessing steps, storage locations, and calculations contribute to the reproducibility and maintainability of the project. The integrated logging provides a robust mechanism for performance analysis, debugging, and documentation.

## I. Project Setup and Data Acquisition

This phase establishes the project foundation and acquires the necessary financial data. The project will leverage a client-side heavy lifting approach with a lightweight backend orchestrator, influenced by the Universal Checklist's architectural considerations.

This distributed architecture requires careful planning of data flow and synchronization between client and server. Specific implementation details of data transfer, processing, and storage will be defined during this phase.

### A. Project Initialization

1. **Project Documentation:** A dedicated log will track the project's evolution, including design decisions, challenges, and solutions. This continuous documentation will ensure a clear record of development and facilitate future analysis and improvements. This aligns with the checklist items "Synthesize project progress overview," "Structure project overview," and "Construct comprehensive summary." Summaries will include sections on Architectural Philosophy, Functional System Design (including the Dynamic Rotating Plane and Dual-Engine Perception), Frontend & On-Device Computation, and Backend Architecture.

2. **Discussion Materials:** Relevant research papers and code repositories will be reviewed and prepared for in-depth discussion, ensuring a deep understanding of existing literature and best practices, informing decisions throughout the project lifecycle. The repository will reflect the project structure outlined in the progress summaries, enabling easy navigation and understanding of the codebase related to the Dynamic Rotating Plane and Dual-Engine Perception. The accompanying paper will provide theoretical background and implementation details of key architectural elements, facilitating informed discussion.

### B. Data Acquisition and Preprocessing

This stage focuses on obtaining and preparing the necessary data for training and evaluating the models. Initially, a simplified data pipeline using OHLCV data from Yahoo Finance and candlestick chart images will be implemented, to be enhanced later as the Dynamic Rotating Plane and Dual-Engine Perception components are integrated.

1. **Data Source:** Financial data (OHLCV) will be initially sourced from Yahoo Finance.

2. **Data Acquisition and Preprocessing:** Download OHLC and relevant index data, and adjust prices (e.g., for splits and dividends).

3. **Data Storage:** Store the processed data in CSV format within dedicated directories: `stock_data/train/` for training and `stock_data/test/` for testing datasets.

4. **5-Day Windowing:** Implement logic to create 5-day windows of data as model input.

5. **Return Label Calculation:** Calculate the return using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the current time step and `h` is the holding period.

6. **Candlestick Chart Generation:** Generate candlestick chart images from the 5-day window data, including volume and a moving average.

7. **Return Label Generation:** Calculate 5-day future returns as labels for training.

8. **Integration of Image and Label Generation:** Combine the candlestick chart image generation and return label calculation to ensure consistency.

9. **Filename Convention:** Implement a clear file naming convention that encodes the holding period within the filenames or uses separate CSV files for different holding periods, ensuring proper alignment between inputs and outputs.

### C. Client-Server Architecture

The project will utilize a client-side heavy lifting approach with a native app frontend (Swift/Core ML or Flutter/hybrid native ML) and a lightweight backend orchestrator (Python, Cloud Run/Functions with Firebase).

**Client-Side Responsibilities:**

- Data acquisition (potentially directly from Yahoo Finance via API).
- Data preprocessing (price adjustments, 5-day windowing).
- Candlestick chart image generation.
- Return label calculation.
- Local data storage.

**Backend Responsibilities:**

- Universal model hub: Storing and distributing the core model to client devices (including platform-specific conversions).
- Model updates: Incorporating client-side generated weight updates into the universal model.
- Data provisioning: Providing access to data that is too large or complex for client-side handling.
- Backend process orchestration (e.g., backtesting using Cloud Tasks).
- API interactions (if the client application doesn't directly access external data sources).

## I. Project Setup and Data Acquisition

This section details the initial steps required to set up the project and acquire the necessary data for model training and evaluation. This includes specifying the data source, outlining preprocessing steps, and defining the data storage structure. Establishing a robust and reproducible data pipeline is key for this phase.

### A. Project Initialization

1. **Project Documentation:** Maintaining comprehensive documentation is crucial throughout the project lifecycle. This includes documenting the project's evolution, design choices, rationale behind decisions, challenges encountered, and their solutions. Thorough documentation will aid in identifying potential gaps in project considerations from both coding and algorithmic perspectives, ensuring all aspects of the project are addressed.

2. **Preparation for Review:** Preparing for a thorough project review involves organizing both the code repository and accompanying documentation. The repository should be clearly structured, well-documented, and conducive to collaborative development. Accompanying documentation should provide context, explain the methodology, and clearly present findings, facilitating a comprehensive review and addressing any identified gaps.

### B. Data Acquisition and Preprocessing

1. **Data Source:** Open-High-Low-Close-Volume (OHLCV) data will be acquired from Yahoo Finance. This widely-used source provides the necessary price and volume information for training and evaluating our models. Additionally, relevant index data will be acquired to provide market context.

2. **Preprocessing:** The acquired OHLCV data will be preprocessed to ensure data accuracy. This includes adjusting for stock splits and dividends.

3. **Data Storage:** The preprocessed data will be stored in CSV format within dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. This organized structure facilitates efficient data management and access.

4. **Windowing:** A 5-day sliding window approach will be implemented to create input sequences for the models, allowing them to learn patterns and trends based on five consecutive trading days' worth of data.

5. **Return Calculation:** The target variable, representing the 5-day future return, will be calculated using the following formula: `(Close(t+5) - Open(t+1)) / Open(t+1)`, where 't' is the last day of the input window.

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day window data, incorporating volume and moving average indicators. These charts provide visual representations of price action and will serve as input for the model.

7. **Data Integration:** The return calculation will be integrated with the candlestick chart generation process to ensure data accuracy and synchronization.

8. **Filename Convention:** The filename convention will incorporate the holding period (5 days) to ensure proper data organization. This can be achieved by encoding the holding period directly in the filename or by using separate CSV files.

While this section defines the core data pipeline, the broader architectural implications for data management and future integration with a feature store for model training and serving will be addressed in subsequent sections. This proactive approach aligns with the project's emphasis on scalability, maintainability, and reproducibility.

## I. Project Setup and Data Acquisition

This section details the initial project setup and the acquisition of the necessary data. A robust data pipeline is crucial for both model training and backtesting, particularly given the need for real-time simulation and analysis. This pipeline will be designed with careful consideration for its integration with the paper trading environment, ensuring data consistency between simulated and live trading scenarios.

### A. Project Initialization

1. **Documentation:** Maintain comprehensive and continuously updated documentation of all project decisions, including the rationale for data sources, preprocessing steps, and any considerations for risk management and the integration of paper trading functionality (including the design and interaction of the `Paper_Brokerage_Simulator` and the `Live_Execution_Enforcer`).

2. **Project Materials:** Prepare detailed documentation of the project architecture, including the data pipeline schema, chosen features (e.g., Order Book Imbalance, Weighted Average Price, Bid-Ask Spread), and the data handling logic. This documentation should be readily available for in-depth discussions and collaboration.

### B. Data Acquisition and Preprocessing

1. **Data Source:** This project will use [Specify Data Source, e.g., Yahoo Finance for OHLCV data or Zerodha for live WebSocket tick data]. The chosen data source must meet the requirements for both live trading and paper trading simulations within the `Paper_Brokerage_Simulator`, ensuring realistic and consistent data across both environments. The rationale for this selection should be documented thoroughly.

2. **Data Preprocessing:** The following preprocessing steps will be implemented, consistently across both live and paper trading environments:

   - [If using Yahoo Finance] Acquire OHLC and relevant index data, adjusting historical prices for splits and dividends to ensure accuracy and consistency.
   - [If using Zerodha] [Describe specific preprocessing for Zerodha data, e.g., handling of tick data, aggregation into OHLCV, etc.].
   - Implement a 5-day rolling window approach for model input and output, creating sequential data segments of 5 consecutive trading days.
   - Calculate the return label using the formula `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` represents the current time step and `h` represents the holding period. This captures the percentage return over the holding period.
   - Generate candlestick chart images with volume and moving average indicators for input to the CNN model. These charts must be paired with their corresponding calculated 5-day future return labels (target variables). The holding period should be clearly encoded in the filenames or using separate CSV files.

3. **Data Storage:** Data will be stored in [Specify storage location, e.g., CSV files in `stock_data/train/` and `stock_data/test/`]. The `Paper_Brokerage_Simulator` will access market data for simulations and maintain separate storage for paper trading portfolios (e.g., 'paper_portfolio' in Firestore) to prevent interference with live trading data. The implementation details for data access and storage in both live and paper trading environments should be documented.

## I. Project Setup and Data Acquisition

This section details the project's initial setup and the acquisition and preparation of the necessary data. Due to the constraints of the Zerodha data source, the initial data acquisition strategy has been adjusted to prioritize order book data over spread calculations.

### A. Project Initialization

This stage involves the initial setup and documentation of the project, including adapting to the limitations of the Zerodha data source.

1. **Document Thought Process (Continuous Updates):** Comprehensive documentation of the decision-making process will be maintained throughout the project. This includes the rationale for shifting from spread-based analysis to order book analysis due to the fixed price increments of Zerodha's market depth data. This documentation will be continuously updated to reflect the evolving project understanding.

2. **Prepare for In-depth Discussion (Paper and Repository):** A dedicated repository will house the code, data, and documentation to facilitate in-depth discussions and analysis. This repository will reflect the shift to order book analysis, with any deprecated code related to spread calculations removed or clearly marked.

### B. Data Acquisition and Preprocessing

This stage encompasses the acquisition of both candlestick and market depth data, along with the necessary preprocessing steps. Given that Zerodha's market depth data has fixed price increments of ₹0.05 between -₹0.25 and +₹0.25, the following adjustments are necessary:

1. **Data Source: Yahoo Finance (OHLCV Data):** Historical OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance for candlestick chart generation.

2. **Data Acquisition and Preprocessing (OHLC, Index Data, Price Adjustments):** Relevant index data will be acquired alongside OHLCV data, and price adjustments will be applied to ensure data consistency.

3. **Data Storage (CSV Files):** Acquired data will be stored in CSV format within designated `stock_data/train/` and `stock_data/test/` directories.

4. **5-Day Windowing:** A 5-day sliding window will be applied to create input sequences and corresponding output labels for the model.

5. **Return Label Calculation:** The return label will be calculated as `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the current time and `h` is the holding period (e.g., 5 days).

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the OHLCV data, incorporating volume and moving average indicators.

7. **Return Label Integration:** The return label calculation will be integrated into the candlestick chart generation process for synchronization.

8. **Filename Convention (Holding Period):** The filename convention will encode the holding period, either directly in the filename or using separate CSV files.

9. **Market Depth Data Source: Zerodha:** Market depth data will be acquired from the Zerodha API.

10. **Discard Spread Calculation:** Spread calculations will be omitted due to the limited precision of Zerodha's market depth data.

11. **Focus on Order Quantity and Count:** The focus will be on the quantities and number of orders at each of the five fixed bid/ask levels provided by Zerodha.

12. **Order Book Imbalance Calculation:** A `CalculateOrderBookImbalance` service will calculate the Order Book Imbalance (OBI), a normalized value between -1.0 and +1.0, representing selling and buying pressure. This OBI value will be used as a fourth dimension (alongside Time, Price, and Volume) in the Dynamic Plane Generator.

13. **Market Depth Heatmap Visualization:** A `GenerateDepthQuantityHeatmap` service will generate a heatmap visualizing market depth over time. This heatmap will use color intensity to represent order quantity at each price level and time, serving as input for a specialized CNN.

## I. Project Setup and Data Acquisition

This section outlines the initial steps for setting up the project and acquiring the necessary data for training the `MarketDepthAnomalyDetector` to identify visual patterns indicative of market shocks. While the current project scope doesn't encompass real-time market data and features like "Price Improvement Rate" or "Book Resilience Score," the fundamental principles of data acquisition and preprocessing remain crucial. This section focuses on establishing the foundational data required for core model development, primarily using OHLCV data from Yahoo Finance.

### A. Project Initialization

1. **Documentation:** Maintain a detailed log of all design decisions, rationale, challenges encountered, and justifications for data sources, preprocessing techniques, and the chosen return label formula. This documentation is essential for transparency and reproducibility.

2. **Collaboration Setup:** Organize all code, documentation, and experimental results within a dedicated repository. Prepare a paper or report providing project context, methodology, and preliminary findings to facilitate collaboration and feedback.

### B. Data Acquisition and Preprocessing

This subsection details the acquisition, preprocessing, and storage of financial data.

1. **Data Source:** Utilize Yahoo Finance to obtain historical Open, High, Low, Close, and Volume (OHLCV) data for the selected assets.

2. **Data Preprocessing:** Download the necessary OHLCV and relevant index data. Implement routines to adjust historical prices for corporate actions such as splits and dividends to ensure data consistency and accuracy.

3. **Data Storage:** Store the preprocessed data in CSV format within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **Windowing:** Implement a 5-day windowing approach to structure the OHLCV data, forming the basis for model input features.

5. **Return Label Calculation:** Calculate the return label using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `h` represents the holding period.

6. **Candlestick Chart Generation:** Generate candlestick chart images for each 5-day window, incorporating volume data and a moving average indicator. These images will serve as visual inputs for the CNN model.

7. **Label Synchronization:** Ensure that the return label calculation is synchronized with the candlestick chart image generation, pairing each image with its corresponding return label.

8. **Filename Convention:** Implement a consistent filename convention that encodes the holding period information, either directly in the filenames or through a separate CSV file, to facilitate clear data management.

## I. Project Setup and Data Acquisition

This section details the initial steps of the SCoVA (Snapshot Computer Vision Algorithm) project, encompassing project initialization, data acquisition, and preprocessing. It establishes the foundation for model training and evaluation by outlining the procedures for acquiring, processing, and structuring the necessary data.

### A. Project Initialization

1. **Project Documentation:** Maintain comprehensive documentation throughout the project lifecycle. This includes recording the project's evolution, rationale for decisions, challenges encountered, and potential solutions.

2. **Project Repository:** Establish a dedicated project repository (e.g., GitHub) containing the project's code, data, and documentation. This facilitates collaboration, version control, and reproducibility.

### B. Data Acquisition and Preprocessing

The following steps outline the data pipeline for generating the candlestick images and calculating the corresponding return labels required for training the SCoVA model.

1. **Data Source:** Acquire historical Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance for the target financial instruments.

2. **Data Preprocessing:** Process the raw OHLCV data, adjusting for stock splits and dividends as necessary. Acquire relevant index data if required for benchmarking or feature engineering.

3. **Data Storage:** Store the preprocessed data in CSV format within designated `stock_data/train` and `stock_data/test` directories.

4. **5-Day Windowing:** Implement a 5-day sliding window to create input sequences for the model.

5. **Return Label Calculation:** Calculate the 5-day future return using the following formula: `(Close(t+5) - Open(t+1)) / Open(t+1)`, where 't' represents the current time, and 'Close(t+5)' and 'Open(t+1)' denote the closing price 5 days after and the opening price 1 day after time 't', respectively.

6. **Candlestick Chart Generation:** Generate candlestick chart images from the 5-day windowed data. Include volume information and a moving average as visual elements within the charts.

7. **Integration of Return Labels and Images:** Associate each generated candlestick chart image with its corresponding 5-day future return label.

8. **Filename Convention:** Implement a clear file naming convention that encodes the holding period (5 days) in the filenames or utilize a separate CSV file to map filenames to their respective holding periods.

This structured data pipeline ensures data consistency and provides the necessary inputs for building and evaluating the SCoVA model. The limitations of real-time order book data for direct market prediction informed the project's focus on developing an "anxiety level" indicator. This indicator, trained on historical order book data, will assess the potential for errors and inform dynamic model adjustments. Further details regarding the acquisition and preprocessing of this historical order book data will be provided in subsequent sections.

## I. Project Setup and Data Acquisition

This section details the initial setup of the SCoVA (Snapshot Computer Vision Algorithm) project and the acquisition and preprocessing of the necessary financial data. A robust and well-defined setup process is crucial for reproducibility and efficient development.

### A. Project Initialization

1. **Document Thought Process:** Maintain a comprehensive log of the project's development, including design decisions, rationale, explorations of alternative approaches (e.g., "Non-Hierarchical Asymmetric" concepts), and any challenges encountered. This documentation is crucial for transparency and understanding the evolution of the SCoVA algorithm.

2. **Prepare for In-Depth Discussion:** Prepare materials for comprehensive discussion and review. This includes a formal paper outlining the project's goals, methodology, and findings, and a well-organized code repository containing the SCoVA implementation. The repository should be structured to facilitate understanding and reproducibility.

### B. Data Acquisition and Preprocessing

This subsection details how SCoVA's reliance on "snapshots" influences data acquisition and preprocessing. The discrete, dynamic nature of the visual data necessitates careful consideration of data sources, formatting, and storage. The specific process of generating these "snapshots" will be detailed in the Model Development and Training section.

1. **Data Source:** Financial data, including Open, High, Low, Close, and Volume (OHLCV) values, will be sourced from Yahoo Finance, a readily available and widely used source of historical market data. Additionally, relevant index data will be acquired.

2. **Data Preprocessing:** Acquired OHLCV and index data will be preprocessed. This includes adjusting historical prices for splits and dividends to ensure data accuracy and consistency.

3. **Data Storage:** Preprocessed data will be stored in CSV format within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. This organized structure facilitates efficient data management.

4. **5-Day Windowing:** A 5-day sliding window approach will be implemented. This involves using a sequence of 5 consecutive days of data as input for the model and predicting the target variable over the subsequent 5-day period.

5. **Return Label Calculation:** The return label, representing the percentage change in price, will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where t is the last day of the input window, h is the holding period (5 days), and `Close(t+h)` is the closing price on the last day of the holding period, and `Open(t+1)` is the opening price on the first day after the input window.

6. **Candlestick Chart Generation:** Candlestick charts visualizing the 5-day input window will be generated. These charts will incorporate volume data and a moving average indicator to enhance visual representation and analysis.

7. **Return Label and Chart Integration:** The return label calculation will be seamlessly integrated with the candlestick chart generation process, ensuring each image is paired with its correct target variable.

8. **Filename Convention:** A clear and consistent file naming convention will be implemented. This convention will incorporate the holding period (5 days) within the filenames or utilize a separate CSV file to store this information for accurate data tracking and organization.

## I. Project Setup and Data Acquisition

This section details the initial steps of establishing the project and acquiring the necessary financial data for model training. A key focus is on engineering features that capture the asymmetric nature of market dynamics, differentiating the behavior of market rises and falls. This structured approach ensures a clear, reproducible workflow for subsequent model development.

### A. Project Initialization

1. **Documentation:** Maintain comprehensive, continuously updated documentation of the project's development. This includes rationale for design choices, feature engineering decisions, data preprocessing steps, and explorations of potential enhancements. This living document will be crucial for understanding the project's trajectory, justifying design choices, and supporting future analysis.
2. **Communication & Collaboration:** Prepare a structured repository and documentation to facilitate in-depth discussions and analysis of the project's architecture and implementation details. This will be particularly important for communicating the complexities of integrating computed feature vectors into the Vision Transformer model.

### B. Data Acquisition and Preprocessing

This stage encompasses acquiring standard financial data, enriching it with engineered features that capture market asymmetries, and storing the processed data for efficient model training.

1. **Data Acquisition & Standard Preprocessing:** Acquire OHLC (Open, High, Low, Close), volume, and index data. Perform standard preprocessing steps such as adjusting for splits and dividends, as necessary. Specific data sources will be documented within the project repository.
2. **Asymmetric Feature Engineering:** Develop an `AsymmetricFeatureEngine` service to compute a rich feature vector capturing the asymmetry in recent price action and volume. This service will process a window of raw data and output a vector of calculated asymmetric features to serve as a context token for the Vision Transformer model. These features include:

   - **Price & Volatility Asymmetry:** Upside vs. Downside Volatility (calculated using semi-deviation), Volatility Skewness, and Volatility Kurtosis. These capture the differences in magnitude and distribution between upward and downward price movements.
   - **Volume & Participation Asymmetry:** Accumulation/Distribution Ratio (measuring volume flow on up vs. down days) and Order-to-Quantity Asymmetry (comparing bid-side and ask-side order-to-quantity ratios). These gauge the conviction and participation levels of buyers and sellers.
   - **Correlation Asymmetry:** Price-Volume Correlation State, calculated separately for positive and negative return periods, to discern periods of market fear and greed.

3. **Data Storage & Organization:** Store the acquired OHLCV data, along with the calculated asymmetric features, in an appropriate format (e.g., Parquet, HDF5) for efficient access during model training. The specific storage format, directory structure, and any databases used will be documented within the project repository.

## I. Project Setup and Data Acquisition

This section details the initial steps of setting up the project and acquiring the necessary financial data. While the current focus is on establishing the core data pipeline, future integration of contextual data, such as market regime identification and terrain-based features, is planned to enhance the model's predictive capabilities. These elements will be integrated in later stages.

### A. Project Initialization

This stage involves setting up the project for organized development and preparing for detailed discussions.

1. **Continuous Documentation:** A detailed log will track the project's evolution, including design decisions, challenges, and potential solutions. This living document will ensure transparency and facilitate future development.

2. **Preparation for In-depth Discussion:** A research paper draft and a code repository will be created and maintained alongside development. The repository will house the project's codebase, enabling version control, collaboration, and reproducibility. The paper will outline the project's goals, methodology, and findings.

### B. Data Acquisition and Preprocessing

This stage focuses on acquiring and preparing the financial data for training and evaluating the model. While full details will be finalized as the project progresses, the anticipated steps include:

1. **Data Source:** Yahoo Finance.

2. **Data Acquisition and Preprocessing:** This will involve acquiring OHLC and index data, adjusting prices as necessary, and potentially other preprocessing steps.

3. **Data Storage:** Data will be stored in CSV files within designated directories. Specific file organization will be detailed later.

4. **Windowing:** A 5-day windowing approach is currently anticipated for input data. This may be subject to change based on experimental results.

5. **Return Label Calculation:** The specific formula and process for calculating future returns will be defined and documented.

6. **Data Visualization:** Candlestick charts will be generated to visualize the data. Implementation details will be documented.

7. **Visualization and Label Integration:** The process for combining the calculated return labels with the candlestick chart images will be defined.

8. **Filename Conventions:** Clear and consistent filename conventions will be established, potentially encoding the holding period or utilizing a separate CSV file for metadata.

### C. Future Data Integration: Contextual Features

While not part of the initial implementation, the following contextual features are planned for future integration:

1. **Market Regime Identification:** An unsupervised clustering model (e.g., Gaussian Mixture Model or Self-Organizing Map) will be trained offline on historical data to identify distinct market regimes (estimated 4-8 regimes). This will generate a Regime ID (integer) to be used as context for the Vision Transformer. This functionality will likely be encapsulated within a new "IdentifyAsymmetricRegime" service. Further planning is required to define the necessary historical asymmetric feature vectors.

2. **Terrain-Based Input:** "Terrain"-based input, categorizing environmental metrics (e.g., humidity, temperature, Air Quality Index) into predefined bins, will be explored for the Vision Transformer. This aims to simplify input while potentially increasing explanatory power. Specific terrain categories, data sources, and the potential impact of information loss due to categorization will be addressed in a later stage.
   Data Acquisition and Preprocessing

This phase focuses on acquiring, preprocessing, and storing the financial data that will be used to train and evaluate the model.

1. **Data Acquisition:** Historical Open, High, Low, Close, and Volume (OHLCV) data will be acquired from Yahoo Finance. This data forms the foundation for generating candlestick charts and calculating returns. Relevant index data may also be incorporated to provide broader market context.

2. **Preprocessing:** The acquired OHLCV data will be preprocessed. This includes handling missing values, adjusting prices for splits and dividends, and potentially normalizing the data.

3. **Storage:** The preprocessed data will be stored in CSV format within dedicated directories: `stock_data/train/` for the training dataset and `stock_data/test/` for the testing dataset.

4. **Windowing:** The data will be organized into 5-day windows. Each window represents a sequence of 5 consecutive trading days and serves as input for the model.

5. **Return Calculation:** The target variable, representing the return, will be calculated using the following formula: `Return(t+h) = (Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the 5-day window and `h` is the holding period (1-5 days).

Candlestick Chart Generation and Labeling

This phase focuses on generating candlestick chart images and associating them with their corresponding return labels.

1. **Chart Generation:** Candlestick charts will be generated from the 5-day window data. These charts will visually represent price and volume information and may incorporate moving averages as additional technical indicators.

2. **Label Integration:** The calculated returns will be associated with their respective candlestick chart images. This combined data forms the training dataset.

3. **Pipeline Streamlining:** The processes of generating candlestick charts and calculating corresponding return labels will be streamlined into a single, efficient pipeline.

4. **Filename Convention:** The filenames of the candlestick chart images, or a separate metadata CSV file, will encode the holding period (h). For example, a filename might follow the format `YYYY-MM-DD__YYYY-MM-DD__3.25__h=3__ABB.ST.png`. This ensures that the data is properly labeled and easily retrievable.

Project Initialization

This phase focuses on setting up the project for success and ensuring clear communication and collaboration.

1. **Continuous Documentation:** Maintain a comprehensive log of the project's evolution, including design decisions, challenges, solutions, and any insights gained. This documentation will be invaluable for tracking progress, ensuring transparency, and facilitating future development.

2. **Preparation for Discussion:** Thoroughly familiarize yourself with the associated research paper and code repository. Ensure a clear understanding of the project's core concepts, methodology, and findings. Prepare all necessary materials for in-depth discussions and be ready to articulate the relationship between the code, the experiments, and the expected outcomes. This preparation will be crucial for effective collaboration and presentations.

### A. Project Initialization

This phase establishes the project's foundation and comprises two key activities: meticulous documentation and thorough preparation for collaborative discussions.

1. **Continuous Project Documentation:** Maintain a comprehensive and evolving record of the project's progress, including design decisions, rationale, encountered challenges, and their respective solutions. This living document, maintained within the project paper and repository documentation, will be invaluable for tracking the project's trajectory, facilitating future analysis and refinement, and supporting later reporting.

2. **Preparation for In-depth Discussions:** Prepare the project paper and repository to facilitate productive discussions and collaboration. The repository should contain well-organized code, data, and documentation. The paper should provide a comprehensive project overview, including:

   - **Data Preprocessing:**

     - **Candlestick Chart Images:** Generate candlestick chart images from five consecutive trading days of OHLCV (Open, High, Low, Close, Volume) data. These images, incorporating candlesticks, volume bars, and a moving average, will serve as input for the CNN model.
     - **Return Labels:** Calculate 5-day future percentage returns using the formula `((Close(t+5) - Open(t+1)) / Open(t+1)) * 100`. These labels will be the target output for the CNN model during training.

   - **Model Architecture and Evaluation:**
     - **CNN Input Data:** The CNN model will _only_ process the generated candlestick images. It will _not_ be trained on raw date values, actual price values, or ticker symbols (unless inadvertently introduced during image generation).
     - **Model Evaluation Metric:** Model performance will be evaluated using a regression metric, such as Mean Squared Error (MSE) or Root Mean Squared Error (RMSE), based on its ability to predict returns over a defined _n_-day horizon. The specific value of _n_ will be determined and documented.

These initial steps ensure a strong foundation, promoting clear communication and a structured approach to project development. Addressing these aspects early will minimize potential ambiguities and streamline subsequent stages.

## A. Project Initialization

This phase lays the groundwork for the project, focusing on establishing a clear direction and preparing for detailed investigations into reproducing and improving upon the CNN-based stock trading strategy presented in [cite referenced paper]. Initial analysis of the paper reveals promising predictive capabilities but also identifies key inefficiencies in the trading strategy's practical implementation, primarily its high turnover and lack of sophisticated trade filtering. This initialization phase will focus on documenting these observations and preparing for a deeper dive into addressing these limitations.

This phase comprises two key steps:

1. **Documenting the evolving thought process:** A living document will be maintained to record the project's evolution. This documentation will capture not only progress updates but also, crucially, the rationale behind decisions, alternative approaches considered, and challenges encountered. This will serve as a valuable reference throughout the project lifecycle. Specifically, this document will initially detail the analysis of the referenced paper, focusing on the points outlined below.

2. **Preparing for in-depth discussions:** A project repository will be established to house the code, data, and experimental results, facilitating transparent communication and collaboration. A structured document outlining the project's goals, methodology, and expected outcomes will also be developed to frame discussions. These resources will be essential for productive discussions regarding the project's technical details and strategic direction, particularly concerning the complexities of refining the trading strategy.

The following key areas derived from the initial analysis of the referenced paper will guide the project's direction:

- **Trading Strategy Inefficiency:** While the referenced model demonstrated predictive potential, its practical application suffered from high turnover (trades every 5 days), uniform portfolio weighting, and a lack of intelligent trade filtering, resulting in transaction costs negating much of the pre-cost alpha. This project will explore refinements such as dynamic position sizing, prediction confidence thresholds, and potentially alternative trading frequencies. The rationale for selecting a 5-candlestick input window, based on the work of Jiang et al. [cite Jiang et al.], will be documented, along with a discussion of the 5-day output window, which mirrors the input but is currently based on practical trading considerations rather than rigorous empirical validation.

- **Small-Cap Performance and Short-Selling Constraints:** The referenced paper reported significant outperformance within the small-cap segment (First North All-Share). However, potential constraints related to short-selling availability within this segment need careful consideration. This project will investigate the impact of these constraints and explore mitigation strategies, including focusing on long-only strategies or incorporating short-selling availability data.

- **Handling of Unsuccessful Trades:** The referenced paper's methodology for handling unsuccessful trades, particularly in the absence of a stop-loss mechanism, requires further clarification. A thorough understanding of how these trades were identified and their impact on overall performance is crucial for replicating and improving upon the reported results. Further investigation into the use of hard labels for training and validation, and the potential benefits of a soft-label approach using a probability distribution, will also be documented.

## A. Project Initialization

This phase establishes the project's foundation, focusing on comprehensive documentation and preparation for in-depth discussions about the project's scope, methodology, and expected outcomes. The following steps are essential:

1. **Document Thought Process:** Maintain a detailed and continuously updated record of the project's evolution. This "living document" should capture the initial project goals, brainstorming sessions, research findings (including the initial research on predicting "rally time" and its feasibility in stock market algorithms as described in the Universal Checklist), decisions made, and the rationale behind each decision. It should also document challenges encountered, explorations of alternative approaches, and any shifts in thinking. This documentation will be invaluable for tracking progress, understanding the project's trajectory, facilitating future analysis, and ensuring transparency.

2. **Prepare for In-Depth Discussion:** Prepare materials to facilitate productive discussions about the technical aspects of the project. This includes:

   - A preliminary project paper outlining the research on rally time prediction, including the chosen data model for label generation (scanning forward in time for target price achievement), the proposed multi-head neural network architecture (using EfficientNet features and predicting both return and rally time), the exploration of survival analysis models for time-to-target predictions, and the plan to enhance trade decision logic with rally time predictions.
   - A well-organized and documented code repository containing the project's code and data, enabling easy navigation and comprehension.

This preparation will ensure discussions are focused, grounded in a shared understanding of the project's goals and methodology, and address the technical feasibility and potential challenges of each approach. It will also provide stakeholders with the necessary information to contribute meaningfully.

## A. Project Initialization

This phase lays the groundwork for the SCoVA project by outlining the initial steps required before delving into data acquisition and model development. The primary focus is establishing a clear understanding of the project scope and preparing for in-depth exploration. This entails two key activities:

1. **Document the thought process (Continuously update progress and thinking):** Maintain a detailed, evolving record of the project's progression. This documentation should capture brainstorming sessions, design decisions, the rationale behind specific choices (e.g., justification for using a Vision Transformer (ViT), rationale for the chosen input length), and any challenges encountered. This living document will be a valuable reference throughout the project lifecycle, particularly as the architecture and training of the ViT progress, given the complexities of variable-length input sequences and positional embeddings.

2. **Prepare for in-depth discussion (Paper and repository):** Compile preliminary research and establish a well-structured code repository to facilitate focused discussions. Gather relevant papers on Vision Transformers, candlestick chart analysis, and sequence modeling. The repository will be the central hub for code development and experimentation. Its structure should facilitate the exploration of different approaches, including:

   - Benchmark comparison experiment: Comparing "static picture vs. picture-pair vs. sequence" architectures to evaluate the best approach for incorporating temporal context.
   - 2-Image paired input dataset generator: Designing and implementing a data generator to create paired candlestick images for the picture-pair architecture.
   - CNN + LSTM hybrid architecture: Exploring a hybrid architecture combining CNNs for feature extraction and LSTMs for temporal modeling.
   - Vision Transformer (ViT) implementation: Implementing a ViT model for analyzing candlestick chart image sequences, considering two options: encoding images with EfficientNet and feeding the resulting vectors into the transformer, or using ViT-style patch embedding. Experimentation will include varying input image counts (N=3, 4, and 5) and implementing strategies for handling variable-length input sequences, such as masking and padding.
   - Feature comparison: Evaluating image subtraction versus feature subtraction for generating delta features within the ViT architecture.
   - Backtesting framework: Developing a framework for evaluating model performance.
   - Image sequence generator: Constructing a generator to prepare data for the ViT model.
   - Data model design: Defining the dataset of image sequences, implementing features like delta features, and defining the input data as sequential candlestick windows. This includes considering the format and structure of the data to be used for training and evaluation.

### Project Initialization

This phase establishes the project's foundation by defining goals, outlining the experimental framework, and preparing for detailed discussions about the architecture, data model, and experimental design. Thorough documentation throughout this phase is crucial.

**A. Defining Project Goals and Evaluation Metrics:**

To objectively compare different memory management strategies (A, B, and C), a comprehensive suite of metrics will be defined. These include performance measures such as the Sharpe Ratio, directional accuracy (predicting up/down movements), Mean Squared Error (MSE), and rally-time prediction accuracy. This multifaceted evaluation provides a holistic view of each strategy's strengths and weaknesses. Furthermore, the project aims to explore how the Vision Transformer (ViT) model can handle real-time, variable-length financial data, specifically for predicting future candlestick images. This shift from predicting return values directly to predicting candlestick images requires careful consideration and documentation regarding its conceptual soundness, comparing human visual interpretation of trading patterns with the model's approach. The potential advantages of image-based prediction—representation richness, uncertainty modeling, causal reasoning, training supervision, interpretability, and generative flexibility—will be analyzed and documented.

**B. Designing the Experimental Framework:**

A rigorous experimental framework will ensure a fair comparison of the memory management options. This framework will use the same dataset across all options while varying the specific memory handling approaches. The analysis will consider the tradeoffs between flexibility, performance, and training efficiency for each option, as well as the impact of added temporal context on predictive power.

**C. Preparing for In-depth Discussions and Development:**

A comprehensive repository containing code and documentation will be established to facilitate collaboration and review. This includes:

- **Prototype Design:** Details of the image-based prediction model prototype, specifying the use of a CNN decoder or a transformer-based image generator (e.g., ViT or U-Net) for forecasting candlestick patterns.
- **Return Extraction:** The methodology for extracting open, high, low, and close values from the generated candlestick images, including techniques like analyzing pixel locations or rendering image data. This process connects image-based predictions to actionable trading signals.
- **Experimental Validation:** The experimental setup for model validation, including details on training, post-processing generated images to extract returns, comparison with baseline models using metrics like RMSE, SSIM/LPIPS, and backtested profit performance.
- **Risk and Drawback Assessment:** Analysis of the potential downsides of the image-based model, including the less direct evaluation compared to numerical prediction, potential compounding errors, loss of direct reward supervision, and the ambiguity of translating predictions into financial implications. This analysis also includes a comparison with existing models like scalar regression and probabilistic return models, considering aspects like output type, supervisory signal, connection to trading decisions, interpretability, risk of ambiguous predictions, data requirements, and modeling complexity.
- **Reusable Components:** The repository will include reusable components like data loaders, masking logic, and ViT wrappers for current and future work with financial time series data.

This thorough initialization process ensures a clear project direction, informed decision-making, and effective communication among stakeholders. The documentation generated will serve as a valuable reference throughout the project lifecycle.

### A. Project Initialization

This phase lays the foundation for the SCoVA project by clarifying project goals and preparing for subsequent development stages. Two crucial steps ensure a strong start:

1. **Document the thought process (Continuously update progress and thinking):** Maintain comprehensive, evolving documentation. This record should encompass not only progress updates but also the rationale behind decisions, alternative approaches considered, challenges encountered, and any shifts in direction. This "living document" promotes transparency, reproducibility, and provides invaluable context for the final dissertation. It should also include relevant literature supporting methodological innovations and a preliminary dissertation structure with chapter outlines, abstract drafts, and a bibliography.

2. **Prepare for in-depth discussion (Paper and repository):** Facilitate focused discussion about project architecture and direction by preparing a conceptual framework (a draft "paper") and a practical foundation (the initialized "repository"). The paper should outline core project ideas and the proposed shift to a 2D plane with rotational axes and a dynamic center point, including a comparative analysis with the 3D approach, focusing on representing complex motions like parabolic trajectories. The repository, even with minimal initial setup, enables concrete, code-focused conversations and ensures alignment between the dissertation and codebase from the outset. This preparation also enables discussions about potential benefits and drawbacks of the 2D approach.

## A. Project Initialization

This phase establishes the theoretical groundwork for representing financial time series data using a dynamic 2D plane instead of a static 3D Cartesian coordinate system. This dynamic plane, conceptually similar to a Frenet frame or tangent plane, features rotational axes and an origin that shifts with market movements. This initial phase prioritizes thorough documentation and preparation for in-depth analysis.

The following key tasks are essential:

1. **Continuously Document the Evolving Thought Process:** Maintaining a comprehensive record of the project's evolution is crucial. This documentation should include:

   - **Rationale and Expected Benefits:** Clearly articulate the reasons for transitioning to a dynamic 2D plane and the anticipated advantages of this approach.
   - **Potential Challenges and Limitations:** Identify potential obstacles and limitations, such as path dependence, singularities, and computational overhead. Also, discuss the feasibility of representing parabolic trajectories within the 2D framework via axis rotation and dynamic origin refocusing.
   - **Connections to Established Mathematics and Physics:** Explore and document the relationships between the dynamic 2D plane and relevant concepts like parallel transport, affine connections, Frenet-Serret frames, the SE(3) group, and local inertial frames in general relativity.
   - **Practical Applications and Limitations:** Document potential applications in fields like robotics, computer graphics, and data compression, along with any inherent limitations.
   - **Algebraic Formalism:** Develop a concise algebraic summary using precise mathematical notation and equations to formally represent the dynamic 2D plane concept.
   - **Future Research Directions:** Outline future research avenues, including formulating an explicit rotation law, investigating curvature invariants, generalizing to surfaces beyond the plane, and exploring the application of the 2D dynamic frame to scenarios like the three-body problem (potentially using pairwise relative coordinates, a barycentric frame, or a shape space representation). Consider recent advances in the three-body problem, such as machine learning approximations and new periodic solutions, and analyze their potential connection to the 2D frame concept.

2. **Prepare for In-Depth Discussions and Analysis:** This involves preparing materials and establishing a platform for collaborative discussions. Create a dedicated repository for code, data, and documentation. Draft preliminary documentation, including an analysis of coordinate transformations, manifolds, "bending space," and the concept of a moving frame on a 1-dimensional differentiable manifold, focusing on the implications of attaching an orthonormal frame to each point on the manifold and rotating it along the price trajectory. Investigate how a parabolic curve, typically represented along the z-axis in 3D, can be encoded within the dynamic 2D chart using axis rotations, and analyze the information balance and degrees of freedom, considering the two plane coordinates (u,v) and the three Euler angles (or rotation matrix) defining the moving frame's orientation. Develop a conceptual 2D prototype to visualize the motion representation and validate the viability of the 2D approach before committing to full development.

This thorough initialization phase will lay a solid foundation for subsequent model development and implementation by establishing a shared understanding of the core representational shift from a 3D to a dynamic 2D perspective.

## Project Initialization

This critical initial phase establishes a solid project foundation through meticulous documentation and preparation for in-depth discussions. It encompasses setting up the development environment, defining visualization strategies, and organizing materials for collaboration.

**1. Continuous Documentation:** Maintaining a detailed record of the project's evolution is paramount. This living document should capture not only decisions made, but also the rationale behind them, alternative approaches considered, and any challenges encountered. This comprehensive record, updated throughout the project lifecycle, will serve as a valuable resource for tracking progress, understanding the project's trajectory, and facilitating future analysis and refinement. Suitable formats include a research journal, a dedicated section within the project's repository, or a combination of both. This documentation will be crucial for connecting the visualizations with underlying data and model interpretations.

**2. Preparation for In-Depth Discussion:** Effective communication and collaboration are essential for project success. This involves preparing a well-structured repository and relevant documentation to facilitate productive discussions with advisors, collaborators, and other researchers.

    * **Repository:**  The codebase should be well-organized, with clean code, comprehensive documentation, and clear instructions for running experiments. This ensures that all parties can quickly grasp the project's core concepts and contribute effectively.

    * **Documentation:**  Prepare a document articulating the research problem, the proposed solution, and anticipated experimental results.  This could take the form of a research paper or a less formal report, depending on the context. The theoretical underpinnings of the project, informed by research in areas such as numerical conditioning of moving frame coordinates, the stability of these coordinates during rapid rotations, and the biological basis of egocentric and allocentric mapping, should be clearly articulated.

**3. Visualization Strategy:** Initial visualization efforts will focus on clear and informative representations of project data using `matplotlib` and `numpy` (the use of `seaborn` is prohibited).

    * **3D Helix:**  A standalone 3D helix visualization (radius 1.0, pitch 0.5, 4 turns) will be generated, with labeled axes (X, Y, and Z) and titled "3D Helix in Laboratory Coordinates."  Subplots should be avoided.

    * **Planar Traces:** Standalone planar trace visualizations will also be created, with a focus on clarity and enhancements like tooltips, annotations, or dedicated help documentation. Subplots should be avoided.

    * **Technical Considerations:**  Data storage and logging for planar coordinates (u, v) and moving frame orientation must be carefully considered to enable spiral path reconstruction while respecting memory and storage limitations. Visualizing the relationship between the 3D helix and its 2D unfolded representation requires careful design due to the single-chart constraint.  Separate charts or animation with frame snapshots are potential solutions, although animation performance ("heavy") needs evaluation. Static, separate charts might offer greater efficiency.

## A. Project Initialization

This critical initial phase establishes a clear project direction and lays the groundwork for productive collaboration. It encompasses meticulous documentation and thorough preparation for in-depth discussions.

1. **Document Thought Process (Continuously Update Progress and Thinking):** A comprehensive log of the project's evolution is crucial. This living document will capture not only the steps taken but also the rationale behind design decisions, challenges encountered, and their solutions. It should also document evolving ideas, experimental results, and any changes in direction. This continuous record will serve as a vital reference throughout the project lifecycle, aiding future analysis, troubleshooting, and communication. This documentation should address emerging challenges and explore potential solutions. For this project, this includes considering the computational cost of incorporating Principal Component Analysis (PCA), potential risks and challenges of dynamic PCA (including overfitting, computational overhead, integration complexity, loss of absolute reference frame, instability, model dependency, complexity of explanation, edge cases, and baseline fairness), and the need for a baseline and comparison strategy to quantify the benefits of dynamic PCA refocusing (compared against no PCA, static PCA). While a future consideration involves plotting Orientation vs. Time and visualizing a straight line trace on a helix (including UI/UX considerations), these will be addressed in later stages.

2. **Prepare for In-depth Discussion (Paper and Repository):** Effective collaboration requires thorough preparation. This includes establishing a well-structured repository for all project materials, encompassing the codebase (well-documented), experimental results, datasets, and any other pertinent files. Concurrently, a formal paper outlining the project's core concepts, theoretical underpinnings, design choices, goals, methodology, and current status should be developed. This paper will serve as the foundation for in-depth discussions and collaborations. Specifically for this project, the paper will detail how candlestick data influences the dynamic projection system, including the chosen mathematical transformation for rotation (e.g., affine transform, PCA rotation, learned rotation) and its parameters. The paper will also clearly articulate the ultimate goal of dynamic projection: whether it's achieving prediction invariance to previous trend direction or enabling the model to focus on relative local movement. Investigating PCA rotation within the model, as a potential method for prioritizing relative local movement, will be a starting point. Architectural considerations, such as dynamically rotating and re-centering candlestick image sequences using PCA, potentially integrated within the Vision Transformer or CNN pipeline, will also be explored in the paper. This dynamic PCA integration aims to create a locally optimized frame of reference based on the latest market movements.

## A. Project Initialization

This section outlines the crucial initial steps required for setting up the project effectively, laying the groundwork for a well-structured and documented research process. This includes preparing for in-depth discussions about the project's architecture, especially the dynamic plane implementation, and establishing a robust documentation practice.

**1. Documentation and Discussion of the Dynamic Plane:**

The core concept of dynamically adjusting the perspective of price data requires thorough documentation and discussion from the outset. This involves:

- **Dynamic Origin Implementation:** Document how the coordinate system's origin shifts dynamically with the price data within a 2D plane analogy, ensuring alignment with the overall architecture.
- **Dynamic Rotation Implementation:** Detail the dynamic rotation process: calculating the local movement vector (explained below), rotating the coordinate frame to align the X'-axis with this vector, and setting the Y'-axis orthogonal to it. The origin will then be translated to the current price point. This rotation will be applied to a small window of price data (e.g., the last 5-10 candlesticks).
- **Local Movement Vector Calculation:** Define the method for calculating the local movement vector (v): `v = P_current - P_previous`, where P_current and P_previous represent the price and time (X,Y) coordinates for the current and previous time steps, respectively.
- **Rotation Matrix Calculation:** Document the calculation of the rotation matrix. First, calculate the rotation angle (θ): `θ = arctan(v_y / v_x)`. Then, construct the rotation matrix R(θ): `R(θ) = [[cos(θ), sin(θ)], [-sin(θ), cos(θ)]]`, which will be applied to the surrounding candlestick window.
- **Dynamic Origin Shift:** Detail the dynamic origin shift after rotation, translating coordinates to place the current price point at the origin (0,0). This centering enables visualization of oscillations as deviations around the zero point on the Y'-axis.

**2. Project Documentation and Repository Preparation:**

- **Continuous Documentation:** Maintain a detailed record of the project's evolution, including brainstorming sessions, design decisions, challenges, and solutions. This documentation is crucial for tracking progress, understanding the rationale behind decisions (especially regarding the dynamic snapshot generation, rotation artifact handling, volatility jump handling, and consistent axis scaling), and facilitating future iterations.
- **Repository Management:** Establish a central repository for all project materials: code, data, documentation, and experimental results. This will serve as a single source of truth, facilitating collaboration and knowledge sharing. Start structuring the research paper, outlining key sections and anticipated content. This framework will guide the organization of findings and ensure a coherent narrative, particularly when addressing technical aspects like the pseudocode for the dynamic rotating snapshot generator and addressing user concerns about computational cost, alternative dimensionality reduction techniques, and mandatory image-based input (candlestick images, Heiken Ashi charts, or other image types). Early documentation should address the rationale for reducing three axes (time, price, and volume) to two rotational axes on a dynamic 2D plane, justifying the dynamic refocusing of the origin, and explaining the connection between the conceptual diagram of the dynamic rotation and the RotatingSnapshotGenerator module. Furthermore, the rationale for the dynamic plane implementation should be clearly articulated, including the simplification of calculations, the decision to use image-based input, and exploring alternative data-driven focus methods such as an ablation study.

## Project Initialization

This phase establishes the foundation for the project, encompassing conceptualization, documentation, and preparation for subsequent development stages. We will focus on the dynamic plane generator, its integration with Vision Transformers (ViTs) or Convolutional Neural Networks (CNNs), and the visualization of its output.

The following key activities will be undertaken during this initialization phase:

1. **Continuous Documentation:** Maintaining comprehensive documentation throughout the project lifecycle is crucial. This living document will track design decisions, challenges, solutions, and the evolution of the project's core concepts, including the dynamic plane generator and its integration with chosen machine learning models (ViTs or CNNs). This documentation should also detail the rationale behind specific implementation choices, including library selections (NumPy, Matplotlib, PIL). A dedicated project journal or collaborative platform will facilitate this ongoing documentation process.

2. **Preparation for In-Depth Discussions and Collaborative Development:** To ensure productive discussions and efficient collaboration, the following resources will be prepared:

   - **Project Repository:** A version-controlled repository will house the project's code, data, documentation, and relevant research papers. This will facilitate transparent collaboration and efficient progress tracking.
   - **Conceptual Diagrams and Examples:** Visual aids, including a diagram illustrating the evolution of the dynamic 2D plane with market movements and example images of both raw candlestick input and transformed output, will be prepared. These visuals will be essential for communicating and validating the dynamic coordinate system and data transformation process. These examples should specifically illustrate how the dynamic origin impacts rotational calculations and snapshot generation.
   - **Pseudocode and Implementation Plan:** Detailed pseudocode for the Rotating Dynamic Plane Generator algorithm, encompassing data input, local window definition, movement vector calculation, Principal Component Analysis (PCA) application, coordinate system rotation, and 2D plane reconstruction, will be developed. This pseudocode will form the basis for the Python implementation, leveraging libraries like NumPy, Matplotlib, and PIL. The implementation plan will address optimization for batch processing of time series data and account for the technical constraint of requiring at least two data points for computation. The relationship between the angle theta, the dynamic origin, and the rotational axes within this dynamic context will be explicitly addressed. The method for defining the local frame of reference at each time step using PCA (or a similar method) on a local window of previous candles (e.g., 5-10) to determine movement vectors and identify principal components will be clearly outlined.
   - **Key Discussion Points:** Prepared discussion points will include:
     - Input formatting for the chosen machine learning model (ViT or CNN) based on the dynamic plane generator's output (images or numerical features).
     - Detailed design and implementation of the Rotating Dynamic Plane Generator, including candlestick data handling, plane rotation logic, snapshot rendering, window selection, movement calculations, dynamic frame construction, and refocusing mechanisms.
     - Optimization strategies for batch processing and addressing the minimum data point requirement.

This comprehensive initialization phase will ensure that subsequent development and discussions are well-informed, productive, and aligned with the project's goals.

## Project Initialization

This crucial initial phase focuses on establishing a solid foundation for the project. This involves documenting key decisions and preparing for in-depth discussions to ensure alignment on project direction, especially regarding the animation framework, dynamic plane visualization, and data handling.

### Documentation

Maintain comprehensive documentation throughout the project lifecycle. This documentation should capture the evolving thought process, design choices, encountered challenges, and proposed solutions. Key areas for documentation include:

- **Data Handling:** Detail the approach for handling long data sequences (Simulate Longer Sequences) and generating datasets for training CNNs and ViTs based on the dynamic plane principle (Batch Generate for Training). This ensures a robust and scalable data pipeline.
- **UI/UX Design:** Document plans for:
  - Visualizing candlestick data before and after transformation, including example images.
  - Experimenting with smoothing techniques like Heikin-Ashi for dynamic plane visualization (Experiment with Smoothing).
  - Developing an animation to illustrate the plane's evolution (Animation of Plane Evolution), addressing potential initialization errors (Fix Animation Initialization Error).
- **Animation Framework:** Document decisions related to:
  - Mitigating PCA instability with limited or collinear data, especially in early animation frames (e.g., delaying PCA calculation or using alternative dimensionality reduction techniques). See Chat1.json message #96.
  - Handling single-point frames gracefully (e.g., displaying a placeholder or a static representation). See Chat1.json message #96.
  - Ensuring correct offset formatting to avoid dimension mismatches, particularly with limited initial data points. See Chat1.json message #96.
  - Delaying rotation and plotting until at least two data points are available to ensure proper animation functionality. See Chat1.json message #96.
- **Dynamic Plane Visualization:** Document the rationale for architectural choices, anticipated challenges, and potential solutions related to dynamic point movement, live frame rotation and recentering, and the minimum number of points required for stable rotation calculations.

### Discussion Preparation

Compile relevant materials, including research papers, code repositories, conceptual diagrams, and code snippets, to facilitate productive discussions. These discussions should cover the documented thought process and address the technical aspects of the project, ensuring shared understanding and alignment among stakeholders.

### A. Project Initialization

This section outlines the crucial initial steps for setting up the SCoVA (Stock Chart Visual Analysis) project, focusing on establishing a robust foundation for developing, testing, and visualizing the dynamic Heiken-Ashi charting technique. These initial steps involve simulating market conditions, generating Heiken-Ashi charts, and implementing core functionalities for data transformation and visualization. Thorough documentation and preparation for collaborative discussions are also emphasized.

1. **Documentation and Collaboration:**

   - **Continuous Documentation:** Maintain a detailed record of the project's evolution, including brainstorming sessions, design decisions, rationale, challenges encountered, potential solutions, and progress updates. This living document will serve as a primary reference throughout the project lifecycle, inform the dissertation writing process, and provide valuable insights for future reference and improvements.

   - **Collaboration Preparation:** Compile relevant materials, including preliminary research findings, code repositories, and design documents, to facilitate productive discussions with collaborators. A well-organized repository and a concise summary paper will ensure efficient communication and alignment on project goals, methodologies, and progress. This includes preparing visual aids, such as example images of Heiken-Ashi input before and after transformation (excluding animation of the process itself), to illustrate the data model.

2. **Environment Setup and Core Functionalities:**

   - **Heiken-Ashi Implementation:** Implement functions for generating and visualizing standard Heiken-Ashi candlesticks from OHLC (Open, High, Low, Close) data. Green candles should represent upward price movement (close ≥ open), and red candles should represent downward movement (close < open).

   - **Dynamic Rotation and Recentering:** Implement a function to dynamically rotate and recenter the Heiken-Ashi data points using the midpoint between the open and close values of each candle. Create a visualization function to display the rotated and recentered Heiken-Ashi data on a 2D plane.

   - **Saving Visualizations:** Save both the standard and rotated Heiken-Ashi charts as PNG images to designated locations (e.g., `/mnt/data/standard_heiken_ashi.png` and `/mnt/data/rotated_dynamic_plane_heiken_ashi.png`) for easy sharing and inclusion in reports.

3. **Market Simulation and Data Generation:**

   - **Complex Price Pattern Simulation:** Implement a simulation of realistic market conditions, incorporating elements like rallies, drops, and recoveries to test the agent's handling of varied market dynamics.

   - **Choppy Market Simulation:** Implement the `generate_choppy_candlesticks(n=30)` function to simulate chaotic, sideways market patterns with frequent, rapid price fluctuations. This will be crucial for assessing the robustness of the dynamic plane transformation under volatile conditions.

4. **Heiken-Ashi Visualization with Simulated Data:**

   - **Standard Heiken-Ashi Visualization:** Generate and save a standard Heiken-Ashi chart from the choppy market data (`/mnt/data/standard_heiken_ashi_choppy.png`).

   - **Rotated Dynamic Heiken-Ashi Visualization:** Apply the dynamic plane transformation to the choppy Heiken-Ashi data and visualize the results, saving the output to `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`. This allows direct visual comparison with the standard chart, highlighting the transformation's impact.

These initialization steps establish a solid foundation for evaluating the dynamic plane implementation and informing subsequent model development and training. The generated charts will provide visual insights into how the transformation affects the representation of different market conditions, particularly choppy, sideways movements. While a functional requirement is to visualize the parabolic trajectory in the reconstructed plane, the initial focus will be on the real dataset, model pipeline, and small validation tests, with the visualization being addressed in a later stage.

## A. Project Initialization

This phase focuses on establishing a solid foundation for the SCoVA project by outlining initial steps and preparatory work. These steps, while preliminary, are crucial for maintaining clarity and ensuring smooth progression throughout the project lifecycle. We will begin by documenting our thought process and preparing resources for in-depth discussions regarding the project's scope, technical challenges, and proposed solutions.

1. **Document Thought Process (Continuously Updated):** A detailed record of the project's evolution will be meticulously maintained. This living document will capture brainstorming sessions, design decisions, the rationale behind chosen approaches, and any encountered challenges. It will serve as a valuable resource for understanding the project's trajectory, facilitating future development, analysis, and knowledge transfer. This documentation will integrate insights from related discussions, including the "Thought Experiment Discussion," specifically regarding PCA Rotation Stabilization Techniques, Loss Functions for Dynamic Space, and shifting the model's learning to relational perception.

2. **Prepare for In-depth Discussions:** Before commencing core development, thorough preparation for detailed technical discussions is essential. This involves compiling relevant research papers, establishing a well-organized code repository, and gathering preliminary findings. This centralized repository will house code, data, documentation, and serve as the primary platform for collaboration and information sharing. The repository and relevant papers will be shared with stakeholders to facilitate focused conversations on key architectural choices, such as the shift towards relational perception in the model's learning process and the algorithmic implementation of error signals within the market movement algorithm. The in-depth discussions will also cover the implementation of PCA, addressing crucial aspects like its impact on image feature representation, relational model design in the PCA space, geometric pattern recognition, interpretability projection back to the original Time-Price-Volume space, and the careful selection of window size and smoothing parameters to mitigate noise and potential overfitting.

## A. Project Initialization

This phase establishes the project's foundation, preparing for in-depth analysis and subsequent development. It involves two key activities: meticulous documentation of the evolving thought process and thorough preparation for technical discussions.

1. **Document the Evolving Thought Process:** Maintaining a comprehensive, continuously updated record of the project's evolution is crucial. This documentation should capture:

   - The rationale behind key decisions, including architectural choices (e.g., PCA plane consistency between prediction and reality matrices, as discussed in item e872c072-61d5-409e-a49c-1aed6fe8b1e1).
   - Explorations of different approaches and the reasons for selecting the chosen methodologies.
   - Challenges encountered and their resolutions.
   - Clarification of key concepts (e.g., the number of rotational angles and distance vectors, as raised in Chat1.json message #117).
   - Detailed explanations of methodologies, including calculations (e.g., error correction, distance error, and angle error, from Chat1.json messages #118).

   This living document will serve as a valuable resource throughout the project lifecycle, facilitating understanding, debugging, future refinements, and contributing to the final dissertation. It explains the "why" behind the "what" of the project's development.

2. **Prepare for In-depth Discussions:** Productive collaboration requires thorough preparation. This includes establishing a central, logically structured, and well-documented repository for code, data, and supporting materials. The repository will house:

   - Code related to frame correction, healing phase logic, dynamic plane generator integration, error visualization, and the frame intervention metric.
   - Data used for analysis and validation.
   - Diagrams illustrating key concepts (e.g., rotations and prediction error, including global frame transformation and local vector misalignment, as suggested in Chat1.json message #118).

   In addition, prepare preliminary documentation (the "paper") outlining the project's scope, goals, initial findings, underlying theory, implemented algorithms, and justification for chosen metrics. This documentation, along with the repository, will equip stakeholders with the information necessary for meaningful contributions and serve as the foundation for detailed discussions and analysis.

### A. Project Initialization

This phase focuses on establishing a solid foundation for the project by documenting the initial thought process, clarifying core concepts related to dynamic PCA plane management and error assessment within a shifting coordinate system, and preparing resources for in-depth discussions. The following crucial steps must be completed:

1. **Document Thought Process (Continuously updated):** Given the complexity of error calculations involving dynamic PCA planes, maintaining a detailed and continuously updated record of the evolving project rationale, design decisions, and insights is essential. This documentation should address:

   - **Robust PCA Frame Management:** Explain the chosen method (Freeze Frame or Reproject Realization) and how it addresses potential shifts in market data distribution while ensuring relational consistency between predicted and realized data points. Discuss how the chosen method mitigates potential model hallucinations from minor market fluctuations. Clearly distinguish these approaches from existing distance and angular error calculations.
   - **Error Calculation with Shifting PCA Planes:** Detail the rationale for calculating deviation errors between PCA1 and PCA2 for both real and predicted values, explicitly addressing the impact of shifting PCA planes on these calculations. Contrast this approach with the limitations of a static plane assumption.
   - **Total Error Calculation:** Provide a clear derivation and explanation of the Total Error calculation (Total Error = Vector Deviation Error + Frame Shift Error), justifying the inclusion of both components.
   - **Frame Drift Error Measurement:** Describe the methodology for measuring frame drift error using principal angles between subspaces (PCA1<sub>t</sub> and PCA1<sub>t+1</sub>, and PCA2<sub>t</sub> and PCA2<sub>t+1</sub>). Explain the formula: Frame Error = α _ Angle between PCA1 vectors + β _ Angle between PCA2 vectors, including the rationale for the tunable weights α and β.
   - **Pseudocode for Total Error Calculation:** Include pseudocode that outlines the step-by-step process for calculating the Total Error, incorporating both Vector Error and Frame Shift Error.

2. **Prepare for In-depth Discussion (Paper and repository):** Compile all relevant documentation, including the documented thought process, pseudocode, and any initial experimental results or numerical examples, into a preliminary paper outlining the problem, proposed solutions, and planned experiments. This paper should include visualizations clarifying the differences between the "Freeze Frame" and "Reproject Realization" approaches. Establish a dedicated repository to manage the codebase, pseudocode for the "Freeze and Correct" module, and related artifacts. Both the paper and repository should clearly demonstrate an understanding of the proposed techniques and their implementation details, including handling PCA frame shifts and projecting predicted and realized data points. These resources should be readily available to facilitate in-depth discussions and collaboration. This preparatory work enables focused and productive conversations about the project's technical details. Specific details like "Short-Term Frame Consistency," "Visual Simulation of Frame Alignment," "Data Structure for PCA Basis," and "Visualizations of Freeze Frame and Projection," while important, will be addressed in subsequent model development and visualization sections.

## A. Project Initialization

This phase establishes the project's foundation by outlining the core concepts, initiating documentation, and preparing for detailed discussions. These initial steps are crucial for maintaining a clear project trajectory and facilitating effective communication.

1. **Document Thought Process (Continuous Updates):** Maintain comprehensive, evolving documentation of the project's progression. This includes design decisions, rationale behind chosen solutions, encountered challenges, and alternative approaches considered. This "living document" should track the development of all project components, including the Error Trend Detector, Healing Phase, error monitoring strategies, correction factor application, decay mechanisms, and the system's transitions between Correction Mode and Healing Phase. It should also detail the rationale for the proposed default weights α, β, and γ, and include a numerical example demonstrating the total error calculation from individual atomic error components. Formal pseudocode for multi-weight error computation and an explanation of angle unit normalization for consistent error aggregation should also be included.

2. **Prepare for In-Depth Discussion (Resources and Repository):** Consolidate all necessary materials for productive discussions. This includes the aforementioned documentation, a dedicated repository for code, data, and project artifacts, and potentially a draft paper outlining project goals and methodologies. The repository will house all simulation-related elements, including those for Vector Deviation, PCA Frame Drift, and Frame Drift Error as a confidence indicator. It should also contain explorations of error component weighting, normalization strategies for distance and angular errors, and the implementation of the weighted error calculation combining these components. This organized approach will streamline communication and support efficient collaboration.

## A. Project Initialization

This phase establishes a robust foundation for the project by addressing data representation, preprocessing for Principal Component Analysis (PCA), and documentation.

**1. Data Preprocessing for PCA:** Price (P), Time (T), and Volume (V) data will be transformed using PCA. Each feature within a rolling window of _N_ data points will be independently centered and scaled using z-score normalization. This involves subtracting the mean (μ) and dividing by the standard deviation (σ) of each feature within the window. This ensures equal feature contribution to PCA, regardless of original units.

The formulas for calculating mean (μ) and standard deviation (σ) for each feature (t for Time, p for Price, v for Volume) are:

- μ<sub>t</sub> = (1/N) \* Σ t<sub>i</sub>
- σ<sub>t</sub> = sqrt((1/N) \* Σ (t<sub>i</sub> - μ<sub>t</sub>)<sup>2</sup>)

Similar calculations apply for Price (p) and Volume (v). The scaled data matrix X<sub>scaled</sub> is then constructed as:

- X<sub>scaled</sub>[,i,.,.] = ((t<sub>i</sub> - μ<sub>t</sub>)/σ<sub>t</sub>, (p<sub>i</sub> - μ<sub>p</sub>)/σ<sub>p</sub>, (v<sub>i</sub> - μ<sub>v</sub>)/σ<sub>v</sub>)

**Specific Feature Handling:**

- **Time (T):** Two options are available:

  - **Relative Time Index:** For windows consistently containing the last _N_ trading candles, use a sequence of integers from 1 to _N_ representing the relative time within each window. This sequence should also be z-score normalized.
  - **Absolute Clock Time:** To incorporate diurnal patterns, calculate time deltas (Δt<sub>i</sub>) using the formula Δt<sub>i</sub> = (timestamp<sub>i</sub> - μ<sub>t</sub>) / σ<sub>t</sub>. Note that large gaps in timestamps can inflate σ<sub>t</sub> and diminish the impact of time on PCA results.

- **Volume (V):** Due to its heavy-tailed distribution, volume data often requires transformation before z-score normalization:
  - **Log Transformation:** Apply a log transformation, such as v<sub>i</sub>' = log(1 + v<sub>i</sub>), to mitigate outlier effects.
  - **Robust Scaling:** Use the median and interquartile range (IQR). Subtract the median volume from each data point and divide by the IQR. This is less sensitive to extreme outliers.

**2. Healing-by-Correctness System Design:** The trading agent will incorporate a healing process driven by predictive accuracy, using a rolling prediction correctness buffer. The correction factor applied during healing will be proportionally reduced as predictive capability recovers, as measured by the rolling buffer.

**3. Project Documentation and Communication:** Thorough documentation of the entire project is crucial, including:

- Design decisions and rationale for chosen methods, including thresholds for the healing system and the `dynamic_decay_rate` function.
- Data preprocessing steps and their justification.
- Results of toy example simulations.
- Supporting literature and research findings.

This documentation will be maintained in a dedicated repository and continuously updated. This repository will also contain all code, experiments, and materials for in-depth discussions, facilitating effective collaboration and a shared understanding throughout the project lifecycle. This will support the development of the dissertation and any associated publications.
Data Transformation and Preparation

The following transformations prepare the data for Principal Component Analysis (PCA):

1. **Fractional Elapsed Time:** Timestamps are converted to fractional elapsed time within each window. This is calculated by subtracting the minimum timestamp from each timestamp and dividing by the total time elapsed within the window (the difference between the maximum and minimum timestamps). This captures the relative timing of events and handles irregularities in data frequency.

2. **Window-Relative Returns:** Raw prices are transformed into window-relative returns (either percentage or log returns), calculated relative to the first price in the window. This centers the data around zero, normalizing price movements and mitigating the influence of absolute price differences.

3. **Scaled Volume:** Volume data undergoes a logarithmic transformation to reduce the impact of extreme values. The log-transformed volume is then normalized using median and interquartile range (IQR) scaling, providing robustness against outliers.

4. **PCA Input Matrix:** A 3-dimensional matrix is constructed with columns representing fractional elapsed time, window-relative returns, and scaled volume. This matrix serves as the input for PCA.

5. **PCA Application:** PCA is performed on the input matrix. The resulting principal components encapsulate the relationships between the three input variables.

Project Initialization

This phase establishes the project foundation, focusing on documentation and preparation for discussion, enabling effective communication and collaboration. It includes:

1. **Continuous Documentation:** A detailed log of the project's evolution, including decisions, rationale, challenges, and potential future directions will be maintained. This ensures transparency and facilitates understanding of the project's development.

2. **Discussion Preparation:** A formal research paper and a well-organized code repository will be prepared. The paper will detail the project's goals, methodology, and results, while the repository will provide the code implementation. This facilitates comprehensive review and in-depth discussion.

## A. Project Initialization

This phase lays the groundwork for the stock market prediction project using dynamic frame construction and visual representation of market data. Before commencing data acquisition and model development, the following steps are crucial:

1. **Document Thought Process (Continuously Update):** Maintain a comprehensive and evolving record of the project's trajectory. This documentation should capture:

   - Design decisions and the rationale behind them.
   - The evolution of the dynamic frame construction, image generation, and self-correction mechanisms.
   - Challenges encountered and potential solutions explored.
   - Architectural considerations, including incorporating multi-scale timeframes (daily, weekly, monthly, quarterly, and yearly), assigning weights to these timeframes, and handling cyclical patterns and external influences (e.g., after-market forces).
   - Specific implementation details for multi-scale temporal models, weighting schemes, and cyclical pattern capture.

   This living document will be invaluable for understanding the project's evolution, facilitating future improvements, and onboarding new team members.

2. **Prepare for In-Depth Discussion (Paper and Repository):** Establish a central repository for all project materials (code, data, and documentation) to streamline collaboration and ensure reproducibility. Prepare preliminary documentation outlining the core concepts, including:

   - **Dynamic Frame Construction:** Using Principal Component Analysis (PCA) on normalized Time, Price, and Volume data to create a dynamic 2D plane based on the two primary components of movement.
   - **Rotation and Refocusing:** Projecting market history onto this dynamic plane and re-centering the origin to the last data point.
   - **Image Generation:** Rendering the transformed 2D market flow as an image, potentially using candlestick or Heiken-Ashi charts, for input to a Vision Transformer model.
   - **Predictive Goals:** Predicting a 2D movement vector (Δx', Δy') within the dynamic plane and the associated "Rally Time."
   - **Self-Correction Mechanism:** Implementing a feedback loop using a Total Error signal (incorporating Vector Deviation Error and Frame Shift Error), along with "Wound Detection" and a "Healing Phase" to maintain model robustness.

This thorough initial preparation will establish a strong foundation for subsequent project phases.
A. Project Initialization

This phase establishes the foundation for the SCoVA project. Before proceeding to data acquisition and model development, two crucial steps are essential:

1. **Document Thought Process (Continuously Updated):** Maintain a comprehensive and continuously updated record of the project's evolution. This documentation should include brainstorming notes, design decisions, rationale for chosen approaches (e.g., Ensemble Model Approach, Multi-Input Transformer Model, specific Weight Assignment Strategies), and explorations of alternative architectures (e.g., Non-Hierarchical Attention Model, Dynamic Rotating Plane method). This living document will serve as a valuable resource for tracking progress, informing future development, and supporting the final dissertation. It should also capture considerations of potential challenges like data pipeline complexity and processing overhead.

2. **Prepare for In-depth Discussion (Paper and Repository):** Establish a structured environment for productive collaboration and discussion. This includes setting up a dedicated repository to house the project code, documentation, and preliminary research. Organize this repository to facilitate version control, code review, and easy access to relevant materials for all stakeholders. Begin drafting the initial structure of the dissertation, outlining potential areas of investigation related to multi-scale data processing and model evaluation. This allows for early feedback and helps guide the project's direction. While UI/UX considerations for potential future applications and specific model architecture details (e.g., ViT, self-correcting mechanisms, data normalization) will be addressed in later stages, initial brainstorming and high-level discussions are encouraged. This preparation ensures that all stakeholders have the necessary context and resources for meaningful discussions throughout the project lifecycle.

### A. Project Initialization

This stage lays the groundwork for the SCoVA project. It focuses on establishing clear documentation practices and preparing for collaborative discussions.

1. **Document the thought process (Continuously update progress and thinking):** Maintaining a comprehensive record of the project's evolution is crucial for transparency, understanding, and future development. This documentation should include:

   - Initial project goals and objectives.
   - Research and exploration of existing solutions and alternative approaches.
   - Design decisions and their rationale.
   - Challenges encountered and solutions implemented.
   - Regular progress updates and any deviations from the initial plan.

2. **Prepare for in-depth discussion (Paper and repository):** Effective communication is essential for successful collaboration. Prepare the following:

   - **Project Repository:** Establish a well-structured code repository with clear documentation within the code (using comments and docstrings) to explain the functionality of different modules and components. This facilitates easy navigation and understanding of the project's codebase.
   - **Preliminary Paper:** Draft a paper outlining the project's scope, methodology, and preliminary results. This document will serve as a basis for discussions with stakeholders and provide a roadmap for the project. Regularly update this document to reflect the project's progress.

These initial steps establish a solid foundation for the SCoVA project, ensuring clarity, transparency, and effective communication throughout the development process.

### A. Project Initialization

This section outlines the crucial initial steps for setting up a well-structured and efficient project. These steps establish a robust foundation for research, development, documentation, and collaboration.

1. **Document the thought process (Continuously update):** Maintain comprehensive documentation of the project's evolution. This living document should capture not only decisions made, but also the rationale behind them, research findings, brainstorming sessions, and any changes in direction. Include explorations into functional requirements, such as determining the optimal number of candles per frame, evaluating transfer learning effectiveness, implementing weighted predictions, utilizing PCA analysis, and conducting hyperparameter testing. Continuously update this documentation throughout the project lifecycle to provide a complete audit trail and valuable insights for future development.

2. **Prepare for in-depth discussion (Paper and repository):** Before commencing development, prepare for productive discussions and collaboration. This involves:
   - **Repository:** Establish a central repository for all project-related materials, including code, data, documentation, and meeting notes. This facilitates version control, streamlined communication, and ensures all team members have access to the latest information.
   - **Paper:** Initiate a document outlining the project scope, objectives, proposed methodology, and anticipated outcomes. This document will serve as the basis for discussions with stakeholders and will evolve alongside the project, eventually becoming the final research paper documenting the project's findings and addressing the explored functional requirements.

### A. Project Initialization

This phase establishes the foundation for the SCoVA project, emphasizing meticulous documentation and preparation for collaborative discussions. Before delving into specific implementation details like UI/UX, we must solidify the project's direction and core principles.

1. **Document thought process (Continuously update progress and thinking):** Maintain a comprehensive log of the project's evolution. This includes not only technical decisions but also the rationale behind them, challenges encountered, and evolving hypotheses. This living document, whether a dedicated file or integrated within the project repository, will be invaluable for tracking progress, identifying potential pitfalls, and ensuring the project remains aligned with its initial goals. It will also be a crucial resource for future analysis and reporting. For SCoVA, this specifically includes documenting the integration of the Dharmic Mandate and its influence on architectural and design choices, along with the application of the four paths of the Bhagavad Gita (Gyaan, Bhakti, Karma, and Raja Yoga). Specific examples of ethical rule implementation (Satya, Shaucha, Santosha) and their justifications should be included.

2. **Prepare for in-depth discussion (Paper and repository):** Effective communication and collaboration are essential for project success. Prepare materials for in-depth discussions with advisors, collaborators, and peers. This includes a well-structured document outlining the project's goals, methodology, and expected outcomes, and a link to the project repository containing all code, data, and relevant documentation. For SCoVA, this repository should include documentation on technical choices such as the use of the Zerodha KiteConnect API. These materials facilitate productive feedback, knowledge sharing, and ensure the project's direction remains well-informed.
   Project Initialization

This phase establishes the foundation for the SCoVA (Swaha) project—a multi-scale, self-correcting, relational spacetime vision transformer for predictive intraday market analysis—by outlining core principles and preparing for collaborative development. It encompasses two key activities:

1. **Continuous Documentation:** Maintain a comprehensive and continuously updated record of the project's evolution. This living document should meticulously detail every technical decision, assumption, rationale, challenge encountered, and solution implemented. The emphasis should be on _why_ a specific approach was chosen, providing clear justification for each step. While grounded in the project's philosophical framework (integrating the Four Paths of Yoga—Gyaan, Bhakti, Karma, and Raja—as outlined in the Bhagavad Gita), the documentation should prioritize concrete technical details, methods, and justifications, avoiding abstract philosophical discussions. This thorough record minimizes future ambiguity and ensures a transparent understanding of the project's trajectory.

2. **Preparation for In-depth Discussion:** Prepare comprehensive materials to facilitate productive technical discussions. This includes a well-structured project repository hosted on `idx.google` and aligned with the project's modular design (starting with Module 1: The Gyaan Shala - Data & Experiment Design, focusing on data management via the Zerodha Kite Connect API). The repository should mirror the thoroughness of the project documentation, providing readily accessible information on all aspects of the technical implementation, including specific features, libraries, chosen methods, and how these address the technical constraints of development within `VS Code with Gemini Code Assist Agent` using Gemini agentic help and deployment to Firebase. Additionally, preliminary drafts of the project paper and any relevant research resources should be compiled. This preparation ensures a shared understanding among stakeholders and establishes a clear path forward. Each module's documentation within the repository will articulate its philosophical grounding, purpose, UI/UX considerations, and technical implementation details.

### A. Project Initialization

This phase establishes the project's foundation, encompassing technical decisions, architectural design, and cost optimization strategies. Given the project's reliance on image processing for computer vision and a federated learning approach distributing workload between a server and iPads, careful initialization is critical.

1. **Document the evolving thought process:** Maintain comprehensive, continuously updated documentation of all design decisions and their rationale. This living document should track the project's evolution, offering valuable context for future development and maintenance. Specifically, address the following:

   - **Architecture and Cost Optimization:** Detail the chosen architecture for both server and iPad components, including rationale and explorations of cost-effective alternatives. This includes analysis of cloud vs. offline/iPad processing, image generation and computer vision optimization techniques, and strategies for minimizing cloud expenses (e.g., caching, lightweight architectures, offloading computationally intensive tasks).

   - **Federated Learning Implementation:** Describe the chosen federated learning approach, including data flow between server and iPads, model update strategies, and anticipated challenges related to this distributed architecture.

   - **UI/UX for Training and Retraining:** Outline the user interface design for model training and retraining. Regardless of the processing strategy (cloud, offline, or iPad), the UI should abstract away underlying infrastructure complexities and provide a user-friendly experience.

2. **Prepare for in-depth discussions:** Compile the technical specifications and design documentation into a formal document and initialize the project repository. This repository will serve as the central hub for all code, documentation, and discussions, ensuring a shared understanding among team members. The repository should include a clear README outlining project objectives and architecture. Key discussion points based on the documentation above should include:

   - A comparative analysis of cloud-based, offline, and iPad-based processing, focusing on cost-effectiveness, performance, and user experience.
   - An evaluation of different image generation and computer vision optimization techniques, considering computational overhead, accuracy, and implementation complexity.
   - A detailed proposal for the application UI/UX, emphasizing user-friendliness and abstraction of infrastructure complexities for model training and retraining.

This detailed initialization process will ensure a strong foundation for the project, facilitating efficient communication and informed decision-making throughout the development lifecycle.

## A. Project Initialization

This project involves training a Vision Transformer (ViT) model on an iPad within a Progressive Web App (PWA), posing significant computational challenges. This initial phase focuses on evaluating the feasibility of this approach, addressing user concerns regarding PWA stability and performance, and preparing for in-depth discussions on potential solutions. This involves two key activities:

1. **Feasibility Analysis and User Concern Documentation:** A thorough investigation into the suitability of PWAs for GPU-intensive tasks like ViT training is crucial. This analysis should cover:

   - **PWA Suitability:** Identify the potential benefits and drawbacks of using a PWA, including risks of browser crashes due to excessive resource consumption. Assess the feasibility of achieving stable, long-term, uninterrupted operation within a PWA environment on resource-constrained iPads. This requires up-to-date research on TensorFlow.js and WebGPU performance within a browser context, specifically on iPadOS.

   - **Performance and Stability on iPadOS:** Evaluate TensorFlow.js and WebGPU performance and stability data specifically on iPadOS. Investigate recent browser memory limits and potential crash rates for GPU-intensive workloads. Address potential challenges in accessing iPad GPU resources effectively through the browser within a PWA. Analyze the impact on both training speed and stability. Explore alternative approaches, such as hybrid training architectures, if initial findings suggest limitations.

   - **User Concerns:** Document user concerns related to PWA stability during intensive computations, including risks of browser crashes, long-running processes, the impact of browser cache limits, and memory management within the PWA environment.

2. **Preparation for In-depth Discussion:** The feasibility analysis and user concern documentation will form the basis for in-depth discussions and inform architectural decisions. This preparation includes:

   - **Technical Documentation:** Create a comprehensive document outlining the planned architecture for both the server and iPad components. Include diagrams and textual descriptions clarifying the responsibilities of each entity, data flow, technologies employed (TensorFlow.js, Canvas API, Web Workers, etc.), federated learning framework, data handling strategies, and model update mechanisms.

   - **Project Repository:** Establish a well-documented repository to house the code for both server and iPad components, including image generation, model training, and communication protocols. The repository should reflect the current state of the project and facilitate future development and maintenance.

   - **Discussion Points:** Prepare for focused discussions covering server-side orchestration (data storage, model management, training coordination), client-side processing (data fetching, image generation, local training, updates to the server), API design for data and model exchange, UI adaptations for progress feedback and resource monitoring, and potential alternative deployment solutions (native applications, cloud-based training) if PWAs prove unsuitable.

## A. Project Initialization

This phase lays the groundwork for the SCoVA project, focusing on documentation, architectural decisions, and preparing for collaborative discussions. The project adopts a hybrid architecture, balancing processing between a native iOS Swift application on the client-side (iPad) and a minimal Python backend. This approach addresses the limitations of previous architectures (like PWAs) regarding memory constraints, long-running task stability, and GPU access for computationally intensive tasks such as image generation and model training.

1. **Document the Evolving Thought Process:** Maintain a living document that chronicles the project's direction, including architectural decisions, rationale, challenges, and solutions. This documentation should clearly articulate:

   - The rationale for migrating to a native iOS Swift application and adopting a minimal backend.
   - The distribution of responsibilities between the backend (acting as an orchestrator and managing data flow via APIs like Zerodha Kite Connect) and the iOS client (responsible for on-device image generation, model training, backtesting, and live inference). This backend function can be visualized as a "traffic controller" managing interactions between the client and external services.
   - Considerations regarding stability, memory management (including batch processing for large datasets), and the trade-offs between client-side responsiveness and server-side resources.
   - The evolution of the user interface, specifically how the "Campaign Runner" will allow users to select execution targets (e.g., "Cloud GPU (Full Training)", "On-Device (Fine-Tuning Only)", "On-Device (Interactive Backtest)").

2. **Prepare for Collaborative Discussions:** Establish a shared code repository and prepare documentation to facilitate in-depth discussions about the hybrid architecture. This documentation should include:
   - The project's scope, objectives, and planned methodology.
   - A detailed explanation of the chosen technologies (Swift for iOS, Python for the backend) and their integration.
   - An analysis of the iOS environment's capabilities for handling client-side processing, including an investigation of memory limits, long-running task stability, and access to the GPU context, especially concerning the DynamicPlaneGenerator's image generation requirements. This analysis should inform the feasibility and compatibility of the existing architecture with the iOS platform.
   - Documentation related to server-side components, including potential use of cloud resources like Google Cloud Run with GPU acceleration (e.g., NVIDIA T4) for tasks like initial model training and large-scale multi-permutation campaigns, with the client acting as a remote control for these jobs. The mechanism for synchronizing weight updates (deltas) from client-side fine-tuning back to the server should also be addressed.

This thorough initialization process will ensure alignment amongst stakeholders, address potential challenges early, and maximize the likelihood of a successful project outcome.

## A. Project Initialization

This phase lays the groundwork for the SCoVA project. While the project's core involves sophisticated model development and training, these initial steps are crucial for organization, efficient progress tracking, and facilitating effective collaboration.

1. **Document Thought Process (Continuously Update):** Maintain a comprehensive, living document that chronicles the project's evolution. This includes not only _what_ decisions were made and _how_ they were implemented, but also _why_. Capture the rationale behind chosen approaches, alternative solutions considered, and any evolving insights. This detailed record enables effective retrospective analysis, informed future decision-making, and facilitates knowledge transfer.

2. **Prepare for In-Depth Discussion (Documentation and Repository):** Effective collaboration requires thorough preparation. Create two key resources:

   - **Project Documentation ("Paper"):** A well-structured document outlining the project's goals, methodology, and expected outcomes. This serves as a high-level overview for discussions with stakeholders and collaborators. Include the rationale for key technical decisions, such as backend and frontend technology choices.

   - **Code Repository:** A dedicated repository containing organized and well-documented code. This enables version control, collaborative development, transparent progress tracking, and practical demonstrations. Initialize the repository early to align with the evolving project documentation.

This initialization phase also involves confirming existing technology choices and investigating potential alternatives:

- **Backend Technology:** Confirm the continued use of Python for backend development.
- **Client-Side Processing:** Reaffirm the project's reliance on extensive client-side processing, leveraging native app permissions and resources.
- **Frontend Technology:** Investigate the feasibility of using Flutter for frontend development, carefully considering its implications for client-side processing and integration with the Python backend.

## A. Project Initialization

This phase establishes the foundation for the SCoVA project, focusing on key architectural decisions, model management, and preparing for detailed discussions and documentation.

1. **Model Management and Portability:** A clear strategy for managing machine learning models is crucial. This includes addressing the different model formats: `.mlmodel` for iOS and `.tflite` for Android. The strategy should outline how models will be stored, versioned, and updated across platforms. Critically, we must determine the portability of Core ML models trained on Apple devices for use and modification on other platforms. This involves understanding the `.mlmodel` format and any dependencies on Apple-specific libraries or hardware. Research potential solutions for cross-platform compatibility, such as:

   - Converting models to a portable format like TensorFlow Lite (`.tflite`).
   - Using a cross-platform training framework like TensorFlow or PyTorch from the outset. This decision significantly impacts long-term maintainability and scalability.

2. **Project Setup and Documentation:** Prepare for in-depth discussions and ensure comprehensive documentation. This includes:

   - **Architectural Design:** Create a clear architectural diagram illustrating the interaction between Dart, Swift, Kotlin, and the respective machine learning frameworks (Core ML and TensorFlow Lite). Highlight the role of Platform Channels in bridging native and cross-platform components.
   - **Model Integration:** Document the chosen model integration strategy, including how models will be embedded, versioned, and updated within the application.
   - **Thought Process Documentation:** Maintain a detailed record of the decision-making process throughout the project lifecycle. This living document will track the project's evolution, rationale behind decisions, challenges encountered, and explored solutions. This facilitates knowledge transfer and informed future development. Utilize tools like a dedicated project journal, shared documents, or commit messages for this purpose.

### A. Project Initialization

This section outlines the crucial initial steps for the SCoVA project, focusing on establishing a clear thought process and preparing for in-depth discussions.

1. **Document the thought process:** Maintain a continuously updated record of the project's evolution, including initial brainstorming, research, design decisions, and any changes in direction. This living document will provide valuable context for future analysis and ensure transparency. Document the rationale behind architectural choices (e.g., Core ML vs. cross-platform frameworks) and any challenges encountered. This documentation should be stored within the project repository.

2. **Prepare for in-depth discussion (Paper and repository):** Prepare a well-structured code repository and a preliminary paper outlining the project's scope. The repository will house the "Universal Source Model" in a framework-agnostic format (PyTorch or TensorFlow), enabling conversion and deployment to various platforms (.mlmodel for iOS, .tflite for Android, TensorFlow.js for web). Include documentation covering on-device training, synchronization mechanisms for updating model weights, and the rationale for technology choices. The paper should provide a roadmap for the research, covering background, methodology, results, and conclusions. Both the paper and repository will be iteratively refined throughout the project.

### A. Project Initialization

Project initialization sets the foundation for a successful SCoVA implementation. This phase focuses on establishing clear documentation practices and preparing materials for collaborative discussions.

1. **Document Thought Process (Continuously update progress and thinking):** Maintain a comprehensive, living document that chronicles the project's evolution. This document should include:

   - **Initial Goals and Objectives:** Clearly define the SCoVA project's aims, the problems being addressed, and the desired outcomes.
   - **Brainstorming and Ideation:** Document brainstorming sessions, capturing initial ideas, rejected concepts, and the rationale behind selected approaches.
   - **Design Decisions and Rationale:** Meticulously document all design choices, explaining the reasoning behind each decision to ensure consistency and provide a clear audit trail.
   - **Challenges and Solutions:** Document encountered challenges, proposed solutions, and the rationale behind chosen solutions. This helps in understanding the project's trajectory and informs future decision-making.

2. **Prepare for In-depth Discussion (Paper and Repository):** Compile and organize materials to facilitate productive and informed discussions. This includes:

   - **Research Papers:** Gather relevant research papers that inform the project's direction and methodology.
   - **Project Repository:** Establish a well-structured repository to house the project's codebase, data, and other relevant artifacts. Ensure clear commit messages and comprehensive documentation within the repository to promote understanding and collaboration.
   - **Pre-Meeting Documentation:** Prepare a concise document summarizing key discussion points, open questions, and proposed solutions to guide discussions and ensure efficient use of meeting time.

## A. Project Initialization

This phase establishes a solid foundation for the SCoVA project by clarifying key security requirements and conducting preliminary research. It involves documenting the evolving thought process and preparing materials for in-depth discussions.

1. **Document Thought Process:** Maintain a detailed, continuously updated record of the project's evolution. This includes documenting decisions made, the rationale behind chosen approaches, challenges encountered, and potential solutions explored. This living document promotes transparency, facilitates knowledge sharing, and supports reproducibility. Regularly record progress, noting completed tasks, ongoing work, and upcoming milestones to ensure transparency and effective project management. This documentation can be maintained in a dedicated project log, a digital notebook, or within the project's code repository.

2. **Prepare for In-Depth Discussion:** Compile relevant research and analysis into a structured format suitable for discussion and collaboration. This includes:

   - **Project Proposal/Paper:** A concise, iteratively updated document outlining the project's scope, methodology, and anticipated outcomes. This ensures alignment among team members and stakeholders.

   - **Code Repository:** A well-structured repository containing the project's codebase, facilitating version control and collaborative development. Include clear documentation, such as a README file explaining the project's purpose, structure, and usage instructions.

   - **Definition of "Uncrackable":** A precise definition of "uncrackable" within the project's context, considering factors like acceptable cracking time, attackers' potential computational resources, and industry best practices. This definition informs the analysis of password length and strength.

   - **Minimum Alphanumeric Password Length for Uncrackability:** Based on the defined "uncrackable" criteria, determine the minimum acceptable length for alphanumeric passwords, considering current and near-future computational capabilities for brute-force attacks.

   - **Research on Brute-Force Cracking Speeds:** Data on current brute-force cracking speeds using high-end GPUs and other relevant hardware. This informs the feasibility assessment of cracking passwords of varying lengths and complexities.

   - **NIST Guidelines on Password Length:** A review of NIST guidelines and recommendations for password length and complexity, compared to the project's specific security needs.

   - **Analysis of Password Length vs. Cracking Time:** A thorough analysis of the relationship between password length, the alphanumeric character set, and cracking time, factoring in current brute-force capabilities and the use of robust hashing algorithms like bcrypt and Argon2. This analysis will inform the choice of a suitable password length and should be included in the repository for discussion.

### A. Project Initialization

This phase focuses on establishing a solid foundation for the project by documenting the initial thought process and preparing for in-depth discussions.

1. **Document the evolving project rationale:** Maintain a comprehensive and continuously updated log of the project's evolution. This includes documenting brainstorming sessions, design decisions, the rationale behind chosen approaches, encountered challenges, and their respective solutions. This documentation will serve as a valuable resource for tracking progress, understanding the project's trajectory, and facilitating knowledge sharing. Consider using a regularly updated log file, a dedicated section in the project documentation, or dated entries in a research journal.

2. **Prepare for collaborative discussions:** Organize all relevant materials, including preliminary research papers, design documents, and a central code repository, to facilitate productive discussions. Ensure the repository is well-documented, easy to navigate, and includes a clear README file outlining the project's goals and structure. Prepare a concise summary of key findings and ensure the codebase is clean, commented, and easily understandable. This thorough preparation will streamline communication and ensure clarity during discussions with advisors, collaborators, or reviewers.

## A. Project Initialization

This phase establishes a robust foundation for project development and testing, emphasizing cost-effective cloud resource utilization. It involves meticulous documentation, preparation for collaborative discussions, and a multi-stage testing approach with mock data and a dummy model.

**Documentation and Collaboration:**

- Maintain comprehensive documentation of the project's evolution, including design decisions, experimental results, and encountered challenges. This "living document" will serve as a valuable resource for ongoing analysis, refinement, and future reference.
- Prepare materials, including preliminary research papers and access to the project repository, to facilitate in-depth discussions and ensure team alignment.

**Cost-Effective Testing Strategy:**

To minimize server costs, particularly given the ₹25,000 budget constraint, the project employs a tiered testing approach:

1. **Mock Data Generation:** Create mock data (CSV or JSON format) representing various market scenarios, including typical behavior and edge cases like price spikes, flat periods, and data gaps. This allows for initial testing and validation without incurring the cost of real market data. This data will be used to thoroughly test the `DynamicPlaneGenerator`, data normalization functions (e.g., `time_frac`, `log_return`, `volume_scaling`), and the data loader.

2. **Dummy Model Development and Smoke Testing:** Develop a small, computationally inexpensive "dummy" neural network mimicking the input/output structure of the intended ViT model. This dummy model enables several key tests:

   - **On-Device Smoke Tests:** Deploy the dummy model to an iOS device for "smoke tests" using a small subset of real data. This verifies the Core ML training setup, including loss calculation, backpropagation, and weight updates. The necessity of training the dummy model during this smoke test will be evaluated; if a pre-trained dummy model suffices, eliminating this training step can further optimize resource utilization.

   - **End-to-End Pipeline Dry Run:** Design an "End-to-End Dry Run" mode within the Experiment Designer UI. This mode executes the entire workflow, from data fetching to model input preparation, but bypasses the actual model training. This validates the data pipeline and server communication without the computational expense of training.

3. **Short Training Runs:** Conduct short, 1-epoch training sessions on the iOS device using minimal date ranges for data. This allows frequent testing and debugging cycles without incurring substantial server costs.

These initial steps ensure a solid foundation by validating core functionalities, data pipelines, and the training mechanism before committing to full-scale model development and training with real market data. This tiered approach enables rapid iteration, efficient resource use, and early issue detection, ultimately contributing to the project's success within budget constraints.

## A. Project Initialization

This phase lays the groundwork for the project, focusing on documentation and preparing for collaborative discussions about architecture, implementation, and initial results. This preparation ensures effective communication and a shared understanding among team members.

1. **Document Thought Process (Continuously Update Progress and Thinking):** Maintain a comprehensive, living document chronicling the project's evolution. This includes:

   - Rationale behind design decisions and chosen approaches.
   - Exploration of alternative approaches and the reasons for their rejection.
   - Challenges encountered and their solutions.
   - A detailed examination of potential "shocker" events within the financial time series data. Define and quantify these events based on metrics like volatility spikes, volume anomalies, and rapid price changes. These definitions will inform the Cognitive Threat Analysis Module (CTAM) development.
   - Tracking progress against the project checklist.

This documentation is crucial for later review, understanding the project trajectory, ensuring continuity, and justifying design choices in the final dissertation.

2. **Prepare for In-depth Discussion (Paper and Repository):** Facilitate productive discussions by preparing necessary materials, including:

   - Preliminary drafts of the dissertation paper, outlining the project's architecture and the integration of the CTAM with the overall framework, specifically the DynamicPlaneGenerator.
   - A well-structured code repository containing initial code, experimental setups, and documentation, enabling easy navigation and understanding. This includes code for the CNN models within the CTAM for "shocker event" pattern analysis and the mechanism for connecting the `DynamicPlaneGenerator` to the Core ML training session.

Key discussion points should include:

    * The response of the DynamicPlaneGenerator to the CTAM's "Threat Level" score, including potential adjustments to the smoothing function or learning rate.
    * The design of lightweight CNN models within the CTAM, balancing accuracy and computational efficiency for real-time anomaly detection.  Document the chosen CNN architectures and the rationale behind their selection in the paper.
    *  A three-stage testing strategy: Unit & Integration Testing with Local Mocks, End-to-End Pipeline Simulation in "Dry Run" Mode, and On-Device "Smoke Tests".  Emphasize the smoke test debugging strategy, particularly for application pipeline troubleshooting.  This includes a smoke test involving a single-epoch training run on a dummy model to validate the training and update pipeline. This smoke test should verify data loading (especially the `DynamicPlaneGenerator` connection), training loop execution (forward pass, loss calculation, backpropagation, and weight updates), and weight extraction. The necessity of this on-device training step should be evaluated considering the trade-offs between performance and resource utilization.

This initial preparation will ensure a robust foundation for subsequent model development, training, and collaborative discussions, accommodating potential changes in scope and direction.

### A. Project Initialization

This section outlines the crucial initial steps for the SCoVA project implementation. A robust foundation is essential before delving into the technical details. This involves establishing clear documentation practices and preparing for collaborative discussions.

1. **Document Thought Process (Continuous Updates):** Maintaining comprehensive and continuously updated documentation of the project's evolution is paramount. This includes:

   - Brainstorming sessions and design decisions.
   - Rationale behind chosen approaches.
   - Challenges encountered and their resolutions.
   - Progress updates and evolving understanding of the system.

This "living document" fosters transparency, aids future development and debugging, and provides context for the architectural overhaul of Project Swaha, particularly regarding the re-architecting of the Continuity role (as described in the universal checklist item).

2. **Prepare for In-Depth Discussion (Materials and Repository):** Effective collaboration requires thorough preparation. This includes:

   - Compiling relevant research papers supporting the chosen methodologies.
   - Establishing a well-structured code repository for version control, collaboration, and code development. This repository will be central to developing the dual-system architecture (Flow Engine and CTAM - Contextual Threat Assessment Module) discussed in "Chat2.json messages #76 & #77".
   - Preparing a project proposal outlining goals, methodology, and expected outcomes. This proposal should address the limitations of the current error-correction model (mean reversion bias and sensitivity to standard deviations) and detail the proposed dual-system approach for handling both normal market behavior ("Flow Engine") and outlier events ("Threat Engine/CTAM") focusing on equities and derivatives threat detection and the fusion into a single Systemic Threat Level (STL) score. The model retraining strategy, accounting for both flow and shock in the data to prevent performance plateauing, should also be included.

These preparations will facilitate productive discussions with stakeholders and team members, ensuring alignment and enabling efficient problem-solving.

## A. Project Initialization

This phase establishes the project's foundation by clarifying the architectural vision and preparing for collaborative discussions. It involves reviewing the existing framework, aligning it with the four pillars of Continuity, Enforcement, Facilitation, and Specialization, and establishing clear communication protocols.

The following key activities are essential during this phase:

1. **Document the thought process:** Maintain comprehensive, continuously updated documentation of the project's evolution. This includes:

   - Deconstruction and analysis of the existing framework.
   - Rationale for component restructuring and re-categorization based on the four pillars.
   - Decisions made, challenges encountered, and justifications for chosen solutions.
   - Design choices and rationale regarding functional pillars, communication protocols, and naming conventions.
     This living document serves as a roadmap, facilitating ongoing communication, traceability, and informed decision-making.

2. **Prepare for in-depth discussions:** Consolidate all relevant materials into a readily accessible format (e.g., a shared document or repository). This includes:
   - The documented thought process.
   - Proposed architectural changes.
   - Supporting research and documentation.
   - Preliminary research papers, initial code repositories, and design documents.
   - Analysis of the proposed architecture based on the four pillars, including deconstruction of complex components.
   - Breakdown of existing components into atomic functions, reassessing their roles and redefining data flow and system topology.
   - Discussion of communication restrictions between specialist modules, data access management by enforcers, and the implementation of the `ExperimentRunner` service.
   - Consideration of the implications of the chosen function naming convention on code clarity and maintainability.

This thorough preparation will enable focused and productive discussions, ensuring alignment among stakeholders and contributing to a shared understanding of the project's goals, methodology, and implementation strategy. It supports the principles of Continuity and Facilitation, crucial for a robust architectural foundation and smooth project execution.

## Project Initialization

This phase establishes the project's foundation, preparing for data acquisition and model development. This involves setting up specialist services for data preprocessing and model training, defining architectural principles, and creating enforcer and facilitator services to manage workflows and ensure secure resource access. Documentation of design choices and implementation details will be maintained in a dedicated repository alongside the code, promoting clear communication and collaborative development.

### Architectural Principles

The following principles will guide the system's architecture:

- **Isolation of Specialists:** Specialist components (e.g., CNN training, dynamic plane implementation) will be isolated, with meticulously managed interactions to minimize dependencies and promote independent development and testing. This prevents unintended side effects from changes within one module.
- **Asynchronous Communication:** Asynchronous communication via a pub/sub model will be used between specialists. This fosters loose coupling, scalability, and prevents performance bottlenecks.
- **Resource Access via Enforcers:** All access to resources (data, models, external services) will be channeled through Enforcer components. This centralized approach enhances security, simplifies resource management, and provides a single point for authentication, authorization, and auditing.
- **Workflow Orchestration via Facilitators:** Facilitator components will orchestrate complex workflows, manage dependencies, trigger actions based on predefined conditions, and provide an overview of project progress, ensuring a clear and manageable execution flow.
- **Inheritance for Code Structure:** Class inheritance will structure services, enforcing a strongly typed separation of concerns. This improves code readability, maintainability, promotes code reuse, and reduces the risk of unintended side effects, contributing to a well-organized and scalable codebase.

### Specialist Services

The following specialist services will be created for data preprocessing and model training:

- **`NormalizeWindow`**: Normalizes raw numerical array data based on a configuration dictionary. Dependencies are explicitly limited, excluding `google-cloud-storage`, `google-cloud-firestore`, and `requests` to minimize conflicts and maintain focused functionality.
- **`ComputePrincipalComponents`**: Performs Principal Component Analysis (PCA) on normalized array data using only the `numpy` library for efficiency and portability. Outputs the top two principal component vectors.
- **`ProjectToPlane`**: Projects data onto a 2D plane defined by basis vectors (presumably from `ComputePrincipalComponents`), simplifying visualization and potentially improving subsequent model performance.
- **`TrainOneEpoch`**: Encapsulates the core model training logic for a single epoch. Receives model artifact bytes, training data tensors, and a configuration, returning updated model artifact bytes. Operates independently of data origin or destination, promoting modularity and reusability.

### Enforcer and Facilitator Services

Key enforcer and facilitator services will manage workflows and secure resource access:

- **`Workflow_Broker` (Facilitator):** Orchestrates interactions between specialist services, managing the project workflow based on tasks received from a Pub/Sub queue. Examples include constructing the dynamic plane and executing experiments.
- **`State_Enforcer` (Enforcer):** A Cloud Function with exclusive read/write access to the Firestore database, ensuring data integrity and security. All database interactions are channeled through this service via a single function with conditional logic.
- **`Resource_Enforcer` (Enforcer):** A Cloud Function managing object storage in Google Cloud Storage with exclusive read/write permissions to GCS buckets. A single function with conditional logic manages all storage operations.
- **`Live_Execution_Enforcer` (Enforcer):** Manages interactions with the Zerodha API for trading, holding exclusive authority to place, modify, or cancel orders via a secure endpoint (e.g., `/executeOrder`).
- **`Pre-flight_Validation_Service` (Facilitator):** Called by the `UI_Gateway` before any operation. Sends requests to the `Resource_Enforcer` for data validation, adhering to security protocols and maintaining separation of concerns.

### UI Gateway

The `UI_Gateway` facilitator service will act as the main API gateway for the frontend application, exposing public-facing endpoints and translating user requests into internal workflows, including those involving the specialist services. While not directly part of the initialization process, it's a crucial architectural component.

## A. Project Initialization

This phase establishes the foundation for the SCoVA project by documenting the initial design thinking and preparing for in-depth discussions about the architecture and functionality. While the overall project architecture will eventually incorporate microservices, distributed tracing, and sophisticated visualization, this initial stage focuses on foundational documentation and preparation.

1. **Document Thought Process (Continuously Update Progress and Thinking):** Meticulously document the initial project ideas, brainstorming sessions, preliminary research, and the rationale behind key decisions. This living document should be continuously updated throughout the project lifecycle to reflect evolving understanding, challenges encountered, and design choices. This comprehensive record will provide valuable context for future development and ensure a clear audit trail. Consider using a structured format and appropriate tools for efficient knowledge management.

2. **Prepare for In-depth Discussion (Paper and Repository):** To facilitate effective collaboration and knowledge sharing, prepare the following:

   - **Repository:** Create a central repository (e.g., a Git repository) to house project code, data, and documentation. Initialize the repository to support the initial development approach (e.g., a co-located monolith for simplified debugging and easier transition to a distributed architecture later).

   - **Preliminary Design Document:** Draft a document outlining project goals, proposed methodology, and expected outcomes. This document should also articulate the core architectural concepts, such as the Dynamic Rotating Plane and the Dual-Engine Perception architecture (including the Flow Engine, Shockwave Prediction Model (SPM), and Systemic Threat Level (STL)). Clearly explain the purpose and intended use cases for the system-wide logbook, including how it will visually represent asynchronous and parallel services, branching threads, and loopbacks during facilitator-specialist interactions. Document initial research into existing IDE logging capabilities and potential tool adaptations to inform the logbook implementation process. While the final architecture will involve microservices and distributed tracing, this initial document should focus on the foundational elements and planned implementation strategy for the co-located monolith.

## A. Project Initialization

This phase establishes a solid foundation for the project by prioritizing continuous documentation and preparing for in-depth discussions.

1. **Document Thought Process:** Maintain a continuously updated, detailed record of the project's evolution. This documentation should capture not only the _what_ and _how_ of decisions, but also the _why_. Document the rationale behind choices, explorations of alternative approaches, reasons for selecting the chosen path, and any challenges encountered. This living document will be invaluable for tracking progress, facilitating collaboration, and providing context for future development and analysis.

2. **Prepare for In-depth Discussion:** Effective collaboration requires a shared understanding of the project's scope, goals, and technical approach. Prepare materials to facilitate in-depth discussions, including a preliminary paper outlining the project's objectives, proposed methodology, and anticipated outcomes. This paper should address key architectural decisions, such as the Four Pillars System Architecture (Specialists, Facilitators, Enforcers, and Continuity), Client-Side Heavy Lifting, the chosen frontend approach (Native App using Swift/Core ML or Flutter/hybrid native ML), the Universal Model Hub, and the Lightweight Backend Orchestrator (Python, Cloud Run/Functions, Firebase). Concurrently, establish and maintain a well-organized, clearly documented code repository. This centralized repository will house the project codebase, data, and documentation, ensuring version control and facilitating efficient knowledge sharing among collaborators.

## Project Initialization

This phase lays the groundwork for the project, addressing initial architectural decisions based on data source constraints and preparing for detailed discussions. The limitations of Zerodha's market depth data precision necessitate a shift from the initially planned spread-based calculations to leveraging order quantity and count at different bid/ask levels. This change influences the development of two new services and requires thorough documentation.

Key initial steps include:

1. **Continuous Project Documentation:** Maintain a comprehensive, living document that chronicles the project's evolution. This includes:

   - Rationale behind design decisions, including the shift away from spread-based calculations due to data limitations.
   - Chosen architectural approaches, with specific details on the `Live_Execution_Enforcer`, `Paper_Brokerage_Simulator`, and their interaction with the Firestore database.
   - Challenges encountered and potential solutions explored.
   - Integration plans for Zerodha live tick data, including data format, method, and usage within the `Paper_Brokerage_Simulator` for realistic fill simulation (handling partial fills and live bid/ask prices).
   - Rationale and design of the `Paper_Brokerage_Simulator`, including how it leverages WebSocket tick data to simulate real-time market conditions.
   - Existing usage of market depth data within the system to inform the development of the `DeriveOrderBookFeatures` specialist.

2. **In-depth Discussion Preparation:** Prepare materials for comprehensive discussions regarding the project architecture and implementation. This includes:
   - Diagrams illustrating the system architecture, including interactions between key components.
   - Relevant code repositories (`Live_Execution_Enforcer`, `Paper_Brokerage_Simulator`) ready for review.
   - Design documentation for the `DeriveOrderBookFeatures` specialist, detailing how it derives features (OBI, WAP, Bid-Ask Spread) from market depth data for the `DynamicPlaneGenerator`.
   - Design documentation for the `CalculateOrderBookImbalance` service, explaining how it calculates OBI from market depth data for use in the `DynamicPlaneGenerator`.
   - Design documentation for the `GenerateDepthQuantityHeatmap` service and its use within the `MarketDepthAnomalyDetector` to identify market shocks.
   - A review of existing research papers relevant to the project's goals.

This thorough initialization ensures all stakeholders are aligned and the project's direction is clearly documented, especially given the initial architectural adjustments. This documentation will be iteratively refined as the project progresses.

### A. Project Initialization

This phase focuses on establishing a clear direction and solid foundation for the SCoVA (Snapshot Computer Vision Algorithm) project. These initial steps are crucial for ensuring a shared understanding of project goals and the technical approach.

1. **Project Definition and Scope:** The project is formally named "SCoVA" (Snapshot Computer Vision Algorithm), reflecting its core methodology of applying computer vision techniques to market data snapshots. This naming convention will be used consistently throughout the project. SCoVA explicitly focuses on discrete, dynamically generated visual snapshots of market data, rather than continuous time-series data. This fundamental design choice will inform all subsequent architectural decisions.

2. **Technical Specifications and Architecture:** Thorough documentation of the SCoVA architecture is essential, considering its snapshot-based approach. This includes specifying the technical details of snapshot generation, the chosen computer vision models, and the overall data pipeline. Furthermore, a detailed record of the project's evolution will be maintained, encompassing design decisions, rationale for chosen approaches, anticipated challenges (such as the impact of high trading costs or short-selling constraints), and potential solutions. This documentation should be continuously updated throughout the project lifecycle. This living document will be a valuable resource for future analysis, refinement, and onboarding of new team members. It will also serve as a basis for productive discussions with stakeholders, ensuring everyone is aligned on the project's objectives and technical details. A version-controlled repository will be established to manage the project's codebase, facilitating collaboration and ensuring code integrity.

## A. Project Initialization

This phase establishes the project's foundation, clarifies the core technical approach, and prepares for future development. The following key steps are essential:

1. **Continuous Documentation:** Maintain comprehensive, evolving documentation of the project's lifecycle. This includes design decisions, rationale, challenges encountered, potential solutions, and experimental results. Specifically, document:

   - The core reliance on computer vision as the primary perception modality.
   - The motivation and expected benefits of exploring asymmetric prediction models, risk management, and self-correction, along with potential challenges.
   - The rationale for exploring a risk-averse loss function and integrating features like Upside vs. Downside Volatility.
   - The ongoing investigation into asymmetric feature engineering, including how features such as Upside vs. Downside Volatility, Volatility Skewness, Volatility Kurtosis, Accumulation/Distribution Ratio, Order-to-Quantity Asymmetry, and Price-Volume Correlation State are incorporated into the model architecture and training process.

2. **Preparation for In-Depth Discussion:** Prepare a research paper and code repository to facilitate in-depth technical discussions and collaboration. The paper should detail the project's goals, methodology, theoretical and empirical analysis, and findings. The repository should house the code implementation, enabling reproducibility. Ensure the following are included:

   - Analysis of potential "non-hierarchical asymmetric" design implications, including consideration of naming conventions such as "Anhad’s Non-Hierarchical Asymmetric Snapshot Computer Vision Algorithm" (ANHASCoVA) or similar variants.
   - Exploration of how "non-hierarchical" and "asymmetric" concepts relate to the current algorithm design and their potential for future development, including a preliminary investigation into a graph-based perceptual model. This model should represent different timeframes (intraday, daily, weekly, etc.) as graph nodes, with edges representing learned influence between them. A Graph Neural Network (GNN) should replace the existing Transformer for fusion, dynamically learning influence between timeframes and adapting to market conditions.
   - Documentation of the model architectures, including specialized loss functions (e.g., the Asymmetric Loss Function) and complex attention mechanisms (e.g., the State-Dependent Attention Mechanism).
   - Relevant literature on asymmetric market behavior, prospect theory, and techniques for handling asymmetric data in machine learning models.
   - Structure the repository to include forthcoming components, such as the `Bull_Flow_Engine`, `Bear_Flow_Engine`, `Portfolio_Risk_Manager`, `HealingController`, `CalculateAsymmetricFeatures` service, and `AsymmetricFeatureEngine` service. The initial commit should include the project documentation outlined above.

## Project Initialization

This phase establishes the project's foundation, focusing on thorough documentation and preparation for collaborative discussions about the chosen architecture, specifically the "Dual-Token Context Injection" method.

1. **Document Thought Process (Continuously Updated):** Maintain a comprehensive, living document detailing the project's evolution. This includes not only the progression of tasks but also the rationale behind key decisions, such as the selection of the "Dual-Token Context Injection" method over alternatives, encountered challenges, and evolving insights. This documentation should capture why this specific approach, incorporating both the regime ID and the raw asymmetric feature vector, was chosen and the potential for feature vector removal if its contribution proves minimal.

2. **Prepare for In-depth Discussion (Documentation and Repository):** Compile the necessary materials to facilitate detailed discussions regarding the project's architecture. This includes documentation explaining the "Dual-Token Context Injection" method, the rationale behind using a computed vector as input for the Vision Transformer (ViT), and the potential for future enhancements related to asymmetry understanding (while remaining mindful of over-engineering). A well-organized code repository (e.g., Git) should be established for collaborative development and version control, ensuring all team members have access to the latest code and relevant research papers. This repository should also contain the initial implementation of the `AsymmetricFeatureEngine` and its features (Price & Volatility Asymmetry Features, Volume & Participation Asymmetry Features, and Correlation Asymmetry Feature). Discussions should also cover the data model implications of using asymmetric features and how the data will be preprocessed and fed into the model.

## Data Acquisition and Preprocessing

This section details the acquisition and preparation of financial data from Yahoo Finance and Nasdaq for model training and evaluation.

1. **Data Sources:** Stock data (Open, High, Low, Close, Volume - OHLCV) for companies listed in the OMXS All-Share and S&P 500 will be acquired from Yahoo Finance, covering the period from 2016 to 2023. Index data for OMXS All-Share, OMXS Large Cap, OMXS Mid Cap, OMXS Small Cap, and First North All-Share will be acquired from Nasdaq. S&P 500 index data will also be sourced from Yahoo Finance. All index data will be treated as equally weighted.

2. **Data Preprocessing:** Acquired OHLCV stock data will be preprocessed, including adjusting close prices for stock splits (dividends will be excluded). Index data will be incorporated into the dataset.

3. **Data Storage:** Preprocessed data will be stored as CSV files in `stock_data/train/` and `stock_data/test/` directories.

4. **5-Day Windowing:** Data will be structured using a 5-day sliding window for both input features and output labels.

5. **Return Label Calculation:** The target variable, representing the future return, will be calculated as: `Return(t+h) = (Close(t+h) - Open(t+1)) / Open(t+1)`, where 't' is the last day of the 5-day input window and 'h' is the holding period (a hyperparameter ranging from 1 to 5 days).

6. **Candlestick Chart Generation:** Candlestick charts will be generated, incorporating volume and a moving average.

7. **Return Label Integration:** The calculated 5-day future returns will be integrated with the candlestick chart image generator.

8. **Filename Convention:** Filenames will encode the holding period or a separate CSV will be used to track this information.
   Data Acquisition and Preprocessing

This section details the acquisition, preprocessing, and preparation of financial data used to train and evaluate the trading model. A key aspect of this project is the use of candlestick chart images paired with corresponding return labels as model input. The following steps outline this process:

1. **Data Source:** OHLCV (Open, High, Low, Close, Volume) data for the selected financial instruments is acquired from Yahoo Finance.

2. **Data Acquisition and Preprocessing:** This step involves acquiring OHLC and index data from Yahoo Finance. Price adjustments are applied as needed (e.g., for splits and dividends) to ensure data accuracy and consistency for subsequent analysis.

3. **Data Storage:** The acquired and preprocessed stock data is stored as CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** A 5-day sliding window is implemented for both input and output data. The input to the model is a 5-day sequence of OHLCV data, represented visually as a candlestick chart. The corresponding output is the predicted return over the subsequent 5 trading days.

5. **Return Label Calculation:** The target variable, or return label, is calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window and `h` is the holding period (h=5, representing 5 trading days). This formula calculates the percentage return from the opening of the next day (t+1) to the close of the final day in the holding period (t+h).

6. **Candlestick Chart Generation:** Candlestick charts are generated from the 5-day windowed OHLCV data, incorporating volume and potentially a moving average indicator for additional context.

7. **Return Label Integration:** The calculation of the 5-day future return (the return label) is directly integrated with the candlestick chart generation process, ensuring each candlestick image has a corresponding return label. This combined data point (image and label) forms the basis for training the CNN.

8. **Filename Convention:** The filenames of the generated candlestick chart images (or a separate CSV file) are designed to incorporate the holding period (e.g., `YYYY-MM-DD__YYYY-MM-DD__3.25__h=5__ABB.ST.png`). This facilitates efficient data management and retrieval based on specific holding periods.

### B. Data Acquisition and Preprocessing

This section details the acquisition, preprocessing, and preparation of financial data for training the Convolutional Neural Network (CNN) model. The model uses 5-day candlestick chart images as input and predicts the corresponding 5-day future return.

1. **Data Source:** Historical Open, High, Low, Close, and Volume (OHLCV) data will be acquired from Yahoo Finance using the `yfinance` library. Specific tickers and date ranges will be documented for reproducibility.

2. **Index Data and Adjustments:** Relevant index data (e.g., S&P 500) will be acquired alongside OHLCV data for potential use in future analysis. Stock prices will be adjusted for splits and dividends to ensure data accuracy and consistency.

3. **Data Storage:** Acquired and preprocessed data will be stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** A sliding window mechanism will create 5-day sequences of OHLCV data for model input.

5. **Return Label Calculation:** The 5-day future return for each window will be calculated using the following formula: `((Close(t+5) - Open(t+1)) / Open(t+1)) * 100`, where `t` represents the starting day of the 5-day window. These returns serve as the target output for the CNN model during training.

6. **Candlestick Chart Generation:** Each 5-day window will be visualized as a candlestick chart, incorporating volume information and a 20-day Simple Moving Average (SMA) for additional context. These charts are the primary input to the CNN.

7. **Data Integration:** Calculated return labels will be directly associated with their corresponding candlestick chart images.

8. **Filename Convention:** A consistent filename convention will be implemented, likely encoding the holding period within the filename or using a separate CSV file to map images to labels. This ensures efficient data loading during model training.

The CNN model training will exclude explicit date, price, and ticker symbol information. The model will focus solely on the visual patterns in the scaled candlestick charts and learn to associate these patterns with the calculated future returns. This approach aims to prevent overfitting to specific tickers or time periods and encourages the model to learn more generalizable patterns in market behavior.

### B. Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data for training a convolutional neural network (CNN) to predict future stock returns. A 5-day window is employed for both input features (candlestick data) and output labels (future returns), although the rationale differs for each.

**Input Window (5-day):** Following established practices from Jiang et al. (2023), a 5-day input window of OHLCV (Open, High, Low, Close, Volume) data is used. This choice is supported by their research demonstrating improved pattern learning with this specific window size.

**Output Window (5-day Future Prediction):** The 5-day future prediction window mirrors the input window length for practical considerations, such as aligning with typical trading frequencies. However, it lacks the same level of empirical validation as the input window, representing a potential area for future optimization.

**Data Sources and Storage:**

1. **Source:** OHLCV and relevant index data will be acquired from Yahoo Finance using the `yfinance` library (or equivalent).
2. **Preprocessing:** Acquired data will be adjusted for splits and dividends to ensure accuracy.
3. **Storage:** Processed data will be stored as CSV files in designated directories: `stock_data/train/` and `stock_data/test/`.

**Data Preparation:**

1. **Windowing:** The data will be organized into 5-day windows, forming the input samples.
2. **Return Calculation:** The target variable, representing the 5-day future return, will be calculated using the following formula, where `h = 5`: `(Close(t+h) - Open(t+1)) / Open(t+1)`.
3. **Candlestick Charts:** Candlestick charts will be generated from the 5-day OHLCV data, including volume and a moving average indicator (details of the moving average type and period will be documented). Each chart will be paired with its corresponding calculated return label.
4. **Filename Convention:** A consistent file naming convention will incorporate the holding period (e.g., 5 days) to facilitate data organization and retrieval.

**Soft Labeling:** To potentially enhance model accuracy and generalization, the use of soft labels instead of hard labels will be explored. This involves:

1. **Discretization:** The continuous range of possible returns will be discretized into bins (e.g., -5% to +5% in 0.5% increments). The specific binning strategy will be carefully determined and optimized.
2. **Soft Label Generation:** Soft labels, representing a probability distribution over the discretized return bins, will be generated using a Gaussian kernel centered around the actual return. The impact of hard labels on validation performance will be analyzed, and suitable methods for generating and utilizing soft labels during training and validation will be researched.

**Future Considerations:** The impact of trading costs on alpha generation will be a key consideration during backtesting and performance evaluation. Factors contributing to these costs, such as frequent rebalancing, transaction friction, uniform weighting, and short-selling constraints, will be addressed during strategy development. Additionally, further investigation into the empirical justification for the 5-day output window and the potential of soft labeling will be conducted.
This section details the acquisition and preprocessing of financial data required for training and evaluating the trading agent. The following steps ensure data consistency and proper formatting for the subsequent model development:

1. **Data Source:** Historical Open, High, Low, Close, and Volume (OHLCV) data will be acquired from Yahoo Finance. This widely-used platform offers readily available historical market data for a broad range of financial instruments.

2. **Data Acquisition and Preprocessing:** Along with OHLCV data, relevant index data will be acquired to provide market context. Price adjustments for splits and dividends will be performed to ensure data accuracy and consistency.

3. **Data Storage:** The acquired and preprocessed data will be stored as CSV (Comma Separated Value) files in dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. This organized structure facilitates efficient data management and access.

4. **5-Day Windowing:** A 5-day sliding window approach will be implemented. Input features for the model will consist of data from five consecutive trading days. The corresponding output (target variable) will be calculated based on this 5-day window and a specified holding period.

5. **Return Label Calculation:** The target variable, representing the predicted return, will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`. This calculates the percentage return from the opening price on the day after the input window (t+1) to the closing price after the holding period (t+h), where 'h' represents the holding period (e.g., 5 days).

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day window OHLCV data. These charts will incorporate volume information and potentially a moving average indicator to visualize price trends. They will serve as visual inputs for the model.

7. **Return Label and Chart Integration:** The processes of generating candlestick chart images and calculating the corresponding return labels will be tightly integrated to ensure data consistency and prevent mismatches between inputs and targets. Each candlestick chart image will have a corresponding return label, forming a data pair for training and evaluation.

8. **Holding Period Encoding:** The filename convention will be updated to incorporate the holding period (h). This can be achieved by encoding 'h' within the filename itself or by using separate CSV files for different holding periods. This facilitates easy identification and management of data corresponding to specific holding periods.

### B. Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data for model training and evaluation. The data preparation pipeline includes collecting data from a reliable source, performing necessary adjustments for data accuracy, organizing the data for efficient access, and generating input features and labels for the model.

1. **Data Source:** Financial data, including Open, High, Low, Close, and Volume (OHLCV), will be acquired from Yahoo Finance.

2. **Data Preprocessing:** The acquired OHLCV data, along with relevant index data, will be preprocessed. This includes adjusting prices for stock splits and dividends to maintain data accuracy and consistency.

3. **Data Storage:** Preprocessed data will be stored as CSV files in designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** A 5-day sliding window approach will be used, creating input sequences of five consecutive trading days of OHLCV data. Each sequence will be associated with a corresponding output label.

5. **Return Label Calculation:** The target variable, representing the return, will be calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`. This calculates the percentage return from the opening price on the day following the 5-day window (t+1) to the closing price after a holding period of h days (t+h). For this project, h=5.

6. **Candlestick Chart Generation:** Candlestick charts will be generated from each 5-day window, visualizing the price action and including volume and a moving average indicator. These charts will serve as input for the CNN model.

7. **Return Label Integration:** The calculation of return labels will be directly integrated with the candlestick chart generation process to ensure each chart is paired with its corresponding label.

8. **Filename Convention:** The filenames of the candlestick chart images, or a separate CSV file, will encode the holding period (h=5). This facilitates easy identification and retrieval of data associated with this specific holding period.

9. **Future Data Preprocessing Considerations:** While this section outlines the initial data preprocessing steps, it's important to acknowledge that this will be an ongoing process. As model development progresses, we may revisit this stage to refine data handling techniques or incorporate additional data sources as needed.

## Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data, focusing on preparing data for training the Vision Transformer (ViT) model. The process involves acquiring raw financial data, transforming it into sequential candlestick chart images, and engineering relevant features.

**A. Initial Data Acquisition and Preparation**

1. **Data Source:** OHLCV (Open, High, Low, Close, Volume) data, along with relevant index data, will be acquired from Yahoo Finance.
2. **Preprocessing:** This includes downloading the OHLC and index data, and adjusting prices for corporate actions such as splits and dividends to ensure data accuracy.
3. **Storage:** The preprocessed data will be stored as CSV files in designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.
4. **Windowing:** The data will be structured using a 5-day sliding window approach for both input features and output labels.
5. **Return Label Calculation:** The target variable, representing the future 5-day return, will be calculated using the following formula: `(Close(t+5) - Open(t+1)) / Open(t+1)`, where 't' represents the current time step.
6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day OHLCV data, incorporating volume information and moving averages. These charts will serve as the visual input for the ViT model.
7. **Synchronization:** The generation of candlestick chart images will be synchronized with the calculation of the corresponding 5-day return labels.
8. **File Naming:** The file naming convention will incorporate the 5-day holding period for clear identification.

**B. Sequential Data Preparation for ViT**

1. **Dataset Design:** A dataset of image sequences will be created using a sliding window approach. Each sequence will comprise candlestick charts from time _t-2_, _t-1_, and _t_, with corresponding labels for the 5-day return.
2. **Sequence Generator:** A dedicated tool will be developed to automate the generation of these image sequences from the stock data using the defined sliding window approach.
3. **Input Data:** The ViT model will use these sequential candlestick windows as input data, allowing it to learn from the temporal dynamics of price movements.
4. **Feature Engineering:** Delta features, capturing the change between consecutive candlestick windows, will be implemented. These can be calculated via image subtraction (comparing pixel differences) or feature subtraction (comparing OHLCV values).
5. **Variable-Length Sequence Handling:** To accommodate varying sequence lengths, shorter sequences will be padded with blank chart images to match the maximum sequence length, and a masking vector will inform the ViT which inputs are padding.
6. **Positional Embeddings:** Positional embeddings will be used to provide the ViT with information about the order of candlestick images within each sequence.
7. **Input Sequence Length Experimentation:** Experiments will be conducted with different numbers of candlestick images (N) as input, starting with N=3 (15 trading days) and incrementally increasing to determine the optimal sequence length.
8. **Input Image Count Constraint Investigation:** The potential constraint of the ViT training layer on the number of input images will be investigated to determine if the number of input images is a configurable parameter.
9. **Backtesting Framework Development:** Development of the backtesting framework will begin in this stage to facilitate continuous evaluation of model performance as it develops and to ensure alignment with the chosen data format and features.

### B. Data Acquisition and Preprocessing

This section details the acquisition and preparation of financial data for model training and evaluation. The primary focus is on preparing data for direct return prediction, with a secondary exploration of candlestick image prediction. This alternative method requires a shift from numerical returns to image sequences.

**Primary Approach: Numerical Return Prediction**

The following steps outline the data pipeline for numerical return prediction:

1. **Data Acquisition (Retrieve OHLC and index data):** OHLC (Open, High, Low, Close) and relevant index data are retrieved from Yahoo Finance.

2. **Data Adjustment (Adjust price data):** Acquired price data is adjusted (e.g., for splits and dividends).

3. **Data Storage (Store adjusted data in CSV files):** Adjusted data is stored as CSV files in designated training (`stock_data/train/`) and testing (`stock_data/test/`) directories.

4. **Windowing (Create 5-day windows):** Data is segmented into 5-day windows to form the input features.

5. **Return Label Calculation (Calculate h-day future returns):** Return labels, representing the target variable, are calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`. This calculates the percentage return from the open of the day after the window (t+1) to the close at the end of the holding period (t+h), where 'h' represents the holding period.

6. **Candlestick Chart Generation (Generate charts with volume and moving average):** Candlestick charts are generated from the 5-day windowed data, visualizing OHLCV data, volume, and a moving average indicator. These charts serve as a visual aid during development and analysis.

7. **Integration (Combine return labels with candlestick charts):** Although the primary approach uses numerical returns, candlestick charts are generated and linked with the corresponding return labels for visual analysis and potential future use.

8. **Filename Updates (Encode holding period in filenames or CSV):** The file naming convention incorporates the holding period (h), either within filenames or using separate CSVs, for clear identification.

**Secondary Approach: Candlestick Image Prediction**

An alternative approach involves predicting future candlestick images instead of directly predicting returns. Predicted images would then be interpreted to extract return values. This shift to visual sequence forecasting (image-to-image prediction) necessitates a separate data pipeline and raises several key considerations:

- **Conceptual Soundness:** Does predicting candlestick images align with how human traders interpret these patterns?
- **Theoretical Advantages:** Potential benefits include richer representation, uncertainty modeling, potential for causal reasoning, enhanced training supervision, visual interpretability, and generative flexibility.
- **Implementation:** Requires modifications to the data pipeline to generate and process sequences of candlestick images as both input and output. This includes extracting meaningful predictions from generated images, possibly through pixel analysis or converting images back to numerical OHLC data.

**Data Preprocessing Considerations for Model Development and Evaluation:**

Regardless of the chosen approach, the following are crucial:

- **Actionable Trading Decisions:** Data preprocessing must support generating clear buy/sell signals, not just predicting visual patterns.
- **Model Evaluation:** The process should facilitate a multi-faceted evaluation, considering both financial returns and the visual plausibility of generated candlestick sequences.
  Data Acquisition and Preprocessing

This section details the acquisition and preparation of the financial data used for training and evaluating the prediction model. While the initial checklist didn't explicitly address data preprocessing steps, it highlighted key principles that inform the process. These principles, coupled with the project outline, guide the development of a robust and relevant data pipeline. The data must not only generate realistic candlestick patterns but also incorporate features that support causal inference, trading relevance, and robustness against overfitting.

Specifically, the preprocessing must consider:

- **Trading Relevance:** The data should highlight actionable trading opportunities, ensuring features and transformations contribute to clear entry and exit points on the generated candlestick charts.
- **Causal Grounding:** Features should reflect underlying market dynamics, not just superficial patterns. Incorporating information like trading volume can help the model learn genuine relationships rather than spurious correlations.
- **Robustness:** Preprocessing should mitigate potential issues inherent in image-based prediction, such as ground truth misalignment and overfitting. This includes careful data cleaning, normalization, and augmentation to ensure diverse market conditions (bull, bear, and sideways trends) are represented.

The following steps outline the data pipeline:

1. **Data Source:** Historical OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance.
2. **Index Data and Adjustments:** Relevant index data will also be acquired. Necessary price adjustments (e.g., for splits and dividends) will be applied to ensure data accuracy and consistency.
3. **Storage:** The processed data will be stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.
4. **5-Day Windowing:** The data will be organized into 5-day windows, each representing five consecutive trading days of OHLCV and index data. These windows serve as the model's input samples.
5. **Return Label Calculation:** The target variable, representing the return, will be calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the 5-day window and `h` is the holding period.
6. **Candlestick Chart Generation:** Visual representations of the 5-day windows will be generated as candlestick charts, incorporating volume and a moving average indicator.
7. **Return Label Integration:** The calculated return label will be paired with its corresponding candlestick chart image.
8. **Filename Convention:** The filename convention will encode the holding period (either directly in the filename or using separate CSV files).

## Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data from Yahoo Finance for the SCoVA project. The process focuses on preparing the data to align with the dynamic 2D plane concept central to the project's theoretical framework. This involves transforming raw Open, High, Low, Close, Volume (OHLCV) data and relevant index data into a representation suitable for modeling stock price movements as trajectories on a manifold within a dynamic 2D plane.

**Data Acquisition:**

1. **Source:** Historical OHLCV and relevant index data will be acquired from Yahoo Finance.
2. **Adjustment:** Acquired price data will be adjusted for splits and dividends to ensure accuracy.
3. **Storage:** Data will be stored as CSV files in designated training (`stock_data/train/`) and testing (`stock_data/test/`) directories.

**Preprocessing:**

1. **5-Day Windowing:** A 5-day sliding window will be applied to the OHLCV data. Each input will consist of five consecutive days of data.
2. **Return Label Calculation:** The target variable, representing the 5-day future return (holding period h = 5), will be calculated as: `(Close(t+h) - Open(t+1)) / Open(t+1)`. This captures the return achieved by holding the asset for the subsequent 5-day period.
3. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day OHLCV windows, incorporating volume and a moving average indicator for added context. These charts will be linked to their corresponding return labels.
4. **Filename Convention:** The filename convention will clearly indicate the 5-day holding period. This may involve embedding the holding period in the filename itself or using a separate CSV file for mapping.

**Dynamic 2D Plane Transformation:**

The core preprocessing step involves transforming the data to align with the dynamic 2D plane concept. This requires further research and implementation of the following:

- **Coordinate Transformations:** Raw data will be transformed into the dynamic 2D plane's coordinate system, accounting for rotational axes and a moving origin, based on research into coordinate transformations, manifolds, and the bending of space.
- **Moving Frame Representation:** Stock price movement will be represented as a trajectory on a 1-manifold. Each point on the trajectory will have an associated orthonormal frame, and the rotation of this frame along the trajectory will be captured.
- **Encoding Curves (Parabolas):** Parabolic trends and other curves in the stock price will be encoded as rotations within the dynamic 2D plane.
- **Information Balance and Degrees of Freedom:** The preprocessing will ensure that the original data's information is effectively encoded within the two coordinates (u,v) and the three Euler angles (or rotation matrix) describing the dynamic frame's orientation. This might involve feature engineering or dimensionality reduction.

Subsequent sections will detail the specific algorithms and implementations for these transformations.

### B. Data Acquisition and Preprocessing

This stage focuses on acquiring and preparing financial data for model training. Visualization considerations inform the preprocessing steps to ensure data integrity and compatibility with later stages.

**Data Acquisition and Preprocessing Steps:**

1. **Data Source:** Acquire Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance.
2. **Index Data and Adjustments:** Acquire relevant index data and adjust prices for splits and dividends to ensure accuracy.
3. **Data Storage:** Store acquired and preprocessed data in CSV files within `stock_data/train/` and `stock_data/test/`.
4. **5-Day Windowing:** Implement logic to create 5-day windows of data for model input.
5. **Return Label Calculation:** Calculate the 5-day future return using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`. This serves as the target variable.
6. **Candlestick Chart Generation:** Generate individual candlestick charts, including volume and moving averages, adhering to the constraint of avoiding subplots. These charts will serve as input for the CNN model.
7. **Integrated Return Labels:** Integrate the 5-day future return calculation into the candlestick chart generation process, ensuring each chart is paired with its corresponding target variable.
8. **Filename Convention:** Update the filename convention to encode the holding period (h), either directly in the filenames or in a separate CSV file.

**Technical Constraints for Visualization:**

- **Matplotlib and NumPy:** All plotting must use `matplotlib` and `numpy`; `seaborn` is prohibited.
- **Single Chart Output:** Each chart generation call must produce a single chart, avoiding subplots. This constraint directly affects the design of candlestick charts and other visualizations.

**Future Considerations:**

- **Orientation Visualization:** A potential future enhancement is visualizing orientation over time, possibly using rotation matrices or quaternion frames, while still avoiding subplots. This should be considered during preprocessing for future compatibility.
- **Helix Trace Visualization:** (This seems incomplete and unrelated to the stock market context. If it's relevant, please provide more context so it can be integrated properly. Otherwise, remove it.)

### B. Data Acquisition and Preprocessing

This section outlines the acquisition and preprocessing of financial data from Yahoo Finance, focusing on creating a consistent and reliable data pipeline for training the prediction model. The data undergoes transformations to create a dynamic, localized perspective of price action for the model's input.

1. **Data Source:** Historical OHLCV (Open, High, Low, Close, Volume) data is acquired from Yahoo Finance.

2. **Index and Adjustment Data:** Relevant index data is acquired, and price adjustments for splits and dividends are applied to ensure data accuracy and consistency.

3. **Data Storage:** Acquired and preprocessed data is stored as CSV files in designated training (`stock_data/train/`) and testing (`stock_data/test/`) directories.

4. **5-Day Windowing:** A 5-day rolling window is used to create input features and corresponding output labels, capturing short-term price trends and volatility.

5. **Return Calculation:** The target variable, representing future returns, is calculated as: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the window and `h` is the holding period.

6. **Candlestick Chart Generation:** Candlestick charts with volume and moving average indicators are generated for each 5-day window as input for the CNN model.

7. **Return Label Generation:** Corresponding 5-day future return labels are calculated for each candlestick chart image.

8. **Integration of Return Calculation:** The return label calculation is integrated with candlestick chart generation, ensuring synchronization between visual input and target value.

9. **Filename Convention:** The filename convention encodes the holding period within the filename or uses separate CSV files for different holding periods.

10. **Dynamic Projection (Local Movement Vector):** A local movement vector (v) is calculated as the difference between the current price (P_current) and the previous price (P_previous): `v = P_current - P_previous`. Both P_current and P_previous include price and (X,Y) time coordinates within the candlestick chart.

11. **Rotation Matrix Calculation:** The angle of rotation (θ) is determined from the local movement vector: `θ = arctan(v_y / v_x)`. This angle is used to construct the rotation matrix R(θ):

    ```
    R(θ) = [[cos(θ), sin(θ)], [-sin(θ), cos(θ)]]
    ```

12. **Rotation Application:** The rotation matrix is applied to a window of candlestick data (e.g., the last 5-10 candlesticks) to rotate the local price data around the origin.

13. **Dynamic Origin Shift:** Following rotation, the coordinates are translated to center the current price action. Details of this translation will be further defined in the model architecture section. The purpose of this dynamic rotation and translation is to provide the model with a localized perspective of price action, potentially improving its ability to generalize and predict future movements.
    Data Acquisition and Preprocessing

This section details the acquisition and preprocessing steps required to prepare the market data for model input. This involves two primary stages: (1) acquiring and preparing standard OHLCV data, and (2) constructing a dynamic 2D plane representation of price movements for input to the CNN or Vision Transformer models.

**Stage 1: Standard OHLCV Data Preparation**

1. **Data Source:** Historical OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance using the `yfinance` library in Python.

2. **Data Preprocessing:** This includes handling missing values, adjusting for stock splits and dividends, and potentially incorporating relevant index data (e.g., S&P 500) for broader market context.

3. **Data Storage:** The preprocessed data will be stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **Windowing:** The data will be organized into 5-day windows, each serving as an input sample.

5. **Return Label Calculation:** The target variable, representing the percentage return from the open of the day following the 5-day window (t+1) to the close of a future day (t+h), will be calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 'h' represents the holding period.

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day window data, including volume and a moving average indicator (e.g., a simple moving average).

7. **Return Label Integration:** Each candlestick chart will be paired with its corresponding calculated return label.

8. **Filename Convention:** Filenames of the candlestick chart images, or a separate CSV file, will encode the holding period 'h'.

**Stage 2: Dynamic 2D Plane Construction**

This stage focuses on creating dynamic candlestick chart snapshots by transforming the 5-day window data onto a continuously recalculated 2D plane. This plane, derived from local price movements, provides the model with a locally accurate and constantly evolving market perspective.

1. **Local Window Definition:** A sliding window encompassing a set number of previous candles (e.g., 5-10) will provide local context.

2. **Movement Vector Calculation:** Movement vectors within the time-price-volume space will be calculated for each data point in the local window.

3. **Principal Component Analysis (PCA):** PCA will be applied to the movement vectors to extract the two dominant axes of movement.

4. **Dynamic Plane Reconstruction:** The top two principal components from PCA will define the axes of the dynamically redrawn 2D plane. The 5-day window data will be projected onto this new coordinate system.

5. **Data Transformation and Centering:** The current price point will serve as the origin (0,0) of the dynamic plane. This centers the movement and represents price oscillations as deviations around 0.

6. **Dynamic Snapshot Generation:** The transformed data will be visualized as candlestick charts. These dynamic snapshots, where time is not explicitly represented horizontally, will serve as input for the CNN or Vision Transformer models.

**Addressing Technical Challenges:**

- **Rotation Artifacts:** Interpolation techniques like anti-aliasing will minimize distortion during image redrawing after rotation.

- **Volatility Jumps:** Smoothing or limiting rotation angle changes between consecutive frames will prevent abrupt transitions and mitigate the impact of extreme price movements.

- **Consistent Axis Scaling:** Standardized units per percentage move on both axes will ensure uniform interpretation of price movements regardless of rotation.

- **Pseudocode Documentation:** A detailed pseudocode pipeline will document the dynamic snapshot generation process for clarity and reproducibility.

Further refinement is required for calculating the rotation angle (theta), moving beyond simple price difference to incorporate the dynamic origin and rotational axes.

## Data Acquisition and Preprocessing

This section details the acquisition, preprocessing, and transformation of financial data into the dynamic 2D plane representations used as input for the Vision Transformer (ViT) or Convolutional Neural Network (CNN). This process involves generating candlestick chart images and then transforming them based on market movements to create the dynamic plane.

The following steps outline the data pipeline:

1. **Data Source:** Historical OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance. This data forms the basis for constructing the candlestick charts.

2. **Additional Data and Adjustments:** Relevant index data will also be acquired. Necessary price adjustments for corporate actions such as splits and dividends will be applied to ensure data accuracy.

3. **Data Storage:** The acquired and preprocessed data will be stored as CSV files in designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** A 5-day sliding window will be applied to the time series data. This window defines the period used for creating both the input features (dynamically generated candlestick images) and the corresponding output labels (future returns).

5. **Return Label Calculation:** Return labels, representing the percentage change in price over a holding period _h_, will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`.

6. **Initial Candlestick Chart Generation:** Standard candlestick charts will be generated from the 5-day windowed OHLCV data, incorporating volume and a moving average indicator. These initial charts serve as a basis for the subsequent dynamic transformations. Example images of these pre-transformation candlestick charts will be provided (see Figure X). _[Placeholder for figure cross-reference]_

7. **Dynamic 2D Plane Generation:** The static candlestick charts will be transformed into a dynamic 2D plane representation. This process involves repositioning the origin of the chart based on the current day's opening price and rotating the chart based on price and volume movements. Details of this transformation process, including the specific algorithms and parameters used, will be provided in a subsequent section. Example images of these transformed candlestick charts will be provided (see Figure Y). _[Placeholder for figure cross-reference]_

8. **Output Format:** The generated dynamic 2D plane representations will be formatted appropriately for input to the chosen model (ViT or CNN). This may involve converting the visual representation into a numerical format suitable for the model.

This pipeline ensures that the data is appropriately processed and transformed into the dynamic 2D plane representation required for training and evaluating the predictive models.

## B. Data Acquisition and Preprocessing

This section details the process of acquiring and preparing the financial data for the project. While the initial steps are outlined below, the dynamic nature of financial markets requires continuous data updates and potential adjustments to the preprocessing pipeline. This process also addresses potential challenges related to data sparsity, particularly in the early stages of data acquisition or under specific market conditions.

- **Data Source:** OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance. This data forms the basis for candlestick chart generation and return calculations.

- **Data Enrichment and Adjustment:** Along with OHLCV data, relevant index data will be acquired to provide market context. Price adjustments for corporate actions (e.g., splits and dividends) will be applied to ensure data accuracy. The process will be robust to potential issues arising from limited initial data points.

- **Data Storage:** Processed data will be stored as CSV files in designated directories: `stock_data/train/` and `stock_data/test/`.

- **5-Day Windowing:** A 5-day rolling window will be used to create input sequences for the model and define the period for calculating future returns. Edge cases with fewer than five data points, especially at the beginning of the dataset, will be handled gracefully. For single-point frames, either a placeholder will be displayed or the frame will be omitted, ensuring compatibility with the animation framework and preventing issues such as PCA instability due to limited data. This approach aligns with the technical constraint of requiring a minimum number of points for accurate calculations and animation.

- **Return Calculation:** Returns will be calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where _h_ represents the holding period. The calculation will handle edge cases at the end of the dataset where `t+h` might exceed available data.

- **Candlestick Chart Generation:** Candlestick charts, incorporating volume and moving average indicators, will be generated as the primary input for the CNN model. The generation process will accommodate scenarios with fewer than five data points within the 5-day window. Chart generation parameters, including the types of moving averages used, will be documented.

- **Return Label Generation:** 5-day future returns will be calculated using the defined formula and serve as the target variable for model training.

- **Synchronized Data Processing:** The return label calculation will be tightly integrated with the candlestick chart generation process, ensuring data consistency and associating each chart with the correct target return.

- **Filename Convention:** The filename convention will be updated to clearly indicate the holding period. This will be achieved either by encoding the holding period directly within the filename or by using separate CSV files for different holding periods.

This continuous data acquisition and preprocessing pipeline ensures that the model is trained and tested on relevant and up-to-date market data while addressing potential challenges posed by data sparsity.

## B. Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data used for model training and visualization. This involves collecting data from Yahoo Finance, generating synthetic data for specific market scenarios, and transforming the data for both standard candlestick and Heiken-Ashi chart representations. The initial focus is on establishing a functional data pipeline, validating it with small tests, and ensuring the data transformations are correctly applied.

**1. Data from Yahoo Finance:**

The primary data source is Yahoo Finance, providing Open, High, Low, Close, and Volume (OHLCV) data. This data will be preprocessed to include index data and any necessary price adjustments. The data will be stored in CSV files organized into training and testing sets, located in `stock_data/train/` and `stock_data/test/`, respectively. A 5-day windowing logic will be applied to both input features and output labels. The return label, representing the 5-day future return, will be calculated as `(Close(t+5) - Open(t+1)) / Open(t+1)`.

**2. Candlestick Chart Generation:**

Candlestick charts will be generated from the OHLCV data, incorporating volume and moving average indicators. These charts will be integrated with the return label calculation. Filenames for these images, or a separate accompanying CSV file, will encode the holding period information.

**3. Heiken-Ashi Chart Generation and Transformation:**

Heiken-Ashi charts will be generated alongside standard candlestick charts for comparison. This involves:

- **Generation:** A function will convert standard OHLC data into Heiken-Ashi candles (time, open_HA, high_HA, low_HA, close_HA).
- **Plotting:** Standard Heiken-Ashi candlestick charts will be created, using green bodies for upward price movements and red bodies for downward movements.
- **Dynamic Rotation and Recentering:** A transformation function will dynamically rotate and recenter the Heiken-Ashi data based on the midpoint of each candle's open and close values.
- **Rotated Plotting:** The transformed Heiken-Ashi data will be visualized on a 2D plane to illustrate the effect of the rotation and recentering.
- **Saving Charts:** Both standard and rotated Heiken-Ashi charts will be saved as PNG files at designated paths (e.g., `/mnt/data/standard_heiken_ashi.png` and `/mnt/data/rotated_dynamic_plane_heiken_ashi.png`).

**4. Simulated Data for Specific Market Regimes:**

To test the robustness of the dynamic plane implementation, synthetic data will be generated to simulate various market conditions, including:

- **Complex Price Patterns:** Realistic market behavior, including rallies, drops, and recovery phases, will be simulated.
- **Choppy/Chaotic Market Conditions:** A simulation of a chaotic, choppy sideways market will test the system's resilience to high volatility and rapid price fluctuations. A function `generate_choppy_candlesticks(n=30)` will generate this data. Both standard and rotated Heiken-Ashi charts will be created from this simulated data and saved as PNG files (e.g., `/mnt/data/standard_heiken_ashi_choppy.png` and `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`).

This comprehensive data preparation process ensures the model is trained and evaluated on a diverse range of market scenarios, validating its performance under both typical and extreme conditions. Further details regarding the dynamic plane implementation and visualization techniques will be provided in subsequent sections.

## Data Acquisition and Preprocessing

This section details the process of acquiring, preprocessing, and preparing financial data for both visualization and model training. The process incorporates a dynamic coordinate system generated by Principal Component Analysis (PCA), requiring careful data handling to ensure stability and meaningful pattern recognition.

### Visualization Data

Data for visualization will be sourced from Yahoo Finance and supplemented with simulated data representing distinct market regimes: Trend-Reversal-Recovery, Choppy Sideways, and a strong linear uptrend. This combination allows for a comprehensive comparison between standard Heiken-Ashi charts and the rotated Dynamic Plane charts under various market conditions. Preprocessing will focus on generating both chart types for each regime and arranging them for side-by-side comparison within a single panel. Special attention will be given to plotting techniques to avoid previously encountered issues with matplotlib.

### Model Training Data

The following steps outline the data pipeline for model training:

1. **Data Source:** OHLCV and relevant index data will be acquired from Yahoo Finance.

2. **Price Adjustment:** Prices will be adjusted for corporate actions such as splits and dividends to ensure accuracy.

3. **Data Storage:** Data will be stored as CSV files in designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** A 5-day sliding window will be used, meaning input data will consist of 5 consecutive days of OHLCV and index data.

5. **Return Label Calculation:** The target variable (return over holding period _h_) will be calculated as: `(Close(t+h) - Open(t+1)) / Open(t+1)`. This represents the percentage change from the open of the day following the 5-day window (t+1) to the close at the end of the holding period (t+h).

6. **Candlestick Chart Generation:** Candlestick charts, including volume and a moving average indicator, will be generated from the 5-day window data.

7. **Return Label Association:** Each candlestick chart will be associated with its corresponding 5-day future return.

### PCA Considerations for Model Training

The following considerations are crucial for incorporating PCA into the data pipeline:

1. **PCA Window Size and Smoothing:** The PCA window size will be a critical parameter, requiring careful selection to balance sensitivity to market movements with stability against noise. Smoothing techniques or stability thresholds will be explored and implemented to mitigate erratic rotations in the PCA space due to market noise, which could lead to overfitting.

2. **Relational Learning Emphasis:** The model should learn relationships and patterns _within_ the dynamic PCA space, rather than relying on fixed interpretations of price, time, and volume. The focus should be on geometric shapes and flows in the normalized PCA space, independent of the original axes' semantic meaning. Traditional technical indicators may be less relevant in this context.

3. **Interpretability Projection:** To facilitate understanding, a mechanism will be developed to project the model's attention back into the original Time-Price-Volume space. This will enable analysis of model behavior and explanation of its decisions.

These additions ensure that the data is appropriately prepared for a model operating on a dynamic, PCA-transformed representation of market data, addressing both technical constraints and the desired model architecture. While the initial data acquisition and preprocessing steps (1-7 above) are currently considered complete, they may require adjustments as the project evolves and model complexity increases, particularly with the potential incorporation of features like an "Error Signal" or changes to the "Model Learning Shift."
Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data for the SCoVA project. The following steps outline the process:

1. **Data Source:** OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance.

2. **Data Acquisition and Preprocessing:** This includes acquiring OHLC and relevant index data, and adjusting historical prices for splits and dividends to ensure data accuracy.

3. **Data Storage:** The acquired and preprocessed data will be stored as CSV files in designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** A 5-day sliding window will be used for both input features and output labels. Each input sample will consist of 5 consecutive days of OHLCV and index data. The corresponding output label will be the return calculated over the subsequent 5-day holding period.

5. **Return Label Calculation:** The return label will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window, `h` is the holding period (5 days), `Close(t+h)` is the closing price on the last day of the holding period, and `Open(t+1)` is the opening price on the first day after the input window.

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day window data, including volume information and a moving average indicator. Each chart will be associated with its corresponding return label.

7. **Filename Convention:** The filenames of the candlestick chart images (or a separate CSV file) will encode the holding period (h) for easy identification and retrieval of data for specific holding periods.

Model Adaptation and Refinement

This section details the algorithms and data structures used for handling prediction errors and dynamically adjusting the model's frame of reference. These mechanisms will inform future iterations of data acquisition and preprocessing, enhancing the model's adaptability to changing market conditions.

**Prediction Error Handling and Dynamic Frame Adjustment**

The following elements are crucial for maintaining model accuracy and responsiveness:

- **Prediction Error Buffer:** A rolling buffer will store recent prediction errors, calculated as the difference between the predicted and actual price movement within the rotated frame. The buffer size (e.g., 5-10 steps) will be configurable, affecting the system's sensitivity to recent errors.

- **Error Trend Detector:** This algorithm analyzes the prediction errors in the rolling buffer, calculating the rolling mean and variance. A dynamic threshold, potentially a multiple of the rolling standard deviation, triggers a correction mode when consistently exceeded.

- **Rolling Frame Correction Algorithm:** Based on the analogy of wound healing, this algorithm corrects the dynamic Principal Component Analysis (PCA) frame based on identified error trends. If the Error Trend Detector signals a significant trend, the algorithm applies small rotation adjustments or damping to the PCA frame. The correction gradually diminishes as errors subside. This algorithm comprises three phases: _error detection_, _frame correction_, and a _healing phase_.

- **Mitigating Plateau in Dual Records:** This algorithm addresses potential stagnation in the long-term record caused by dual records lagging behind market changes. Further details on this mitigation strategy are needed.

- **Frame Coincidence Correction with Rolling Rewiring:** This algorithm refines frame correction using a rolling mechanism that gradually returns the system to its normal state after corrections. It addresses the problem of frame coincidence, where PCA axes align suboptimally. Further details on "rolling rewiring" are needed.

These mechanisms will be integrated into the model's learning process. Data acquisition and preprocessing will be refined based on model performance, leading to a more robust and adaptive system.
Data Acquisition and Preprocessing

This section details the acquisition, preprocessing, and preparation of financial data for model training.

1. **Data Source:** OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance.

2. **Data Acquisition and Preprocessing:** This involves downloading OHLC and relevant index data. Historical prices will be adjusted for splits and dividends to ensure accuracy.

3. **Data Storage:** The preprocessed data will be stored as CSV files in `stock_data/train/` (training data) and `stock_data/test/` (testing data).

4. **5-Day Windowing:** A 5-day sliding window will be used for both input features and output labels. Each input will consist of five consecutive trading days' worth of data.

5. **Return Label Calculation:** The target variable, representing the 5-day future return, will be calculated as: `(Close(t+5) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window. This formula calculates the percentage change in price from the opening price of the day following the window (t+1) to the closing price five days later (t+5).

6. **Candlestick Chart Generation:** Candlestick charts will be generated for each 5-day window, incorporating volume and a moving average. These charts will serve as input for the CNN model.

7. **Return Label Integration:** The return label calculation will be integrated with the candlestick chart generation process, ensuring each chart is paired with its corresponding label.

8. **Filename Convention:** A consistent file naming convention will be implemented, either by encoding the holding period (5 days) in the filenames or using separate CSV files for different holding periods.

This pipeline ensures the data is properly prepared for subsequent model development and training.

### B. Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data from Yahoo Finance, encompassing OHLCV (Open, High, Low, Close, Volume) and relevant index data. Data will be stored as CSV files in `stock_data/train/` and `stock_data/test/` directories. A key aspect of preprocessing is the implementation of a 5-day rolling window for both input features and output labels, aligning with the short-term prediction horizon.

The following steps outline the data pipeline:

1. **Data Acquisition:** OHLCV and index data are acquired from Yahoo Finance.
2. **Data Preprocessing:** Raw data is adjusted and formatted for model compatibility. This includes handling missing values and potential outliers. Index data is incorporated to provide broader market context.
3. **5-Day Windowing:** A rolling 5-day window is applied to create input features. Each data point represents a 5-day period of OHLCV and index data.
4. **Return Label Calculation:** The target variable, representing the 5-day future return, is calculated using the following formula: `(Close(t+5) - Open(t+1)) / Open(t+1)`. This captures the percentage change in price from the open of the day following the 5-day window to the close of the fifth day.
5. **Candlestick Chart Generation:** Candlestick charts are generated for each 5-day window, incorporating volume and a moving average indicator (the specific moving average period will be defined in a later section). These charts serve as a visual representation of price action within each window.
6. **Data Storage:** Processed data, including the calculated return labels and corresponding candlestick chart images, are stored in CSV files within the designated `stock_data/train/` and `stock_data/test/` directories. The filename convention will incorporate the holding period (5 days in this case) for clear identification.

While the core focus of this section is data preparation, it's important to acknowledge the dynamic nature of financial markets. Subsequent sections will detail how the model accounts for potential market shifts and incorporates mechanisms for continuous performance evaluation and adaptation.

### B. Data Acquisition and Preprocessing

This section details the acquisition and preprocessing steps required to prepare financial data for model input, specifically for Principal Component Analysis (PCA). The process focuses on transforming raw price, time, and volume data into a robust and normalized dataset.

1. **Data Acquisition:** Historical financial data, including price, timestamp, and volume, is acquired for the chosen asset. A rolling window approach is used, where calculations are performed within a sliding window of _N_ data points.

2. **Data Transformation:** Two key transformations are applied to the raw data within each rolling window:

   - **Time Transformation:** Timestamps are converted to fractional elapsed time. This is calculated by subtracting the minimum timestamp from each timestamp within the window and then dividing by the total time span of the window (the difference between the maximum and minimum timestamps). This representation captures the relative timing of events within the window and handles irregular time intervals.

   - **Price Transformation:** Raw prices are transformed into window-relative returns. Either percentage change or log returns are calculated relative to the first price in the window. This anchors the price series to zero at the beginning of each window, mitigating the influence of absolute price magnitudes on the PCA and allowing for better comparison across different assets and time periods.

3. **Volume Preprocessing:** Volume data is preprocessed using a two-step approach to handle potential outliers and large fluctuations:

   - **Logarithmic Transformation:** A logarithmic transformation, `v' = log(1 + v)`, is applied to compress extreme values and reduce the impact of outliers.

   - **Robust Scaling:** The log-transformed volume data is then scaled using the median and interquartile range (IQR). This robust scaling method is less sensitive to outliers compared to traditional z-score normalization. Specifically, the median is subtracted from each log-transformed volume data point, and the result is divided by the IQR.

4. **Data Normalization for PCA:** After the transformations and scaling, the time and price data are normalized using z-score standardization. This ensures that both features contribute equally to the PCA. The mean (μ) and standard deviation (σ) are calculated for each feature within the rolling window, and the data is scaled as follows: `X_scaled = (X - μ) / σ`.

5. **PCA Implementation:** A 3-dimensional matrix is constructed with the fractional elapsed time, relative return (or log return), and scaled volume data. Principal Component Analysis is then performed on this matrix using Singular Value Decomposition (SVD):

   ```python
   import numpy as np
   u, s, vh = np.linalg.svd(X_scaled, full_matrices=False)
   axes = vh[:2]   # Extract the first two principal components
   ```

   This code snippet extracts the first two principal components for further analysis.

## Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data, transforming it into a suitable format for the CNN model. This process involves creating dynamic plane snapshots from candlestick and volume data, capturing key market patterns.

1. **Data Acquisition and Initial Visualization:** Raw OHLCV (Open, High, Low, Close, Volume) data is acquired. Candlestick charts are generated from this raw data to provide a baseline visualization of price and volume action.

2. **Data Transformation:** The following transformations are applied to the raw data:

   - **Log Returns:** Price data is converted to log returns to stabilize variance and emphasize relative price changes.

   - **Log Volume:** Volume data is log-transformed using `np.log1p(volume)` to mitigate the impact of extreme values. Outliers in the log-transformed volume are clipped at the 5th and 95th percentiles. Similarly, extreme outliers in the log return of price are clipped at the 5th and 95th percentiles.

   - **Normalization:** `time_frac` (fractional elapsed time within the trading day), log return of price, and processed volume data are min-max scaled to the [-1, 1] range. The minimum and maximum values for scaling are calculated across the entire 5-day window to ensure consistency.

   - **Principal Component Analysis (PCA):** Following normalization, the transformed `time_frac`, log return of price, and volume data are combined into a matrix. PCA, using Singular Value Decomposition (SVD), is applied to this matrix. Centering the matrix before PCA is optional. The resulting two principal components form the 2D dynamic plane snapshot.

3. **Dynamic Plane Snapshot Visualization:** For illustrative purposes, five distinct examples of price and volume action are generated, each representing a different market pattern (e.g., uptrend with rising volume, downtrend with volume spikes, reversal, sideways chop). For each example, a pair of images is created:

   - **Pre-Transformation Candlestick Chart:** The original candlestick chart with volume, providing a visual reference.
   - **Post-Transformation Dynamic Plane Snapshot:** The transformed data visualized as a 2D representation.

Each example uses a single-day chart with 10-minute intervals. The resulting 10 images (5 pre-transformation and 5 post-transformation) are presented sequentially for clear visual comparison.

## Data Acquisition and Preprocessing

This stage details the acquisition and preparation of financial data for model training and validation. The data pipeline consists of the following steps:

1. **Data Source:** Obtain historical Open, High, Low, Close, and Volume (OHLCV) data from Yahoo Finance.

2. **Data Acquisition and Adjustment:** Download OHLCV data for the target stocks and corresponding index data. Adjust historical prices for splits and dividends to ensure data consistency.

3. **Data Storage:** Store the acquired and preprocessed data as CSV files within dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** Implement a 5-day rolling window for time series analysis. Each input will consist of five consecutive days of OHLCV data, used to predict the target variable.

5. **Return Label Calculation:** Calculate the return label, representing the percentage change between the closing price at time _t+h_ (where _h_ is the holding period) and the opening price at time _t+1_, normalized by the opening price at _t+1_: `(Close(t+h) - Open(t+1)) / Open(t+1)`.

6. **Candlestick Chart Generation:** Generate candlestick charts from the 5-day window data, incorporating volume and a moving average as visual features.

7. **Return Label Association:** Calculate the 5-day future returns and associate these labels with their corresponding candlestick chart images.

8. **Integrated Label Generation:** Integrate the return label calculation with the candlestick chart generation process to ensure each image is linked to its respective return.

9. **Filename Convention:** Implement a clear file naming convention that encodes the holding period, either within the filenames themselves or using a separate CSV file for efficient data organization.

This section outlines the complete data acquisition and preprocessing pipeline for the project, covering data sources, adjustments, storage, formatting, windowing, and label generation. The following section will detail image generation and transformation using the prepared data.

## Image Display and Transformation

Generated images will be displayed individually in the output channel to facilitate clear visualization and analysis of each transformation. The following explorations of candlestick chart patterns and their representation in a dynamic plane using Principal Component Analysis (PCA) will be performed:

- **Candlestick Pattern Generation:** Candlestick charts augmented with volume data will be generated, representing five distinct market patterns: an uptrend with rising volume, a downtrend with volume spikes, a reversal (down then up), sideways chop, and a breakout followed by stabilization.

- **Volume Representation:** Investigate whether explicit volume bars are necessary in the transformed dynamic plane snapshot, or if the transformed data points sufficiently capture volume information.

- **Breakout/Spike/Stabilize PCA Examples:** Generate five distinct examples of the "breakout/spike/stabilize" pattern, visualized through PCA. Varying data volumes across these examples will allow for assessing the visual similarities and differences in the resulting PCA patterns.

- **Breakout/Spike/Stabilize Scenario Visualization:** Generate five variations of the "breakout/spike/stabilize" scenario, each with a different volume profile, visualized using two chart types:

  1. Original candlestick chart with volume, using 10-minute intervals.
  2. Transformed dynamic plane projection, utilizing time, log-return, and log-volume, scaled to the [-1, 1] range and rotated using PCA to the PC1 and PC2 components.

- **PCA Pattern Analysis:** Compare the five PCA patterns generated from the "breakout/spike/stabilize" scenarios, focusing on how variations in volume magnitude and timing influence the trajectory's shape in PCA space. Key observations include:
  1. Clustering of data points before the spike.
  2. The volume-driven jump to one extreme of PC2 during the spike.
  3. Formation of another cluster or trajectory after the spike during stabilization.

## B. Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data used to train and evaluate the trading models. The process involves acquiring raw OHLCV (Open, High, Low, Close, Volume) data from Yahoo Finance, followed by several crucial preprocessing steps to prepare the data for the Vision Transformer model.

The initial implementation focuses on a 5-day window of data, transformed into candlestick chart images. The pipeline for this involves:

1. **Acquisition:** Acquiring OHLC and index data from Yahoo Finance.
2. **Adjustment:** Adjusting prices for splits and dividends.
3. **Storage:** Storing data in CSV files within designated train/test directories (`stock_data/train/`, `stock_data/test/`).
4. **Windowing:** Implementing a 5-day rolling window for input features.
5. **Label Calculation:** Calculating return labels using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 'h' represents the holding period.
6. **Image Generation:** Generating candlestick charts with volume and moving average. These charts integrate the calculated return label. Filenames incorporate the holding period (h) for tracking and analysis.

The following preprocessing steps detail the creation of the dynamic input images from this 5-day window:

1. **Normalization:** Normalizing Time, Price (using log-returns), and Volume (using log-transformation and robust scaling) to a uniform range of [-1, +1]. This ensures comparability between features and prevents any single dimension from disproportionately influencing the analysis while preserving chronological order.
2. **Dynamic Frame Construction:** Applying Principal Component Analysis (PCA) to the normalized data within the 5-day window. This identifies the two primary axes of correlated movement (PC1 and PC2), defining the dynamic 2D plane.
3. **Rotation and Refocusing:** Projecting the 5-day market action history onto the 2D plane (PC1 and PC2) and re-centering the view so the last data point becomes the origin (0,0). This focuses the model on relative movement from the current market state.
4. **Image Rendering:** Rendering the transformed 2D data as a candlestick chart image, which serves as input to the Vision Transformer.

Future development aims to incorporate a multi-scale temporal model, including 10-minute, daily, weekly, monthly, quarterly, and yearly data, alongside considerations of after-market activity. This will require a more complex data pipeline and introduce significant processing overhead. The architecture for integrating these different timescales, potentially through a weighted sum approach, needs further definition. Specifically:

- **Multi-Scale Data Integration:** How will data from different timeframes (10-minute, daily, weekly, monthly, quarterly, yearly) be integrated with the existing 5-day window? What weighting scheme will be used, and how will this be optimized?
- **After-Market Data:** How will after-market data be acquired and integrated into the model?
- **Data Pipeline Complexity and Overhead:** The increased complexity of the data pipeline requires careful planning and efficient implementation strategies to manage the expected processing overhead. Further investigation and performance testing will be necessary.

Addressing these considerations will be crucial for the successful implementation of the multi-scale temporal model.

## Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data for the SCoVA project. The process leverages Yahoo Finance as the primary data source for Open, High, Low, Close, and Volume (OHLCV) data, along with relevant index data for the chosen asset universe (e.g., NIFTY 50, NIFTY 500). Acquired price data is adjusted for corporate actions such as splits and dividends to ensure accuracy.

The data preprocessing pipeline includes the following steps:

1. **Normalization:** Financial data (Time, Price, and Volume) is normalized to a uniform range of [-1, +1]. Time is represented as fractional elapsed time within the trading day. Price data is transformed using log-returns, while volume data undergoes a log transformation followed by robust scaling. This normalization ensures that all features contribute equally to model training, preventing features with larger magnitudes from dominating the learning process.

2. **Windowing:** A 5-day rolling window mechanism is implemented to prepare data for both model input and output generation. The model receives a sequence of five daily candlesticks as input, while the corresponding output, the 5-day future return (the target variable), is calculated based on the price movement over the subsequent five days. This 5-day window also serves as the basis for the dynamic plane calculations and visualizations.

3. **Return Label Calculation:** The 5-day future return is calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 't' represents the last day of the input window and 'h' represents the holding period of 5 days.

4. **Data Storage:** Preprocessed data is stored as CSV files in designated directories (`stock_data/train/` and `stock_data/test/`).

5. **Visualization:** Standard candlestick charts, incorporating volume and moving average indicators, are generated to provide a visual representation of the 5-day window and are linked to the calculated return labels. While the system can be adapted to display other candlestick types (e.g., Heiken-Ashi), this implementation utilizes standard candlesticks. Visualizations are dynamically updated with each new candlestick. The filename convention for these charts and associated data either encodes the holding period or utilizes separate CSV files for organization.

While the system supports user-configurable options for data sources, timeframes, and specific features incorporated into the dynamic plane visualizations, this section describes the core data acquisition and preprocessing steps implemented for the SCoVA project.

## B. Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data required for training and evaluating the model. The process involves collecting raw data from Yahoo Finance, incorporating relevant market indices, adjusting for corporate actions, and transforming the data into a structured format suitable for generating candlestick chart images and corresponding return labels.

1. **Data Source:** Historical Open, High, Low, Close, and Volume (OHLCV) data, along with corresponding index data, will be acquired from Yahoo Finance. This widely-used platform offers readily available historical stock price data and market indices, which are crucial for capturing market trends and individual stock performance.

2. **Data Adjustments:** Acquired price data will be adjusted for splits and dividends to maintain data accuracy and consistency. This ensures that historical price movements accurately reflect true market performance.

3. **Data Storage:** The preprocessed data will be stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. This organized structure facilitates efficient data management and access during model development.

4. **5-Day Windowing:** A sliding 5-day window will be applied to the data. Each input to the model will consist of OHLCV and index data for a 5-day period, creating a sequential dataset that captures short-term market trends.

5. **Return Label Calculation:** The target variable, representing the 5-day future return, will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 't' represents the last day of the 5-day window and 'h' is the holding period (in this case, 5 days). This formula calculates the percentage return from the open of the day following the window (t+1) to the close of the day five days later (t+5).

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day windowed data. These charts will visualize the OHLCV data, incorporating volume and a moving average indicator to provide additional context for the model.

7. **Integration of Return Labels and Charts:** The calculation of return labels will be tightly integrated with the candlestick chart generation process. This ensures that each generated chart is paired with its corresponding return label, creating a synchronized dataset for model training.

8. **Filename Convention:** The filenames of the generated data will incorporate the holding period (h=5) to facilitate easy identification and retrieval during later stages of the project. This can be achieved by directly encoding the holding period in the filenames or using a separate CSV file for managing this information.

**Future Development:** While Yahoo Finance currently serves as the primary data source, future development should consider expanding to include intraday data for all stocks in both the Indian and US markets, across all available intraday intervals. This expanded scope will significantly increase data volume, requiring efficient data management and storage solutions, but offers the potential to enhance model generalization and capture finer-grained market dynamics.

### B. Data Acquisition and Preprocessing (Continuity)

This section details the acquisition and preparation of market data for the project. The transition from Yahoo Finance to the Zerodha KiteConnect API significantly streamlines this process and offers enhanced integration with trading functionalities.

- **Data Source: Zerodha KiteConnect API:** All historical and live market data, including OHLCV (Open, High, Low, Close, Volume) and index data, will be acquired using the Zerodha KiteConnect API. This replaces the initially planned use of Yahoo Finance and provides access to a broader range of Indian market data.

- **Live Data Streaming: Zerodha KiteConnect Websocket:** Real-time data for continuous model training and adaptation will be streamed via a websocket connection established through the Zerodha KiteConnect API.

- **Order and Portfolio Management Integration:** The Zerodha KiteConnect API will also manage order generation, execution, and portfolio tracking, unifying data acquisition with trading actions.

- **Data Preprocessing:** Acquired data will undergo preprocessing, including price adjustments (if necessary) and formatting for model input. Specific preprocessing steps and their rationale will be documented.

- **5-Day Windowing:** A 5-day rolling window will be implemented for both input features (historical data) and output labels (future returns). The implementation details of this windowing logic will be documented.

- **Return Label Calculation:** The return label will be calculated as `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `h` represents the holding period. This calculation will be integrated with the data preprocessing pipeline.

- **Candlestick Chart Generation:** Candlestick charts, incorporating volume and moving averages, will be generated for visualization and potentially as model input. The parameters used for chart generation will be documented.

- **Data Storage:** The preprocessed data, including candlestick chart images, will be stored in a structured format to facilitate efficient access and retrieval. Details regarding the storage location and format will be provided.

- **Data Management and Documentation:** Adhering to robust data management principles, all data acquisition, preprocessing, and storage procedures will be thoroughly documented, including validation steps, data formats, and any transformations applied.

This shift to Zerodha KiteConnect necessitates a review and potential restructuring of the project architecture. These architectural considerations, including the influence of Dharmic principles (Gyaan, Bhakt, Karam, and Raaj Yoga), will be explored in the architectural overview section. The chosen data acquisition and preprocessing strategy ensures a continuous flow of high-quality data, forming a solid foundation for the Gyaan Shala (The House of Wisdom) module and subsequent model development.
Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data, a crucial foundation for structured experimentation and backtesting. These steps ensure data consistency and reproducibility.

1. **Data Source:** Open, High, Low, Close, and Volume (OHLCV) data will be acquired from Yahoo Finance, a widely-used and readily-accessible source. The acquisition method, including any libraries or APIs (e.g., the `yfinance` library in Python), along with code snippets and the specified time range, will be meticulously documented.

2. **Index Data and Price Adjustment:** Relevant index data will be acquired for benchmarking and potential use as model features. Critically, prices will be adjusted for splits and dividends to maintain accuracy. The precise adjustment procedure, including formulas, will be explicitly documented.

3. **Data Storage:** Acquired and preprocessed data will be stored in CSV files within dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. Comprehensive documentation of file naming conventions will prevent confusion.

4. **5-Day Windowing:** Data will be organized into 5-day windows, serving as model input. A detailed explanation of the windowing logic, including handling of overlapping or non-overlapping windows, will be provided.

5. **Return Label Calculation:** The target variable, representing future returns, will be calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the 5-day window, `h` is the holding period (explicitly clarified in the documentation, likely 5 days in this case), `Close(t+h)` is the closing price on the `t+h` day, and `Open(t+1)` is the opening price on the day immediately following the 5-day window. A step-by-step breakdown of the formula and its implementation will be included.

6. **Candlestick Chart Generation:** Candlestick charts visualizing the 5-day window data will be generated, incorporating volume and a specified moving average indicator (type and period documented). Libraries, methods, parameters, and settings for chart generation will be extensively documented.

7. **Return Label Integration:** Return label calculation (with `h=5`) will be seamlessly integrated with candlestick chart generation, ensuring data integrity and preventing mismatches between images and labels. The precise alignment method will be documented.

8. **Filename Convention:** A clear filename convention, encoding the holding period (e.g., 5 days) within the filename itself or using separate CSV files for different holding periods, will be implemented and thoroughly documented. This is crucial for organizing data for the "Campaign Runner" and subsequent analysis in the "Gyaan Mandir" module.

9. **Kurukshetra Integration:** This structured data pipeline will ensure consistent delivery of appropriately formatted data to drive experiment templates and facilitate automated backtesting campaigns within the “Kurukshetra” module.

## B. Data Acquisition and Preprocessing

This section details the acquisition, preprocessing, and preparation of the financial data used to train the CNN and ViT models. Given the potential computational intensity of generating and processing large volumes of candlestick chart images, cost-optimization strategies are considered from the outset.

### Data Preparation Steps

1. **Data Source:** Open, High, Low, Close, and Volume (OHLCV) data, along with relevant benchmark index data (e.g., S&P 500), will be acquired from Yahoo Finance.

2. **Data Preprocessing:** Acquired data will be preprocessed to ensure accuracy. This includes handling stock splits and dividends, potentially using adjusted close prices provided by Yahoo Finance or implementing a separate adjustment mechanism.

3. **Data Storage:** Preprocessed data will be stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** Data will be organized into 5-day rolling windows. Each window of OHLCV data will serve as an input sample for the model.

5. **Return Label Calculation:** The target variable, representing the 5-day future return, will be calculated using the following formula: `(Close(t+5) - Open(t+1)) / Open(t+1)`, where `t` represents the last day of the 5-day input window. This calculates the return from the open of the day following the window to the close five days later.

6. **Candlestick Chart Generation:** Candlestick charts will be generated from each 5-day window of OHLCV data. These charts will include volume and a moving average indicator to provide a richer visual representation of price action.

7. **Data Integration:** Calculated return labels will be associated with their corresponding candlestick chart images to ensure the model receives both the input (image) and the target output (return) during training.

8. **File Naming Convention:** A consistent file naming convention will be implemented, potentially incorporating the holding period (5 days) into the filenames or using separate CSV files for different holding periods.

### Cost Optimization Strategies

To mitigate potential cloud computing costs associated with image generation and processing:

- **Offline Image Processing:** The feasibility of generating candlestick chart images offline will be explored. This could involve using a separate, more cost-effective system or pre-generating images in batches for later use during training.

- **Optimized Image Generation:** The image generation process will be optimized to reduce computational overhead. This might include caching pre-processed images or using a more lightweight image generation library.

- **User-Controlled Training:** The application UI will provide users with control over model training and retraining, regardless of where image generation occurs. This might involve mechanisms for triggering offline jobs or managing data transfer between systems.

### Alternative Approaches (Exploration)

- **iPad Processing:** The possibility of offloading image generation and/or model training to the user's iPad will be investigated to potentially reduce server load and costs. This exploration will consider memory constraints and data access challenges on the client device, potentially leveraging federated learning or a hybrid approach combining server-side and client-side processing. The feasibility of this approach depends on balancing the iPad's computational capabilities with the need to process potentially large datasets.

### B. Data Acquisition and Preprocessing (Continuity)

This section outlines the continuous process of acquiring and preparing market data for model training and evaluation. The process is split between server-side orchestration and client-side processing.

- **Server-Side Data Acquisition and Preprocessing:** The server acts as the central hub, storing and serving OHLCV (Open, High, Low, Close, Volume) and index data from Yahoo Finance. It preprocesses this data by adjusting prices (e.g., for splits and dividends). This processed data is then made available to the client (iPad) via API endpoints.

- **Client-Side Data Handling and Transformation:** The iPad fetches the preprocessed OHLCV and index data from the server. Using JavaScript libraries and the Canvas API, the client generates candlestick chart images from 5-day windows of data, incorporating volume and moving average indicators. Concurrently, it calculates the 5-day future return, serving as the target label for each image. TensorFlow.js facilitates local model training on the iPad using these generated image-label pairs. Model updates are then sent back to the server for aggregation and update of the global master model. Web Workers handle these background processes to maintain UI responsiveness.

- **UI/UX Enhancements for Client-Side Processing:** To enhance user experience and provide visibility into the client-side processes, the Campaign Runner and Experiment Designer UI will be updated. New status indicators will track data downloading, image generation, local model training, and model update uploads. The UI will also display local resource monitoring (e.g., CPU and memory usage) during these operations.

### Architectural Considerations for Client-Side Training

Given the computational demands of Vision Transformer training, the feasibility of performing this task within the iPad's PWA environment requires careful investigation. Key considerations include:

- **PWA Suitability for GPU-Intensive Tasks:** Can PWAs effectively leverage GPU processing for computationally intensive tasks? This investigation must address potential drawbacks such as browser crashes and performance limitations, particularly during extended training periods.

- **PWA Stability under GPU Load:** What is the probability of browser crashes when running GPU-heavy operations within a PWA on an iPad? This assessment should consider iPad capabilities, browser limitations, and resource management strategies.

- **Resource Constraints within PWAs:** A comprehensive examination of potential limitations, stability, and performance concerns associated with resource-intensive Vision Transformer training within a PWA is necessary. The focus should be on the feasibility of long-term, uninterrupted operation.

- **Technical Challenges of GPU-Intensive PWA Training:** Addressing the core technical challenge of running this training within a PWA requires in-depth research on TensorFlow.js performance and WebGPU capabilities for training large models in a browser.

- **Alternative Training Solutions:** Should PWAs prove unsuitable, alternative solutions must be explored. Options include native applications or cloud-based training to provide the necessary performance and stability.

The final implementation details of data acquisition and preprocessing, including the filename conventions for storing training data (incorporating the 5-day holding period), will be determined based on the findings of this architectural feasibility study.

### B. Data Acquisition and Preprocessing

This section details the process of acquiring, preprocessing, and preparing the financial data for model training. This structured approach ensures data quality and consistency, laying a solid foundation for reliable model development.

1. **Data Source:** Historical OHLCV (Open, High, Low, Close, Volume) and relevant index data will be acquired from Yahoo Finance, a widely used and readily available source for financial market data.

2. **Data Adjustments:** Price adjustments will be applied to account for corporate actions such as stock splits and dividends, ensuring data accuracy.

3. **Data Storage:** Acquired and preprocessed data will be stored in CSV files within organized directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **Windowing:** A 5-day sliding window approach will be employed. The model input will consist of five consecutive days of OHLCV data.

5. **Return Label Calculation:** The return will be calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window, and `h` is the holding period (in days). This represents the return from the open of the day after the window to the close of the holding period.

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day window data, visualizing price action and incorporating volume and moving average indicators for additional context.

7. **Return Label Integration:** The return label calculation will be integrated with the candlestick chart image generation process to ensure data synchronization and efficient processing.

8. **Filename Convention:** The filename convention will encode the holding period, either within the filenames themselves or through separate CSV files for different holding periods, facilitating easy identification and retrieval.

This comprehensive data acquisition and preprocessing pipeline will contribute to the development of robust and reliable models.

### B. Data Acquisition and Preprocessing (Continuity)

This section details the acquisition, preprocessing, and preparation of financial data for model training. The following steps outline the process:

1. **Data Source:** Historical OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance.

2. **Additional Data and Adjustments:** Along with OHLCV data, relevant index data will be acquired. Price adjustments for splits and dividends will be applied to ensure accuracy.

3. **Data Storage:** The acquired and preprocessed data will be stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **Windowing:** The data will be organized into 5-day rolling windows. Each window will serve as an input sample for the model, representing five consecutive trading days of OHLCV and index data.

5. **Return Label Calculation:** The target variable (return label) will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window, `h` is the holding period (5 days), and `Close(t+h)` is the closing price on day `t+h`. This represents the percentage return from the open of the day following the 5-day window (t+1) to the close of the holding period (t+h).

6. **Candlestick Chart Generation:** Candlestick charts will be generated from each 5-day window, including volume and a moving average indicator.

7. **Return Label Integration:** The return label calculation will be integrated with the candlestick chart generation process. Each chart will be associated with its corresponding 5-day future return.

8. **Filename Convention:** The filename convention will encode the holding period (5 days) to ensure clear identification of each data sample. This could be achieved through the filename itself or by using separate CSV files.

## B. Data Acquisition and Preprocessing

This section outlines the process of acquiring, preprocessing, and preparing financial data for model training, ensuring data consistency and the creation of appropriate input features.

1. **Data Source (Yahoo Finance):** Stock data, including Open, High, Low, Close, and Volume (OHLCV), will be acquired from Yahoo Finance using a library like `yfinance`. The specific instruments and data timeframe will be defined in the project configuration.

2. **Index Data and Price Adjustments:** Along with OHLCV data for individual stocks, relevant index data will also be acquired. Price adjustments for stock splits and dividends will be applied to ensure data accuracy.

3. **Data Storage:** Acquired and preprocessed data will be stored as CSV files in dedicated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Rolling Window:** A 5-day rolling window will be used to create input features. Each data point will consist of the OHLCV data for the preceding five trading days.

5. **Return Calculation:** The target variable, representing the return, will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the current time, and `h` is the holding period (e.g., h=5 for a 5-day holding period).

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day window data, incorporating volume and a moving average (the specific moving average period will be defined). These charts will serve as visual inputs for the CNN model.

7. **Return Label Generation:** Corresponding to each candlestick chart, a return label representing the 5-day future return (as defined in step 5) will be calculated.

8. **Integrated Data and Label Generation:** The processes of generating candlestick chart images and calculating corresponding return labels will be integrated for efficient data preparation.

9. **Filename Convention:** The filename convention will be updated to clearly indicate the holding period. This can be achieved by embedding the holding period in the filename itself or by storing data for different holding periods in separate CSV files.

## B. Data Acquisition and Preprocessing (Continuity)

This section details the acquisition and preprocessing of financial data for the SCoVA project. A phased approach is employed to balance the need for thorough testing and development against budgetary constraints (₹25,000). This strategy minimizes server costs during initial development while ensuring robust model training with the full dataset in later stages.

**Phase 1: Minimized Data and On-Device Training (Cost-Effective Development)**

- **Data Source:** Yahoo Finance, providing Open-High-Low-Close-Volume (OHLCV) and relevant index data. Price adjustments for splits and dividends will be handled to ensure data accuracy.
- **Minimized Dataset:** A reduced subset of the full dataset will be used for initial testing and debugging.
- **On-Device Training:** Initial model training will be limited to one epoch and executed on the iOS development device to minimize server reliance.
- **Data Storage:** Data will be stored in CSV files within designated `stock_data/train/` and `stock_data/test/` directories.

**Phase 2: Full Data and Server-Side Training (Final Model Development)**

- **Full Dataset Integration:** The complete dataset will be used for final model training to achieve optimal performance.
- **Server-Side Training:** Final model training will leverage server-side resources for computational efficiency.

**Data Preprocessing Steps (Consistent across both phases):**

1. **Data Acquisition:** Acquire OHLCV and index data from Yahoo Finance.
2. **Price Adjustment:** Adjust historical prices for splits and dividends.
3. **5-Day Windowing:** Implement a sliding 5-day window for model input.
4. **Return Label Calculation:** Calculate the holding period return using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the input window and `h` is the holding period (explicitly defined as 5 days).
5. **Candlestick Chart Generation:** Create candlestick charts for each 5-day window, including volume and a moving average indicator (period to be determined).
6. **Return Label Integration:** Associate calculated return labels with their corresponding candlestick chart images.
7. **Filename Convention:** Encode the holding period within filenames or a separate CSV file for clear identification.

This phased approach allows for complete application run-throughs, including data acquisition, preprocessing, model training, and prediction, while minimizing server costs during the initial development stages. This aligns with the project's budgetary constraints and ensures robust model development in the later stages.

### B. Data Acquisition and Preprocessing (Continuity)

This section details the acquisition and preprocessing of financial data, establishing a robust and reliable data pipeline.

1. **Data Source:** OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance. This commonly used source provides reliable and consistent data, crucial for project continuity. Specific tickers and date ranges will be defined in the project configuration.

2. **Index Data and Price Adjustments:** Alongside OHLCV data, relevant index data (e.g., S&P 500) will be acquired to provide market context. Historical prices will be adjusted for splits and dividends to ensure data accuracy.

3. **Data Storage:** Acquired and preprocessed data will be stored as CSV files in a structured directory: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** Data will be organized into 5-day windows. Input features will consist of data from the past 5 days, and the output will be a prediction for the subsequent period (defined in the model configuration).

5. **Return Label Calculation:** The target variable (return label) will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the current time and `h` is the holding period (in this case, h=5). This represents the return from the open of the next day (t+1) to the close of the final day in the holding period (t+h).

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day windowed data, incorporating volume information and a moving average indicator. These charts serve as visual inputs for the CNN models.

7. **Return Label Integration:** The return label calculation will be tightly integrated with the candlestick chart generation process to ensure data and label synchronization.

8. **Filename Convention:** The filename convention for generated images and/or accompanying CSV files will include the holding period for easy identification and management.

9. **Future Data Incorporation:** While this stage focuses on OHLCV data from Yahoo Finance, future development will incorporate additional visual inputs, including futures charts and options chain data, as outlined in the broader project goals. This pipeline is designed to be extensible to accommodate these future data sources.

## Data Acquisition and Preprocessing (Continuity)

This section details the continuous and consistent acquisition and preparation of data, crucial for the project's success. Continuity ensures a robust, repeatable, and long-term operational pipeline, transforming raw data into a suitable format for model input. The following steps, while involving various roles (Specialization, Facilitation, Enforcement), prioritize establishing a reliable and continuous data stream.

1. **Data Source: Yahoo Finance:** OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance using an automated and robust process to ensure consistent data flow. This involves utilizing the Yahoo Finance API or web scraping techniques, prioritizing reliability and repeatability.

2. **Data Preprocessing and Adjustment:** Raw data will be preprocessed to ensure consistency and model compatibility. This includes acquiring associated index data and adjusting historical prices for corporate actions like splits and dividends. Selecting and consistently applying appropriate adjustment methodologies is crucial for data integrity.

3. **Data Storage:** Processed data will be stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data. A clear and consistent file naming convention, incorporating the holding period, will be implemented to facilitate automated processing and analysis. This structured approach supports efficient data access and management.

4. **5-Day Windowing:** A 5-day sliding window will be applied to the OHLCV and index data. Each input to the model will consist of five consecutive days of data, and the corresponding output will be the predicted return for the subsequent period (e.g., the next day's return).

5. **Return Label Calculation:** The target variable, return, will be calculated using the formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 't' is the last day of the input window and 'h' is the holding period. This formula provides a standardized measure of investment performance over the holding period.

6. **Candlestick Chart Generation:** Candlestick charts, incorporating volume and moving averages, will be generated for each 5-day window. These visualizations will provide a graphical representation of price action and technical indicators, aiding in subsequent analysis and model development.

7. **Integration of Return Labels with Candlestick Charts:** The calculated return labels (step 5) will be directly associated with their corresponding candlestick chart images (step 6), ensuring data integrity and facilitating analysis. This integration streamlines the data pipeline and ensures data consistency.

### B. Data Acquisition and Preprocessing (Continuity)

This section outlines the process of acquiring and preparing data for the model, ensuring a continuous and reliable data flow. This involves generating candlestick chart images, calculating return labels, and implementing robust validation services.

**Data Preparation and Transformation:**

Raw OHLCV (Open, High, Low, Close, Volume) data undergoes several transformations before being used as model input. A 5-day windowing approach provides the necessary temporal context. Return labels, the target variable for training, are calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 'h' represents the holding period. This holding period is encoded within the filenames or a separate CSV file to maintain data consistency.

**Candlestick Chart Generation:**

Candlestick charts, incorporating volume and moving average indicators, provide a crucial visual representation of the data. These charts are generated automatically and integrated seamlessly with the data processing pipeline, ensuring a standardized input format for the model.

**Data Validation and Monitoring:**

Two key services ensure data integrity and operational efficiency throughout the project lifecycle:

- **Pre-flight Validation Service:** This service verifies all necessary conditions before model training or backtesting. Checks include:

  - **Data Integrity:** Validation of OHLCV and index data for completeness, correctness, and consistency, including checks for missing values, outliers, and correct data types.
  - **Environment Health:** Verification of required software libraries, dependencies, and hardware resources.
  - **Template Schema Validity:** (If applicable) Validation of data schemas or templates against defined standards.

- **Post-flight Analytics Service:** This service analyzes the results after training or backtesting. It performs the following actions:
  - **Campaign Summaries:** Generation of concise summaries of campaign performance, including key metrics and statistics.
  - **Model Registry Updates:** Updating the model registry with newly trained models and their performance metrics.
  - **Pattern Identification:** Analysis of results to identify recurring patterns or trends, informing future strategies and refining data preprocessing procedures.

These services, coupled with the standardized data transformations and visualizations, establish a robust and continuous data pipeline, crucial for the project's success. They proactively address potential issues, streamline the workflow, and ensure data integrity throughout the project lifecycle.

### B. Data Acquisition and Preprocessing

This section details the acquisition and preprocessing of financial data for model training and backtesting within the SCoVA project. The process, encapsulated within a dedicated `ContinuityService` subclass to ensure maintainability and architectural consistency, transforms raw data from Yahoo Finance into structured input features. This service should interact with other services, potentially including `EnforcementService` subclasses for data validation, and manage the overall data flow. Detailed technical specifications and pseudocode, mirroring the requirements for the base classes, should be drafted.

The following steps, implemented as methods within the `ContinuityService` subclass, outline the data pipeline:

1. **Data Source:** Raw OHLCV (Open, High, Low, Close, Volume) data is acquired from Yahoo Finance.

2. **Additional Data and Adjustments:** Relevant index data is acquired alongside OHLCV data. Price adjustments (e.g., for dividends and splits) are applied to ensure accuracy.

3. **Data Storage:** Acquired and preprocessed data is stored as CSV files in `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **Windowing:** A 5-day sliding window is applied to the data, creating input samples consisting of five consecutive days of OHLCV and index data.

5. **Return Label Calculation:** The target variable (return label) is calculated as: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where `h` is the holding period (likely 5 days). This represents the return from the open of the day after the window to the close of the final day of the holding period.

6. **Candlestick Chart Generation:** Candlestick charts, incorporating volume and a moving average indicator, are generated from the 5-day window data for visual input to the CNN model.

7. **Return Label Integration:** Return label calculation is integrated with candlestick chart generation for synchronized data and image creation.

8. **Filename Convention:** The holding period (`h`) is encoded in the filenames of the candlestick chart images or a separate CSV file for easy identification and retrieval during training and evaluation.

9. **Mitigating Network Bottlenecks:** Solutions are implemented to minimize network bottlenecks during data retrieval and processing. This includes exploring service discovery, load balancing, and optimized API design.

10. **Asynchronous Communication:** Asynchronous communication (preferably Google Cloud Pub/Sub) is used for non-critical data requests to improve responsiveness and prevent blocking operations.

11. **Performance Testing:** Comprehensive end-to-end performance tests are implemented from the outset, and latency related to data retrieval and preprocessing is continuously monitored. This facilitates early identification of bottlenecks and ensures efficient handling of real-time data streams.
    Data Acquisition and Preprocessing

This section details the process of acquiring and preparing the financial data used for training and evaluating the models. The following steps will be handled client-side to leverage user device computational power, minimizing server load and latency. This client-side processing also allows for rapid iteration and experimentation with different data preprocessing techniques. The data generated on the client will be structured for compatibility with the backend model training pipeline. These steps transform raw financial data into a format suitable for the CNN and ViT models, including the computationally intensive tasks of generating candlestick chart images and calculating corresponding return labels.

1. **Data Source:** Historical Open-High-Low-Close-Volume (OHLCV) data, along with relevant index data, will be collected from Yahoo Finance.

2. **Price Adjustments:** Price data will be adjusted for splits and dividends to ensure accuracy.

3. **Data Storage:** Acquired and preprocessed data will be stored as CSV files in designated training (`stock_data/train/`) and testing (`stock_data/test/`) directories.

4. **5-Day Windowing:** A 5-day sliding window will be used to create input sequences and corresponding output labels. Each input instance will consist of OHLCV data for five consecutive trading days.

5. **Return Label Calculation:** The target variable (return label) will be the percentage change between the closing price at time _t+h_ (where _h_ is the holding period, 5 days) and the opening price at time _t+1_, normalized by the opening price at _t+1_: `(Close(t+h) - Open(t+1)) / Open(t+1)`.

6. **Candlestick Chart Generation:** Candlestick charts will be generated for each 5-day window, incorporating volume information and a moving average indicator (the specific period will be defined later).

7. **Return Label Integration:** Return label calculation will be integrated with the candlestick chart generation process, ensuring each chart has its corresponding label.

8. **Filename Convention:** The filename convention will be updated to reflect the 5-day holding period, either by encoding it within the filename or using separate directories. This will facilitate data management for different trading strategies and experimental setups.

### B. Data Acquisition and Preprocessing

This section details the acquisition and preparation of market data for subsequent model training and backtesting. The primary data source is Yahoo Finance, providing OHLCV data. Additional data, such as relevant market indices, may also be incorporated.

The following steps outline the data acquisition and preprocessing pipeline:

1. **Data Source:** OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance. The specific tickers and time period for data acquisition will be defined in the project configuration.

2. **Index Data and Price Adjustments:** Along with OHLCV data, relevant index data may be acquired. Price adjustments for splits and dividends are crucial for data integrity and will be performed during preprocessing. The specific indices and the methodology for price adjustments will be documented.

3. **Data Storage:** Acquired and preprocessed data will be stored as CSV files in designated training (`stock_data/train/`) and testing (`stock_data/test/`) directories. Specific file naming conventions will be defined to ensure data organization.

4. **5-Day Windowing:** Data will be structured into 5-day windows. This windowing will be applied to both input features and output labels using overlapping windows. The first day of the window will represent _t_ and the last day will represent _t+4_.

5. **Return Label Calculation:** The target variable, 5-day future return, will be calculated using the following formula, where _t+5_ is the day following the 5-day window:

   ```
   Return = (Close(t+5) - Open(t+1)) / Open(t+1)
   ```

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day windowed data. These charts will include volume and a 20-day simple moving average. The charting library used will be [specify library name], and the images will be saved in [specify image format].

7. **Data Association:** Calculated return labels will be associated with their corresponding candlestick chart images through a consistent file naming convention that includes a shared identifier.

8. **Encoding Holding Period:** The 5-day holding period will be implicitly encoded by the 5-day windowing and candlestick chart generation process. This eliminates the need for explicit encoding in filenames or a separate CSV file.

## B. Data Acquisition and Preprocessing

This section details the process of continuously acquiring and preprocessing financial data for training and evaluating the SCoVA (Snapshot Computer Vision Algorithm).

1. **Data Source:** OHLCV (Open, High, Low, Close, Volume) data will be acquired from Yahoo Finance using the `yfinance` library. Specific tickers and date ranges will be defined for the dataset.

2. **Index Data and Price Adjustments:** Along with OHLCV data, relevant index data (e.g., S&P 500) will be acquired for benchmarking and potential feature engineering. Stock prices will be adjusted for splits and dividends to ensure data consistency.

3. **Data Storage:** The acquired and preprocessed data will be stored as CSV files in designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.

4. **5-Day Windowing:** A 5-day rolling window will be applied to structure the input data. Each input to the model will consist of five consecutive days of OHLCV data.

5. **Return Label Calculation:** The target variable, representing the future return over a holding period (h), will be calculated using the following formula: `(Close(t+h) - Open(t+1)) / Open(t+1)`, where 't' is the last day of the input window.

6. **Candlestick Chart Generation:** Candlestick charts will be generated from the 5-day windowed OHLCV data. These charts will include volume information and a moving average indicator (e.g., 20-day SMA).

7. **Return Label Integration:** The calculation of the return labels will be directly integrated with the candlestick chart generation process. This ensures each candlestick chart image is paired with its corresponding future return.

8. **Filename Convention:** The filenames of the candlestick chart images will incorporate the holding period (e.g., h=5). This can be achieved by embedding the holding period in the filename itself or by using a separate CSV file to map filenames to their respective holding periods.

9. **Future Enhancements:** While not included in the initial implementation, future enhancements should consider incorporating execution quality metrics. For example, a "Price Improvement Rate" based on a rolling average could be added as a context token or feature. Additionally, metrics like "Book Resilience Score," derived from order book data, could provide valuable context. These enhancements will require adjustments to the data pipeline.

### B. Data Acquisition and Preprocessing

This section details the acquisition and preparation of financial data for the SCoVA project. Two distinct approaches are employed: one for generating candlestick images and another for incorporating asymmetric features.

**Candlestick Image Generation:**

This pipeline focuses on creating candlestick chart images as input for the visual model. It involves the following steps:

1. **Data Source:** OHLCV (Open, High, Low, Close, Volume) and index data are acquired from Yahoo Finance.
2. **Data Adjustment:** Prices are adjusted for splits and dividends to ensure accuracy.
3. **Data Storage:** Data is stored in CSV files within designated directories: `stock_data/train/` for training data and `stock_data/test/` for testing data.
4. **Windowing:** A 5-day rolling window is applied to the OHLCV data.
5. **Return Label Calculation:** 5-day future returns are calculated using the formula: `(Close(t+5) - Open(t+1)) / Open(t+1)`. Note: `h=5` represents the 5-day holding period.
6. **Candlestick Chart Generation:** Candlestick charts are generated from the 5-day window data, incorporating volume and a moving average indicator.
7. **Integration:** Return labels are synchronized with their corresponding candlestick chart images.
8. **Filename Convention:** The holding period (5 days) is reflected in the filenames or through the use of separate CSV files.

**Asymmetric Feature Integration:**

This pipeline focuses on calculating and integrating asymmetric features that reflect market dynamics. These features are generated using an `AsymmetricFeatureEngine` and serve as context tokens for the Vision Transformer. The following features are calculated from raw OHLCV data:

- **Price and Volatility Asymmetry:**
  - Upside vs. Downside Volatility (using semi-deviation)
  - Volatility Skewness
  - Volatility Kurtosis
- **Volume and Participation Asymmetry:**
  - Accumulation/Distribution Ratio
  - Order-to-Quantity Asymmetry (comparing bid-side and ask-side ratios)
- **Correlation Asymmetry:**
  - Price-Volume Correlation State (calculated separately for positive and negative return candles)

The `AsymmetricFeatureEngine` receives a window of raw OHLCV data as input and outputs a vector of these calculated features. This integration of asymmetry is crucial for capturing market nuances and improving the model's predictive capabilities.

## II. Model Development and Training

This section details the development and training of the models used to predict stock market trends. The process centers around Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), enhanced with a novel dual-token context injection method. This method incorporates both numerical features and market regime information into the model's input, improving its ability to discern subtle patterns and market dynamics.

The dual-token context injection mechanism works by embedding two distinct tokens into the ViT architecture: a numerical feature vector and a regime identifier. This approach allows the model to leverage both visual and numerical information, balancing simplicity with future extensibility.

The following considerations guided the development of this mechanism:

- **Simplified Feature Integration:** Numerical features are computed and packaged into a vector. This vector serves as a token for the ViT, streamlining the integration process and enabling the ViT to utilize both candlestick images and calculated features during training and prediction.

- **Contextual Understanding in ViT:** The ViT architecture is adapted to process this context token alongside the candlestick image tensor. This modification allows the self-attention mechanism within the ViT to learn relationships between visual patterns in the candlestick images and the numerical context.

- **Balancing Complexity and Extensibility:** While further enhancements for understanding market asymmetry and volatility are recognized as potentially beneficial, the current implementation favors a simpler approach to minimize potential pipeline alterations. The design, however, is extensible and allows for future incorporation of more sophisticated feature engineering if warranted.

## Data Acquisition and Preprocessing

This section details the acquisition and preprocessing steps for the financial data used in the project. The pipeline is designed to accommodate future integration of the dual-token context injection method.

The current pipeline includes:

1. **Data Source:** OHLCV (Open, High, Low, Close, Volume) data is acquired from Yahoo Finance.
2. **Additional Data and Adjustments:** Relevant index data is acquired, and appropriate price adjustments (e.g., for splits and dividends) are applied to ensure data consistency and accuracy.
3. **Data Storage:** Processed data is stored in CSV format within designated training (`stock_data/train/`) and testing (`stock_data/test/`) directories.
4. **Windowing:** A 5-day sliding window is applied to both input features and output labels.
5. **Return Calculation:** Return is calculated as `(Close(t+h) - Open(t+1)) / Open(t+1)`, representing the percentage change in price over the holding period _h_.
6. **Candlestick Chart Generation:** Candlestick charts, incorporating volume and moving average indicators, are generated from the processed data.
7. **Return Label Generation:** Future returns are calculated for the 5-day period, aligning with the input window.
8. **Data Synchronization:** The return label calculation is coupled with the candlestick chart generation process, ensuring data consistency.
9. **Filename Convention:** The filename convention encodes the holding period _h_, ensuring clear organization and traceability.

To support the future integration of the dual-token context injection method, the following enhancements are planned:

- **Regime ID Inclusion:** The pipeline will be extended to acquire and integrate a "Regime ID," a discrete identifier representing the prevailing market regime. The methodology for determining and associating this ID with the financial data will be defined and implemented.
- **Asymmetric Feature Vector Inclusion:** The pipeline will incorporate a "raw asymmetric feature vector." The specific features comprising this vector will be defined, calculated, and integrated as separate inputs. This non-destructive approach preserves the existing OHLCV data.

These additions, while not yet implemented, are planned to ensure a seamless transition to the dual-token approach.

## II. Model Development and Training

This section details the development and training of models used to predict stock market trends based on 5-day candlestick chart images. The primary focus is on Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), exploring their ability to identify predictive visual patterns.

### A. Model Architecture

Two primary architectures are investigated:

1. **CNN-based Models:** CNNs process the candlestick images to generate buy/sell signals. Numerical outputs will be documented to ensure transparency in the image-to-decision transformation. Hybrid architectures combining CNNs with LSTMs or Transformers will also be explored to capture potential sequential information. Input data explicitly excludes dates, prices, and ticker symbols to isolate the impact of visual patterns. The models are trained to predict the return between `t+1` and `t+h` (where `t` is the last day of the chart and `h` is the holding period, ranging from 1 to 5 days), calculated as `Return(t+h) = (Close(t+h) - Open(t+1)) / Open(t+1)`. Training utilizes Mean Squared Error (MSE) or Root Mean Squared Error (RMSE).

2. **Vision Transformer (ViT):** ViTs offer an alternative approach for processing sequential candlestick data. Implementations using EfficientNet or patch embedding will be explored. Input consists of sequential windows of candlestick images. Feature engineering, including delta features (capturing changes between consecutive candlesticks), will be incorporated. A dedicated dataset of image sequences will be created. Key considerations include the input image limit for the ViT training layer, implementing positional embeddings for sequential information, and handling variable-length input via masking and padding. An efficient input pipeline will be developed to handle sequences of _N_ images.

The overarching goal is not only predictive accuracy but also causal grounding—understanding _why_ specific patterns lead to particular market movements. This involves analyzing learned features and connecting them to established market dynamics.

### B. Training and Validation

Robust training and validation processes are crucial:

- **General Training Parameters:** A learning rate of 0.001 and a batch size of 32 will be used, based on empirical evidence and prior research. Prediction magnitude filtering will be employed, executing trades only when the absolute predicted return exceeds a predefined threshold (optimized during backtesting).

- **CNN-Specific Training:** Training uses a mean squared error loss function. A validation set (30% of the training data) and early stopping (stopping after two epochs without validation loss improvement) prevent overfitting. The model with the lowest validation loss is selected for testing.

- **Data Scale-Up and Sequence Generation (ViT):** Training data will be scaled up. A sliding window image generator will create candlestick image sequences for the ViT. An initial 3-image sequence dataset will validate the sequence generation and training pipeline. This framework supports exploration of variable holding periods.

### C. Performance Evaluation

Model performance will be evaluated using Jensen's alpha and the Sharpe ratio. The equally weighted index serves as the benchmark, and the risk-free rate is derived from the 3-month Swedish Krona Short Term Rate (SWESTR). Trading costs (20 basis points per round trip) are deducted from portfolio returns. All performance results are annualized for standardized comparison.

### D. Documentation and Code Connection

Detailed documentation of the thought processes and decisions will be maintained throughout the project using a platform like Canvas. This ensures transparency and facilitates future analysis and refinement. The codebase will be meticulously linked to the experiments described in the accompanying research paper to ensure reproducibility.

## II. Model Development and Training

This section details the development and training of the predictive model, focusing on its architecture, data preparation, and evaluation methodology. The primary model is a Convolutional Neural Network (CNN) trained on candlestick chart images to predict future stock returns.

### A. Model Architecture

The core of the prediction model is a CNN designed to interpret visual patterns in candlestick charts. The model receives as input a candlestick chart image representing five consecutive trading days and predicts the percentage return over the subsequent five days. Crucially, the model operates solely on the visual representation of price action, excluding explicit numerical price values, date information, or ticker symbols. This design choice prevents the model from learning spurious correlations and ensures predictions are based purely on price patterns.

The CNN transforms the input image into a numerical output representing the predicted 5-day future return. This transformation process, along with the precise interpretation of the numerical output in the context of trading decisions, will be thoroughly documented.

The input candlestick chart images are generated using the `matplotlib` library (chosen due to deployment environment constraints over the initially preferred `mplfinance` library). Each image incorporates price candlesticks, volume bars, and a moving average line to provide a comprehensive visual representation of market activity. The images are generated from OHLCV (Open, High, Low, Close, Volume) data, and the corresponding return labels are calculated using the following formula: `((Close(t+5) - Open(t+1)) / Open(t+1)) * 100`. This formula represents the percentage return from the open of the day following the input window (t+1) to the close five days later (t+5). Rigorous validation ensures accurate representation of the candlestick data (open, high, low, close prices), correct time windows, and consistent data point counts within each image. Edge cases for the last _n_ data points of each graph are also addressed.

To enhance the model's ability to capture temporal dependencies, future work will explore integrating the CNN with Long Short-Term Memory networks (LSTMs) or Transformer architectures. Furthermore, incorporating fundamental and macroeconomic data alongside the visual input will provide broader market context and potentially improve predictive accuracy.

### B. Training and Evaluation

Model training will utilize a large-scale dataset comprising thousands of candlestick chart images paired with their corresponding return labels. This represents a significant scale-up from initial prototyping and is crucial for robust performance and generalization to unseen data. The model will be trained to minimize prediction error, employing appropriate regression metrics such as Mean Squared Error (MSE) or Root Mean Squared Error (RMSE).

A robust evaluation process using rolling walk-forward validation will be employed to assess model performance on unseen data and ensure adaptability to changing market conditions. This rigorous testing will provide critical insights into the model's generalizability and long-term viability. Future documentation will detail specific training parameters and the complete validation strategy.

## II. Model Development and Training

This section details the architecture, training process, and enhancements for the Convolutional Neural Network (CNN) models used to predict stock market trends based on candlestick chart images.

### A. Model Architecture

The model architecture incorporates several key design choices and enhancements aimed at improving prediction accuracy and robustness:

- **Input Window:** A 5-candlestick input window is used. This choice, while inspired by the work of Jiang et al. demonstrating the effectiveness of this window size for pattern learning, warrants further investigation and potential optimization in future work. The corresponding 5-day future prediction (output) window currently mirrors the input length for design symmetry and practical considerations, but its optimality should be explored.

- **Separate CNNs for Holding Periods:** Separate CNNs will be trained for each holding period (1 to 5 days) to allow for specialized learning and potentially improved performance at different prediction horizons. Performance will be evaluated and compared across models using metrics such as validation loss, Sharpe ratio, alpha, and Mean Squared Error (MSE).

- **Historical Prediction Error Profiling (HPEP):** A novel HPEP module will be implemented to enhance backtesting accuracy and model robustness. This module, integrated into `test_model.py` (post-training) and `trade.py` (during filtering), will generate a confidence profile by binning validation predictions based on predicted return ranges and calculating the accuracy for each bin. This HPEP map will then be used during backtesting to filter trades, executing only those exceeding a predefined `confidence_threshold` hyperparameter. A hybrid approach combining this confidence-based filtering with ranking will also be explored to optimize capital allocation. A working prototype with dummy data will be developed for initial testing and demonstration.

- **Feature Attribution:** Tools like Grad-CAM and SHAP will be employed to visualize the areas of the candlestick charts that the CNN focuses on during prediction, providing insights into model behavior and potential optimization opportunities.

### B. Training and Validation

The training and validation process incorporates considerations for HPEP implementation and hyperparameter optimization:

- **Confidence-Based Filtering and Ranking:** The HPEP map, generated during training, will be integrated into the backtesting validation process to filter and rank trades based on historical confidence levels.

- **Hyperparameter Optimization:** Several key hyperparameters will be optimized, including the `confidence_threshold` for trade execution and the `holding_days`. Joint optimization of these and other relevant hyperparameters will be explored using techniques such as grid search, random search, or Bayesian optimization.

### C. Soft Label Exploration

Currently, the model uses hard labels for training and validation. An investigation into the potential benefits of using soft labels, representing a probability distribution over a range of values, will be conducted. This approach aims to capture market prediction uncertainty and potentially improve trading accuracy, particularly during validation. This exploration will involve developing methods for generating soft labels and integrating them into the training pipeline.

## II. Model Development and Training

This section details the development and training of the models used to predict stock market trends based on candlestick chart images. The core model architecture revolves around Convolutional Neural Networks (CNNs) trained on these images, with explorations into uncertainty-aware architectures like Monte Carlo Dropout, Deep Ensembles, and Bayesian CNNs. A key enhancement is the implementation of soft labeling and a corresponding change in the loss function to improve prediction accuracy and quantify uncertainty.

### A. Model Architecture and Loss Function

The standard CNN architecture is modified to accommodate soft labeling and probabilistic predictions. This involves several key adjustments:

- **Discretizing Return Space:** The continuous range of possible returns is discretized into bins (e.g., -5% to +5% in 0.5% increments). The CNN predicts the probability of the return falling into each bin, providing a more nuanced understanding of potential returns and mitigating overfitting.

- **Modifying the Output Layer:** The output layer is replaced with a fully connected layer followed by a softmax activation function. This ensures the outputs represent a probability distribution over the defined return bins.

- **Implementing Soft Labels:** Instead of hard labels (single scalar return values), soft labels are used. These represent a probability distribution over the return bins, generated using a Gaussian kernel centered around the actual observed return. This allows the model to learn from the inherent uncertainty in predicting future returns and can improve generalization performance.

- **Changing the Loss Function:** To accommodate soft labels, the loss function is changed from Mean Squared Error (MSE) to a more appropriate function like Categorical Cross-Entropy or KL-Divergence. This enables the model to learn a probability distribution over potential outcomes.

- **Exploring Uncertainty-Aware Architectures (Optional):** Techniques such as Monte Carlo Dropout, Deep Ensembles, or Bayesian CNNs are explored to further enhance uncertainty quantification. These methods provide a measure of confidence in the model's predictions, crucial for risk management.

### B. Training, Validation, and Backtesting

Model training focuses on incorporating the changes required for soft labels and evaluating the impact of probabilistic predictions.

- **Prototype with Probabilistic Trading:** A prototype trading strategy leverages the probabilistic output of the soft label CNN. Trading decisions are based on confidence thresholds (e.g., executing a trade only if P(return > 0) exceeds a predefined threshold).

- **Backtesting with Soft Labels:** The prototype strategy's performance, incorporating probabilistic predictions, is thoroughly backtested. This provides insights into the effectiveness of soft labels and confidence thresholds for trading decisions.

### C. Addressing Trading Costs and Performance

A significant challenge from previous iterations was the impact of high trading costs on overall performance. This iteration addresses these inefficiencies:

- **Mitigating High Trading Costs:** Previous high trading costs, reducing annual returns by approximately 10%, stemmed from frequent rebalancing, neglecting transaction friction, uniform weighting, and short-selling constraints. These issues are addressed to minimize costs and improve alpha generation. Strategies include reducing trading frequency, incorporating transaction costs into the model, optimizing weighting schemes, and exploring alternative handling of short-selling constraints.

- **Analyzing Performance:** A detailed performance analysis is conducted, including a breakdown of alpha by long vs. short positions, per-index Sharpe ratios before and after costs, and investigating the impact of short-selling constraints, particularly in the small-cap segment. The analysis will also examine how unsuccessful trades (where outcomes contradict predictions) are handled without a stop-loss mechanism, informing future risk management refinements. The successful application to the First North All-Share index, generating a positive alpha of +8.89% annually after transaction costs (+37.57% vs. benchmark of -27.88%), serves as a starting point for broader performance improvement and consistency across all portfolios.

## II. Model Development and Training

This section details the development and training of models to predict stock market trends, potential returns, and the time horizon for realizing those returns (rally time). The models will be designed to predict trend direction, return magnitude, and rally time.

### A. Model Architecture

A multi-head neural network architecture, specifically a Convolutional Neural Network (CNN) with multiple output heads, will be employed. This approach allows for simultaneous prediction of different aspects of market behavior. The CNN will leverage features extracted from an EfficientNet model. The output heads will include:

1. **Trend Direction Prediction:** This head predicts the direction of the trend (increase or decrease) as a scalar value.

2. **Return Magnitude Prediction:** This head predicts the magnitude of the price change (potential profit or loss) as a scalar value.

3. **Rally Time Prediction:** This head predicts the "rally time," defined as the number of candlestick periods required to reach the target return.

The network will be trained using a weighted sum of Mean Squared Error (MSE) losses for each head to balance the optimization of all prediction objectives.

In addition to the CNN architecture, survival analysis models (e.g., DeepSurv, DeepHit, Weibull Time-To-Event models) will be explored. These models offer an alternative approach to predicting time-to-target, particularly beneficial for handling censored data (where the target price isn't reached within the observation window) and expressing uncertainty in the time-to-target prediction.

### B. Output Encoding for Trading Signals

The models will generate continuous values for trend and reward magnitude. However, discrete trading signals (BUY, SELL, HOLD) are necessary for practical trading. Several encoding methods are under consideration:

- **One-hot Encoding:** This approach uses a binary vector, with each element corresponding to a specific trading signal, allowing the model to output a probability distribution over possible actions.

- **Label Smoothing:** Used in conjunction with one-hot encoding, this technique adjusts target probabilities during training to mitigate overconfidence and improve generalization.

The choice of encoding will influence the model architecture, loss function, and evaluation metrics.

### C. Training and Validation

The training and validation process will adhere to best practices, including early stopping and appropriate learning rate and batch size selection. A labeled dataset for rally time will be generated from existing stock data and incorporated into the training process. Appropriate metrics for evaluating rally time prediction accuracy will be researched and implemented. The training process will be extended to accommodate a portfolio management strategy incorporating:

- **Risk-Based Weighting:** Portfolio positions will be weighted based on prediction confidence, the inverse of historical volatility, and the signal-to-noise ratio of predicted vs. actual returns observed during validation. This prioritizes trades based on their risk profiles.

### D. Risk Management Module

A dedicated risk management module will be developed, including:

- **Stop-Loss Mechanism:** This mechanism will exit trades when they move against the predicted direction by a predefined percentage within a specified timeframe. For example, a position with a predicted return of +3.0% might be closed if the stock drops -2.5% within two days. Various stop-loss levels will be backtested to evaluate their performance impact.

- **Dynamic Trade Filtering:** A filtering mechanism will be integrated to execute trades only when the predicted return magnitude exceeds a predefined threshold (e.g., > X%). This filter can also incorporate Historical Prediction Error Profiling (HPEP) data, allowing trades only when the historical accuracy for the given prediction bin is above a certain threshold. Effective filter thresholds will be determined through backtesting with historical or simulated data.

### E. Model Enhancement and Refinement

This phase will focus on model improvements based on trade performance analysis. Research will be conducted on alternative algorithmic exit strategies beyond a simple hardcoded stop-loss. A mechanism will be developed to capture and integrate insights from losses and refine the model accordingly.

## II. Model Development and Training

This section details the development and training of the predictive model, focusing on its architecture, training methodologies, and enhancements designed to improve learning from mistakes and incorporate temporal dependencies.

### A. Model Architecture

Several architectures will be explored and compared to determine the most effective approach for processing candlestick chart image sequences:

- **Vision Transformer (ViT):** A Vision Transformer (ViT) will be implemented to process sequences of three consecutive 5-day candlestick charts. Two variations will be investigated:

  1. **EfficientNet Feature Extraction:** Encoding each image using EfficientNet and feeding the resulting feature vectors into the transformer.
  2. **Patch Embedding:** Utilizing ViT-style patch embedding, where each image is divided into patches, flattened, and augmented with positional encoding before being input to the transformer.

- **CNN + LSTM Hybrid:** A hybrid model combining a Convolutional Neural Network (CNN) for image feature extraction and a Long Short-Term Memory (LSTM) network for capturing temporal dependencies between candlestick patterns. This approach leverages the strengths of CNNs for image processing and LSTMs for sequential data.

- **Benchmark Comparison:** A benchmark comparison will evaluate the performance of different input strategies: single static images, paired images, and image sequences. This comparison will inform the most effective method for incorporating temporal context.

- **Paired Image Data Generator:** A data generator will be created to produce paired image inputs for training, achieved by concatenating or stacking images.

### B. Training and Validation

This subsection details the strategies employed to train and validate the chosen model architecture(s), focusing on learning from mistakes and incorporating temporal dependencies.

- **Learning from Mistakes:** Three approaches will be employed to address recurring trading errors:

  - **Sample Re-weighting:** Misclassified trades with high associated losses will be assigned higher weights during retraining. This cost-sensitive approach prioritizes avoiding costly mistakes.
  - **Bootstrapping of Hard Cases:** After each training epoch, samples with high loss will be identified and added to a "hard sample bootcamp" set for focused retraining.
  - **Meta-Model for Trade Review:** A secondary "meta-learner" model will be trained to predict potential trade failures before execution, using the initial prediction, trade setup characteristics, and actual outcomes.

- **Incorporating Temporal Dependencies:** The following adjustments will be implemented to leverage the sequential nature of candlestick data:
  - **Enhanced Reward Logic:** The reward logic will consider the influence of preceding candlestick patterns, incorporating the sequence of images into the reward calculation.
  - **Sequential Input Processing:** The model's input layer will be adapted to accept sequences of images, enabling the model to learn from temporal patterns and implement the enhanced reward logic.

### C. Model Enhancement and Refinement

This subsection details the enhancements implemented to refine the model's performance and risk management, including sophisticated exit strategies.

- **Error Map Analysis:** An error map will be created during validation to track predicted and actual returns, focusing on significant divergences. Analyzing patterns in these incorrect predictions will identify potential model weaknesses and inform future training iterations.

- **Sophisticated Exit Strategies:** The following dynamic exit strategies will be implemented:
  - **Volatility-Aware Thresholds:** Exit thresholds will be dynamically adjusted based on recent market volatility using the Average True Range (ATR).
  - **Time-Based Confidence Decay:** Trades will be exited if the predicted reward does not materialize within the expected timeframe.
  - **Prediction Divergence:** If model retraining leads to significant prediction divergence, the trade will be exited.
  - **Portfolio Contextual Exits:** Underperforming trades relative to their prediction group will be exited.

These combined enhancements aim to create a more adaptive, robust, and profitable trading model.

## II. Model Development and Training

This section details the development and training process for predictive models designed to forecast stock market trends based on candlestick chart images. The focus is on Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and potentially hybrid architectures, incorporating delta features and handling sequential data.

### A. Model Architectures

Several model architectures will be investigated to achieve accurate predictions of market trends and reward magnitudes:

- **Dual-Input CNN:** This model accepts the current candlestick chart image and a delta frame representing the difference between the current and previous frame. This explicitly encodes frame-to-frame changes, potentially improving the model's ability to capture short-term trends.

- **Vision Transformer (ViT) with Sequential Candlestick Windows and Delta Features:** This architecture leverages the ViT's ability to handle sequential data and long-range dependencies. Key considerations include:

  - **Delta Feature Integration:** Both image subtraction and feature subtraction will be compared to determine the most effective method for calculating delta features.
  - **Variable-Length Input Sequences:** The model will handle variable-length sequences up to a defined maximum (e.g., 5 charts). Shorter sequences will be padded with blank chart images, and a mask will inform the transformer which parts of the sequence to ignore.
  - **Positional Embeddings:** Positional embeddings will provide the ViT with information about the order of images within the sequence. This applies to both training a ViT from scratch and utilizing a pre-trained model with positional extension.
  - **Input Image Count Experimentation:** Experiments will be conducted with varying input sequence lengths (N=3, 4, and 5 candlestick images) to determine the optimal number of images for achieving the best performance.

- **Prediction Targets:** The models will be designed to predict one or more of the following: a scalar reward value, a categorical label (BUY/SELL/HOLD), or a combination of reward and rally time using a multi-head output structure. This allows flexibility in defining the prediction target and tailoring the model to different trading strategies.

### B. Training and Validation

The training process incorporates several key elements:

- **Dataset Creation:** A sliding window approach will create sequences of candlestick chart images (e.g., from time _t-2_, _t-1_, and _t_) with corresponding labels (return value, rally time, and signal class). An image sequence generator tool will automate this process.

- **ViT Training with Variable-Length Sequences:** Training the ViT will utilize masking and padding to handle variable-length sequences effectively.

- **Backtesting:** A backtesting framework will evaluate model performance under realistic market conditions, enabling assessment and identifying areas for improvement.

## II. Model Development and Training

This section details the development and training of the models used to predict stock market trends based on candlestick chart images. The core innovation lies in predicting future candlestick images rather than directly predicting returns, leveraging the rich information encoded within these visual representations. This approach reframes the problem as a visual sequence forecasting task, specifically image-to-image prediction.

### A. Model Architecture

This project centers on developing a robust model architecture capable of processing candlestick image data and generating accurate predictions. Two primary approaches are being considered: a Convolutional Neural Network (CNN) decoder and a transformer-based image generator (e.g., Vision Transformer - ViT, or U-Net). A prototype design for an image-to-image candlestick forecaster will be developed using one of these architectures.

This image-based prediction approach introduces several key considerations:

- **Candlestick Image Prediction:** The model will predict the next candlestick image(s) in a sequence. This requires exploring architectures suitable for image-to-image prediction, potentially adapting existing CNN, LSTM, Transformer, or ViT architectures.

- **Return Extraction:** A mechanism for interpreting the predicted candlestick images and extracting corresponding return values will be developed. This will involve translating visual patterns into quantifiable return predictions, potentially by analyzing pixel locations to determine predicted open, close, high, and low values, or rendering the image data into a usable format.

- **Comparison with Traditional Models:** A comprehensive comparison will be conducted between the image-based model and traditional models (e.g., scalar regression, probabilistic return models). This comparison will encompass output type, supervisory signal, link to trading, richness of learned structure, interpretability, risk of ambiguity, data requirements, and modeling complexity.

- **Theoretical Advantages:** The potential advantages of this image-based methodology will be analyzed, including:

  - **Representation Richness:** Capturing complex patterns and relationships within candlestick data.
  - **Uncertainty Modeling:** Incorporating and representing uncertainty in predictions.
  - **Causal Reasoning:** Learning and representing causal relationships between candlestick patterns and market movements.
  - **Training Supervision:** Using images as a form of supervision during training.
  - **Interpretability:** Gaining insights into the model's decision-making process.
  - **Generative Flexibility:** Generating a range of potential future scenarios.

- **Reusable Components:** Reusable components will be developed to facilitate this architectural shift and future work with financial time series data. This includes specialized data loaders for candlestick image sequences, masking logic for variable-length sequences, and wrappers for ViT models.

- **Risk Assessment:** A thorough risk assessment will address the challenges of image-based prediction, including less direct evaluation metrics, potential compounding of errors, loss of direct reward supervision, and ambiguity in translating predictions into financial implications.

### B. Training and Validation

A robust experimental framework will be used to validate the effectiveness of the chosen architecture. This involves training the selected deep learning model (ViT or U-Net) using generated candlestick images. Post-training, predicted returns will be extracted from the generated images and compared against baseline models using metrics such as RMSE, SSIM/LPIPS (for image quality), and backtested profit performance. This framework will also incorporate rigorous memory management strategies (Options A, B, and C) evaluated for their impact on flexibility, performance, and training efficiency. The experiments will maintain a consistent dataset across all options, varying only the memory management approach, and evaluate the effect of added temporal context on predictive power. Model performance will be measured using key metrics, including Sharpe ratio, directional accuracy, Mean Squared Error (MSE), and rally-time prediction accuracy.

## II. Model Development and Training

This section details the development and training of the model, focusing on architectural considerations and ensuring its predictions are causally grounded, not merely based on superficial pattern recognition. A key concern is addressing the potential fragility of image-based prediction training and evaluation.

### A. Model Architecture

The model architecture should not only predict realistic charts but also understand _why_ certain patterns occur, ensuring **causally grounded predictions**. This goes beyond replicating visual patterns to understanding the market dynamics that create them, improving generalizability to new, unseen market situations. A potential solution is a **dual-module framework**:

- **Chart Generator Module:** Creates candlestick chart images.
- **Trade Evaluator Module:** Analyzes generated charts for actionable trading opportunities, identifying clear entry and exit points.

This separation allows specialized evaluation of the trading value of generated charts, distinguishing realism from trading value—a realistic chart may not necessarily translate into profitable trades.

### B. Training and Validation

Training and evaluating image-based prediction models presents unique challenges. It's crucial to address the potential **fragility of training and evaluation** by mitigating issues like:

- **Ground Truth Alignment:** Ensuring accurate correspondence between generated images and actual market data.
- **Image Fidelity Loss Functions:** Choosing appropriate loss functions that capture relevant aspects of chart similarity for trading, not just visual resemblance.
- **Mode Collapse:** Preventing the model from converging on a limited set of outputs, hindering its ability to capture diverse market behaviors.
- **Pattern Overfitting:** Avoiding overfitting to specific patterns in the training data, which can lead to poor generalization.
- **Evaluating Trading Relevance:** Developing metrics that assess the generated images' ability to inform profitable trading decisions.

The training process must also investigate the relationship between the model's accuracy in predicting visual patterns and its actual financial performance. This analysis helps determine how effectively the model translates visual predictions into profitable trades. The model prioritizes correctly identifying visual trends over directly maximizing financial returns, allowing it to learn underlying market mechanics reflected in candlestick patterns. Furthermore, the model must determine the necessity of a trade; not every identified pattern warrants action. Even with accurate visual predictions, financial implications can be ambiguous. Therefore, the model's output should be evaluated for its ability to inform clear and actionable trading decisions.

### C. Benchmark Model and Future Development

A benchmark model leveraging image prediction as a latent feature will be developed to improve risk management by identifying potentially high-risk trades or periods with limited trading opportunities. This will inform further model development and training. The training and validation of this benchmark model will be documented, including details on the training process, validation methodology, and performance metrics.

### D. Model Architectures Explored

Several model architectures are being explored, including Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), each with specific input requirements and training procedures:

- **CNN-based Models:** CNNs generate trade signals based on visual patterns, focusing on the top/bottom 10% of predicted price movements. Trade execution logic will be evaluated using actual t+1 to t+5 prices. The possibility of hybrid architectures combining CNNs with LSTMs or Transformers is being explored. The numerical outputs of the CNNs and their transformation process will be documented.

- **Vision Transformer (ViT) Models:** ViTs process sequential candlestick data using EfficientNet or patch embedding for input processing. Positional embeddings are implemented for ViT training, with masking and padding handling dynamic-length input. The ViT is trained with variable-length sequences using an input pipeline designed for N images. A key constraint is determining the input image count for the ViT training layer.

### E. Comparative Analysis

A rigorous experimental protocol will compare this image-based prediction approach against traditional scalar-based return prediction models, assessing their respective strengths and weaknesses under various market conditions.

## II. Model Development and Training

This section details the development and training of the model for stock price prediction. The model's core innovation is a shift from a traditional 3D Cartesian coordinate system to a dynamic 2D plane representation. This 2D framework utilizes rotational axes and a dynamic origin that moves with price fluctuations, offering a potentially more efficient and accurate way to capture the dynamics of candlestick data. This approach draws inspiration from concepts like Frenet frames and tangent planes.

### A. Model Architecture

The model architecture centers around representing price movements within a dynamic 2D plane. This plane incorporates rotating axes and a shifting origin, analogous to an observer tracking a moving object. This representation necessitates exploring several key research areas:

- **Coordinate Transformations and Manifolds:** The implications of using a dynamic 2D plane require exploring the mathematical concepts of coordinate transformations, manifolds (specifically, the 2D plane as a moving frame on a 1-dimensional manifold), and the bending of space. This provides a theoretical basis for understanding the dynamic plane's behavior and limitations.

- **Encoding Movement in the 2D Plane:** A key challenge is encoding 3D movement, exemplified by parabolic trajectories, within the 2D plane. This involves understanding how axis rotations effectively capture this information. The initial prototype will visualize a parabolic trajectory by rotating the X and Y axes at the trajectory's starting point and dynamically shifting the origin to the endpoint. This allows for a direct comparison with a standard 3D representation, considering accuracy and efficiency, and analyzing how movements along different axes are visualized in both frameworks.

- **Information Balance and Degrees of Freedom:** The dynamic 2D representation uses two coordinates (u,v) and three Euler angles (or a rotation matrix) to describe the frame's orientation. Analyzing the information balance and degrees of freedom within this representation is crucial to ensure the model captures necessary information without unnecessary complexity.

### B. Training and Validation

Model training aims to accurately predict visual patterns in 5-day candlestick chart images, excluding dates, prices, and tickers, while ensuring causal grounding for these predictions. Two distinct models will be trained: one for predicting trend direction and another for predicting reward magnitude.

The training process incorporates best practices:

- **Dataset Generation:** A large-scale dataset of candlestick image sequences will be created using a sliding window image generator. A smaller dataset of 3-image sequences will also be generated for initial training and experimentation.
- **Training Methodology:** Convolutional Neural Networks (CNNs) will be trained using a dedicated validation set and early stopping to prevent overfitting. The learning rate and batch size will be carefully tuned.
- **Evaluation Metrics:** Model performance will be evaluated using clear metrics such as Mean Squared Error (MSE) or Root Mean Squared Error (RMSE).

### C. Mathematical Formalization and Further Exploration

The model's development will be supported by:

- **Formalization:** A concise algebraic summary of the dynamic 2D plane, including precise mathematical notation and equations for coordinate transformations, will be developed.

- **Theoretical Grounding:** The dynamic 2D plane concept will be connected to established mathematical and physical formalisms, including parallel transport, affine connections, Frenet-Serret frames, the SE(3) group, and local inertial frames in general relativity. We will also analyze practical uses and limitations, considering path dependence, singularities, and computational overhead.

- **Three-Body Problem Analogy:** The model architecture will explore mapping the 2D dynamic frame to the three-body problem, investigating strategies like pairwise relative coordinates, barycentric coordinates, and shape space representations. Recent advancements in understanding the three-body problem, including machine learning approximations and new periodic solutions, will be examined.

- **Ongoing Thought Experiments:** The architecture will incorporate learnings from thought experiments focusing on developing an explicit rotation law, examining curvature invariants, and generalizing the concept from a plane to more complex surfaces. These insights will inform the design of the model's internal representations and transformations. Appropriate literature framing and dissertation materials will be developed to support these innovations.

## II. Model Development and Training

This section details the development and training of the model used for predicting stock market trends. The model's architecture draws inspiration from biological navigation principles, emphasizing a dynamic 2D representation of 3D price movements and ensuring numerical stability.

### A. Model Architecture

The core of the model architecture lies in representing 3D price information within a dynamic 2D framework, encoding orientation as state information. This approach is inspired by human navigation and leverages a moving frame of reference for enhanced numerical stability. Key architectural components include:

- **Dimensionality Reduction:** The model reduces the dimensionality of 3D price data to a 2D representation. This involves mapping 3D price movements onto a dynamic 2D chart, carefully addressing the challenge of reconstructing past price paths when orientation isn't explicitly recorded. The mapping incorporates separate handling of orientation and scaling, and considers the implications of representing complex, potentially chaotic, market dynamics within this simplified 2D form. Further investigation will explore how recently discovered periodic orbits (e.g., those from 2025) are represented within this 2D framework. Mathematical concepts such as coordinate charts, the Frenet-Serret frame, and stereographic projections will inform this reduction.

- **Dynamic Frame of Reference:** Mimicking human navigation, the model employs both egocentric (relative to the current price point) and allocentric (relative to a fixed market context) frames of reference. This is implemented through a dynamic coordinate system centered on the current price point, drawing inspiration from neuroscience research on place and grid cells, particularly the work of O'Keefe.

- **Moving Frame Coordinates for Numerical Stability:** To prioritize numerical stability, the model uses moving frame coordinates, analogous to those used in robotics and UAV trajectory optimization. The numerical conditioning of this approach will be evaluated and compared to using global coordinates (e.g., standard price and volume). The inherent advantages of body-fixed frames for numerical stability will be exploited, especially during periods of rapid price fluctuations. Stability during these periods will be addressed using angular-momentum conserving integrators (Lie-group methods), comparing their performance to simpler integration schemes like Euler. The memory and computational costs associated with storing 2D coordinates and orientation data (using quaternions or rotation matrices) will be compared to directly storing 3D positions.

### B. Training and Validation

A robust training and validation process is crucial for any implementation of this architecture. This will involve defining appropriate loss functions, training procedures, and evaluation metrics suitable for the 2D representation. A demonstration or visualization will be beneficial to illustrate the practical application of these theoretical concepts.

### C. Demonstrator Implementation (Optional)

A demonstrator could provide valuable insights into the practical application of the 2D dynamic frame. This could involve visualizing and allowing for experimentation with the application of the 2D frame to 3D price data. Potential features include using Jacobi coordinates, projecting onto the shape sphere, integrating the reduced Hamiltonian, and reconstructing original 3D trajectories. Incorporating concepts like the "shape great-circle ansatz" and R(t) periodicity to explore new price movement patterns within the 2D representation could also be considered.

### D. Narrative Generation Service Update

The Narrative_Generation_Service can be updated to leverage insights from the 2D representation. By mapping raw asymmetry values to descriptive labels corresponding to assigned Regime IDs (e.g., "Nervous Fear"), the service can provide more interpretable and contextually relevant explanations of market behavior.

### E. Research Directions

An intriguing research direction involves comparing the model's approach to human navigation of information spaces. Considering the human body/head as the origin of the coordinate system could provide insights into more intuitive and potentially more effective model architectures. Investigating the benefits and limitations of the chosen model architecture, including scenarios where certain symmetries might be redundant, is also crucial. Further analysis should explore the model's handling of singularities, chaotic systems, and its ability to effectively traverse the solution space, particularly for forward-looking predictions.

## II. Model Development and Training

This section details the development and training of a model capable of learning and predicting patterns within a dynamically rotating 2D chart representation of a financial time series, visualized as an upward spiral. The model aims to capture the evolving dynamics of the data as it progresses along this spiral.

### A. Model Architecture

The model's core revolves around dynamically rotating the 2D chart's axes to create a visual representation of an upward spiral, or helix. This unconventional representation requires careful consideration of the model architecture.

- **Input:** The primary input is the 2D chart represented as a helix, using `u` and `v` coordinates for position on the spiral. The axes' orientation encodes upward movement along the helix.

- **Helix Parameters:** The model incorporates the helix's radius and pitch as parameters. These are crucial for calculating curvature and torsion, which influence the patterns the model learns.

- **Helix Traversal:** Two helix traversal strategies are considered: (1) constant speed along the `u`-axis with a rotating frame, or (2) constant speed along a circular path with a slowly tilting frame. The chosen strategy impacts how the model interprets temporal dynamics.

- **Orientation:** A robust rotation implementation is required to reflect the helix's curvature. This might involve vertical axis rotation or rotational matrices. An orientation stream (E(s)) will capture the spiral's geometric information, aligning with the chosen helix generation and movement strategy.

- **Data Storage:** Planar coordinates (u, v) and the moving frame's orientation will be stored using an arc length counter for planar coordinates and quaternions or rotation vectors for orientation. This facilitates spiral path reconstruction and must respect memory constraints while being compatible with visualization requirements.

### B. Training and Validation

Training and validation require a specialized approach due to the dynamic input. The training dataset will be generated dynamically based on the helix parameters and movement logic to ensure exposure to a diverse range of spiral representations.

### C. Visualization

Visualizations are crucial for understanding model behavior. `matplotlib` and `numpy` will be used for plotting and numerical computations, respectively. `seaborn` is explicitly excluded for consistency and code simplicity.

Key visualizations include a 3D helix and an improved 2D trace visualization. Each will be generated as a separate figure, adhering to the technical constraint against using subplots. The 3D helix visualization will depict a helix with a radius of 1.0, a pitch of 0.5, and 4 turns, clearly labeled with X, Y, and Z axes. Improvements to the 2D trace visualization will focus on enhanced visual representation, tooltips/annotations, or dedicated documentation to clarify its meaning.

### D. Performance Considerations

Visualizing the concept of a straight line trace on a 3D helix appearing straight in a 2D representation aligned with a moving frame presents a challenge. While animation is a potential solution, its performance implications must be carefully evaluated. If performance becomes a bottleneck, static, separate charts will be prioritized.

A future enhancement could involve a third plot visualizing orientation evolution, potentially using rotation matrices or quaternion frames. This is not a requirement for the initial phase and would also need to be a standalone figure.

## II. Model Development and Training

This section details the development and training of predictive models for stock trading, focusing on a novel approach using dynamic projection and dimensionality reduction techniques. This approach aims to simplify the representation of market dynamics by focusing on relative price movements and potentially uncover new insights into price trends.

### A. Model Architecture

The core of the model architecture revolves around processing candlestick image sequences and predicting future price trends. A key innovation is the integration of a dynamic projection system based on Principal Component Analysis (PCA). This system dynamically rotates and re-centers the input data to prioritize local price action over absolute price levels, addressing the challenge of how each new candlestick influences the feature space.

Several architectural options for integrating PCA are being considered:

1. **Pre-processing:** Applying PCA directly to the raw data before feeding it into the model. This involves window selection, feature preparation, PCA computation, principal direction identification, feature space rotation, and data re-centering.

2. **Embedding Transformation:** Applying PCA to the image embeddings learned by a Convolutional Neural Network (CNN) or Vision Transformer (ViT). This allows the model to learn relevant features before dimensionality reduction and rotation.

3. **Integrated PCA Layer:** Incorporating a dedicated PCA layer within the network architecture. This enables end-to-end training but requires addressing technical considerations for backpropagation through this dynamic transform, potentially using differentiable approximations of PCA. The impact on model gradients during batch and sequential processing, as well as online vs. offline training, needs evaluation.

4. **Data Augmentation:** Implementing PCA-based rotation as a data augmentation technique. This involves rotating the input images based on the principal components, potentially improving robustness to different market conditions.

This dynamic projection aims to create a locally optimized frame of reference based on the latest price movement, allowing the model to learn and predict based on relative price changes regardless of the overall trend or absolute price levels. Further investigation will clarify how this dynamic projection interacts with other model components and its impact on prediction accuracy.

### B. Training and Validation

The effectiveness of the dynamic PCA integration and the overall model performance will be rigorously evaluated using the following metrics:

- **Accuracy Metrics:** Mean Squared Error (MSE), Mean Absolute Error (MAE), and Directional Accuracy (percentage of correctly predicted price movements).
- **Financial Metrics:** Sharpe Ratio (risk-adjusted return), Hit Rate (percentage of successful trades), and Profit Metrics (total profit, average profit per trade).
- **Computational Metrics:** Compute Efficiency (training and inference time) and Model Convergence (stability and convergence of the training process).

This comprehensive evaluation will provide insights into the model's predictive power, robustness, and practical applicability for stock trading.

## II. Model Development and Training

This section details the development and training of the model, focusing on the dynamic plane implementation, which dynamically rotates and translates the coordinate system of the input candlestick data. This approach aims to center recent price action, allowing the model to focus on fluctuations around the current trend.

### A. Model Architecture

The model architecture centers around transforming candlestick data using a dynamic plane. This transformation enhances the model's ability to recognize patterns in volatility and trend reversals by emphasizing relative price changes. The transformation involves the following steps:

1. **Local Movement Vector Calculation:** The local movement vector (v) is calculated at each timestep, representing the change in price and time between the current candlestick (P<sub>current</sub>) and the previous candlestick (P<sub>previous</sub>):

   ```
   v = P<sub>current</sub> - P<sub>previous</sub>
   ```

   P<sub>current</sub> and P<sub>previous</sub> are represented by their (x, y) coordinates of (time, price).

2. **Rotation Matrix Calculation:** The angle of rotation (θ) is determined from the local movement vector:

   ```
   θ = arctan(v<sub>y</sub> / v<sub>x</sub>)
   ```

   This angle is used to construct the rotation matrix R(θ):

   ```
   R(θ) = [[cos(θ), sin(θ)], [-sin(θ), cos(θ)]]
   ```

   This matrix is applied to a surrounding candlestick window (e.g., the last 5–10 candlesticks).

3. **Dynamic Rotation Implementation:** The coordinate system is dynamically rotated at each timestep (or at intervals of a few candlesticks). The rotation aligns the new x′-axis with the calculated local movement vector, and the y′-axis orthogonal to it. This effectively aligns the coordinate system with the current price trend.

4. **Dynamic Origin Shift:** Following rotation, the origin of the coordinate system is translated to the current price point (P<sub>current</sub>), centering the price movement and placing the current price at (0,0). Fluctuations around the trend are then represented as deviations from zero on the y′-axis.

5. **Dynamic Snapshot Generation:** The rotated and re-centered candlestick data are rendered into images. These dynamic snapshots serve as input to the model (e.g., a CNN or Vision Transformer). This approach aims to capture relative price movements more effectively than traditional static candlestick charts.

6. **Addressing Potential Challenges:** Several challenges associated with this dynamic approach need careful consideration:

   - **Rotation Artifacts:** Mitigation strategies, such as anti-aliasing, will be employed during image generation to minimize visual artifacts introduced by rotation.

   - **Volatility Jumps:** Smoothing or limiting rotation angle deltas between consecutive frames will address abrupt rotations caused by sudden price movements.

   - **Consistent Axis Scaling:** Maintaining consistent axis scaling (units per % move) across all generated frames is crucial for effective model training.

   - **Baseline Comparisons and Ablation Study:** Model performance will be compared against baselines (no PCA, static PCA, simpler transformations, alternative focus methods) and an ablation study of PCA without re-centering and vice-versa will be conducted to quantify the benefits of the dynamic approach. Furthermore, potential issues such as overfitting to noise within small windows, increased computational overhead, pipeline integration complexity, loss of the absolute reference frame, data instability, model dependency on the transformation, and interpretability challenges will be thoroughly analyzed.

7. **Pseudocode Pipeline:** A detailed pseudocode pipeline for the dynamic snapshot generator will be developed to document and facilitate implementation. This pipeline will outline the steps of data transformation, rotation, and image generation.

## II. Model Development and Training

This section details the development and training of the predictive model. The core innovation lies in a dynamic plane representation of market data, which is then used as input for either a Vision Transformer (ViT) or a Convolutional Neural Network (CNN). This dynamic representation continuously redraws the coordinate system based on local price movements, effectively creating a locally relevant 2D representation of the market's state.

### A. Model Architecture

The model's architecture centers around the dynamic 2D plane concept. Instead of traditional static representations, market data (time, price, and optionally volume) is transformed into a dynamic coordinate system. This transformation, coupled with either a ViT or CNN, aims to capture the temporal dynamics of market movements in a visually interpretable format, enabling the chosen model to learn complex patterns and predict future price movements. The dynamic plane generation process provides a novel approach to feature engineering, capturing the relative changes in price and volume over time within a locally relevant frame of reference. This contrasts with simpler rotations based on trend direction or PCA on windowed price variance because it reconstructs the entire 2D measurement plane at each step, allowing the model to learn from both dominant trajectories and residual/oscillatory market behaviors.

1. **Rotating Dynamic Plane Generator:** A dedicated module, tentatively named `RotatingSnapshotGenerator`, generates the rotating dynamic plane. This module takes candlestick data (time, price, and optionally volume) as input and outputs a 2D representation suitable for the ViT or CNN. Key functionalities include:

   - **Data Input and Window Selection:** The module ingests candlestick data and defines a local window of recent data points (e.g., 5-10 previous candlesticks).
   - **Movement Vector Calculation:** Movement vectors are calculated within the selected window, representing changes in time, price, and volume.
   - **Dynamic Frame Construction using PCA:** Principal Component Analysis (PCA) is applied to these movement vectors to extract the two dominant principal components. These components become the axes of the dynamically redrawn 2D plane.
   - **Rotation and Refocusing:** The 2D plane is rotated and refocused, with the origin dynamically shifting to the latest data point. This ensures the model’s frame of reference remains centered on the most recent market activity. This rotation accounts for the parabolic curve representing price movement in the original, conceptualized 3D model (time, price, volume).
   - **Output Generation:** The module outputs the 2D plane data, formatted as images or numerical features compatible with the chosen model (ViT or CNN). Optionally, snapshots of the dynamic plane can be rendered for visualization and analysis.

2. **Vision Transformer/CNN Integration:** The output of the `RotatingSnapshotGenerator` is fed into either a ViT or a CNN. Experiments will be conducted comparing the performance of both models with static and dynamic input frames.

3. **Pseudocode and Implementation:** Detailed pseudocode for the `RotatingSnapshotGenerator` will be developed, encompassing all steps from data handling to the rotation process. This pseudocode will then be implemented in Python using libraries like NumPy, Matplotlib, and PIL. The implementation will be optimized for batch operation to efficiently process large datasets and reduce training time.

4. **Conceptual Diagram:** A conceptual diagram will be created to illustrate the evolution and shifts of the 2D plane with each market movement, providing a clear visual representation of the dynamic coordinate system's behavior and the rotation operation on a sequence of price movements. This diagram will also clarify the re-evaluation of the angle theta calculation, which needs to incorporate the dynamic origin and rotational axes within the 2D plane, ensuring accurate representation of market movements.

## II. Model Development and Training

This section details the development and training of the CNN and ViT models, focusing on data representation, transformations applied by the Dynamic Plane Generator, and addressing specific technical constraints. A key aspect is ensuring the models receive appropriate image inputs and understanding the effect of the dynamic plane transformations.

### A. Model Architecture

This stage centers on the visual representation of financial time series data and how it's processed by the models. The models are trained on datasets generated based on the dynamic plane principle. This involves generating batches of these datasets for both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to facilitate efficient training. Further architectural details and specific model configurations are addressed in other sections of this document.

- **Input Data Visualization:** Example images of the raw candlestick data, before transformation by the Dynamic Plane Generator, will be provided. This visualization serves as a baseline for comparison and illustrates the initial format of the input data.
- **Transformed Input Visualization:** Example images will also demonstrate the candlestick input after applying the dynamic plane transformations, including origin refocusing and alignment with principal rotating axes. This visualization is crucial for understanding the effect of the dynamic plane transformations on the input data and how these changes are presented to the CNN/ViT models.

### B. Training and Validation

Training and validation are deeply intertwined with the dynamic nature of the input data. The Dynamic Plane Generator plays a crucial role in preparing the visual input for the models. The training process leverages datasets generated based on the dynamic plane. To effectively train the models with potentially longer sequences, the simulation will be extended accordingly to handle variable-length input sequences. Additional training and validation specifics, such as learning rates, batch sizes, and validation strategies, are addressed elsewhere in this document.

- **Dynamic Plane Generator Functionality:** The Dynamic Plane Generator is a key component in the model pipeline, rendering an image representation of the dynamic 2D plane at each timestep as direct input for the CNN/ViT models.
- **Minimum Data Point Constraint:** The implementation considers the technical constraint of the Dynamic Plane Generator requiring at least two data points for dynamic plane computation. This constraint informs the data preparation and training processes.

### C. Dynamic Plane Implementation

This section elaborates on the implementation of the Dynamic Plane Generator, including visualization and operational principles, and addresses key improvements and functionalities.

- **Dynamic Plane Visualization:** An animation will visualize the step-by-step evolution of the plane as new data points are added. This visualization aids in understanding the transformations during dynamic plane generation.
- **Smoothing Techniques:** Experimentation with different smoothing techniques, like Heikin-Ashi, will be explored for the dynamic plane visualization to reduce noise and potentially improve model performance.
- **Animation Enhancements:** The animation will dynamically update the displayed plane, providing insights into how each data point affects its orientation. A key bug fix addresses animation initialization errors encountered with no or only one initial point. The animation now correctly handles these edge cases by delaying its start until at least two points are available.

### D. Addressing Technical Constraints

Several technical constraints related to the dynamic rotation and its impact on the animation and PCA calculations require careful handling:

- **Minimum Points for Animation and PCA:** Both the animation framework and PCA calculations require a minimum of two data points to function correctly. This is particularly relevant in the initial frames when visualizing dynamically rotated points after PCA/SVD transformations. Attempts to animate or perform PCA with a single point will result in errors. The implemented solution delays rotation and plotting until sufficient data is available.
- **PCA Instability with Collinear Data:** Calculating PCA becomes unstable when data points are collinear, a concern particularly in early frames with fewer data points. This instability can lead to errors in the dynamic plane rotation. Mitigation strategies for this instability will be detailed in a subsequent section.

## II. Model Development and Training

This section details the development and training process, focusing on data transformation, visualization, and model architecture designed to handle complex market patterns, including chaotic and choppy price movements. The primary input data consists of OHLC candlestick data, which will be transformed into Heiken-Ashi candlesticks and visualized using both standard and dynamically rotated charting techniques.

### A. Data Generation and Transformation

1. **Choppy Data Generation:** To simulate realistic market conditions, including choppy sideways movements, a function `generate_choppy_candlesticks(n=30)` will generate synthetic candlestick data (Open, High, Low, Close, and potentially Volume and Time). This data will serve as a robust testbed for the model's performance under challenging market scenarios. Beyond choppy conditions, the model will also be trained on simulated data incorporating more complex price dynamics, such as rallies, drops, and recovery phases, to enhance generalization to real-world market behavior.

2. **Heiken-Ashi Transformation:** The generated and simulated candlestick data will be transformed into Heiken-Ashi candles using a dedicated function. This function will accept an array of standard candles (Time, Open, High, Low, Close) and output an array of Heiken-Ashi candles (Time, Open_HA, High_HA, Low_HA, Close_HA).

3. **Dynamic Rotation and Recentering:** A dynamic rotation and recentering technique, `dynamic_rotate_recenter_heiken`, will be applied to the Heiken-Ashi data. This function calculates the midpoint between the open and close values of each Heiken-Ashi candle and uses these midpoints as the basis for rotation and recentering, aiming to highlight underlying trends in the data.

### B. Visualization and Saving

Both standard and dynamically rotated Heiken-Ashi charts will be generated and saved as PNG images for comparison and analysis.

1. **Standard Heiken-Ashi Visualization:** Standard Heiken-Ashi candles will be visualized using `plot_heiken_ashi_candlestick`. This chart will represent up moves (Close >= Open) with green bodies and down moves (Close < Open) with red bodies. The generated chart will be saved to `/mnt/data/standard_heiken_ashi_choppy.png`.

2. **Rotated Heiken-Ashi Visualization:** The dynamically rotated and recentered Heiken-Ashi data will be visualized on a 2D plane using `plot_rotated_heiken`. This visualization will allow for a visual assessment of how the dynamic rotation reveals trends within the data. The rotated chart will be saved to `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`.

### C. Model Architecture and Animation Considerations

The visualization component will incorporate dynamic point movement and live frame rotation. To ensure stability and smoothness, especially during the initial stages with limited data, the following architectural decisions will be implemented:

1. **Minimum Points for Plotting and Rotation:** Plotting operations will commence with at least two valid data points, and plane rotation will only begin after accumulating at least three stable points. This prevents errors within the animation framework and addresses potential instability in Principal Component Analysis (PCA) calculations with limited data.

2. **Handling Single-Point Frames:** A mechanism will be implemented to gracefully handle frames containing only a single data point. This could involve displaying an empty canvas or a placeholder graphic to maintain visual consistency.

3. **Formatting Offsets:** Offsets will be formatted correctly to prevent dimension mismatches during calculations, which can lead to incorrect animation behavior.

4. **Smooth Rotation Matrices:** Rotation matrices will be smoothed or stabilized to avoid dimensional blowups and ensure visually smooth transitions in the animation, preventing jerky or unnatural movements during rotation.

5. **Standalone Animation Simulator:** A standalone animation simulator will be built to aid in development and visualization. This simulator will incorporate the delayed rotation and smoothing techniques, providing a step-by-step visualization of the dynamic plane for debugging and refinement. This simulator will also incorporate a mechanism to handle single data point instances gracefully.

## II. Model Development and Training

This section details the development and training of the market prediction model. It covers the visualization and analysis of market regimes within the dynamic plane representation, the model's architecture, and considerations for interpretability and usability. Finally, it addresses the incorporation of error signal mechanisms and loss function design.

### A. Market Regime Simulation and Visualization

Three distinct market regimes—Trend-Reversal-Recovery, Choppy Sideways, and Strong Uptrend/Sharp V-shaped Recovery—will be simulated and visualized within the dynamic plane framework. This simulation will provide diverse market conditions for analysis.

### B. Comparative Visualization and Analysis

Standard Heiken-Ashi charts will be visually compared alongside their corresponding dynamic plane representations for each simulated market regime. This comparison, presented in a combined visualization panel for all three regimes, will highlight how each method represents these distinct market conditions, offering insights into the potential advantages of the dynamic plane approach. The visualization process will involve manually replotting both chart types onto a subplot grid. A thorough analysis will then be conducted on how the dynamic plane represents these regimes, investigating how the transformations affect visual representation and exploring implications for model training and performance. These findings will be summarized for inclusion in the dissertation.

### C. Model Architecture

The model architecture needs to handle the dynamic nature of the input data and the complexities introduced by the dynamic plane implementation. Key considerations include:

- **Relational Model Design:** The model should prioritize learning relationships and structures between data points within candlestick chart images, focusing on recognizing patterns within the dynamic coordinate system generated by Principal Component Analysis (PCA) rather than fixed interpretations of price, time, and volume.
- **Geometric Pattern Recognition:** The model should identify geometric shapes and flows within the normalized PCA space, deviating from traditional technical indicators.
- **PCA's Impact on Feature Representation:** The impact of PCA on the representation of numerical features (Price, Time, and Volume) needs careful investigation, particularly how these representations vary based on input patterns and whether the model's focus shifts between emphasizing different feature combinations.
- **Window Size and Smoothing:** Careful tuning of the PCA window size and smoothing/stability thresholds is crucial to avoid excessive, noise-driven rotations and potential overfitting, especially in noisy market environments.

A key architectural decision involves shifting the model's learning from absolute price perception to a relational understanding of price movements within the dynamic PCA space. For example, instead of learning "If price rises over time," the model should learn "If the dominant movement along the local principal axis shows rising oscillations." This shift requires careful design of the loss function and consideration of how the model interprets movements within this dynamic space.

### D. Interpretability and Usability

The dynamic axis rotation inherent in the model's design obscures the direct economic interpretation of the model's activations. An "Interpretability Projection" mechanism is needed to project the model's focus back into the original Time-Price-Volume space, enabling human understanding and analysis of the model's decision-making process.

### E. Loss Function and PCA Stability

The loss function will be tailored for the dynamic PCA space, encouraging the model to focus on meaningful movement patterns. Techniques to stabilize the dynamic PCA axes will be implemented to prevent the model from being confused by jittery rotations caused by noise or small window sizes.

### F. Error Signal Incorporation

An "Error Signal" component, analogous to biological error correction mechanisms, will be investigated and implemented. This component needs to account for increasing uncertainty and delayed feedback when predicting longer-term market movements.

## II. Model Development and Training

This section details the development and training of the predictive model, incorporating mechanisms for error feedback and dynamic plane adjustments inspired by biological processes. The core concept is to implement a rolling frame correction, mimicking wound healing, to address issues like plateaus in dual records and frame coincidence. Furthermore, the model will integrate several error signal mechanisms to enhance its adaptability and robustness.

### A. Model Architecture

This subsection details the architectural enhancements related to error signals and dynamic plane adjustments.

- **Dynamic Rotating Plane Enhancements:** The dynamic rotating plane algorithm will be augmented with the following error signal mechanisms:

  - **Market Movement Error Signal:** An error signal, derived from the difference between predicted and actual market movements, will adjust the model's internal market representation. This mirrors the human brain's error correction mechanisms (Chat1.json message #112).

  - **Frame Confidence Correction:** Discrepancies between predicted and actual market movements will trigger adjustments to the rotational frame assumptions. This iterative correction process refines the model's understanding of dynamic plane projections (Chat1.json message #112).

  - **Prediction Error Memory:** A rolling memory of recent prediction errors across dynamic frames will influence the rotation's weight. Consistent misalignments between the PCA frame and the market structure will lead to adjustments, improving the dynamic plane's stability based on historical performance (Chat1.json message #112).

  - **Feedback-Driven Frame Smoothing:** During periods of high prediction error, the rotation speed will be temporarily reduced to increase smoothing and mitigate the impact of market volatility (Chat1.json message #112).

  - **Dual-Frame Estimation:** Two overlapping frames—"optimistic" (immediate rotation) and "stable" (slow smoothing)—will be maintained. Predictions will be dynamically weighted between these frames based on observed market consistency, combining rapid updates with stability (Chat1.json message #112).

- **Rolling Frame Correction Algorithm:** This algorithm, inspired by the biological process of wound healing, incorporates the following components:

  - **Prediction Error Buffer:** A rolling buffer (optimal size determined experimentally, e.g., 5-10 steps) will store recent prediction errors, calculated as the difference between the signed prediction and the realized movement within the rotated frame.

  - **Error Trend Detector:** This component analyzes the rolling mean and variance of errors in the buffer. A threshold (e.g., 1-2x the rolling standard deviation) will trigger frame correction.

  - **Frame Coincidence Correction with Rolling Rewiring:** Upon significant error deviation, adjustments (e.g., small rotations or damping) will be applied to the dynamic PCA frame. A rolling rewiring mechanism will gradually revert the frame to its normal state as errors subside.

- **Integration with Existing Components:**

  - **Dynamic Plane Generator Integration:** The rolling frame correction algorithm will be integrated into the existing `DynamicPlaneGenerator` class as a modular function.
  - **Lightweight Prediction-Error Feedback:** This will be integrated into the training process, potentially using auxiliary loss functions or by monitoring frame stability (ref: Implement Lightweight Prediction-Error Feedback).
  - **Removal of Static Error Value in PCA:** Removing the static error value in PCA calculations will improve the accuracy of dynamic plane adjustments (ref: Remove static error value in PCA).

### B. Training and Validation

The validation process will be crucial for evaluating the effectiveness of the dynamic plane adjustments and error signal mechanisms. A sketch of the error-signal augmented dynamic plane algorithm (ref: Sketch Error-Signal Augmented Dynamic Plane Algorithm) will guide the implementation of the error feedback loop during training. A lagging rotation deactivation strategy (ref: Lagging Rotation Deactivation Strategy) will be determined to optimize the balance between responsiveness and stability. Simulating a "peripersonal vs. extrapersonal" gap within backtesting results (ref: Simulate Peripersonal vs. Extrapersonal Gap) will help define the model's reaction to errors of varying magnitudes.

## I. Dynamic Plane Management

This section details the management and maintenance of the dynamic 2D plane, focusing on frame correction and stability.

### C. Dynamic Plane Implementation (Specialization)

This subsection details the implementation of rolling frame correction within the dynamic 2D plane. A robust frame correction mechanism is crucial for maintaining the stability and reliability of the dynamic plane representation.

- **Frame Intervention Metric:** A soft evaluation metric will be developed to monitor the frequency of frame interventions over a trading year. This metric will quantify how often the correction algorithm adjusts the PCA frame, providing insights into the system's fluidity, adaptivity, and overall stability.

- **Frame Correction Action:** When the error trend detector (described in Section II) signals a correction, a small rotation adjustment or damping is applied to the PCA frame. This adjustment involves reweighting the principal axes, shifting away from pure PCA towards greater stability. The magnitude of this adjustment is a critical parameter that will require careful tuning.

- **Healing Phase Logic:** After a correction, a healing phase gradually reduces the correction magnitude as the error returns to an acceptable range. Exponential decay is a potential mechanism for this reduction, with the decay rate being another tunable parameter.

- **Error Visualization:** A visualization will be created to illustrate the "error spike → correction → healing decay" process over a simulated sequence. This will aid in understanding and debugging the correction process and its impact on the dynamic plane.

## II. Model Development and Training

This section details the development and training of the predictive model, encompassing architectural considerations, error detection, handling, and correction within the dynamic 2D plane.

### A. Model Architecture (Specialization)

The model architecture incorporates several key components for accurate prediction and robust error handling:

- **Enhanced Error Trend Detector:** This component utilizes a sophisticated approach to error trend detection, moving beyond simple rolling means and variance calculations. It considers the dynamic interplay of price, time, and volume to capture complex relationships and their influence on prediction errors. This detector also evaluates deviations from expected movement within the dynamic 2D frame, calculating both distance and angular error for a comprehensive understanding of prediction accuracy.

### B. Training and Validation (Enforcer)

The training and validation processes incorporate specific requirements focused on error analysis:

- **Deviation Vector Monitoring:** The model's training monitors deviation vectors in the dynamic 2D plane, calculating the vector difference between predicted and realized movement vectors. This provides a quantifiable measure of prediction accuracy in terms of both direction and magnitude.

- **Angular Error Tracking:** The training process tracks the angular error between predicted and realized vectors within the rotated 2D plane, calculated using the arccosine of the normalized dot product of the vectors. This provides crucial insights into the directional accuracy of the predictions.

### C. Loss Functions and Error Correction

Addressing prediction error and drift is crucial for model robustness. This involves developing specialized loss functions and integrating an error correction module.

- **New Loss Functions:** New loss functions will be implemented to penalize both scalar and angular drift during training, improving the model's ability to maintain accurate predictions over time.

- **Composite Error Score:** A composite error score, combining distance and angular error using tunable weighting factors (alpha and beta), will provide a nuanced evaluation of prediction accuracy.

- **Dynamic Rolling Error Correction Module:** The enhanced Error Trend Detector will be expanded into a dynamic rolling error correction module. This module will actively adjust predictions based on detected error trends, enhancing model stability and accuracy.

### D. Visualization and Simulation

Visualizations will be used to gain deeper insights into error accumulation and the behavior of the model.

- **Visual Simulation of Vectorial Misalignments:** A visual simulation will demonstrate how small vectorial misalignments accumulate differently compared to simple price error, aiding in understanding the impact of angular drift on overall prediction accuracy.

### E. Implementation and Refinement

The pseudocode for the error detection, analysis, and correction components will be refined into robust, production-ready code, ensuring efficient implementation within the model architecture. A key aspect of the model's architecture is the dynamic 2D plane representation. The number of rotational angles and distance vectors used in this representation will be documented and validated against the intended design. Any discrepancies will be addressed through corrective actions in either the implementation or the documentation.

## II. Model Development and Training

This section details the development and training of the predictive models, focusing on the implementation of a dynamic Principal Component Analysis (PCA) frame of reference and ensuring accurate alignment of predicted and realized price movements. A key consideration is maintaining consistency within the short-term prediction timeframe (1-5 candlesticks). Given this short horizon, structural market drift is assumed negligible compared to potential prediction errors. Consequently, continuous recalculation of the dynamic frame for every price micro-movement is unnecessary unless operating in extremely volatile, high-frequency trading scenarios.

The model architecture incorporates this dynamic PCA frame and mechanisms for error correction and movement projection. The training and validation process utilizes these mechanisms to refine the model's predictive accuracy. Key aspects are outlined below:

### A. Model Architecture

The following points address specific architectural choices related to the dynamic PCA frame:

- **Dynamic Frame Freezing:** The dynamic PCA frame, computed at time 't', is frozen for predicting movements over a short future horizon (e.g., 1-5 candlesticks). This assumption of relative market structure stability over this short interval simplifies the prediction and evaluation process. By using a consistent frame, we avoid recomputing PCA at 't+1' for error calculation. This is referred to as the "Freeze Frame" method.

- **Realized Movement Reprojection:** The realized movement at 't+1' is reprojected back into the PCA frame established at time 't' using the original PCA basis (rotation matrix). This "Reproject Realization" method allows direct comparison between predicted and realized movements within a consistent coordinate system. This approach is crucial because the PCA frame at 't+1' might differ due to market dynamics.

- **PCA Plane Consistency:** A critical investigation will ensure the consistency of the 2D plane derived from PCA between the prediction and realized data matrices. Since price and volume values differ between these matrices, potentially leading to different PCA axes, a transformation or alignment step might be necessary. This investigation will determine if and how such adjustments are needed.

### B. Training and Validation

The following elements are crucial for training and validating the model with the dynamic PCA frame:

- **Error Metrics:** Two primary error metrics are used:

  - **Distance Error:** The magnitude difference between the predicted and realized displacement vectors within the dynamic plane. This quantifies the error in the magnitude of the predicted movement.
  - **Angle Error:** The orientation difference between the predicted and realized direction vectors within the dynamic plane. This quantifies the error in the direction of the predicted movement. Note that the initial frame creation rotation (global frame transformation via PCA) is not considered in these error calculations, as the focus is on the _relative_ movement within the established dynamic plane.

- **"Freeze and Correct" Module:** Robust pseudocode and implementation of a "Freeze and Correct" module will incorporate the Freeze Frame and Reproject Realization methods. This module will manage the application of the dynamic PCA frame and error calculations.

- **Lightweight Frame Adjustment:** Small, corrective adjustments to the frame are proposed when accumulated angular errors exceed a predefined threshold. This aims to maintain frame accuracy while minimizing computational overhead.

- **Visualizations and Simulations:** Visualizations and simulations will be created to illustrate the dynamic frame mechanism, including the Freeze Frame and Reproject Realization methods, and the error correction process. These will clarify the approach and aid in verifying correct implementation. A diagram illustrating the two layers of rotation – global frame transformation (PCA) and local vector misalignment – and how the prediction error relates to them will be included. Furthermore, visualizations demonstrating the "freeze frame" and "projection" functionalities, contrasting them with traditional distance/angular error calculations based on a static PCA plane, will be provided.

- **Efficient Data Structures:** A lightweight data structure will be designed for efficient storage and retrieval of the PCA basis (rotation matrix) for each candlestick window. This is crucial for the Reproject Realization method.

## II. Model Development and Training

This section details the model's development and training, focusing on handling dynamic PCA frame shifts and associated error calculations for robust predictions. Effectively managing potential shifts in market data distribution is crucial for maintaining relational consistency between predicted and realized data points, preventing model "hallucinations" due to minor market fluctuations. Two methods, "Freeze Frame" and "Reproject Realization," will be evaluated for their ability to address this challenge.

### A. Dynamic Error Calculation

The current error checking mechanism, assuming static PCA planes (PCA1 and PCA2), requires adaptation for dynamic shifts. Instead of calculating the error between vectors on the static PCA plane, the deviation errors between predicted and realized values for both PCA1 and PCA2 will be used. A comprehensive error calculation, encompassing both vector deviation within the dynamic local frame and the frame shift error (change between PCA axes), will be implemented as follows:

- **Total Error = Vector Error + Frame Shift Error**

The Frame Shift Error, representing the difference between two sets of basis vectors (frames), will be measured using the principal angles between the two subspaces. In this 2D context, this involves calculating the angle between PCA1 at time _t_ and PCA1 at time _t+1_, and similarly for PCA2. The Frame Shift Error is calculated as a weighted sum:

- **Frame Shift Error = α _ Angle between PCA1 vectors + β _ Angle between PCA2 vectors**

where α and β are tunable weights controlling the relative importance of each principal angle.

The Vector Error incorporates both the distance and angular deviation between predicted and realized vectors within the current frame:

- **Vector Error = α₁⋅d<sub>vec</sub> + α₂⋅θ<sub>vec</sub>**

where α₁ and α₂ control the relative importance of distance (d<sub>vec</sub>) and angle (θ<sub>vec</sub>).

### B. Weighted Error Integration and Confidence Indication

A key aspect of model development is implementing a comprehensive, weighted error calculation system. This system quantifies discrepancies between predicted and actual market movements, incorporating both vector deviations and PCA frame drift, and is integral to the model's learning and decision-making.

The model will employ a weighted total error calculation:

- **Total Error = γ₁⋅Vector Error + γ₂⋅Frame Shift Error**

where γ₁ and γ₂ control the overall trade-off between prediction error and frame instability. This weighted approach allows granular control over the influence of each error type (distance, angle, PCA1 drift, and PCA2 drift) through the parameters α₁, α₂, β₁, β₂, γ₁, and γ₂. This facilitates fine-tuning to prioritize prediction accuracy and frame stability.

Furthermore, the Frame Shift Error itself can be a valuable confidence indicator. Higher frame drift may signal lower confidence in predicted market movements, potentially suggesting a hold strategy instead of executing a trade, enhancing the model's decision-making beyond simple buy/sell signals.

### C. Simulation, Validation, and Normalization

To ensure accurate aggregation of distance and angular errors, given their different units and scales, normalization or another suitable adjustment method will be explored. The optimal weighting strategy for these error components will also be investigated.

A small-scale simulation will be implemented to demonstrate vector deviation and PCA frame drift, facilitating analysis and validation of the error calculation mechanisms. This will ensure the model correctly interprets and responds to these critical factors. Pseudocode for the multi-weight error computation, including normalization of angle units (mapping degrees to the range [0, 1]), will be provided. This normalization facilitates cleaner aggregation with other error components.

## II. Model Development and Training

This section details the development and training of the model, focusing on the implementation of a "Healing-by-Correctness" system. This system dynamically adjusts the model's behavior based on its recent predictive accuracy, enabling a more robust and adaptive trading strategy.

### A. Model Architecture (Specialization)

The model architecture incorporates a rolling error buffer, dynamic correction mechanisms, and a performance-based healing system. These components work together to monitor prediction errors, trigger corrective actions, and adapt the model's responsiveness to market conditions.

- **Rolling Error Buffer and Statistics:** A rolling buffer will store the total error accumulated over a defined number of recent time steps (e.g., 5-10). Rolling statistics, specifically the mean and standard deviation of these errors, will be calculated to provide a dynamic measure of model performance. These statistics are used to trigger the "Wound Phase" when performance deteriorates significantly.

- **Wound and Healing Phases:** A "Wound Phase" is activated when the mean error exceeds a threshold, typically a multiple (e.g., k=2) of the rolling standard deviation. This indicates a period of poor performance requiring corrective action. A "Healing Phase" begins when the mean error drops below a lower threshold, signaling a return to acceptable performance.

- **Correction Factor and Dynamic Adjustment:** During the "Wound Phase," a correction factor is applied to the model's predictions. This correction could involve adjustments to parameters like PCA rotation or smoothing factors. The correction factor dynamically decays as performance improves during the "Healing Phase," allowing a smooth transition back to normal operation. The model can re-enter the "Wound Phase" if errors spike again.

- **Prediction Correctness Tracking and Buffer:** The model tracks the correctness of its predictions, assigning a score of +1 for correct predictions and 0 otherwise. Correctness is determined by comparing the predicted and actual market movement direction and magnitude. These scores are stored in a rolling buffer (similar to the error buffer) to track recent predictive accuracy.

- **Performance-Based Healing (Healing-by-Correctness):** The decay rate of the correction factor is dynamically adjusted based on the mean prediction correctness from the rolling buffer. Higher accuracy accelerates the decay, allowing the model to respond more quickly to current market trends. Conversely, lower accuracy slows the decay, maintaining a stronger correction until performance consistently improves. This "Healing-by-Correctness" mechanism ensures the model adapts its behavior based on actual performance, rather than relying on arbitrary time-based healing.

- **Visualization and Modular Implementation:** Visualizations will be created to monitor the system's behavior and the impact of the healing phases on model predictions. The entire error detection and healing system will be implemented as a clean, modular component to ensure maintainability and facilitate future enhancements. This structured approach improves the model's resilience and adaptability to changing market conditions. Further research and experimentation will optimize the system's parameters and effectiveness.

## I. Model Enhancement and Refinement

This section details enhancements to improve the model's long-term performance and adaptability, specifically focusing on a dynamic healing mechanism.

### A. Dynamic Healing by Correctness

The model is augmented with a dynamic healing mechanism that adjusts based on its prediction accuracy. This mechanism aims to improve long-term performance by allowing the model to recover from periods of reduced accuracy.

- **Healing Logic Update:** The existing healing logic will be modified to use a dynamically adjusted correction factor. Instead of a fixed value, this factor will be based on the mean prediction correctness over a rolling window of the last _N_ steps. Higher rolling prediction correctness (e.g., ≥ 80%) will proportionally reduce the correction factor, allowing the model to rely more on its own predictions. Conversely, if correctness deteriorates, the correction factor will be held steady or increased, enabling larger corrections to steer the model back on track.

- **Dynamic Decay Rate Function:** The decay rate of the correction factor will be calculated by a function, `dynamic_decay_rate(mean_correctness)`, and tied to the mean prediction correctness. A suggested implementation is: `Decay Rate = 1 - (mean_correctness - healing_threshold)`. This ensures faster decay when mean correctness significantly exceeds the healing threshold and slower decay when correctness is only marginally above it.

- **Healing Thresholds:** Initial healing thresholds will be set within a reasonable trading context. For example, a threshold of 75-80% directional correctness could be a starting point, representing a level of accuracy where the model can be considered reliable.

- **Pseudocode Implementation:** Formal, modular pseudocode will be written for the complete Healing-by-Correctness system, detailing the interaction between prediction correctness tracking, dynamic decay rate calculation, and correction factor application.

- **Toy Example Simulation:** A toy example will be simulated to demonstrate the entire healing mechanism flow: from the initial reduction in predictive accuracy, through the application of corrections, to ultimately regaining predictive accuracy. This serves as a proof of concept and aids in understanding the system's dynamics.

This dynamic healing approach aims to create a more robust and adaptable model, capable of maintaining performance over extended periods and diverse market conditions.

## II. Model Development and Training

This section details the development and training of the predictive models, focusing on data preprocessing, representation, and transformations to improve performance and stability.

### A. Data Preprocessing for PCA

Price (P), time (T), and volume (V) data within a rolling window of _N_ data points undergo preprocessing before Principal Component Analysis (PCA) to ensure features contribute equally and prevent scaling issues. This involves:

1. **Normalization:** Each feature (P, T, and V) is independently centered and scaled using z-score normalization. For each time window, the mean (𝜇) and standard deviation (𝜎) are calculated for each feature:

   $\\mu_t = \\frac{1}{N}\\sum_i t_i,\\quad \\sigma_t = \\sqrt{\\frac{1}{N}\\sum_i(t_i-\\mu_t)^2}$

   (and similarly $\\mu_p,\\sigma_p$ ; $\\mu_v,\\sigma_v$). The scaled features are then calculated as:

   $X_{\\text{scaled}}[,i,.,.] = \\Bigl(\\tfrac{t_i-\\mu_t}{\\sigma_t}, \\tfrac{p_i-\\mu_p}{\\sigma_p}, \\tfrac{v_i-\\mu_v}{\\sigma_v}\\Bigr).$

2. **Time Handling:** Timestamps are handled using one of two methods:

   - **Relative Time Index:** Within each window, a sequence of integers (1, 2, ..., _N_) represents the relative time, which is also z-score normalized.
   - **Absolute Clock Time:** Time deltas are calculated as $\\Delta t_i = \\frac{\\text{timestamp}_i - \\mu_t}{\\sigma_t}$ to capture diurnal patterns. Large gaps in timestamps can inflate $\\sigma_t$ and require careful consideration.

3. **Volume Transformation:** Due to the heavy-tailed distribution of volume data, a transformation is applied before z-score normalization:
   - **Log Transformation:** $v_i' = \\log(1 + v_i)$ compresses outliers.
   - **Robust Scaling:** The median and Interquartile Range (IQR) are used for scaling. Each volume data point has the median volume subtracted and is then divided by the IQR.

### B. PCA Implementation

After preprocessing, PCA is performed on the scaled data matrix $X_{scaled}$ using Singular Value Decomposition (SVD):

```python
u, s, vh = np.linalg.svd(X_scaled, full_matrices=False)
axes = vh[:2]   # two principal directions in T-P-V space
```

This implementation utilizes the first two principal components.

### C. Model Architecture

The model architecture incorporates the following data transformations:

- **Transform Price into Relative Returns:** The model uses relative returns (percentage change or log returns) relative to the first price in the window. This anchors the price to zero at the start of each window, mitigating the impact of extreme price spikes and focusing on price movement within the window.

- **Encode Time as Fractional Elapsed Time:** Time within each window is encoded as a fractional value between 0 and 1, calculated as elapsed time divided by the total time span of the window. This preserves chronological order while addressing issues arising from linear indexing with unevenly spaced time intervals.

### D. Training and Validation

The training and validation process incorporates the data transformations and addresses potential issues with unseen values:

- **Handle Unseen Values for Price and Volume:** The model's input pipeline must handle unseen price and volume values robustly. This prevents record highs or lows from disproportionately impacting predictions. A robust scaler, less sensitive to outliers, or techniques like percentile clipping can be employed for dynamic scaling.

### E. Dynamic Plane Implementation and Model Refinement

While the dynamic plane implementation may not be directly affected by these transformations, it receives the normalized and transformed data as input. Model enhancements and refinements will consider these preprocessing steps. The potential use of a live feed of Last Traded Prices (LTPs) for time tracking is also under consideration.

## II. Model Development and Training

This section details the development and training process of the model, encompassing data preprocessing, feature engineering, and dimensionality reduction using Principal Component Analysis (PCA).

### A. Data Preprocessing

Before model training, the raw price, volume, and time data undergo several crucial preprocessing steps to enhance model robustness and performance. These transformations ensure data consistency, mitigate the impact of outliers, and prepare the data for subsequent feature extraction.

1. **Volume Transformation:** Raw volume data is log-transformed using `np.log1p(volume)` to handle the typically wide range of values. Subsequently, extreme outliers are clipped at the 5th and 95th percentiles to further reduce the influence of potentially spurious extreme values.

2. **Price Transformation:** Price data is transformed into log returns relative to the opening price of the respective time window. Similar to volume, extreme outliers in these log returns are clipped at the 5th and 95th percentiles.

3. **Time Representation:** Timestamps within each window are converted to fractional elapsed time (`time_frac`). This is calculated by subtracting the minimum timestamp from each timestamp and then dividing by the total time elapsed within the window (the difference between the maximum and minimum timestamps).

4. **Min-Max Scaling:** Following the transformations above, `time_frac`, log return of price, and log-transformed volume are each min-max scaled to the [-1, 1] range. Critically, this scaling uses the minimum and maximum values observed across the entire dataset, ensuring consistent feature scaling and preventing features with larger magnitudes from dominating the model training process.

### B. Feature Extraction: Principal Component Analysis (PCA)

After preprocessing, the transformed time, price, and volume data are combined into a single matrix. Principal Component Analysis (PCA) is then applied to this matrix for dimensionality reduction. Given the symmetric [-1, 1] scaling applied in the preprocessing step, centering the matrix before PCA is generally unnecessary as the data is already approximately zero-mean. PCA identifies the principal components that capture the most significant variance within the data, reducing dimensionality while retaining essential information about the relationships between time, price, and volume. These principal components serve as input features for the prediction model.

## II. Model Development and Training

This section details the development and training of the CNN and ViT models, focusing on data transformation and image generation for model input. The core concept involves transforming candlestick and volume data into a sequence of dynamic plane snapshots suitable for these models.

### A. Data Preparation and Visualization

The following steps outline the data preparation and visualization process:

1. **Candlestick Chart Generation:** Candlestick charts with accompanying volume data are generated for five distinct market patterns: uptrend with rising volume, downtrend with volume spikes, reversal (down then up), sideways chop, and breakout spike then stabilize. These charts use 10-minute intervals for a granular view of intraday price action.

2. **Data Transformation:** The candlestick and volume data undergo a transformation involving log returns, Principal Component Analysis (PCA) rotation, and normalization. Specifically, time, log return, and log volume are normalized to the range [-1, 1] before applying PCA rotation to create a 2D representation. This “dynamic plane” transformation aims to capture essential price and volume movement features in a format readily interpretable by the models.

3. **Dynamic Plane Snapshot Generation:** For each market pattern, a sequence of five candlestick charts and their corresponding transformed dynamic plane snapshots are generated. This sequence provides temporal context for the models.

4. **Image Output and Visualization:** The generated image pairs (original candlestick chart and transformed dynamic plane snapshot) are displayed individually for clear comparison and analysis. This approach avoids information loss from combining images and facilitates a better understanding of the transformation's impact. Visualizations focus on the following:

   - **Transformation Impact:** Comparing pre- and post-transformation images allows for a visual inspection of the effects of the dynamic plane transformation. This helps validate the transformation process and identify potential issues.
   - **Volume in Transformed Images:** An investigation will be conducted to determine the informativeness of volume bars in the transformed dynamic plane snapshots. The sufficiency of the transformed data points alone will be evaluated.
   - **Breakout Spike then Stabilize Pattern:** Five variations of the "Breakout Spike then Stabilize" pattern, each with a unique volume profile, are visualized to explore the impact of volume on the transformed representation.
   - **PCA Pattern Analysis:** Five distinct examples of the "Breakout Spike then Stabilize" pattern are analyzed using PCA. Data volumes are varied across these examples to assess visual similarities and differences in the resulting PCA patterns, focusing on pre-spike clustering, the spike's trajectory in PC2, and post-spike stabilization.

This multifaceted approach to visualizing and analyzing candlestick data, including volume and PCA transformations, provides a robust foundation for model development and training. The insights gained inform subsequent model architecture choices and feature engineering strategies.

## II. Model Development and Training

This section details the development and training of the algorithmic trading bot, Swaha, designed for the Indian equities market. Swaha will assist in its own development by leveraging provided documentation (`Swaha.md`) and user interaction with Anhad. This iterative development process means architectural specifics will emerge through testing and refinement, but the core focus remains algorithmic trading within the Indian equities market. Swaha's persona and functionality will be continuously refined based on Anhad's feedback, the evolving project requirements documented in `Swaha.md`, and actual market performance. Swaha will introduce itself and interact with Anhad according to its defined persona within its documentation.

### A. Model Architecture (Specialization)

The core of Swaha's predictive capabilities lies in a vision transformer (ViT) model that analyzes dynamically generated images of market data. This process involves several key steps:

1. **Data Preprocessing and Normalization:** Time, price (using log-returns), and volume (log-transformed and robustly scaled) are normalized to a uniform range of [-1, +1]. This ensures balanced feature influence, preserves chronological order, and maintains comparability.

2. **Dynamic Frame Construction using PCA:** Principal Component Analysis (PCA) is applied to the normalized time, price, and volume data within a rolling window. The two principal components (PC1 and PC2) define a dynamic 2D plane, capturing the primary axes of correlated market movement.

3. **Projection, Rotation, and Refocusing:** Recent market action is projected onto this dynamic 2D plane. The viewpoint is then re-centered, placing the last data point at the origin (0,0) to emphasize the most recent market dynamics.

4. **Image Generation:** This transformed 2D representation is rendered as an image, potentially using candlestick or Heiken-Ashi charts, to serve as input for the ViT model.

5. **Prediction and Output:** The ViT model predicts a 2D movement vector (Δx', Δy') within the dynamic plane, representing the anticipated trajectory of market movement. It also predicts "Rally Time," estimating the duration for this predicted movement.

6. **Self-Correction and "Healing" Mechanism:** A feedback loop incorporating a Total Error signal ensures model adaptability. This signal comprises:

   - **Vector Deviation Error:** The distance and angular difference between predicted and actual movement vectors.
   - **Frame Shift Error:** The discrepancy between the predicted and actual shift in the dynamic plane.

   When the Total Error exceeds a dynamic threshold (potentially based on a multiple of the rolling standard deviation), a "Wound Detection" triggers a "Healing Phase." This phase gradually restores full dynamism to the model based on its regained predictive accuracy, allowing it to adapt to evolving market conditions and recover from periods of poor performance.

7. **Multi-Scale Temporal Information and After-Market Forces:** Future development will incorporate multi-scale temporal modeling and consider weighted periodicity to capture broader market influences beyond the immediate data window. This will enhance Swaha’s ability to interpret longer-term trends and after-market effects.

The primary objective is to develop a deployable algorithmic trading bot. This will be achieved by integrating the trained ViT model, along with its dynamic image generation and self-correction mechanisms, into a fully functional trading system. Regular evaluation and refinement based on live market data will be crucial for optimizing Swaha's performance and ensuring its alignment with the evolving requirements of the Indian equities market.

## II. Model Development and Training

This section details the architecture, training process, and data pipeline of the stock price prediction model. The model incorporates a multi-timeframe approach using a Vision Transformer (ViT) with a hierarchical attention mechanism. This architecture allows the model to dynamically weigh information from different timeframes (10-minute, daily, weekly, monthly, quarterly, and yearly) to enhance predictive accuracy. A self-correcting mechanism based on prediction error further refines model stability.

### A. Model Architecture

The core predictive engine is a Vision Transformer (ViT) that processes data derived from the Dynamic Rotating Plane method across multiple timeframes.

- **Dynamic Rotating Plane:** For each timeframe, a dynamic 2D plane is constructed using Principal Component Analysis (PCA) applied to normalized Time, Price, and Volume data. This plane, re-centered on the latest data point, forms the basis for generating input representations. This approach captures the local market dynamics within each timeframe.
- **Multi-Timeframe Input:** The ViT receives input from all timeframes simultaneously. These inputs, generated from the dynamic rotating planes, are processed as a sequence of tokens, allowing the ViT to learn dependencies between different timescales.
- **Non-Hierarchical Attention Mechanism:** A hierarchical attention mechanism dynamically queries and weighs information from the different timeframes based on the intraday context. This allows the model to focus on the most relevant time scales for a given prediction.
- **Self-Correcting Mechanism:** A self-correcting mechanism monitors a "Total Error" signal, combining "Vector Deviation Error" and "Frame Shift Error". When the "Total Error" spikes, a "correction mode" dampens the rotation of the dynamic planes. This dampening allows the model to self-correct and stabilize as prediction accuracy improves.

### B. Data Pipeline and Preprocessing

The data pipeline handles the integration and preprocessing of multi-timeframe data.

- **Multi-Timeframe Data Integration:** The pipeline integrates data from various timeframes (10-minute, daily, weekly, monthly, quarterly, and yearly). Each timeframe dataset is prepared using the Dynamic Rotating Plane method. Synchronization, format consistency, and efficient loading procedures are crucial considerations.
- **Data Normalization:** Time, Price, and Volume are normalized before being fed into the model. Time is represented as fractional elapsed time. Price is normalized using log-returns. Volume is log-transformed and then robustly scaled. All three features are then scaled to a uniform range of [-1, +1]. This ensures consistent input data and prevents features with larger magnitudes from dominating the learning process.

### C. Training and Validation

The training and validation process involves creating multi-scale datasets and evaluating the model's performance.

- **Dataset Generation:** Datasets are generated for each timeframe using the Dynamic Rotating Plane method. These datasets provide a set of context images for every prediction point, enabling the model to learn from different temporal perspectives.
- **Model Evaluation:** Model performance is evaluated against a baseline intraday model. Attribution analysis will be conducted to understand the model's reliance on different timeframes. Performance will also be analyzed during major market events to assess robustness in volatile conditions.

### D. Model Deployment and Maintenance

This section outlines procedures for model deployment within the Swaha trading application, including initial training, backtesting, live trading, model revisions, and trade ledger reporting. A robust and adaptable model is crucial for effective application functionality. Further details on this aspect will be provided in a subsequent section.

## II. Model Development and Training

This section details the development, architecture, training, and refinement of the predictive models used to generate trading signals. The design emphasizes flexibility and iterative development, allowing for extensive user customization and ongoing model enhancement.

### A. Model Architecture

The model architecture is configurable through user-defined settings, enabling exploration of different model types and training strategies. Key configurable elements include:

- **Data and Timeframe Configuration:** Users specify the asset universe (e.g., NIFTY 50, NIFTY 500, Custom Watchlist) and the date range for the experiment. The specified date range is automatically partitioned into training and validation sets. A flexible lookahead period (e.g., 1, 3, 5, or 10 candlesticks) allows training for predictions over various time horizons.

- **Dynamic Plane Configuration:** Users control the model's input representation by specifying the candlestick type, local window size, and included features (price, time, volume). This allows experimentation with different representations of market data.

- **Model and Learning Architecture Configuration:** Users select from various model architectures and control hyperparameters, optimizers (e.g., Adam, SGD, AdamW, with configurable learning rates, weight decay, and schedulers), and loss functions. For Vision Transformer (ViT) models, users can configure parameters such as patch size, embedding dimensions, transformer layers, attention heads, and dropout rate. Multi-scale context fusion methods (attention-based, concatenation, weighted average) are also available.

- **Self-Correction System Configuration:** A self-correction mechanism allows adjustment of the wound detection threshold (sensitivity to frame instability) and the healing trigger (prediction accuracy threshold for self-correction).

### B. Training and Validation

Real-time performance metrics are provided during training and backtesting, including training/validation loss charts, equity curves, benchmark comparisons, and key performance indicators. This facilitates effective monitoring and evaluation.

### C. Live Visualization

A live visualization feature provides a side-by-side comparison of the standard Heiken-Ashi chart and the dynamically rotated and re-centered 2D plane image fed into the model. This view updates with each new candlestick, offering real-time insight into the model's input data.

### D. Future Enhancements

Future development will focus on enhancing model and learning architecture options to provide greater flexibility and address a wider range of use cases. The UI for the Training & Backtesting Module will be redesigned with a two-column layout incorporating more detailed controls and monitoring tools. This includes adding information currently missing from the Model & Learning Architecture display to improve usability and user understanding.

## II. Model Development and Training

This section details the development and training of the predictive models, incorporating mechanisms for multi-scale context fusion, performance monitoring, and dynamic data handling.

### A. Model Architecture

This subsection outlines the architectural enhancements for incorporating multi-scale context, specialized data inputs, and context-aware features.

- **Multi-Scale Context Fusion:** To enhance predictive accuracy, the model will fuse information from different time scales. Users can select from various fusion methods:

  - **Attention-Based (ViT):** Leveraging the attention mechanism of a Vision Transformer architecture.
  - **Concatenation:** Combining feature representations from different time scales.
  - **Weighted Average:** Averaging feature representations, weighted by their respective time scales.

- **Input Candles based on Categories:** The model will utilize candlestick data categorized by market capitalization, sectors, and share price bins. This allows specialized training and potentially improved performance within specific market segments.

- **Context Awareness:** The model will dynamically adapt to market conditions. The optimal number of candlesticks per frame and the total number of frames will be determined experimentally. Furthermore, predictions will be weighted based on various periodicities (daily, weekly, monthly, quarterly, and yearly), and the optimal configuration of frames, candles per frame, and stock categorization will be determined for each.

- **Transfer Learning:** The efficacy of transfer learning will be evaluated across different markets (US-US, US-India, and India-India) to potentially address data scarcity and improve generalization.

- **PCA Analysis:** The entire model development and training process will be repeated using a 2-dimensional dynamic plane derived from Principal Component Analysis (PCA) to explore dimensionality reduction for potential performance gains.

### B. Training and Validation

This subsection details the training process, including performance monitoring, healing mechanisms, and hyperparameter tuning.

- **Performance-Based Healing Trigger:** A healing mechanism will be triggered if the model's prediction accuracy falls below a user-defined threshold.

- **Time-Based Healing Trigger:** An additional healing mechanism will activate if the error remains above a specified threshold for a defined duration, both of which are configurable hyperparameters.

- **Total Error Metric:** A comprehensive "Total Error" metric combining Vector Error (difference between predicted and actual vector representations) and Frame Shift Error (misalignment between predicted and actual time frames) will be used. Both components will have adjustable weights.

- **Hyperparameter Permutation Testing:** A "try all permutations" approach, training the model for a single epoch with every possible combination of pre-defined hyperparameter values, will be used to establish baseline performance and guide subsequent hyperparameter optimization.

### C. Data Handling

This subsection outlines the data acquisition strategy.

- **Expanded Data Acquisition:** Intraday data will be downloaded for all stocks in both India and US markets across all available intervals, providing a comprehensive dataset for training and evaluation.

- **Dynamic Capital Allocation:** A dynamic capital allocation strategy will be researched and implemented, utilizing a starting capital and distributing trades based on a probability distribution.

- **Error Detection and Healing:** Research and implementation of dynamic, non-time-based error detection and healing strategies are necessary. These strategies should identify and correct issues during model training and operation without relying on fixed time intervals.

### D. Dynamic Plane Configuration

This subsection details enhancements to the dynamic plane representation.

- **Smooth Frame Rotations:** Smooth frame rotations will be implemented within the Dynamic Plane Configuration, utilizing a user-adjustable smoothing factor (likely an exponential moving average on the rotation matrix) to mitigate jittery frame changes during periods of high market volatility.

## II. Model Development and Training

This section details the development and training of the models within the Swaha project. The project's philosophical foundation, rooted in the four paths of Yoga, influences the model development lifecycle. Specifically, the Gyaan Shala (House of Wisdom) module informs data management and experiment design choices, prioritizing data integrity and the pursuit of objective market analysis. While the technical implementation leverages Python, Progressive Web App (PWA) technology, Firebase, and VS Code with Gemini Code Assist Agent, the overarching goal is to develop models that operate with wisdom, discipline, and detachment, embodying the Swaha philosophy.

### A. Model Architecture

The model architecture is currently under development, with further details to be provided in subsequent documentation. The design is deeply influenced by the four paths to liberation (Gyaan, Bhakti, Karma, and Raja Yoga) from the Bhagavad Gita. How each Yoga influences specific model components will be elaborated upon in future revisions. However, the core principles guiding the architecture are:

- **Data Integrity:** Immutable logs and rigorously cleaned data ensure transparency and accuracy.
- **Balanced Approach:** The system balances profit maximization with overall portfolio stability.
- **Non-predatory Strategies:** The model is designed to avoid exploitative trading practices.

### B. Training Process

The training process is structured around the concept of "Kurukshetra (The Field of Action)," emphasizing skillful action in executing backtests and live trading. This structured approach includes:

- **Experiment Designer:** A visual, node-based or block-based interface (version 1.2) allows for the creation, saving, and management of reusable experiment templates. These templates define the data flow and configurations for each experiment, facilitating systematic variation and tracking.
- **Campaign Runner:** This tool (version 2.1) executes large-scale, automated backtesting campaigns based on saved experiment templates, enabling thorough evaluation of different model configurations and hyperparameters.
- **Live Trading Dashboard:** A real-time dashboard (version 2.2) provides an overview of live trading activities, internal model state, and performance, allowing for continuous monitoring and improvement. The seamless integration between the experimental phase and live trading facilitates iterative model refinement.

### C. Zerodha KiteConnect Integration

The entire architecture is built upon the Zerodha KiteConnect API for:

- **Data Acquisition:** Historical data is downloaded using KiteConnect.
- **Live Data Stream:** A websocket connection via KiteConnect provides live training data.
- **Order and Portfolio Management:** Order generation, portfolio creation, and tracking are managed through KiteConnect.

### D. Dharmic Mandate

Ethical constraints based on Satya (truthfulness), Shaucha (purity), and Santosha (contentment) are integral to the system, ensuring the agent operates within defined ethical boundaries.

## II. Model Development and Training

This section details the development and training process for the stock market prediction models. It covers the chosen model architecture, its technical implementation, the supporting infrastructure for experiment management, and considerations for technical constraints.

### A. Model Architecture

The models primarily use 5-day candlestick chart images as input to Convolutional Neural Networks (CNNs). This approach captures relevant market information encoded in the Open, High, Low, Close, and Volume (OHLCV) data. The process of converting raw OHLCV data into these images will be documented. The specific CNN architecture, including layers, activation functions, and any pre-trained components, will be detailed.

Key architectural components include:

- **Trade Signal Generation:** CNNs generate buy/sell signals by selecting the top and bottom 10% of predictions, respectively. The rationale and implementation of this thresholding process will be explained.
- **Trade Execution Logic:** The logic for executing trades based on the CNN signals will be defined, including how actual prices from t+1 to t+5 are used to evaluate signal effectiveness. Calculations will incorporate slippage and commissions, and these details will be documented.
- **CNN Output Transformation:** The transformation process for the CNN's numerical outputs (e.g., normalization, scaling) will be documented, including mathematical formulas and code snippets.
- **Hybrid Architectures:** Exploration and implementation of hybrid architectures combining CNNs with LSTMs or Transformers will be documented, including the rationale, integration methods, expected benefits, and potential drawbacks of each approach.

The following aspects of the model will be meticulously specified:

- **Input Data Format:** The precise format and size (_n_) of the input data, including data types, dimensions, and any preprocessing steps.
- **Evaluation Metric:** The chosen evaluation metric (e.g., MSE, RMSE) will be stated and justified, including its mathematical formula and interpretation within the project context.
- **Data Exclusions:** Specific data exclusions for the CNN (e.g., dates, real prices, tickers) and the reasons for their exclusion will be clearly stated.
- **Training Process:** The CNN training process, including training parameters, optimization algorithms, data augmentation techniques, libraries, frameworks, and hyperparameter ranges, will be thoroughly documented.

### B. Experiment Management and Execution Infrastructure

To streamline the model development and training workflow, the following infrastructure components will be implemented:

- **Experiment Designer Canvas UI:** A Flutter-based UI will allow visual design of experiments, enabling configuration of different pipeline stages and parameter permutation ranges.
- **Campaign Runner UI:** Another Flutter-based UI will manage experiment campaigns, enabling selection of experiment templates, automatic permutation calculation, choice of execution modes (sequential, parallel), and job queue monitoring via Firestore integration.
- **CampaignOrchestrator Backend:** A Cloud Function will orchestrate experiment campaigns, generating hyperparameter permutations, managing tasks via Google Cloud Tasks, and integrating with Firestore for state tracking.
- **ExperimentRunner Backend:** A Cloud Run service will execute individual experiments, managing data fetching, model initialization, training, backtesting, performance metric calculation, and result storage in Firestore.

### C. Addressing Technical Constraints

- **Kite Connect API Rate Limit:** The data ingestion subsystem will respect the Kite Connect API's rate limit of 10 requests per second by implementing appropriate delays (e.g., `time.sleep(0.1)`) between API calls.

## II. Model Development and Training

This section details the model development and training process, focusing on a distributed approach leveraging the computational capabilities of iPads while maintaining a central server for coordination and aggregation. This architecture aims to minimize server costs associated with image generation and processing while providing a responsive and efficient training process.

### A. Model Architecture and Workflow

The core of the model development revolves around a distributed architecture, with iPads handling client-side processing and a central server managing the overall training process. This approach offers several advantages, including reduced server load, efficient parallel processing, and enhanced user experience.

- **Client-Side (iPad):** iPads play a crucial role in data preparation and model training:

  1. **Data Retrieval and Image Generation:** iPads fetch raw market data from the server and generate candlestick chart images using JavaScript libraries (e.g., Canvas API, a suitable JavaScript graphics library). This pre-processing step converts the raw data into a visual format suitable for the model.
  2. **On-Device Model Training:** TensorFlow.js facilitates local model training on the iPad, leveraging its GPU for performance. Web Workers ensure responsive UI performance by handling training in the background.

- **Server-Side:** The central server orchestrates the training process and maintains the global model:

  1. **Data Provisioning:** The server provides raw market data to the iPads.
  2. **Model Aggregation and Distribution:** The server aggregates model updates received from each iPad, incorporating them into a global model. This global model is then redistributed to the iPads for the next training iteration.

- **Federated Learning Framework:** The interaction between iPads and the server adheres to a federated learning paradigm. This framework optimizes data transfer and aggregation processes.

### B. Training and Validation

The distributed training process is designed for efficiency and scalability:

- **Efficient Data and Model Update Handling:** Optimized methods are employed to minimize communication overhead and maintain iPad responsiveness. These include:

  1. **Compact Update Packages:** The server receives and applies small, efficient update packages from each iPad.
  2. **Client-Side Caching:** Caching mechanisms on the iPad reduce redundant computations and data transfer.
  3. **Background Processing (Web Workers):** Web Workers on the iPad ensure the device remains interactive during resource-intensive training processes.

- **User Interface:** A user interface (UI) provides control over the training process, abstracting the underlying distributed architecture. This UI enables users to initiate and monitor training, adjust parameters, and review results. This ensures a seamless user experience regardless of the distributed nature of the training process.

## II. Model Development and Training

This section details the model development and training process, focusing on a hybrid architecture that leverages both server-side and client-side (iPad) resources. This approach balances the need for powerful computation with the advantages of using local device data and interactive features, mitigating potential performance and stability issues on resource-constrained devices.

### A. Model Architecture

A hybrid approach will distribute training tasks between the server and client (iPad) as follows:

- **Server-Side Training:** The server will handle computationally intensive tasks, including initial model training, large-scale multi-permutation campaigns, and hyperparameter tuning. Leveraging Google Cloud Run jobs with GPU acceleration (e.g., NVIDIA T4) ensures robust and efficient processing of large datasets and complex models like Vision Transformers (ViTs).
- **Client-Side Training (iPad):** The iPad will focus on model fine-tuning (delta training) using recent local data. This allows for personalized model adaptation and interactive exploration of strategies using features like interactive backtesting for shorter periods and live inference. Web Workers will manage these computations separately from the main thread to minimize performance impact and the risk of browser crashes. Weight updates (deltas) calculated during fine-tuning will be sent back to the server via WebSockets for integration into the global model.

This hybrid architecture addresses potential limitations of client-side training, such as browser crashes due to resource constraints or prolonged GPU usage, by offloading the most demanding tasks to the server. Batch processing on both the server and client further optimizes memory usage and addresses constraints like the PWA cache limit.

### B. Training and Validation

The training and validation process reflects the hybrid architecture:

- **Initial Training and Validation:** Initial model training, including hyperparameter tuning and large-scale experiments, will be conducted and validated on the server using Google Cloud Run with GPU acceleration. A held-out dataset on the server will be used for validation.
- **Client-Side Fine-tuning and Validation:** The iPad will perform fine-tuning and preliminary validation using a subset of local data. This allows for rapid iteration and immediate feedback. The server will perform a more comprehensive validation of the aggregated model after incorporating the client-side updates. This ensures robust performance assessment across diverse data. Federated averaging will be used to combine the client model updates into the global model while preserving privacy.

## II. Model Development and Training

This section details the model development and training process, outlining the responsibilities of both the backend (Python) and the client (iOS app). The process is designed to leverage the strengths of each platform for an efficient and robust workflow.

### A. Model Architecture and Specialization

The model training process is split between the backend and the client. The backend manages the overall experiment structure and stores the master models, while the client performs computationally intensive tasks like image generation and model training/fine-tuning. This client-heavy architecture empowers the iOS app to handle these tasks directly on the device, optimizing performance and reducing reliance on constant server communication.

**Client-Side (iOS App):**

- **Core ML Integration:** Leverages Core ML for efficient model execution on Apple devices. Existing models (PyTorch/TensorFlow) will be converted to the `.mlmodel` format.
- **Metal Framework:** Utilizes Apple's Metal framework for direct GPU access and computation, enhancing performance for image generation and rendering within the DynamicPlaneGenerator component. This is crucial for computationally intensive operations like PCA, rotations, and rendering of numerical data to produce images or tensors.
- **Local Data Storage:** Training data (OHLCV) is stored locally using a high-performance database solution (e.g., Core Data or Realm) for fast, offline access during training.
- **DynamicPlaneGenerator:** Implemented as a native Swift module leveraging Metal, ensuring optimal performance and integration with the Core ML training pipeline.
- **Model Training/Fine-tuning:** Performs the primary model training and fine-tuning operations using local data and sends lightweight updates (deltas) to the backend.
- **Interactive Backtesting:** Conducts shorter-term backtesting directly on the device, facilitating rapid evaluation of strategies and parameters.

**Backend-Side (Python):**

- **Model Hub:** Stores and serves master versions of trained Core ML models, acting as a central repository.
- **Orchestration and Aggregation:** Receives model updates (deltas) from the client and applies them to the global model, enabling continuous improvement based on distributed client training.
- **Experiment Management:** Manages experiment templates and high-level results from research campaigns, providing a structured approach to experimentation.
- **Data Serving:** Manages and serves the master database of raw numerical data to the iOS app on request.
- **API Management:** Handles secure connections and authentication with the Zerodha Kite Connect API, abstracting this complexity from the client application.
- **Model Versioning:** Stores and serves different versions of the model, facilitating potential ensemble learning strategies.

This hybrid approach combines the performance benefits of on-device processing with the centralized management and coordination provided by the backend. Further details regarding specific model architectures (e.g., CNNs, LSTMs, or Transformers) and training procedures will be documented in subsequent subsections.

## II. Model Development and Training

This section details the development, training, and deployment of the prediction models, including considerations for cross-platform compatibility and performance. The initial development leverages the computational capabilities of the iOS platform for computationally intensive tasks like on-device image generation, model training/fine-tuning using Core ML, backtesting, and live inference. Subsequent deployment targets web and Android platforms, prioritizing a mobile-first approach.

### A. Model Architecture

The model architecture utilizes Convolutional Neural Networks (CNNs), potentially combined with Long Short-Term Memory networks (LSTMs) or Transformers, to process candlestick chart images and predict market trends. A decoupled architecture separates backend data provisioning from frontend computational tasks, enabling flexibility in cross-platform deployment. The Python backend provides:

1. Raw numerical data
2. The latest master model file
3. Aggregation of model updates

This allows the frontend to focus on data processing, image generation, and prediction.

### B. Training and Validation

The iOS app performs the heavy lifting during the initial training and validation phase, leveraging its processing power for efficient model training and fine-tuning. Further details on the training process, dataset, and evaluation metrics are required.

### C. Dynamic Plane Implementation

The DynamicPlaneGenerator, crucial for generating candlestick chart images, is implemented natively in Swift for iOS. For cross-platform deployment, the following adaptations are planned:

- **Progressive Web App (PWA):** Re-implementation in TypeScript/JavaScript using libraries like ndarray or a lightweight matrix math library for PCA calculations, along with the HTML5 Canvas API for rendering.
- **Native Android App:** Implementation in Kotlin, leveraging Jetpack Compose for UI and potentially a custom rendering solution or a suitable library for visualizations.

### D. Model Enhancement and Refinement

After initial training and validation on iOS, the model will be fine-tuned for each target platform. Error rate monitoring will trigger re-training when necessary, ensuring sustained performance over time.

### E. Cross-Platform Deployment

Following the iOS development and experimentation phase, the core functionality will be ported to other platforms:

- **Progressive Web App (PWA):** TensorFlow.js will handle machine learning tasks.
- **Native Android App:** TensorFlow Lite will be used for optimized on-device machine learning.

The model will be converted to the appropriate format for each target platform: TensorFlow.js JSON format for PWA and TensorFlow Lite (.tflite) format for Android. The image processing pipeline will be streamlined for these deployments, primarily focusing on daily predictions and occasional re-tuning based on error rate monitoring. The potential transition to Flutter for frontend development requires further investigation into:

1. Flutter's suitability for high-performance, on-device machine learning tasks using TensorFlow Lite.
2. Flutter’s graphics capabilities for rendering and manipulating dynamic visualizations, potentially requiring a re-implementation of the DynamicPlaneGenerator.

## II. Model Development and Training

This section details the development, training, and refinement of the machine learning models used for predicting stock market trends based on candlestick chart images. The section focuses on model architecture, training procedures, and integration with the Flutter mobile application, including considerations for on-device training and performance optimization.

### A. Model Architecture

This subsection addresses the core model architecture, focusing on candlestick image processing, Flutter's graphical capabilities, TensorFlow Lite integration, and the feasibility of on-device training.

- **Candlestick Image Processing:** The core model architecture revolves around processing candlestick chart images. A thorough assessment of Flutter's graphical capabilities is crucial. This includes understanding its capacity to render complex visuals, particularly the output of the Dynamic Plane Generator (discussed in Section II.C), and identifying potential performance bottlenecks, especially with animations and transformations required for dynamic snapshots.

- **Flutter's Graphical Capabilities and Custom Shaders:** If Flutter's standard rendering capabilities prove insufficient for the Dynamic Plane implementation, the feasibility and performance implications of leveraging custom shaders will be investigated.

- **TensorFlow Lite Integration and On-Device Training:** Parallel to model development, we will investigate Flutter's machine learning capabilities, specifically TensorFlow Lite integration via `tflite_flutter`. This assessment will focus on inference performance and, crucially, explore the possibilities and maturity of on-device training within the Flutter ecosystem. Because `tflite_flutter` primarily focuses on inference, a deeper investigation into on-device training support in Flutter is required, including exploring potential solutions involving custom implementations and platform channels for native code integration. This research will determine the feasibility of on-device training and inform architectural choices for the CNN and ViT models.

- **Flutter vs. Native Swift Comparison:** A comparative analysis between Flutter and native Swift development is necessary to justify the framework choice. This comparison will encompass development speed, performance trade-offs, ease of accessing native APIs (especially for graphics and machine learning), and the maturity of on-device training capabilities in each environment.

### B. Training and Validation

The training and validation processes are crucial. The existing pipeline, including training and validation set usage, early stopping, learning rate and batch size adjustments, and large-scale data preparation, will be maintained. Specific implementation details may require adaptation based on the chosen Flutter/Python integration methods. Further details on this process are outside the scope of the architecture discussion.

### C. Dynamic Plane Implementation

The Dynamic Plane implementation, involving coordinate rotations, local movement vector calculations, and dynamic snapshot generation, will be adapted for the Flutter frontend. The existing pseudocode and module design may require adjustments. Core functionalities, such as data normalization and percentile clipping, will remain consistent. Details on the Dart implementation of the Dynamic Plane Generator are provided in a later section.

### D. Model Enhancement and Refinement

Model enhancements like prediction magnitude filtering, historical prediction error profiling, and soft labeling will be reviewed for compatibility with the Flutter implementation. Regardless of the frontend, exploring and implementing these techniques remains a priority. The integration of fundamental and macro data, as well as memory management strategies, will also be considered. Further details will be provided in a later section.

The overall architecture decisions regarding the transition from Swift to Flutter will significantly impact the implementation details within these subsections. Thorough investigation and documentation of the Flutter/Python integration, especially concerning data transfer, authentication, and API design, are crucial for a successful transition.

## II. Model Development and Training

This section details the development, training, and management of the machine learning models for the project, emphasizing portability and compatibility across iOS and Android. A "Universal Source Model" approach will be employed, using a framework-agnostic format (e.g., PyTorch or TensorFlow) within a Python backend. This model serves as the single source of truth for the model's architecture and weights, ensuring consistency and simplifying updates.

### A. Model Architecture and Format Selection

The Universal Source Model resides on the Python backend. For deployment to client devices, platform-specific model formats will be used: `.mlmodel` for iOS (leveraging Core ML's performance) and `.tflite` for Android (due to its portability and broad support). This necessitates a robust conversion process, detailed below.

### B. Training and Validation

The training framework will be selected considering performance, development effort, and maintainability, balancing the advantages of Core ML for iOS with the need for cross-platform compatibility. The chosen framework will support training the Universal Source Model on the backend and enable on-device training/fine-tuning of the client-side models.

A key aspect is the synchronization mechanism. After on-device training, updated weights (or deltas) are transmitted from the client to the backend, updating the Universal Source Model with user-specific insights while preserving privacy.

### C. Model Conversion and Deployment

An automated process will convert the Universal Source Model into platform-specific formats: `.mlmodel` for iOS, `.tflite` for Android, and potentially TensorFlow.js for web deployment. This automation ensures efficient deployment of model updates.

### D. Backend Infrastructure and Model Management

The Python backend manages the Universal Source Model and facilitates client updates. This includes:

- **Model Updates:** A robust process will integrate weight updates received from client devices into the Universal Source Model.
- **Automated Re-distribution:** Upon updating the Universal Source Model, the backend automatically converts and redistributes the updated models to client devices, maintaining cross-platform consistency.
- **Version Control:** A version control system will track model changes and ensure reproducibility.
- **Model Storage and Deployment:** A defined strategy will manage model storage and deployment, considering factors like app size and update mechanisms.

This approach streamlines model development, training, and deployment across iOS and Android, maximizing portability and minimizing platform-specific issues while allowing for personalized on-device training.

## II. Model Development and Training

This section details the development and training of the predictive models central to the SCoVA project.

### A. Model Architecture

The core model architecture revolves around processing candlestick chart images and predicting future returns. Several architectures will be explored, including Convolutional Neural Networks (CNNs), hybrid models, and Vision Transformers (ViT).

- **Input Data:** Models will receive sequences of 5-day candlestick chart images. These images are generated without dates, prices, or ticker symbols to ensure the model focuses solely on visual patterns. The format and size of this input data will be rigorously verified. For the ViT model, sequential candlestick windows will be used.
- **CNN Model:** A CNN will be trained to generate buy/sell signals by identifying patterns in the candlestick images. It will predict both trend direction and reward magnitude, aiming to accurately predict visual patterns while understanding the underlying market dynamics. The CNN's numerical outputs and their transformation into trading signals will be clearly documented. The top/bottom 10% of predictions will trigger buy/sell signals, with trade execution logic evaluated using actual prices from t+1 to t+5.
- **Hybrid Architectures:** The potential of combining CNNs with Long Short-Term Memory (LSTM) networks or Transformers will be explored to leverage sequential information within the candlestick data.
- **Vision Transformer (ViT):** A ViT will be implemented specifically for processing sequential candlestick data, potentially using EfficientNet or a patch embedding approach. Key implementation aspects include: feature engineering with delta features; dataset design for image sequences; determining the ViT input limit; implementing positional embeddings; handling dynamic input length through masking and padding; training the ViT with variable-length sequences; and developing the input pipeline for processing _N_ images.
- **Evaluation Metric:** Model performance will be assessed using a clearly defined evaluation metric, likely Mean Squared Error (MSE) or Root Mean Squared Error (RMSE).

### B. Training and Validation

The training and validation process will adhere to best practices to ensure robust model performance.

- **Training and Validation Sets:** Models, particularly the CNNs, will be trained using a dedicated training set and validated against a separate validation set. Early stopping will be employed to prevent overfitting.

## II. Model Development and Training

This section details the model development and training process. Initial development will focus on minimizing server costs due to budget constraints (₹25,000), with later stages incorporating full-scale training and robust security measures.

### A. Model Architecture

Given the budget constraints, the initial model training will occur directly on the iOS device. This approach involves using a limited dataset and single-epoch training runs to minimize computational load and associated server costs. This allows for full application testing and debugging while remaining within budget. Full-scale training with larger datasets and multiple epochs will be conducted later after verifying stability and completing the initial debugging phase. The model will be designed to recognize visual patterns in candlestick chart data. Security considerations, detailed below, will be integrated into the architecture from the outset, influencing data handling procedures and algorithm selection.

### B. Training and Validation

Initial training and validation will also be performed on the iOS device using a limited dataset and single-epoch training. Early stopping will not be implemented during this initial phase, as the focus is on verifying functionality and debugging rather than optimizing model performance. Rigorous training and validation with larger datasets, longer training durations, and early stopping will be implemented in later stages. Access to training data and model parameters will be restricted to authorized personnel.

### C. Data Scaling and Preparation

A sliding window image generator will be used to create a large-scale dataset of 3-image sequences from candlestick data. This dataset will enhance the model's generalization capabilities and will be used for full-scale training after the initial on-device training phase. A smaller sample dataset will be created for initial development and testing on the iOS device. Secure data storage and integrity checks will be implemented throughout the data processing pipeline.

### D. Hyperparameter Tuning

The learning rate and batch size will be carefully tuned for optimal performance during the full-scale training phase.

### E. Security Measures

While initial development prioritizes cost-effectiveness, security remains paramount, especially given the sensitive nature of financial data. The following security measures will be incorporated throughout the project lifecycle:

- **Master Password Authentication:** A master password will be required to access training scripts and sensitive data.

- **Security Questions:** Security questions will be implemented for password recovery.

- **Optional Security Enhancements:** The following security enhancements will be explored:

  - **Visual Cryptographic Key Authentication:** Requiring a user-defined visual pattern in addition to the master password.
  - **Challenge-Response Protocol:** Implementing user-specific challenge-response questions.

- **Secure Password Handling:** Robust hashing algorithms (e.g., scrypt) and protection against brute-force attacks will be employed. Leveraging platforms like Firebase Authentication with Identity Platform will be considered.

These security measures will be fully integrated into the model architecture, training process, and data handling procedures to ensure data integrity and confidentiality. Further details on security implementation will be provided in the dedicated security section of this document.

## II. Model Development and Training

This section details the model development and training process, emphasizing cost-effective strategies and rigorous testing to minimize cloud compute expenses and ensure robust on-device training within the Core ML framework. A multi-stage testing approach using mock data and a dummy model validates the system before deploying to live environments.

### A. Model Architecture and On-Device Training Considerations

The model architecture development considers on-device training integration and its associated verification steps:

- **On-Device Training Necessity:** An evaluation will determine the necessity of performing one epoch of training on the device with a pre-trained or dummy model, considering the trade-offs between potential performance gains and resource utilization (memory, processing power, battery life).
- **Model Update Mechanism:** The process for updating the universal model on the backend after on-device training requires thorough investigation and documentation. This is crucial for version management and consistent performance.
- **On-Device Training Smoke Test:** A single-epoch training run on a dummy model will validate the training and update mechanism. This smoke test verifies:
  - Correct data loading and preprocessing.
  - Flawless training loop execution (forward pass, loss calculation, backpropagation).
  - Successful extraction of updated weights after training.

### B. Training and Validation Process

The training and validation process incorporates a multi-stage testing approach:

1. **Unit/Integration Tests with Mock Data:** Individual model components and their interactions are tested using mock data (CSV/JSON) representing various market conditions, including edge cases (price spikes, flat periods, data gaps). This ensures correct functionality before integration.
2. **End-to-End Dry Run:** A complete dry run simulates the entire pipeline using mock data and the dummy model. This identifies potential bottlenecks and issues before live deployment.
3. **On-Device Smoke Test with Dummy Model:** On-device smoke tests using the dummy model confirm system functionality in the target environment. This crucial step validates:
   - Seamless integration of the `DynamicPlaneGenerator` with the Core ML training session, ensuring correct input of transformed image tensors.
   - Successful execution of a full training step (forward pass, loss calculation, backpropagation) on the device, verifying Core ML's ability to handle the computational load.

This rigorous testing process, fully documented in the methodology section of the technical documentation, ensures the reliability and efficiency of the model training pipeline.

## II. Model Development and Training

This section details the development and training of the core models for the SCoVA project, focusing on real-time event detection ("shocker events") in financial markets using computer vision techniques on diverse financial data. A key component is the Convolutional Threat Assessment Module (CTAM), which generates a Systemic Threat Level (STL) score to inform other system components.

### A. Model Architecture

The model architecture incorporates computer vision techniques and specialized anomaly detection models within the CTAM to analyze various financial instruments for predictive capabilities.

- **Multi-Source Visual Inputs:** The CTAM ingests visual data from multiple sources, including real-time snapshots of primary equity charts, corresponding futures charts, and visualized options chain data presented as heatmaps. These diverse inputs provide a comprehensive view of market conditions.

- **Specialized Anomaly Detection Models (CNNs):** Specialized Convolutional Neural Networks (CNNs) are trained as "threat detectors" within the CTAM. Separate, lightweight CNNs are trained to detect specific anomalies within each data source:

  - **Equities:** CNNs focus on detecting gaps and volume spikes within equity charts.
  - **Futures, Options, and Derivatives:** CNNs analyze options chain heatmaps and futures charts for anomalies, expanding the model's ability to detect a wider range of market disruptions.

- **CTAM Fusion and Systemic Threat Level (STL) Assessment:** The outputs from the specialized CNN detectors are combined to produce a single Systemic Threat Level (STL) score. This fusion can be achieved through a dedicated fusion model or a weighted-average function, depending on performance and complexity considerations.

- **Shocker Event Definition:** "Shocker events" are defined and quantified by identifying characteristics within the time series data that represent such events. These characteristics include volatility spikes, anomalous trading volumes, and rapid price fluctuations, and are used by the CTAM for event detection.

### B. Training and Validation

The training and validation process focuses on enabling real-time event detection and incorporating the CTAM's STL score.

- **Real-time Event Detection:** A core objective of the training process is to enable the model to detect events unfolding in real-time. The training data and methodology will be structured to prioritize and optimize real-time performance.

- **CTAM Integration and Influence:** The generated STL score is integrated into the core SCoVA systems, influencing several key functionalities:

  - **Proactive Pratyahara (Withdrawal):** The STL informs decisions regarding proactive withdrawal from the market.
  - **Dynamic Plane Adjustment:** The STL modulates the smoothing factor within the Dynamic Plane.
  - **Final Prediction Context:** The STL serves as a context token for the final prediction mechanism.

- **Model Retraining Strategy:** The CTAM employs a retraining strategy to maintain its effectiveness. (Further details on this strategy will be provided in a subsequent section.)

### C. Three-Stage Testing Protocol

A three-stage testing protocol ensures robust model development and a cost-effective debugging process:

1. **Unit & Integration Testing:** Individual components and their interactions are rigorously tested using local mocks to isolate and verify functionality. This includes verifying weight extraction after a dummy training step and synchronization of small weight updates between the application and the Python backend for updating the "universal model."

2. **End-to-End Pipeline Simulation:** The entire data processing and prediction pipeline is simulated in a "dry run" mode to validate functionality and performance without impacting real-world systems.

3. **On-Device Smoke Test:** A final "smoke test" is performed on the target device with a dummy model to verify deployment readiness and identify any remaining issues. This low-cost debugging method will be prioritized in future development and troubleshooting efforts.

This proactive development approach emphasizes anticipating market downturns ("cognizance of chances of injury") rather than reacting to them ("treating the injury"). By leveraging real-time event detection, computer vision, and diverse financial data, the model aims to predict and potentially mitigate the impact of market disruptions. A comprehensive technical document will be produced, encompassing all discussed and agreed-upon items, including details about the model's weight update process and the synchronization mechanism with the backend.

## II. Model Development and Training

This section details the development and training of the predictive models used by the trading bot. The system leverages two distinct models: the "Flow Engine" for normal market conditions and the "Shockwave Prediction Model" (SPM) for volatile periods. This dual-system approach allows the bot to adapt its strategy dynamically, maximizing profit potential across a broader spectrum of market conditions. The Flow Engine details are assumed to be defined elsewhere in this document. This section focuses on the SPM development and the integration mechanism between the two models.

### A. Model Architecture

The SPM is designed to predict short-term (1-3 candlestick) price direction and magnitude following significant market events ("shocker events"), such as flash crashes, earnings gaps, major news-driven price spikes, and periods of extreme volatility. Two potential architectures are being considered:

- **Simplified CNN:** A Convolutional Neural Network (CNN) offers rapid processing of candlestick data, allowing for quick reactions to visual patterns. This simplicity may be advantageous for capturing immediate market reactions.

- **Transformer with Optimized Attention:** A Transformer architecture, modified for optimized attention on immediate shockwave follow-through, could capture nuances missed by a standard Transformer. This approach might offer higher predictive accuracy but potentially at a higher computational cost. Further investigation is required to determine the optimal balance between performance and complexity.

### B. Adaptive Strategy Weighting (Seesaw Mechanism)

The predictions of the Flow Engine and the SPM are combined using a dynamic weighting mechanism called the "seesaw." This mechanism utilizes a Systemic Threat Level (STL), a metric (assumed to be defined elsewhere) representing current market volatility. The final prediction is calculated as follows:

```
Final Prediction = (1 - STL) * FlowEngine_Prediction + (STL) * SPM_Prediction
```

When market volatility is low (STL near 0), the bot primarily relies on the Flow Engine. During high volatility (STL near 1), the SPM takes precedence. This adaptive weighting allows the bot to dynamically adjust its strategy based on the current market environment.

### C. Training and Validation

The SPM will be trained on a curated dataset of historical "shocker events." The prediction target is a short-term (1-3 candlestick) movement vector, capturing the immediate, chaotic market behavior following a shockwave. This short-term focus aligns with the reactive nature of the considered architectures.

While a team restructuring based on Continuity, Enforcement, Facilitation, and Specialization, and a Gita-inspired restructuring of microservices are relevant to overall system design, they are not directly relevant to model development and training and are addressed elsewhere in this document.

## II. Model Development and Training

This section details the development and training of predictive models, emphasizing a robust architecture designed around the four pillars of Continuity, Enforcement, Facilitation, and Specialization. The existing framework will be significantly refactored and re-engineered to optimize performance and maintainability.

The redevelopment process begins with a thorough deconstruction of the current architecture. Each component will be analyzed according to the four pillars to determine its primary role. Complex components like the DynamicPlaneGenerator and Multi-Scale Vision Transformer will be deconstructed, and their functionality reassessed in light of user needs. This deconstruction aims to simplify the system and improve understanding of individual component contributions.

Next, these components will be further decomposed into atomic functions. This atomization process clarifies each function's responsibilities and its alignment with the four pillars. Data flow and system topology will be redesigned to optimize communication pathways and ensure efficient processing.

A comprehensive architectural reassessment will then recategorize each atomic function according to the four pillars, focusing on aligning the data flow with the designated roles of each component to ensure seamless communication and collaboration. The interplay between Continuity, Enforcement, Facilitation, and Specialization will be central to this restructuring.

The architecture will be redesigned based on user feedback and critique. Components will be re-assigned to the appropriate pillars based on their refined roles. Stringent communication protocols will be established, particularly for Specialist components, which will interact exclusively through designated Facilitator modules. This restructuring will enforce the newly defined system principles.

Finally, the blueprint will undergo a deep refactoring to reflect these changes. The DynamicPlaneGenerator and Multi-Scale Vision Transformer will be dissected, their functions atomized, and re-assigned to the appropriate pillars. Communication protocols will be redesigned, ensuring Specialist components interact solely through Facilitators. This rigorous adherence to the new system principles will establish a robust foundation for model development and training, promoting scalability, maintainability, and optimal performance.

Subsequently, robust processes and supporting services will be implemented to ensure smooth and reliable model training and backtesting. These include:

- **Pre-flight Validation Service:** Verifies all necessary conditions are met before initiating training or backtesting campaigns, including data integrity, environment health, and template schema validity.

- **Post-flight Analytics Service:** Analyzes campaign results and updates the system's knowledge base, generating campaign summaries, updating the model registry, and identifying archetypal patterns.

- **Real-time Process Monitor:** Tracks the health and resource consumption of active jobs, monitoring resource utilization and training stagnation.

- **Execution State Controller:** Manages the state of running processes, enabling pausing/resumption and graceful termination.

- **API Gateway & Orchestrator:** Serves as the central access point for frontend requests, handling requests and orchestrating workflows.

Finally, specialized services will support core model development within the SCoVA project, handling data transformation, model inference, and analysis. These include:

- **Normalization Service:** Normalizes numerical arrays for stable and efficient model training.

- **PCA Service:** Performs Principal Component Analysis for dimensionality reduction and feature identification.

- **Coordinate Rotation Service:** (Further details required for this service’s functionality).

## II. Model Development and Training

This section details the development and training of the core models for predicting stock market trends based on candlestick chart image sequences. The process adheres to architectural principles that promote modularity, maintainability, and scalability. Specialized services handle data preprocessing, model training, and dimensionality reduction, laying the foundation for subsequent model development using CNNs, ViTs, and other components.

### A. Data Preprocessing and Training Services (Specialization)

The model development process leverages specialized services to prepare data and train the models. This includes data normalization, dimensionality reduction using Principal Component Analysis (PCA), and a dedicated training service decoupled from data sourcing and storage. These services are designed with strict dependency limitations to maintain focused functionality, minimize external dependencies, and ensure performance.

- **`NormalizeWindow` Service:** This service normalizes the raw numerical input array based on a provided configuration dictionary. This crucial preprocessing step ensures stable and consistent model training. It is designed with a strict dependency limitation, forbidding the import of `google-cloud-storage`, `google-cloud-firestore`, and `requests`.

- **`ComputePrincipalComponents` Service:** This service performs Principal Component Analysis (PCA) on the normalized data. It takes the normalized array as input and returns the top two principal component vectors, reducing the dimensionality of the input data while retaining the most important features. Only `numpy` is allowed as an external library to ensure performance and minimize dependency conflicts.

- **`ProjectToPlane` Service:** Using the original data and the basis vectors generated by `ComputePrincipalComponents`, this service projects the data onto a 2D plane, further simplifying the data representation for visualization and subsequent model training.

- **`TrainOneEpoch` Service:** This service encapsulates the model training process for a single epoch. It receives model artifact bytes, training data tensors, and configuration parameters as input and returns updated model artifact bytes. Critically, this service is agnostic to the data's origin or destination, promoting modularity and reusability.

### B. Architectural Principles

Beyond the specialized services, the following architectural guidelines are followed throughout the model development and training phase:

- **Isolated Specialists:** Model components (e.g., CNN for image processing, ViT for sequential data, trend prediction modules) are developed as isolated units. This ensures independent development, testing, and refinement, minimizing unintended side effects and promoting code reusability. Interactions are carefully managed through well-defined interfaces and data exchange protocols.

- **Asynchronous Communication:** Where applicable, asynchronous communication patterns using a pub/sub model are employed between specialist components, particularly for long-running processes like training or image sequence generation. This prevents blocking operations and improves system responsiveness.

- **Resource Access Control (Enforcers):** Access to critical resources (training data, model parameters, hardware) is strictly controlled via designated Enforcer components. This ensures data integrity, prevents resource conflicts, and provides centralized resource monitoring and management.

- **Workflow Orchestration (Facilitators):** Facilitator components orchestrate complex workflows (data preprocessing, model initialization, training, validation, performance evaluation). This simplifies complex procedures and improves management of the model development lifecycle.

- **Inheritance for Code Structure:** Class inheritance structures services throughout the codebase, enforcing a strongly typed and well-defined separation of concerns. This promotes code clarity, maintainability, and extensibility, simplifying long-term maintenance and feature integration.

The combination of specialized services and these architectural principles creates a robust and adaptable pipeline for model development and training. The modular design promotes maintainability and allows for easy integration of future enhancements, including the incorporation of specific model architectures like CNNs and ViTs.

## II. Model Development and Training

This section details the development and training of the predictive models. The process will leverage a secure, maintainable microservice architecture, implemented initially as a co-located monolith for simplified development and debugging, with potential for later distribution. This approach balances the benefits of modularity and scalability with the need for efficient resource management and real-time performance.

**Security and Maintainability:**

The architecture adheres to a four-pillar principle emphasizing security and maintainability. Several key services, categorized by their role, will enforce secure access and manage resources:

- **Enforcement Services:** These services manage exclusive access to critical resources.

  - **`State_Enforcer`**: Manages all Firestore database interactions via a dedicated service account, ensuring data integrity and security for model training data, parameters, and logs. Its single-function design with conditional logic streamlines maintenance.
  - **`Resource_Enforcer`**: Manages object storage within Google Cloud Storage (GCS) via a dedicated service account, securing model checkpoints, datasets, and other artifacts. Its centralized, single-function design promotes maintainability.
  - **`Live_Execution_Enforcer` (For Live Trading Models)**: Holds exclusive authority to interact with the Zerodha API for order placement, modification, and cancellation via a secure `/executeOrder` endpoint, mitigating unauthorized trading risks.

- **Facilitation Service:**

  - **`Workflow_Broker`**: Orchestrates complex workflows, including dynamic plane generation and experiment execution. Triggered by Pub/Sub tasks, it manages model training steps and information flow between components, automating the training pipeline.

- **Continuity Service:**
  - **`Pre-flight_Validation_Service`**: Called by the `UI_Gateway`, this service validates data and resources required for model development, interacting with the `Resource_Enforcer` to maintain security and data integrity.

**Class Inheritance and Design Consistency:**

To ensure architectural consistency and maintainability, all services will be built upon four base classes: `ContinuityService`, `EnforcementService`, `FacilitationService`, and `SpecialistService`. This inheritance-based approach provides a standardized structure and avoids conflicting design strategies. The development process will follow these steps:

1. **Define Base Classes:** Define the four base classes, encapsulating core functionalities and behaviors for each service category.
2. **Draft Specifications and Pseudocode:** Create detailed technical specifications and pseudocode for each base class and the initial set of inheriting services, outlining methods, attributes, and responsibilities.
3. **Implement Class Inheritance:** Enforce the inheritance approach; all services _must_ inherit from one of the defined base classes.
4. **Adhere to Project Constraints:** Follow established project constraints during implementation to maintain design consistency.

**Performance Optimization:**

To mitigate potential performance bottlenecks inherent in the microservice architecture (even in its initial co-located state), the following strategies will be implemented:

- **Service Discovery and Load Balancing:** Distribute traffic efficiently across service instances to prevent overload and minimize latency.
- **Optimized API Design:** Minimize data transfer and communication overhead through efficient data formats, request/response structures, and compression techniques.
- **Asynchronous Communication:** Utilize Google Cloud Pub/Sub for non-critical requests to improve system responsiveness.
- **Continuous Performance Testing:** Integrate end-to-end performance tests to monitor latency and identify potential bottlenecks early, allowing for timely adjustments.

**System-Wide Logbook:**

A comprehensive system-wide logbook will track all aspects of model development and training, including function execution times for all services, facilitating performance analysis and debugging. This centralized logging approach will be crucial for understanding and optimizing the system's behavior.

## II. Model Development and Training

This section details the development and training of the predictive models, addressing various aspects from architecture and specialized functionalities to the training process and performance monitoring. It consolidates information previously presented in a fragmented manner, providing a cohesive overview.

### A. Model Architecture and Specialized Functionalities

The model architecture departs from traditional static candlestick image analysis, introducing two key innovations: a dynamic rotating plane and a dual-engine perception system. Additionally, the architecture incorporates multi-scale periodicity analysis and "rally time" prediction.

- **Dynamic Rotating Plane:** This representation perceives market dynamics on a two-dimensional plane that dynamically re-centers and rotates based on Time, Price, and Volume, offering a more adaptable and responsive view of market conditions.
- **Dual-Engine Perception:** This system employs two distinct models: a Flow Engine for stable market conditions and a Shockwave Prediction Model (SPM) for volatile events. A weighted "seesaw" mechanism, governed by a Systemic Threat Level (STL), combines their predictions, dynamically adjusting reliance based on market stability.
- **Multi-Scale Periodicity:** The model ingests and processes intraday, daily, and weekly data to understand cyclical market patterns, effectively fusing information from these different granularities.
- **"Rally Time" Prediction:** The model predicts the expected duration, measured in candlesticks, for a predicted movement to materialize, adding a temporal dimension to predictions.

### B. Training Process and Infrastructure

Model development will primarily leverage client-side resources (client-side heavy lifting) through a native app frontend (Swift/Core ML or Flutter/hybrid native ML). This approach minimizes server costs and enables rapid iteration. The backend comprises a Universal Model Hub and a Lightweight Backend Orchestrator.

- **Client-Side Training:** Client devices handle computationally intensive tasks like image generation and model training. Weight updates are sent back to the server, fostering collaborative learning.
- **Universal Model Hub (Backend):** This central repository stores the source model, which is converted to platform-specific formats and distributed to client devices.
- **Lightweight Backend Orchestrator:** Implemented using Python (Cloud Run/Functions with Firebase), this orchestrator serves data to clients, manages the universal model, orchestrates backtesting (using Cloud Tasks), and handles API interactions.

### C. Performance Monitoring and Enhancement

To enhance performance and facilitate debugging, a distributed tracing system using OpenTelemetry is implemented, alongside advanced error signals for training.

- **Distributed Tracing:** OpenTelemetry is used to track requests across microservices (Continuity, Enforcement, Facilitator, and Specialist), providing granular insights into the training workflow. Context propagation via HTTP headers and Pub/Sub metadata ensures accurate tracking. Visualization will be handled by a tool chosen from Google Cloud Trace, Jaeger, or Grafana Tempo, prioritizing Gantt chart visualizations for bottleneck identification.
- **Advanced Error Signal (Training):** A "Total Error" signal, combining Vector Deviation Error and Frame Shift Error, provides a comprehensive metric for evaluating model performance during training.
- **Bottleneck Identification and Refactoring:** A dedicated logbook will track function execution times, including pauses during facilitator-specialist interactions, network calls, and API calls. Visualization capabilities, potentially integrated with the IDE, will aid in identifying and addressing bottlenecks. This iterative process of development, logging, analysis, and refactoring will be crucial for creating a robust and performant model training pipeline.

## II. Model Development and Training

This section details the development and training of the predictive models used in the SCoVA project. It addresses model architecture, algorithmic considerations for portfolio construction and risk management, training and validation procedures, and the integration of explainable AI (XAI) for transparency and compliance.

### A. Model Architecture

While specific architectural details for the "Flow" and "Shock" prediction engines are provided elsewhere, this section outlines key algorithmic components crucial for integrating these models into a complete trading system:

- **Portfolio Construction:** A Portfolio Construction service will translate individual stock predictions into a cohesive portfolio using optimization algorithms like Mean-Variance Optimization, Risk Parity, and Hierarchical Risk Parity. This service determines the final list of trades and their respective position sizes.

- **Adaptive Seesaw Blending:** A Meta-Model will dynamically blend predictions from the "Flow" and "Shock" engines, adapting to market conditions using inputs like Systemic Threat Level (STL) and market volatility. This dynamic approach optimizes performance across various market regimes.

- **Explainable AI (XAI) Integration:** The model architecture will incorporate XAI components to provide insights into model predictions. This will assist in understanding the rationale behind specific trading signals generated, for example, by CNN or ViT models based on candlestick patterns. This enhanced transparency will aid in understanding and trusting the model's decisions.

### B. Training and Validation

Specific training and validation procedures for individual models are detailed elsewhere. However, the overall process will not only monitor performance metrics but also assess the effectiveness of the explanation generation methods.

### C. Dynamic Plane Implementation

The Dynamic Plane implementation, while focusing on data transformations, is relevant to model explainability. Understanding how transformations impact the input data is crucial for providing accurate narratives about trade decisions. Further details are provided in the Dynamic Plane section.

### D. Model Enhancement and Refinement

Model refinement will consider the impact of changes on explainability, evaluating the stability and consistency of explanations generated across different model versions.

**Portfolio Risk Management:** A Portfolio Risk Management service will enforce portfolio-level risk rules, including maximum drawdown limits, concentration limits, and volatility targeting. This service reviews and approves trades before execution, mitigating potential risks and ensuring adherence to predefined risk parameters.

**Explainability and Reporting:** A Narrative Generation Service will leverage XAI methods like LIME and SHAP, attention maps, and an LLM, in conjunction with the Feature Store, to produce detailed, human-readable explanations for each trade. These explanations will clarify why specific features and patterns in the candlestick data led to the model's predictions, enhancing transparency and supporting regulatory compliance. These narratives, complete with timestamps, will justify the reasoning behind specific trades, including context from the feature store, market conditions, meta-model predictions, the status of both "Flow" and "Shock" models, available capital, and risk exposure. This approach ensures not only a performant model but also one that meets the requirements for transparency and regulatory compliance.

## II. Model Development and Training

This section details the development, training, and validation of the models, incorporating live market data, feature engineering, and Large Language Models (LLMs) for enhanced predictive capabilities and explainability.

### A. Model Architecture

This section outlines the core model architecture, incorporating CNNs, ViTs, hybrid approaches, and LLMs. It also details the integration of real-time market depth features and the mechanism for generating narrative explanations for trading decisions.

- **CNNs, ViTs, and Hybrid Models:** The architecture uses 5-day candlestick chart images as input for CNNs and sequential candlestick windows for ViTs. Feature engineering and hybrid model exploration remain central to the development process.
- **Real-time Market Depth Features:** A dedicated service, `DeriveOrderBookFeatures`, processes live tick data from a WebSocket feed to calculate features like Order Book Imbalance (OBI), Weighted Average Price (WAP), and Bid-Ask Spread. These features are input to the `DynamicPlaneGenerator`, enhancing its sensitivity to real-time market dynamics. This live data integration bridges the gap between historical backtesting and live trading, enabling more realistic simulations.
- **LLM Integration for Explainability:** LLMs are integrated to generate narrative explanations for trading decisions. This involves collecting and structuring specific data points for LLM prompts, including the input candlestick chart image, relevant internal states of the CNNs and ViTs, and broader system parameters (STL, Seesaw weights, CorrectionFactor, risk exposure, and available capital). The generated narrative encompasses market perception, Flow Engine and Shockwave analysis (if applicable), the model's prediction with confidence level, and the final execution decision.

### B. Training and Validation

This section details the training and validation procedures for the models, including the integration of paper trading and the LLM component.

- **CNN and ViT Training:** Established training and validation procedures for CNNs and ViTs remain crucial, including early stopping, learning rate and batch size tuning, and large-scale dataset preparation.
- **LLM Training and Narrative Evaluation:** The LLM component is trained by refining prompt templates and evaluating the generated narratives for accuracy and clarity.
- **Paper Trading Integration:** A `Paper_Brokerage_Simulator`, mimicking the Zerodha Kite Connect API, facilitates realistic simulations without risking real capital. The `Live_Execution_Enforcer` is modified to support both LIVE and PAPER modes, allowing seamless switching between real and simulated trading. The simulator handles order execution and portfolio management, maintaining its state within Firestore and using live market data for realistic simulated fills, including partial fills based on live bid/ask prices and volume.
- **Karma Ledger Integration:** Narrative explanations from the LLM are integrated into a Karma Ledger, creating a comprehensive and auditable record of each trading decision and its rationale. This audit trail is essential for understanding and improving system behavior.

### C. Paper Portfolio Management

The `Paper_Brokerage_Simulator` manages the paper portfolio's state (cash balance, open positions, order statuses) within a dedicated 'paper_portfolio' collection in Firestore, ensuring data consistency and enabling performance analysis over time.

### D. Live Trading Dashboard UI/UX

The Live Trading Dashboard UI includes a "Paper Trading" toggle, allowing users to seamlessly switch between live and paper trading modes and view their paper portfolio performance for evaluating simulated strategies.

## II. Model Development and Training

This section details the architecture and training process of the models developed for predicting market movements. The models incorporate market depth data, addressing limitations encountered with the Zerodha API's fixed price increments. Instead of relying on spread calculations, the models leverage order book data, focusing on order quantity, count at different price levels, and derived features like Order Book Imbalance (OBI).

### A. Model Architecture

Several specialized models and services are being developed to analyze market dynamics and enhance prediction accuracy. These include:

- **Market Depth Anomaly Detection:** A Convolutional Neural Network (CNN), the `MarketDepthAnomalyDetector`, will analyze market depth heatmaps to identify visual patterns indicative of market shocks (e.g., large orders, order withdrawals, one-sided order book depletion). The output, `P(OrderBookShock)`, representing the probability of an order book shock, will be integrated into the Comprehensive Threat Assessment Model (CTAM) alongside other threat probabilities (e.g., `P(PriceShock)`, `P(DerivativesShock)`).

- **Order Book Imbalance (OBI) Calculation and Integration:** The `CalculateOrderBookImbalance` service will compute the OBI, a normalized value between -1.0 and +1.0, representing selling and buying pressure respectively. This OBI value will be used in several ways:

  - As a fourth dimension (alongside Time, Price, and Volume) in the data window provided to the `DynamicPlaneGenerator`.
  - As input to the `OrderBookVolatility` component, which calculates the standard deviation of OBI values over recent time intervals. This "Order Book Volatility" will then adjust the `CorrectionFactor` within the Self-Correction & Healing Controller, improving adaptability to market instability.

- **Market Depth Heatmap Generation:** The `GenerateDepthQuantityHeatmap` service will create a heatmap visualization of order book dynamics. This heatmap, representing order quantity at different price levels and time steps, will serve as input to the `MarketDepthAnomalyDetector`.

- **Order Book State Vector:** The `ComputeOrderBookState` service will generate an "Order Book State" vector. This vector, incorporating features like Overall Imbalance (OBI), Depth Asymmetry, and "Wall" Detection, will be used as context for the Vision Transformer model, alongside the dynamic plane image. This aims to improve predictive power without modifying the core DynamicPlane input (Time-Price-Volume).

- **Top-of-Book Pressure Gradient Monitoring:** The CTAM will monitor the ratio of Level 1 Bid Quantity to Level 1 Ask Quantity. Significant changes in this ratio, indicative of potential market shocks, will increase the Systemic Threat Level (STL), triggering the Shockwave Prediction Model.

Further investigation is planned regarding optimal integration strategies for OBI data, including its use for prediction, error awareness, and dynamically adjusting weighting between different predictive models (e.g., "flow" vs. "shock" models). The system's handling of intraday limit orders will also be refined to reflect exchange behavior and ensure consistency between the model's logic and actual trading operations. This may necessitate adjustments to the model's trade execution logic.

## II. Model Development and Training

This section details the development and training of the predictive models. The overall strategy has shifted from direct market movement prediction to a focus on error detection, dynamic adaptation, and execution quality. This involves developing a novel "Anxiety Model" to assess risk and dynamically adjust trading behavior, integrating execution quality metrics into performance evaluation, and enhancing the Dynamic Plane's responsiveness to market conditions.

### A. Model Architecture

The core model architecture revolves around an "Anxiety Model" and incorporates enhancements to leverage real-time market depth and execution quality data.

- **Anxiety Model:** This meta-learning model predicts the potential for errors in the primary trading algorithm (Dynamic Plane) based on real-time market depth data. It learns to correlate market conditions with past algorithm performance, using features like Liquidity Gradient (concentration of supply and demand behind best bid/ask), order count at each price level, Order-to-Quantity Ratio, Rate of Change of Order Book Imbalance (OBI), Level 1 Dominance, and Book "Flicker" Rate. The Anxiety Level generated serves as input to the Error Detector and Weight Shifter modules, enabling dynamic adaptation. The model will be trained using backtest data, with the target variable being the Total Error (Vector Deviation + Frame Shift) from the DynamicPlane algorithm. A post-hoc analysis will correlate DynamicPlane actions with historical order book data to further refine the Anxiety Model’s training.

- **Trading Mode Switching:** The Anxiety Model dynamically switches between "flow" (stable markets) and "shock" (volatile markets) trading modes based on its assessment of market conditions and potential for errors. This allows the system to adapt to varying market dynamics. The Narrative_Generation_Service (Explanation AI) provides detailed explanations for these mode switches, including the underlying market conditions and their implications.

- **Price Improvement Rate Integration:** A "Price Improvement Rate" feature, calculated by the `CalculatePriceImprovementRate` service as a rolling average of price improvement received on trades, will be integrated as a context token within the Vision Transformer (ViT) model. This aims to allow the model to learn correlations between price improvement and potential short-term trend reversals.

- **Enhanced Order Book State Feature:** The `ComputeOrderBookState` service will generate a "Book Resilience Score," calculated as the ratio of quantity at Level 1 to the quantity at Levels 2-5. This score, combined with the Order Book Imbalance (OBI), provides a richer feature vector. A low resilience score, coupled with bullish signals from the Flow Engine, enhances the conviction of buy signals from the ViT.

### B. Training and Validation

Model training and validation will incorporate the new features and focus on the Anxiety Model's performance. The price improvement rate allows for more accurate tracking and representation of model profitability, directly incorporating execution quality into performance evaluation.

### C. Dynamic Plane Implementation

The Dynamic Plane implementation integrates an execution quality feedback loop. The Self-Correction & Healing Controller monitors the rolling average execution quality. Deteriorating execution quality (increasing slippage and reduced price improvement) increases the CorrectionFactor of the DynamicPlane, making its perception more conservative to prevent accumulating prediction errors from poor execution. This dynamically adjusts the model's sensitivity based on real-time execution feedback.

## II. Model Development and Training

This section details the development and training of the core predictive models. This document covers the Anxiety Model, which processes high-frequency market data to generate an "Anxiety Level", and the SCoVA (Snapshot Computer Vision Algorithm) model, which analyzes candlestick chart images to predict market movements.

### A. Anxiety Model Architecture

The Anxiety Model architecture is designed to process high-frequency market depth data and generate an "Anxiety Level" representing the perceived level of uncertainty or risk in the market. This Anxiety Level, ranging from 0.0 to 1.0, serves as a key input to other system components, influencing both the Error Detector and Weight Shifter.

The Anxiety Model derives the Anxiety Level from engineered features based on raw market depth data, including:

- **Order-to-Quantity Ratio:** Calculated at the top 5 levels of the order book.
- **Rate of Change of Order Book Imbalance (OBI):** Measures the dynamic changes in order book imbalance.
- **Level 1 Dominance:** Quantifies the influence of the best bid and offer prices.
- **Book "Flicker" Rate:** Captures the frequency of changes in the order book.

Specifically, the Anxiety Level influences the system as follows:

- **Error Detector:** Higher Anxiety Levels increase the CorrectionFactor within the Self-Correction & Healing Controller, enabling more aggressive adjustments.
- **Weight Shifter:** An increasing Anxiety Level shifts weight from the Flow Engine to the Shockwave Prediction Model, prioritizing the latter's predictions during periods of heightened market anxiety. This influence is faster than that of the Systemic Threat Level (STL).

### B. SCoVA Model Architecture

SCoVA utilizes computer vision to analyze candlestick chart images and predict market movements. The model's architecture is being redesigned using a graph-based approach. Instead of a hierarchical multi-scale context model, each timeframe (intraday, daily, weekly, etc.) is represented as a node within a graph. Edges connect these nodes, representing the learned influence between the different timeframes. A Graph Neural Network (GNN) serves as the fusion mechanism, replacing the previously employed Transformer architecture.

A key functional requirement of this graph-based model is dynamic influence learning. The GNN is trained to dynamically learn the influence between different timeframes based on prevailing market conditions. This allows the model to effectively adapt to changing market dynamics.

### C. Asymmetric Prediction Models and Feature Engineering

Asymmetric approaches are being incorporated to enhance performance and risk management. This includes:

- **Asymmetric Prediction Models:** Separate models, a Bull_Flow_Engine and a Bear_Flow_Engine, are trained specifically for bull and bear markets, respectively. A high-level regime-detection model selects the appropriate engine based on current market conditions.
- **Asymmetric Feature Engineering:** A new service, CalculateAsymmetricFeatures, calculates asymmetric features like Upside Volatility and Downside Volatility. These features are packaged into a context vector and fed as input to the Vision Transformer.

### D. Training and Validation

Training and validation processes are tailored to the specific model architectures:

- **Anxiety Model:** Details of the training and validation process for the Anxiety Model are not yet defined and will be addressed in future documentation.
- **SCoVA Model:** The GNN within SCoVA is trained to dynamically learn the influence between timeframes. This dynamic learning is incorporated into the training and validation process.
- **Asymmetric Prediction Models:** The Bull_Flow_Engine and Bear_Flow_Engine are trained and validated independently using respective datasets and standard procedures, including early stopping.

## II. Model Development and Training

This section details the architecture and training of the Vision Transformer (ViT) model, focusing on integrating contextual information to capture nuanced market dynamics and asymmetry. The core concept is that market rises and falls exhibit distinct characteristics, and this asymmetry is incorporated through specialized feature engineering and regime detection.

### A. Model Architecture

The core architecture revolves around a Vision Transformer (ViT) that processes Dynamic Plane image tensors. To enrich the model's understanding, two key enhancements are implemented: integrating an asymmetric feature vector and a market regime identifier as context tokens.

- **Asymmetric Feature Integration:** A dedicated `AsymmetricFeatureEngine` service calculates a vector of features representing asymmetries in recent price action and volume. This vector serves as a context token for the ViT, influencing its self-attention mechanism. The `AsymmetricFeatureEngine` receives a window of raw data as input and outputs the calculated asymmetry features, which include:

  - **Price and Volatility Asymmetry:**

    - Upside vs. Downside Volatility (using semi-deviation): Quantifies volatility during upward and downward price movements separately.
    - Volatility Skewness: Measures the asymmetry of the volatility distribution.
    - Volatility Kurtosis: Captures the "tailedness" of the volatility distribution, indicating the likelihood of extreme volatility events.

  - **Volume and Participation Asymmetry:**

    - Accumulation/Distribution Ratio: Measures the relative volume flow during upward and downward price movements, providing insights into buying and selling pressure.
    - Order-to-Quantity Asymmetry: Compares the bid-side and ask-side order-to-quantity ratios, reflecting the relative aggressiveness of buyers and sellers.

  - **Correlation Asymmetry:**
    - Price-Volume Correlation State: Calculates the correlation between log-returns and log-volume separately for positive and negative return candles, differentiating between market regimes characterized by fear or greed.

- **Market Regime Detection:** An unsupervised clustering model (e.g., Gaussian Mixture Model (GMM) or Self-Organizing Map) identifies distinct market regimes. Trained offline on historical asymmetric feature vectors, this model classifies the current market regime in real-time. The resulting Regime ID (an integer) serves as a second context token for the ViT, providing additional contextual information about the current market state.

- **Context Token Integration:** The ViT is modified to accept both the Dynamic Plane image tensor and the two context tokens (asymmetric feature vector and Regime ID) as input. These tokens influence the self-attention mechanism, enabling the model to learn relationships between the visual information from the images and the contextual information.

### B. Loss Function

A risk-averse loss function will be used to penalize underestimation of losses more heavily than overestimation, aligning with the project's goal of downside protection. This will be achieved by applying a greater penalty when the actual return is worse than the predicted return. The specific implementation details will be determined through experimentation. This approach prioritizes minimizing the risk of significant losses over maximizing potential gains.

### C. Attention Mechanism Enhancement (Future Consideration)

While not implemented initially, a state-dependent attention mechanism is a potential future enhancement. This would adapt the ViT's attention mechanism to dynamically adjust its focus based on the prevailing market state (e.g., volatility or a calculated "threat level"). This would allow the model to prioritize different aspects of the input data based on the current market environment. This enhancement will be considered if initial results warrant further exploration.

## II. Model Development and Training

This section details the architecture and training process of the models, focusing on incorporating contextual information using a dual-token approach within a Vision Transformer (ViT).

### A. Model Architecture

The core model architecture utilizes a ViT designed to process candlestick chart images. To enhance the model's understanding of market regimes and specific trade dynamics, a dual-token context injection method is implemented. This involves injecting two distinct tokens into the ViT input alongside the image data:

1. **Regime ID Token:** This categorical token represents the current market regime identified by a separate, unsupervised clustering model (e.g., Gaussian Mixture Model or Self-Organizing Map). This provides high-level context for the model's predictions. The unsupervised clustering model will be trained offline using historical asymmetric feature vectors to identify 4-8 distinct market regimes.

2. **Asymmetric Feature Vector Token:** This continuous token encapsulates a raw, asymmetric feature vector (e.g., upside/downside volatility, skewness) providing granular detail about the specific market conditions. The ViT will learn to leverage this information to refine its predictions and provide more nuanced trade signals. The importance of this vector will be assessed during training. If it doesn't provide additional explanatory power beyond the regime identifier, its inclusion will be reevaluated to optimize computational efficiency.

This dual-token approach offers flexibility and a non-destructive way to integrate contextual information, balancing discrete regime explanations (via the Regime ID) and detailed market information (via the feature vector). The ViT's self-attention mechanism will learn to weigh the importance of each token in conjunction with the visual information from the candlestick chart. This is in contrast to embedding feature information directly within the images, which could obscure existing visual features. The dual-token approach allows the model to interpret broader regime changes in context with specific data points. This approach also supports the Narrative_Generation_Service by providing comprehensive context, enabling richer explanations of trade rationale.

### B. Training and Validation

The training process involves the following key steps:

1. **Unsupervised Clustering Model Training:** An unsupervised clustering model (GMM or Self-Organizing Map) is trained offline on historical asymmetric feature vectors to identify distinct market regimes. This training is independent of the main ViT training.

2. **ViT Training:** The ViT is trained using candlestick chart images, Regime ID tokens, and Asymmetric Feature Vector tokens. The model will be evaluated on its ability to accurately predict desired trading signals (e.g., buy/sell). Appropriate metrics such as Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) will be used for evaluation and performance tracking. Hyperparameter tuning and optimization will be performed to maximize model performance.

Further investigation will be conducted to evaluate the impact of different terrain categorizations on model performance. This involves comparing performance using individual environmental metrics versus terrain-based input and quantifying the impact on prediction accuracy and other relevant metrics. However, a potential trade-off exists between model simplicity and information loss due to the reduction in input features when using terrain categorization instead of raw environmental metrics (e.g., humidity, temperature, Air Quality Index). This potential loss of granularity will be investigated.
The model architecture for predicting stock market returns leverages a Convolutional Neural Network (CNN) trained on 5-day candlestick chart images. These images encapsulate Open, High, Low, Close (OHLC) price and volume data, but exclude explicit dates, actual price values, and ticker symbols. The CNN predicts both trend direction and return magnitude by recognizing visual patterns within these charts.

The CNN’s output is used to generate buy/sell signals based on the top/bottom 10% of predicted returns. Specifically, the model ranks predicted returns for a universe of stocks. The top 10% are selected for buy signals (long positions), while the bottom 10% generate sell signals (short positions). Trade execution logic, using actual prices from t+1 to t+5 (not available to the model during prediction), evaluates the effectiveness of these signals. This process ensures alignment between the 5-day input window and the 5-day evaluation period. A detailed explanation of the transformation of the CNN's image input into numerical trade signals will be documented.

The primary evaluation metric will be Mean Squared Error (MSE) or Root Mean Squared Error (RMSE). The return label used for training is calculated as: `Return(t+h) = (Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last day of the 5-day input window, and `h` (a hyperparameter ranging from 1 to 5) is the holding period.

A key refinement to the architecture is _Prediction Magnitude Filtering_, where trades are executed only when the absolute predicted return exceeds a predetermined threshold, under the hypothesis that higher magnitude predictions are more reliable. Various threshold values will be optimized during backtesting.

Beyond the core CNN model, this project will explore hybrid architectures combining CNNs with LSTMs or Transformers to capture sequential information across multiple candlestick windows. A Vision Transformer (ViT) will also be implemented, processing sequences of candlestick windows. This implementation will incorporate delta features, a dedicated input pipeline for handling variable-length sequences of _N_ images, positional embeddings, masking, and padding.

The input data format, specifically the representation of OHLC data within the 5-day window, will be rigorously verified. The final dataset size (_n_) will be determined, and edge cases for the final _n_ data points of each candlestick graph will be addressed.

## Model Architecture

This section details the architecture of the CNN-based trading agent, including input data, model variations, and enhancements for robust trading.

### Input Data and Preprocessing

The model operates on sequences of five-day candlestick chart images. Each image encapsulates Open, High, Low, and Close (OHLC) prices, volume, and a moving average derived from historical OHLCV data. Crucially, external identifiers like company names or ticker symbols are excluded, ensuring the model focuses solely on price action. The target output for each image sequence is the calculated five-day future return. This framework leverages the familiar visual representation of candlestick charts as direct input for the machine learning model.

### Core Model and Variations

The primary model is a Convolutional Neural Network (CNN) trained to predict five-day future returns from sequences of five-day candlestick chart images. To capture temporal dependencies and enhance performance, hybrid architectures combining CNNs with Long Short-Term Memory networks (LSTMs) or Transformers will be explored.

While the core model emphasizes visual patterns, the potential for incorporating fundamental and macroeconomic data will be investigated. This could involve integrating such data as additional input features or utilizing a separate model to process this information and combine its output with the CNN's predictions.

### Trade Selection and Uncertainty Estimation

The current trading strategy uses a decile ranking system (buying the top 10% and selling the bottom 10% of predictions). An alternative approach based on maximum prediction accuracy percentage will be evaluated and compared to the decile ranking method.

To enhance robustness, model uncertainty estimation will be incorporated using techniques like Monte Carlo Dropout, Bayesian CNNs, or Ensemble Models. This allows the agent to prioritize high-conviction trades by considering the standard deviation of predicted returns alongside the predicted return itself.

### Confidence-Based Filtering and Ranking

A hybrid confidence-ranking approach will be implemented, combining confidence-based filtering with ranked predictions. Trades will be filtered using a confidence metric derived from Historical Prediction Error Profiling (HPEP). Post-training, a confidence profile is built by binning validation set predictions based on predicted return magnitudes and calculating the accuracy within each bin. During backtesting, trades are executed only if the historical accuracy for the corresponding prediction bin exceeds a predefined threshold. This threshold acts as a tunable hyperparameter. Implementation details within `test_model.py` (post-training) and `trade.py` (filtering) are crucial, and a working prototype with dummy data is recommended.

### Hyperparameter Optimization and Training

A key hyperparameter, `confidence_threshold`, representing the minimum required prediction accuracy for trade execution, will be introduced and tuned using methods like grid search or Bayesian optimization. Joint optimization of hyperparameters, including `confidence_threshold` and `holding_days` (trade duration), will also be explored.

The choice of a five-candlestick input window is supported by the findings of Jiang et al. (2023). Separate CNNs will be trained for holding periods from one to five days, and performance metrics (validation loss, Sharpe ratio, alpha, and Mean Squared Error) will be tracked for each model to determine optimal strategies.

## A. Model Architecture and Specialization

This section details the architecture of the Convolutional Neural Network (CNN) model used for trade signal generation, along with planned modifications to improve trading accuracy and incorporate risk management. While the initial implementation demonstrated potential for positive alpha generation, particularly within the small-cap First North All-Share index (+8.89% annual alpha after transaction costs, +37.57% benchmark outperformance), several enhancements are planned. These address key weaknesses related to trading implementation, strategy, and the handling of uncertainty in predictions.

**Initial Implementation and Challenges:**

The current model processes 5-day candlestick chart images to predict market movements, generating buy and sell signals based on the top and bottom 10% of predicted price movements. However, this strategy exhibits several limitations:

- **High Turnover:** Executing trades every 5 days leads to excessive transaction costs, eroding profitability. Note that while a 5-day input window is supported by existing research, the corresponding 5-day prediction (output) window is an independent design choice and lacks similar empirical validation. This output window length may have been chosen for symmetry or practical considerations but requires further investigation.
- **Uniform Trade Weights:** Assigning equal weight to all trades fails to capitalize on varying prediction confidence levels.
- **Lack of Smart Trade Filtering:** Including low-confidence predictions contributes to high turnover and reduces overall performance.
- **Impact of Short-Selling Constraints:** The positive alpha generated within the small-cap segment needs further analysis considering potential limitations on short selling due to borrow availability. A deeper understanding of the handling of unsuccessful trades, especially in the absence of a stop-loss mechanism, is also crucial.

**Architectural Enhancements and Specialization:**

The following modifications will be implemented to address the identified weaknesses:

**1. Transition to Probabilistic Predictions (Soft Labeling):**

Instead of predicting a single scalar return value, the model will be enhanced to predict a probability distribution over discretized return bins (e.g., -5% to +5% in 0.5% increments). This shift involves several architectural changes:

- **Output Layer Modification:** The output layer will be replaced with a fully connected layer followed by a softmax activation function to generate a valid probability distribution.
- **Soft Label Generation:** Instead of hard labels, the model will be trained using soft labels representing a probability distribution over the return bins, likely generated using a Gaussian kernel centered around the observed return. This approach captures the inherent uncertainty in financial markets.
- **Loss Function Change:** The loss function will be changed from Mean Squared Error (MSE) to one compatible with probability distributions, such as Categorical Cross-Entropy or KL-Divergence.

**2. Risk Management and Portfolio Optimization:**

- **Prediction Confidence Thresholding:** Trades will be executed only when the predicted return magnitude or confidence level exceeds a predetermined threshold. Alternatively, Historical Prediction Error Profiling (HPEP) could be used to filter trades based on the historical accuracy of predictions within a specific range.
- **Dynamic Trade Filtering Layer:** A prototype dynamic trade filtering layer, using either predicted return volatility or HPEP data, will selectively allow trades based on their predicted risk and potential return. Backtesting will be conducted to optimize filtering thresholds.
- **Stop-Loss Mechanism:** A stop-loss mechanism will be implemented to mitigate losses from adverse market movements.

**3. Uncertainty-Aware Architectures (Optional):**

To further enhance the model's ability to express uncertainty, techniques like Monte Carlo Dropout, Deep Ensembles, or Bayesian CNNs may be incorporated to provide a measure of confidence alongside the predicted probability distribution. This would allow for more nuanced trading decisions.

## A. Model Architecture and Trade Management

This section details the model architecture for predicting stock market trends, focusing on return magnitude and rally time prediction. Crucially, it also outlines the integrated trade management and exit logic that governs trading decisions based on these predictions.

**Model Architecture:**

A multi-head Convolutional Neural Network (CNN) forms the core of the prediction model. This CNN uses a pre-trained EfficientNet as a feature extractor, feeding into two separate heads:

- **Return Regression Head:** Predicts the expected return as a scalar value, using a Mean Squared Error (MSE) loss function.
- **Rally Time Head:** Predicts the number of candlestick periods (k) required to reach the target return, also using an MSE loss function.

The overall loss function is a weighted sum of the MSE losses from both heads, balancing the importance of predicting both magnitude and time.

We will also explore survival analysis models (e.g., DeepSurv, DeepHit, Weibull Time-To-Event models) to predict the probability distribution of reaching the target return over time. These models handle censored data effectively and provide a more nuanced understanding of time-to-target prediction by expressing uncertainty over time.

The model receives candlestick chart images as input. Further details regarding the input data format, specific layers, and training procedures are detailed elsewhere in this document.

**Trade Management and Exit Logic:**

The following exit strategies are integrated into the trading framework:

- **Dynamic Stop-Loss:** A stop-loss mechanism triggers an exit if the stock price moves adversely by a specified percentage relative to the predicted return (e.g., exit if the price drops 2.5% when a 3.0% return was predicted). This dynamic approach improves upon fixed holding periods. A prototype module will be developed and backtested to optimize stop-loss levels.

- **Volatility-Aware Exit Thresholds:** Exit thresholds dynamically adjust based on recent market volatility using the Average True Range (ATR). Exits are triggered if adverse price movement exceeds a multiple of the current ATR (e.g., 1.5 \* ATR), preventing premature exits during high volatility.

- **Time-Based Confidence Decay:** Trades are exited if the predicted reward doesn't materialize within the predicted rally time, acknowledging the limited validity of predictions.

- **Prediction Divergence Exit:** If retraining occurs, significant divergence (e.g., 3-4%) between current and previous predictions for a given trade triggers an exit, ensuring consistency with the most up-to-date market understanding.

- **Portfolio Contextual Exit:** Underperforming trades within similar prediction groups are exited based on relative performance, adding a layer of internal attribution.

- **Risk-Based Position Sizing:** A risk-based weighting scheme, considering prediction confidence, inverse historical volatility, and the signal-to-noise ratio of predicted versus actual returns, determines position sizes. This prioritizes higher-conviction, lower-risk trades.

**Model Validation and Refinement:**

During validation, an error map tracking predicted versus actual returns will identify significantly inaccurate predictions. Analyzing these errors will inform adjustments to the training process, such as filtering or re-weighting training samples, to improve future predictions.

### A. Model Architecture

This section details the architecture of the model for predicting stock market trends based on candlestick chart images. The primary architecture is a Vision Transformer (ViT) designed to process sequential input and leverage delta features. A dual-input CNN model is also explored, along with various prediction mechanisms.

**ViT with Sequential Candlestick Windows and Delta Features:** The core model is a ViT, chosen for its ability to capture long-range dependencies in sequential data. The model will process sequences of N candlestick chart images (where N will be experimentally determined, initially focusing on N=3, 4, and 5, representing 15, 20, and 25 trading days, respectively). Delta features, representing changes between consecutive charts, will be incorporated to enhance sensitivity to price action. These delta features will be calculated by either subtracting pixel values directly or subtracting features extracted by a pre-trained model (e.g., EfficientNet). The efficacy of both methods will be compared.

**Dual-Input CNN Model:** A comparative CNN architecture with a dual-input structure is also under investigation. This model receives the current candlestick chart and a delta frame (the difference between the current and previous charts) as input. This design explicitly captures frame-to-frame changes to assess their impact on predictive performance.

**Input Data and Feature Engineering for the ViT:** The ViT model will utilize sequences of candlestick charts as input, providing temporal context. Delta features will be calculated between consecutive charts in the sequence to highlight the dynamics of price and volume changes. Two methods for calculating delta features will be investigated: subtracting raw pixel values and subtracting features extracted from the images.

**Dataset Design for the ViT:** A dataset of image sequences will be constructed using a sliding window approach. Each sequence will comprise N candlestick charts (where N is the sequence length). The corresponding labels for each sequence will include:

- **Return Value:** The calculated return over a defined holding period.
- **Rally Time:** The duration of any price rally.
- **Signal Class:** A categorical label (e.g., BUY, SELL, HOLD).

**Prediction Targets:** The models will be designed to predict one or more of the following: a scalar reward value, a categorical label (BUY/SELL/HOLD), or a combination of reward and rally time using a multi-head output structure. This flexibility enables experimentation with various prediction targets and trading strategies, aligning with the goal of predicting both the direction and magnitude of market movements.

**Alternative Architectures Considered (Benchmarking):** To determine the optimal approach for incorporating temporal context, the performance of the ViT and dual-input CNN models will be benchmarked against simpler models using single static images and pairs of consecutive images. This comparison will inform the final model selection.

While a sequence generator and backtesting are mentioned in the project checklist, those elements pertain to the training/validation and testing/evaluation phases, respectively, and are therefore outside the scope of this architectural overview.

### A. Model Architecture (Specialization)

This section details the architecture of the model used for predicting stock market trends. The core innovation is the use of a Vision Transformer (ViT) to predict future candlestick images rather than directly predicting numerical returns. This image-based approach is hypothesized to capture richer information and potentially mimic how human traders interpret visual patterns. This section also covers the specialized input pipeline for the ViT, memory management strategies, and associated risk assessments.

**ViT Input Pipeline and Data Handling:** A dedicated input pipeline will be developed to handle _N_ candlestick images as input tokens for the ViT model. To accommodate variable-length input sequences, a maximum sequence length will be defined (e.g., 5 charts). Shorter sequences will be padded with blank chart images, and a mask vector will inform the transformer which parts of the sequence are actual data and which are padding. This allows the model to be trained with variable-length sequences, leveraging the masking and padding mechanism to improve generalization to real-world scenarios. Positional embeddings will be implemented to provide the ViT model with information about the order of candlestick images within the input sequence, enabling the model to understand temporal relationships and make more accurate predictions.

**Predicting Candlestick Images:** This model shifts from predicting numerical return values to predicting future candlestick images. The model learns to generate images of future candlestick patterns, from which returns are then extracted. This visual sequence forecasting paradigm is inspired by how human traders visually interpret candlestick patterns. This approach necessitates a thorough evaluation to ensure it effectively captures this human-like visual interpretation and translates to improved predictive power.

**Theoretical Advantages of Image-Based Prediction:** Predicting images instead of raw numerical returns offers several potential advantages:

- **Representation Richness:** Candlestick images encapsulate more information than just open, high, low, and close prices, implicitly representing patterns and relationships that might be lost when reducing to numerical returns.
- **Uncertainty Modeling:** Generating an image allows for representing the uncertainty inherent in future price movements, potentially reflected in the clarity or "fuzziness" of the predicted candlestick image.
- **Causal Reasoning:** Learning to generate visually recognizable candlestick patterns may implicitly teach the model the underlying causal relationships between these patterns and subsequent market movements.
- **Richer Supervision:** Images as training targets may provide richer supervision signals compared to simple numerical returns.
- **Interpretability:** The generated images can offer more intuitive insights into the model's predictions and reasoning process.
- **Generative Flexibility:** This approach may enable the use of generative models to explore different possible future scenarios.

**Risk and Drawback Assessment:** The image-based prediction approach introduces risks and drawbacks requiring careful consideration:

- **Indirect Evaluation:** Evaluating model performance based on image similarity (e.g., SSIM or LPIPS) doesn't directly reflect trading profitability.
- **Compounding Errors:** Small discrepancies in pixel values during image generation can compound and lead to significant deviations in predicted returns.
- **Loss of Direct Reward Supervision:** Training on image similarity instead of direct return prediction may lead to the model prioritizing visual fidelity over profitable trading signals.
- **Ambiguity of Financial Implication:** Visual similarity doesn't always translate to clear financial implications.

**Evaluation of the Candlestick Pattern Prediction Approach:** The efficacy of this approach will be rigorously compared to traditional numerical return prediction, focusing on improvements in prediction accuracy while acknowledging the potential increase in complexity.

**Memory Management:** Three memory management options (A, B, and C) will be experimentally evaluated using a consistent dataset. The experiment will compare their flexibility, performance, and training efficiency. Metrics for evaluation include Sharpe Ratio, Directional Accuracy, Mean Squared Error (MSE), and Rally-Time Prediction Accuracy. The impact of added temporal context on predictive power will also be investigated. This structured experiment will inform the final memory management strategy selection.

**Reusable Components:** Reusable components for data loading, masking logic, and ViT wrappers will be developed to facilitate experimentation and future research with financial time series data.
Candlestick patterns can yield different market outcomes depending on broader context. This document details the model architecture for predicting stock market trends based on candlestick chart images, focusing on visual pattern recognition and its translation into actionable trading decisions.

**Comparison with Existing Models:**

This visual generator model differs from traditional scalar regression and probabilistic return models:

| Feature                       | Visual Generator Model                        | Scalar Regression | Probabilistic Return Model |
| ----------------------------- | --------------------------------------------- | ----------------- | -------------------------- |
| Output Type                   | Image                                         | Scalar value      | Probability distribution   |
| Supervisory Signal            | Image similarity                              | Return value      | Return distribution        |
| Link to Trading               | Indirect (via extracted returns)              | Direct            | Direct                     |
| Richness of Learned Structure | Potentially higher (captures visual patterns) | Lower             | Moderate                   |
| Interpretability              | Lower (black box nature of image generation)  | Higher            | Moderate                   |
| Risk of Ambiguity             | Higher                                        | Lower             | Lower                      |
| Data Requirements             | Higher (image data)                           | Lower             | Lower                      |
| Modeling Complexity           | Higher                                        | Lower             | Moderate                   |

**Return Extraction from Predicted Images:**

Extracting meaningful financial data from generated candlestick images is crucial. Two potential methods are:

- **Pixel Location Analysis:** Analyzing pixel locations of key candlestick features (open, close, high, low) to derive corresponding price values.
- **Rendering Predicted Image Data:** Converting the predicted image data back into a numerical representation directly interpretable as OHLC values. This requires a well-defined mapping between image pixels and price values.

**Prototype Design:**

The prototype image-to-image candlestick forecaster will use either a CNN decoder or a transformer-based image generator, determined through experimentation. It will generate future candlestick images based on an input sequence of historical candlestick images.

**Model Architecture:**

The model architecture aims to predict stock market trends from 5-day candlestick chart images, stripped of explicit date, price, and ticker information to emphasize visual patterns. Convolutional Neural Networks (CNNs) generate trade signals based on these patterns, identifying potential buy and sell opportunities based on the top and bottom 10% of predicted values. These numerical outputs are then transformed into actionable trade signals.

The model's objective is to predict both trend direction (up or down) and the magnitude of potential reward, using separate prediction mechanisms for each. The training goal is not just high prediction accuracy but also understanding the underlying reasons for observed patterns, ensuring causally grounded predictions for robust trade signal generation. Performance will be evaluated using metrics like Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).

Several architectural enhancements are being explored:

- **Hybrid Models:** Combining CNNs with LSTMs or Transformers to leverage sequential dependencies.
- **Vision Transformer (ViT):** Implementing a ViT using either EfficientNet or patch embeddings, processing sequential candlestick windows with delta features as additional input. This requires careful design of the image sequence dataset, including masking, padding, and positional embeddings for variable-length sequences. Developing an efficient input pipeline for processing multiple images for the ViT is crucial.

A key challenge is grounding predictions in causal relationships, not just visual patterns. The model should understand _why_ specific candlestick patterns lead to particular market movements, not just _what_ they look like. This causal grounding is crucial for generalization to unseen market conditions. A dual-module approach is being considered: one module generates predicted candlestick charts, and a second, distinct module evaluates those charts for actionable trading opportunities, providing clear entry and exit signals. This allows specialized development and optimization of each component.

Additionally, a simpler 2D representation of motion using rotational axes and a dynamic center point is being investigated as an alternative to a more complex 3D model. A key investigation will compare...
This section details the model architecture, specializing in a dynamic 2D representation of market data as an alternative to a fixed 3D Cartesian frame. This approach draws inspiration from concepts like Frenet frames and tangent planes, dynamically projecting market data onto a constantly evolving 2D plane.

This 2D representation utilizes a plane with rotational axes and a dynamic origin that shifts with market movements. Several key architectural considerations arise:

- **Coordinate Transformations and Manifolds:** The model leverages coordinate transformations and manifolds to achieve this dynamic representation. Further research will explore these concepts, including the notion of "bending space," to refine the implementation and interpretation of the 2D plane. Specifically, the dynamic plane can be analyzed as a moving frame on a 1-dimensional differentiable manifold representing the time series of market data. Attaching an orthonormal frame to each point on this manifold and rotating it along the price trajectory provides a framework for understanding evolving market dynamics.

- **Encoding Curves in the Moving Chart:** Representing curves, such as parabolic trajectories, within this dynamic 2D chart is crucial. Research will focus on encoding such curves, typically represented along the z-axis in a 3D frame, through rotations of the axes within the 2D plane. This concept is analogous to visualizing a parabolic trajectory by rotating the X and Y axes at the trajectory's starting point and dynamically shifting the origin to the endpoint. This serves as a proof of concept, utilizing axis rotation and dynamic origin refocusing instead of fixed axes and an absolute origin.

- **Information Balance and Degrees of Freedom:** Balancing information and degrees of freedom is critical. The 2D representation uses two coordinates (u,v) and three Euler angles (or a rotation matrix) to describe the frame's orientation. Careful analysis of this balance is crucial for effective model development.

Applying this 2D framework to the three-body problem presents a unique challenge: representing three bodies in 3D space within a 2D framework. Several approaches will be investigated:

- **Pairwise Relative Coordinates:** Using relative coordinates between pairs of bodies reduces dimensionality while preserving interaction information.
- **Barycentric Frame:** A barycentric frame, with the origin at the center of mass, could offer a simplified and stable reference point.
- **Shape Space Representation:** A 2D triangle shape space could capture the overall geometry of the system, focusing on relative configurations.

This investigation will also incorporate recent advancements in the three-body problem, including machine learning approximations for solutions within the 2D framework and the representation of newly discovered periodic solutions.

Analogously, this 2D representation will be applied to stock market prediction, abstracting 3D candlestick data (price, volume, time) into a 2D plane. Key considerations include:

- **Dimensionality Reduction:** The model will employ dimensionality reduction techniques, carefully considering their mathematical soundness. This involves exploring concepts like coordinate charts, Frenet-Serret frames, stereographic projections, shape space, and moving frames. The balance between information completeness and the introduction of orientation parameters will be a central focus.

- **Mapping Explanation:** A clear explanation of the 3D to 2D mapping is crucial, detailing how orientation and scaling are handled and how market complexities are captured or simplified.

- **Moving Frame Implementation:** A moving frame, similar to a Frenet-Serret frame, will represent the 3D candlestick data curves within the 2D chart. This implementation will utilize two coordinates (u, v) for point location and three parameters for the chart’s position and rotation within the original 3D space. Potential simplifications, such as fixing a "center of mass" (e.g., a reference price or average volume), will be explored.

Finally, a thorough analysis of the benefits and limitations of these abstractions is necessary, considering diverse use cases beyond financial markets (e.g., robot arm trajectories, VR headset paths, N-body dynamics, computer graphics). This broader perspective will help identify potential weaknesses and strengths. The analysis will also address the implications of incorporating orientation data, including situations where translational and rotational symmetries might be redundant, ensuring that model complexity is justified by performance gains. Additionally, the potential for singularities within the continuous frame fields and the persistence of chaos, particularly in the context of the three-body problem and the 2D representation, will be investigated.

## Model Architecture and Visualization

This section details the model architecture, focusing on its specialization for navigating market dynamics visualized as a 3D helix projected onto a 2D chart, and the specific considerations for implementing this visualization. The core model leverages CNNs, LSTMs, Transformers, and ViTs (detailed elsewhere in the project documentation). This section addresses how the model interacts with 2D and 3D representations of market data.

The model addresses the challenge of representing complex, three-dimensional market dynamics (price, volume, time) within the simplified two-dimensional space of a candlestick chart. This necessitates incorporating orientation or state information to avoid overfitting to historical data and enable effective forward traversal of the market space. Inspired by biological navigation principles, the model utilizes both egocentric (relative to the current candlestick window) and allocentric (broader market context) frames of reference, similar to O'Keefe's work on place and grid cells. This dual-frame approach allows the model to capture both local price fluctuations and overall market trends, informing the design choices for processing sequential input and extracting features related to both relative and absolute price changes.

To visualize the model's understanding of market traversal, a 3D helix metaphor is employed. This helix represents the model's path through the market space. The visualization challenge lies in dynamically manipulating a 2D chart's axes to simulate movement along this 3D helix. Two potential strategies are considered:

1. **Constant Speed along the u-axis with Rotating Frame:** The point moves linearly along the horizontal axis (u-axis) while the chart's axes rotate and tilt to simulate the upward spiral motion, representing the helix's curvature. The axes continuously align with the helix's tangent and normal vectors.

2. **Constant Speed along a Circular Path with Slowly Tilting Frame:** The point follows a circular path in the 2D plane, and the frame of reference tilts gradually to mimic the upward movement of the helix. This requires careful synchronization between the circular motion and the tilting frame.

The chosen strategy (documented elsewhere) must adhere to defined helix parameters (radius, pitch) that dictate the curvature and torsion of the spiral. An orientation stream `E(s)` guides the axis alignment using either vertical axis rotation or rotational matrices, ensuring accurate visual representation.

The following visualizations are generated to aid in understanding the model's behavior and interpreting its predictions:

- **Visual Data Representation:** Candlestick charts visualize the raw OHLCV data and serve as a core input to the CNN models. Additional visualizations illustrate data preprocessing steps (normalization, windowing).

- **3D Helix Visualization:** A standalone 3D helix (radius 1.0, pitch 0.5, 4 turns) is plotted using `matplotlib` with labeled X, Y, and Z axes and the title "3D Helix in Laboratory Coordinates."

- **2D Trace Visualization:** A separate, clear 2D visualization represents the 3D helix projected onto a 2D plane. This visualization prioritizes clarity and may incorporate tooltips, annotations, or accompanying documentation for enhanced interpretability.

- **Data Storage for Visualization:** Planar coordinates (u, v) and orientation data (potentially using an arc length counter and quaternions or rotation vectors) are stored to enable reconstruction of the visualized spiral path. The storage mechanism is designed to respect memory and storage constraints.

The visualizations are generated using `matplotlib` and `numpy`. The use of `seaborn` is excluded. Each visualization is presented as a separate figure to ensure clarity and facilitate focused analysis. This emphasis on clear and effective visualization is crucial for both understanding the underlying data patterns and interpreting the model's predictions.

## Model Architecture and Dynamic Projection

This section details the model architecture, focusing on its specialization for processing candlestick chart data and the integration of a dynamic projection system. We deviate from a conventional three-dimensional framework for analyzing market movements and explore a novel two-dimensional rotational plane framework. A comparative analysis between the traditional 3D and proposed 2D approaches will assess their respective advantages and disadvantages in representing market dynamics. This analysis will consider the impact of the dynamic origin and rotating axes within the 2D model on interpreting price movements and trade signals. Furthermore, we will investigate the feasibility and potential benefits and challenges of fully transitioning to the 2D representation. A concrete experimental plan will outline specific implementation steps, success metrics, and anticipated hurdles in moving from the 3D framework to the 2D plane.

The core of this 2D framework is a dynamic projection system that rotates and re-centers candlestick image sequences using Principal Component Analysis (PCA). This dynamic projection prioritizes _relative local movement_ over _absolute position_ within the chart. By operating in a locally optimized frame of reference, the model learns patterns in relative price behavior, independent of absolute price levels, enhancing its generalization across diverse market conditions and achieving prediction invariance to previous trend direction.

This dynamic projection system integrates with either a Convolutional Neural Network (CNN) or a Vision Transformer (ViT). For each new candlestick or group of candlesticks, PCA re-centers and rotates the feature space before the subsequent prediction. This “rotation” corresponds to a PCA rotation of the embedded image/feature vectors, aligning the principal components of the data with the feature space axes and capturing the directions of maximal variance in local price movements. This approach differs from affine transformations or learned rotations by leveraging the inherent variance structure within the candlestick data.

Integrating dynamic PCA into the CNN/ViT pipeline presents several implementation options, each with its own technical considerations:

1. **Pre-processing on raw data:** Applying PCA directly to the raw data before feeding it to the model. This requires a comprehensive preprocessing pipeline encompassing time window selection, feature preparation, PCA computation, principal direction identification, feature space rotation, data re-centering, and input preparation.

2. **Applying PCA to learned image embeddings:** Leveraging the CNN's feature extraction capabilities by applying PCA to the learned image embeddings.

3. **PCA as a fixed layer in the network:** Integrating PCA as a fixed layer within the network, enabling the model to learn the transformation alongside other parameters. This approach necessitates addressing the non-differentiability of standard PCA computation for backpropagation, potentially through differentiable approximations or training modifications.

4. **Image-level rotation as a data augmentation layer:** Implementing PCA-based rotation as a data augmentation technique to enhance model robustness to variations in data orientation. This requires careful consideration of implementation details, including feature extraction, dynamic PCA calculation, and input formatting.

Crucially, integrating dynamic transformations, especially PCA as a network layer, raises key technical considerations for backpropagation:

- **Non-differentiability of PCA computation:** Standard PCA's non-differentiability poses challenges for backpropagation.
- **Gradient flow:** Ensuring proper gradient flow back to the original features.
- **Processing method:** Choosing between batch and sequential processing.
- **Training mode:** Determining online versus offline PCA computation.
- **Gradient impact:** Assessing the impact of dynamic transforms on model gradients and potential instability.
- **Differentiable approximations:** Exploring differentiable approximations of PCA.

## Model Architecture (Specialization)

This section details the model's specialized architecture designed to handle image-based input and incorporate a dynamic, rotational axis paradigm. The model accepts images as input, such as candlestick charts or Heiken Ashi charts, which dictates the initial stages of the architecture.

A key feature is the dynamic plane representation of price movements. This involves rotating and translating the coordinate system based on local price action, providing a dynamically adjusted market perspective. This dynamic plane is generated and used as the primary input for the model.

### Dynamic Plane Implementation

The dynamic plane is constructed as follows:

1. **Local Movement Vector Calculation:** The local movement vector (v) is calculated as the difference between the current and previous price points (both represented by their time and price coordinates):

   ```
   v = P_current - P_previous
   ```

   where `P_current` and `P_previous` represent the current and previous price and time coordinates, respectively.

2. **Dynamic Rotation Implementation:** The coordinate system is dynamically rotated at each time step to align the X'-axis with the local movement vector. The Y'-axis is then orthogonal to the new X'-axis. This rotation is applied to a small window of recent candlestick data (e.g., the last 5-10 candlesticks).

   - **Rotation Matrix Calculation:** The rotation angle (θ) is calculated using the arctangent of the local movement vector components:

     ```
     θ = arctan(v_y / v_x)
     ```

     This angle is used to construct the rotation matrix R(θ):

     ```
     R(θ) = [[cos(θ), sin(θ)], [-sin(θ), cos(θ)]]
     ```

     This rotation matrix is then applied to the surrounding candlestick window.

3. **Dynamic Origin Shift:** The origin of the coordinate system is translated to the current price point (0,0) after rotation. This centers the price movement, effectively leveling the trend horizontally, with price oscillations represented as deviations around 0 along the Y'-axis.

### Dynamic Snapshot Generation

The model utilizes dynamically generated candlestick chart images as input. These images are created by redrawing the candlestick data relative to the newly rotated and translated axes. This process transforms the representation of time, making it non-linear within the image frame and aiming to capture relative price movements more effectively.

The following technical challenges are addressed:

- **Rotation Artifacts Handling:** Image distortions introduced by rotation are mitigated using interpolation techniques like anti-aliasing.
- **Volatility Jump Handling:** Abrupt rotations caused by large price movements are smoothed or limited by constraining the change in rotation angle between frames.
- **Consistent Axis Scaling:** A uniform scaling of the axes (units per % move) is maintained across all generated frames to prevent biases in the model.

A detailed pseudocode pipeline for the dynamic rotating snapshot generator will be documented separately. A `RotatingSnapshotGenerator` module will encapsulate this logic. A conceptual diagram illustrating the dynamic rotation on a sequence of price movements will also be provided.

### Dimensionality Reduction and Risks

This dynamic plane approach reduces the complexity of a traditional 3D model (time, price, volume) to a 2D plane with two rotational axes. This simplification aims to reduce computational complexity while capturing the essence of price movement.

However, using dynamic PCA introduces potential risks, including overfitting to noise in small windows, computational overhead, integration complexity, loss of the absolute reference frame, instability and jitter, model dependency on the transform, complexity of explanation, edge cases, and baseline fairness. These risks will be mitigated through careful implementation and evaluation.

The performance of this approach will be rigorously evaluated against baselines like no PCA, static PCA, simple transforms, and alternative data-driven focus methods. An ablation study of PCA without re-centering and vice-versa will also be conducted. The computational cost will be assessed, and optimizations or alternative dimensionality reduction techniques explored if necessary. The evaluation will utilize metrics including Mean Squared Error (MSE), Mean Absolute Error (MAE), Directional Accuracy, Sharpe Ratio, Hit Rate, Profit Metrics, Compute Efficiency, and Model Convergence.

## Model Architecture

This section details the architecture of the models used, focusing on the dynamic plane generation process and its integration with both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).

The core of the architecture revolves around a novel approach: representing candlestick data within a dynamically generated 2D plane. This plane, the output of the Rotating Dynamic Plane Generator, serves as input for both CNNs and ViTs. The generator takes candlestick data (time, price, and optionally volume) as input. It constructs the plane based on local movement patterns within a sliding window (e.g., 5-10 candles). This process involves calculating movement vectors within the window, applying Principal Component Analysis (PCA) to these vectors to define the primary axes of the plane, and rotating the plane based on the evolving market dynamics. The top two principal components from the PCA form the axes of this dynamically redrawn 2D plane, capturing the dominant trajectory of price movement and any residual or oscillatory behavior. A crucial aspect is the recalculation of the rotation angle (theta), which governs the plane's orientation. This calculation considers the dynamic origin and rotational axes within the 2D plane, moving beyond a simple price difference calculation.

A detailed pseudocode algorithm for the Rotating Dynamic Plane Generator will be developed, encompassing: data input, defining the local window, calculating movement vectors, applying PCA, rotating the coordinate system, and rendering the 2D plane as an image. A conceptual diagram will visualize the evolution and shifts of the dynamic 2D plane with each market movement, further illustrating the dynamic coordinate system. The generator will be implemented in Python using libraries such as NumPy, Matplotlib, and PIL, and optimized for batch operation to efficiently process entire time series of candlestick data. The generated 2D plane, formatted as an image, will then be the input for the ViT and CNN models.

**Specific Architectural Considerations for ViTs:**

- **Dynamic Candlestick Snapshots:** The ViT will utilize the dynamically generated 2D plane snapshots as input, offering a novel approach to representing market dynamics for transformer-based models. An experiment will compare the performance of models trained on these dynamic frames versus traditional static candlestick chart images (See _Experiment: Static vs. Dynamic Frames_ below).

- **Pipeline Integration:** The Rotating Dynamic Plane Generator will be integrated directly into the ViT training pipeline for efficient data flow.

- **Configurable Window Size:** The generator will incorporate a configurable window size parameter, controlling the number of past data points considered when generating the dynamic snapshots. This allows adaptability to different market conditions and trading strategies. (See _Experiment: Window Size Parameter_ below)

- **Optional Volume Integration:** The architecture allows for the optional inclusion of volume data as a third dimension in the movement vector calculations within the generator. This allows exploration of different feature combinations and their impact on model performance. (See _Experiment: Volume Inclusion_ below)

**Input Data Visualization and Transformation Examples:**

To enhance understanding, the following visualizations will be provided:

- **Original Candlestick Charts:** Example images of raw candlestick chart inputs (before transformation) will demonstrate the initial data presented to the model.

- **Transformed 2D Plane Snapshots:** Example images of the transformed inputs (after dynamic plane generation) will showcase the effect of the coordinate system transformation.

- **Dynamic Plane Animation:** An animation will visualize the step-by-step redrawing of the plane as new data points become available, illustrating the dynamic nature of the transformation.

**Experiments and Functional Requirements:**

- **Experiment: Static vs. Dynamic Frames:** A comparative experiment will evaluate the effectiveness of using dynamic frames versus static candlestick frames as ViT input.

- **Experiment: Window Size Parameter:** Experiments will be conducted to determine the optimal window size for various market conditions.

- **Experiment: Volume Inclusion:** Experiments will assess the impact of incorporating volume data on model performance.

- **Minimum Data Points:** The generator requires a minimum of two data points to function correctly. This constraint will be considered during data preprocessing and model training.

## A. Model Architecture

This section details the model architecture, focusing on its ability to handle complex market patterns, its utilization of Heiken-Ashi candlesticks, and the technical considerations for dynamic plane rotation and animation.

### Data Input and Transformations

The model accepts standard OHLC candlestick data as input. This data is transformed into Heiken-Ashi candles using a dedicated function. This function takes an array of standard candles (time, open, high, low, close) and outputs an array of Heiken-Ashi candles (time, open_HA, high_HA, low_HA, close_HA). These Heiken-Ashi candles are then visualized as a candlestick chart, with green candles representing upward price movement (close >= open) and red candles representing downward movement (close < open).

Subsequently, a dynamic rotation and recentering transformation is applied to the Heiken-Ashi data, using the midpoint between each candle's open and close values. This transformed data is visualized on a 2D plane. Both the standard Heiken-Ashi chart and the dynamically rotated and recentered chart are saved as PNG files (`/mnt/data/standard_heiken_ashi.png` and `/mnt/data/rotated_dynamic_plane_heiken_ashi.png`, respectively) for use in the training pipeline.

### Dynamic Plane Visualization and Animation

The model's visualization component animates the movement of data points within a dynamically rotating and re-centering frame.

**Architectural Decisions:**

- **Minimum Data Points for Rotation:** To ensure stability, principal component analysis (PCA) and dynamic plane rotation are initiated only after three data points are available.

**Tasks:**

- **Rotation Matrix Smoothing:** Rotation matrices will be smoothed to prevent dimensional inconsistencies and ensure smooth transitions between animation frames.
- **Standalone Animation Simulator:** A separate simulator will be developed to visualize the dynamic plane step-by-step, incorporating delayed rotation, initial plane smoothing, and detailed visualization of each process step.

**Functional Requirements:**

- **Dynamic Point Movement:** The visualization must animate the movement of data points within the frame.
- **Live Frame Updates:** The animation must display real-time frame rotation and re-centering as new data points are introduced.

### Handling Technical Constraints

The model architecture incorporates specific mechanisms to handle technical challenges related to dynamic plane rotation and animation, particularly during the initial phases with limited data:

- **Single-Point Frames:** To prevent errors when the animation framework requires a minimum of two points, single-point frames are handled gracefully by displaying either a placeholder or an empty canvas.
- **Delayed Rotation and Plotting:** Rotation and plotting procedures are delayed until at least two valid data points are available, preventing errors in PCA/SVD calculations.
- **Offset Formatting:** Proper formatting of offsets prevents dimension mismatches, especially in initial frames with limited data points.
- **PCA Instability:** The model addresses potential instability in PCA calculations when data points collapse into a line, a situation that can occur in early frames. Mitigation strategies for this issue are being explored.

### Model Training and Testing

The core model utilizes candlestick chart images as input. Initial development prioritizes preparing the real dataset and constructing a batch generator for these images. Small tests will validate this core pipeline. The model will be tested using simulated data reflecting realistic market scenarios (rallies, drops, recoveries) and chaotic sideways market conditions to evaluate robustness. The model architecture accommodates both standard candlestick data and the transformed dynamic plane representation.
Simulating and Visualizing Market Regimes within a Dynamic Plane

This section details the visualization and analysis of different market regimes within the dynamic plane generated by Principal Component Analysis (PCA). This analysis informs model training and potential feature engineering by illustrating how market behavior manifests in the transformed coordinate system.

To demonstrate the impact of dynamic plane transformation on different market conditions, we will generate synthetic data representing a "choppy" market regime characterized by extreme volatility and rapid price fluctuations. This will be achieved using the `generate_choppy_candlesticks(n=30)` function, which outputs candlestick data (time, open, high, low, close). Both standard and rotated dynamic Heiken-Ashi charts will be generated from this data. The standard Heiken-Ashi chart will be created using `generate_heiken_ashi` and `plot_heiken_ashi_candlestick` and saved to `/mnt/data/standard_heiken_ashi_choppy.png`. The rotated dynamic Heiken-Ashi chart, generated using `dynamic_rotate_recenter_heiken` and `plot_rotated_heiken`, will be saved to `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`. Comparing these visualizations allows for assessing the impact of the dynamic plane transformation on representing choppy market data.

Following the choppy market visualization, we will analyze three distinct market regimes: trend, reversal, and sideways. The visualization of these regimes on the dynamic plane will build upon existing visualizations, illustrating how they appear in the transformed coordinate system. Standard Heiken-Ashi charts will be compared directly with their rotated dynamic plane counterparts for each regime using a subplot grid to avoid matplotlib's figure stacking issues. Two regimes, trend-reversal-recovery and choppy sideways, have been identified. A third regime, potentially a strong linear uptrend or a sharp V-shaped recovery, will be simulated to provide a comprehensive view of the dynamic plane's behavior across diverse market conditions.

The individual visualizations for each regime will be combined into a single, comprehensive panel for easier comparison and analysis. A subsequent analysis will examine the behavior of the different market regimes within the dynamic plane and summarize the implications for model learning. This summary will be formatted for inclusion in the dissertation's chapter drafts, providing a theoretical grounding for the use of the dynamic plane.

Model Architecture: Geometric Pattern Recognition and Interpretability

This model's architecture centers on geometric pattern recognition within the dynamic coordinate system generated by PCA. Instead of relying on the fixed semantic meanings of price, time, and volume, the model learns the relationships and structures between these data points after PCA transformation. This approach prioritizes identifying patterns within the transformed space rather than absolute feature values. The model is trained to recognize geometric shapes and flows within this normalized PCA space, moving away from traditional technical indicators that depend on fixed price/volume relationships.

The dynamic axis rotation introduced by PCA requires an "Interpretability Projection" mechanism to project the model's focus back into the original time-price-volume space. This allows for human interpretation of model activations and insights. Finally, careful consideration will be given to the PCA window size and the implementation of smoothing or stability thresholds to mitigate noise-driven rotations in the PCA space and prevent overfitting.

Model Architecture: Shifting from Absolute to Relational Understanding

A core architectural shift involves moving the model's learning from absolute feature perception (e.g., "price rises over time") to a relational understanding of price movements (e.g., "dominant movement along the local principal axis shows rising oscillations"). This shift emphasizes relative changes within the dynamically defined space, allowing the model to generalize across different market conditions and volatilities.

Model Architecture: Error Signals and Dynamic Frame Adjustments

The model incorporates feedback mechanisms to refine the dynamic rotating plane algorithm based on prediction accuracy. Several strategies are proposed:

- **Error Signal Mechanism:** Compares predicted and actual market movements, generating an error signal used to adjust the model's internal representation of market structure.
- **Frame Confidence Correction:** Analyzes agreement between predicted and actual movements, triggering corrections in the rotational frame assumptions based on detected mismatches.
- **Prediction Error Memory:** Maintains a rolling memory of recent prediction errors to adjust the weighting given to the rotation based on consistent misalignments.
- **Feedback-Driven Frame Smoothing:** Adaptively controls the rotational speed based on prediction error, slowing rotation during periods of high error to increase smoothing.
- **Dual-Frame Estimation:** Maintains "optimistic" and "stable" local frames, weighting predictions between them based on observed market consistency to balance responsiveness and stability.

## Model Architecture (Specialization)

This section details the specialized architecture of the model, addressing error signal mechanisms, dynamic plane adjustments, and enhanced error trend detection. The architecture incorporates principles inspired by biological processes like wound healing, aiming for a balance between rapid adaptation to short-term changes and stability grounded in longer-term trends.

**Dynamic Frame Correction and Adaptation:**

The model employs a rolling frame correction algorithm applied to the dynamic PCA frame to address persistent prediction errors. This algorithm mimics a biological healing process and operates in three phases:

1. **Error Detection:** A rolling buffer (e.g., 5-10 steps) stores recent prediction errors, calculated as the difference between the predicted and realized movement in the rotated frame. The rolling mean and variance of these errors are analyzed. A threshold, defined as a multiple (e.g., 1-2x) of the rolling standard deviation, triggers the correction mode.

2. **Frame Correction Action:** When the error threshold is exceeded, small rotational adjustments or damping are applied to the PCA frame. The specific implementation of these adjustments requires further investigation.

3. **Healing Phase:** The applied adjustments are gradually removed, allowing the frame to return to its normal state, preventing overcorrection and maintaining adaptability. This rolling correction addresses potential performance plateaus caused by static weighting in dual record interactions, ensuring responsiveness to evolving market dynamics.

A "rolling rewiring" mechanism, operating similarly to the frame correction, addresses frame coincidence issues. This mechanism also gradually returns to a normal state after applying corrections, ensuring smooth transitions.

**Lagging Rotation Management:**

The model incorporates a lagging rotation mechanism within the dynamic plane algorithm. A strategy will be developed to dynamically manage this lag, determining when to reduce or deactivate it to return to a non-lagged state. This optimization balances noise reduction and responsiveness to market changes. The specific deactivation criteria require further investigation.

**Simulation of Peripersonal and Extrapersonal Space:**

To improve contextualization and the distinction between immediate and distant market movements, a method will be developed to simulate the biological concept of peripersonal versus extrapersonal space within backtesting. The exact implementation requires further definition.

**Prediction-Error Feedback Integration:**

Lightweight prediction-error feedback will be integrated into model training, likely through auxiliary loss functions or by monitoring frame stability and adjusting accordingly. This aims to improve prediction accuracy by allowing the model to learn from its mistakes. A previously used static error value in PCA calculations will be removed due to potential inaccuracies.

**Enhanced Error Trend Detection:**

The model architecture includes an enhanced Error Trend Detector operating within a dynamic 2D frame. This detector goes beyond simple rolling statistics and incorporates price, time, and volume influences to achieve a deeper understanding of market dynamics. It monitors deviation vectors and angular error between predicted and realized movement vectors within the 2D plane. Angular error is calculated using the arccosine of the normalized dot product of the vectors.

**Loss Function Refinement:**

New loss functions will be developed to penalize both scalar (price difference) and angular (directional) drift, improving the prediction of both magnitude and direction of market movements. The specific formulation of these functions requires further investigation (see "New Loss Functions for Model Training").

### A. Model Architecture (Specialization)

This section details the architecture of the model, specifically focusing on error correction within the dynamic 2D plane representation of market data and maintaining consistency across temporal dynamics.

**Error Correction Implementation:** The model refines predictions through an error correction mechanism based on the discrepancies between predicted and realized movement vectors within the dynamic 2D plane. Two key metrics drive this correction:

- **Distance Error:** This metric quantifies the magnitude difference between the predicted and realized displacement vectors.
- **Angle Error:** This metric quantifies the orientation difference between the predicted and realized direction vectors.

The initial frame creation rotation, based on Principal Component Analysis (PCA), is not factored into the error correction calculation. The focus remains solely on the deviation between predicted and realized vectors within the transformed dynamic plane. A diagram illustrating the relationship between the global frame transformation (PCA), local vector misalignment, and the resulting prediction error will be created (See related task documentation).

The dynamic plane implementation uses two rotational angles and two distance vectors, ensuring consistency and clarity. Any discrepancies between this documentation and the current implementation will be addressed in a follow-up task.

**Dynamic PCA Frame Consistency and Transformation:** Maintaining consistency in the 2D plane derived from PCA is crucial. Since price and volume values can differ between prediction and reality matrices, the resulting PCA axes could vary. An investigation is required to determine if a transformation or alignment step is necessary to ensure comparability between these planes, contributing to a robust and reliable prediction model.

**Freezing the Dynamic Frame for Prediction (Freeze Frame):** To address market data’s dynamic nature, the model employs a "freeze frame" approach. The PCA frame calculated at time 't' is frozen for predicting and evaluating movements over a short future horizon (e.g., 1-3 candlesticks), assuming market structure remains relatively stable within this interval. This allows both predicted and realized movement vectors to be interpreted within the same context, simplifying the error correction mechanism and avoiding PCA recalculation at 't+1'.

**Reprojecting Realized Movement (Reproject Realization):** For accurate error assessment, the realized movement at 't+1' is reprojected back into the PCA frame valid at 't' using the original PCA basis (rotation matrix). This ensures consistent error calculations and model adjustments within a well-defined reference frame, even if the PCA calculation at 't+1' yields a different result.

**Implementation Details for Freeze Frame and Reproject Realization:** For short-term predictions (1-5 candlesticks), maintaining a consistent frame of reference is paramount. Continuous recalculation of the dynamic frame is deemed unnecessary for these short horizons, except in extremely volatile environments. To implement the "Freeze Frame" and "Reproject Realization" techniques effectively:

- **Data Structure for PCA Basis:** A lightweight data structure will store the PCA basis (rotation matrix) for each window, facilitating the reprojection of realized movement vectors.
- **Pseudocode for Freeze Frame & Reproject Realization:** Formal pseudocode will detail the implementation steps of both techniques.

**Visualizations and Simulations:** To enhance understanding and validation:

- **Visual Simulation of Frame Alignment:** A visual simulation will demonstrate the alignment of predicted and realized outcomes using these methods.
- **Visualizations of Freeze Frame and Reprojection:** Visual examples will showcase these functionalities within the application, clarifying their distinction from existing distance/angular error calculations based on static PCA planes.
  This model architecture dynamically adapts to market conditions by incorporating a rolling error buffer, trend detection, and a healing phase to maintain prediction accuracy and stability. The system calculates a weighted total error, accounting for both vector deviation and frame shifts.

**Error Calculation:** Two primary error components contribute to the total error: Vector Error and Frame Shift Error. The Vector Error represents the deviation between predicted and realized movement within the dynamic local PCA frame. The Frame Shift Error quantifies the change in the PCA axes (PCA1 and PCA2) between prediction time and realization time. This approach, referred to as the "Reproject Realization" method, maintains consistency by comparing predictions and realizations within the context of the initial prediction frame, unlike traditional distance/angular error calculations that may assume a static PCA plane. A further refinement called the "Freeze Frame" method will be explored and compared to “Reproject Realization.” Both methods mitigate the impact of market shifts on error calculation.

The Total Error is calculated as a weighted sum of these components:

- **Vector Error = α₁⋅d<sub>vec</sub> + α₂⋅θ<sub>vec</sub>** where `d_vec` represents the distance error and `θ_vec` represents the angular error within the PCA frame.

- **Frame Shift Error = β₁⋅θ<sub>PCA1</sub> + β₂⋅θ<sub>PCA2</sub>** where `θ_PCA1` and `θ_PCA2` represent the angular changes in PCA1 and PCA2, respectively.

- **Total Error = γ₁⋅Vector Error + γ₂⋅Frame Shift Error**

The weights (α₁, α₂, β₁, β₂, γ₁, and γ₂) are treated as hyperparameters and optimized during training. Normalization techniques will be applied to ensure consistent scaling across different error types before aggregation.

**Error Trend Detection and Healing Phase:** The model incorporates a self-regulating mechanism based on a rolling error buffer storing the total error over a defined window (e.g., 5-10 time steps). Rolling statistics (mean and standard deviation) are calculated on this buffer. A "Healing Phase" is triggered when the mean error exceeds a dynamic threshold, typically defined as a multiple (e.g., `k=2`) of the rolling standard deviation. This threshold crossing indicates a period of significant prediction inaccuracy, often associated with market shifts.

During the Healing Phase, a correction factor is applied to the PCA frame construction to mitigate the accumulated error and guide the model back to stability. This correction factor decays as errors decrease, enabling a smooth transition back to normal operation. If errors spike again during the Healing Phase, the system remains in this corrective mode.

The specific implementation details of the Error Trend Detector, including the rolling window size and the threshold for triggering the Healing Phase, will be further defined during development. Pseudocode will be provided to illustrate the multi-weight error computation and the normalization process. A numerical example demonstrating the calculation of the total error from individual atomic error components (e.g., price, volume, time) will also be developed. The initial values for the weighting parameters (α, β, and γ) will be derived from trading intuition and physics principles.

## II. Model Development and Training

### A. Model Architecture (Specialization)

This section details the model architecture specialized for processing candlestick chart images and predicting future returns. The model incorporates dynamic decay and performance-based healing, moving away from time-based decay towards a system driven by predictive accuracy.

**Input and Feature Extraction:**

- The model accepts sequences of 5-day candlestick chart images as input, encapsulating short-term price and volume information.
- Dates, real prices, and tickers are excluded from the CNN input, focusing solely on visual patterns.
- Delta features, capturing changes in price and volume over time, are engineered and incorporated.

**Convolutional Neural Network (CNN):**

- CNNs analyze candlestick chart images and generate initial trade signals based on potential tops and bottoms. The top and bottom 10% of predicted price movements trigger buy and sell signals, respectively.
- The transformation process from CNN output to actionable trade signals is documented for clarity and reproducibility.

**Vision Transformer (ViT):**

- A Vision Transformer (ViT) processes sequential candlestick data, incorporating EfficientNet or patch embedding for feature representation.
- Sequential windows of candlestick data serve as input for the ViT.
- Positional embeddings retain sequential information.
- Masking and padding handle variable-length input sequences.
- An input pipeline efficiently feeds _N_ images to the ViT, with a defined maximum input sequence length.
- The ViT is trained to handle variable-length sequences.

**Hybrid Architectures:**

- The architecture explores combining CNNs with LSTMs or Transformers, leveraging CNNs for image feature extraction and LSTMs/Transformers for sequential data processing.

**Trade Execution and Evaluation:**

- Trade execution is simulated using actual prices from t+1 to t+5 for realistic evaluation.
- Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) is the primary evaluation metric.

**Dynamic Decay and Performance-Based Healing:**

- **Dynamic Decay Rate Adjustment:** The lag introduced to account for previous disruptions is dynamically adjusted based on prediction accuracy. Higher accuracy accelerates lag reduction, enabling faster adaptation to stable market conditions.
- **Performance-Based Healing:** The correction factor applied after disruptions is reduced proportionally to the recovery of predictive accuracy, measured by a rolling prediction correctness buffer.
- **Prediction Correctness Tracking:** Each prediction receives a score: +1 for correct, 0 for incorrect, independent of frame or data representation issues.
- **Rolling Prediction Correctness Buffer:** A rolling buffer stores the last _N_ prediction correctness scores. The buffer's average represents mean prediction correctness, used to calculate the dynamic decay rate and correction factor.
- **True Prediction Value Tracking:** Actual prediction values are continuously tracked to calculate the prediction correctness score and adjust the decay rate and healing process.
- **Healing Logic Update:** High rolling prediction correctness (e.g., above 80% over _N_ steps) proportionally reduces the correction factor. Conversely, deteriorating correctness maintains or increases the factor.
- **Dynamic Decay Rate Function:** A function, `dynamic_decay_rate(mean_correctness)`, calculates the correction factor's decay rate based on mean prediction correctness, accelerating decay as correctness surpasses a defined healing threshold.

**Model Training Goal:**

- Accurately predict visual patterns in candlestick charts that correspond to profitable trading opportunities.
- Ensure predictions are causally grounded through analysis of the underlying reasons for identified patterns.

This architecture learns complex relationships between candlestick patterns and future price movements, forming the basis of a robust trading strategy.

### A. Model Architecture (Specialization)

This section details the architecture of the models used for predicting stock price movements. The models primarily rely on time series data, candlestick chart images, and potentially a live feed of last traded prices (LTPs). This section focuses on the data preprocessing techniques employed to prepare these diverse inputs for the core prediction models (CNNs, LSTMs, or Transformers, discussed elsewhere in this document).

**Time Series Data Preprocessing:**

For time series data consisting of price, time, and volume, the following preprocessing steps are applied:

1. **Normalization:** Price, time, and volume data within a rolling window of _n_ data points are independently normalized using z-score normalization. This involves centering and scaling each feature to zero mean and unit variance:

   $X_{scaled}[,i,.,.] = \Bigl(\tfrac{t_i - \mu_t}{\sigma_t}, \tfrac{p_i - \mu_p}{\sigma_p}, \tfrac{v_i - \mu_v}{\sigma_v}\Bigr)$

2. **Time Handling:** Time data can be represented either as a relative index within the window or as absolute clock time (potentially capturing diurnal patterns). If using absolute time, time deltas are calculated and normalized. However, caution is advised as large gaps in timestamps can skew the results.

3. **Volume Transformation:** Due to the often heavy-tailed distribution of volume data, a log transformation ($v_i' = log(1 + v_i)$) is applied to compress outliers before z-score normalization. Alternatively, robust scaling using the median and interquartile range (IQR) can be employed.

4. **Dimensionality Reduction:** Principal Component Analysis (PCA), implemented using Singular Value Decomposition (SVD), is applied to the scaled data matrix ($X_{scaled}$). Typically, the first two principal components are selected as the primary features.

**Candlestick Image Data Preprocessing:**

When using candlestick images as input, the underlying numerical data is transformed as follows:

1. **Relative Returns:** Raw price values are converted to relative returns (percentage change or log returns) relative to the first price in the input window. This anchors the price to zero at the window's beginning, providing a more stable and consistent input.

2. **Fractional Elapsed Time:** Time within the candlestick window is encoded as a fractional value between 0 and 1, representing the elapsed time divided by the total window duration. This preserves chronological order and handles irregular time spacing.

**Handling Practical Data Challenges:**

The system addresses the following data handling challenges:

1. **Unseen Values:** The model's input pipeline robustly handles unseen or extreme price and volume values, ensuring the scaling mechanism doesn't distort visualizations or impact model performance.

2. **Live LTP Feed (Optional):** Integrating a live feed of LTPs for time tracking can be explored. However, potential drawbacks, such as the loss of historical context and the challenges of maintaining a chronological record, must be carefully considered.

**Combined Data Preprocessing:**

When combining time series data with other features, a unified preprocessing pipeline is used:

1. **Fractional Elapsed Time:** Timestamps are converted to fractional elapsed time within each window.

2. **Window-Relative Returns:** Raw price data is transformed into window-relative returns (percentage or log returns) relative to the first price in the window.

3. **Robust Volume Scaling:** Volume data undergoes a log transformation followed by median and IQR scaling.

4. **PCA:** A 3-dimensional matrix (fractional time, relative returns, scaled volume) is constructed and PCA is applied.

## Model Architecture and Data Preprocessing

This model utilizes a Convolutional Neural Network (CNN) architecture with a specialized input pipeline designed to process candlestick chart images derived from 10-minute interval OHLCV (Open, High, Low, Close, Volume) data. The model receives two sets of five candlestick chart image sequences, each sequence representing one day of trading data. These paired image sets represent the data before and after a specific transformation process, offering the CNN two perspectives of price and volume action.

The first set of images depicts the raw candlestick data, providing a visual representation of the untransformed price action, similar to the input a traditional trading algorithm might use.

The second set consists of transformed images, generated using a normalization process designed to highlight features potentially obscured in the raw data. This transformation involves several key steps:

1. **Log Transformation and Percentile Clipping:** Volume data is log-transformed (`np.log1p(volume)`) to normalize its distribution. Subsequently, both the log-transformed volume and the log return of the price are clipped at the 5th and 95th percentiles to mitigate the impact of extreme values.

2. **Min-Max Scaling:** Fractional elapsed time, the clipped log return of price, and the clipped log-transformed volume are then min-max scaled to the range [-1, 1]. This ensures consistent input ranges across all features, preventing features with larger magnitudes from dominating the model and improving the performance of the subsequent Principal Component Analysis (PCA).

3. **Principal Component Analysis (PCA):** The scaled time, price, and volume data are combined into a matrix, and PCA is applied. This dimensionality reduction technique identifies key underlying patterns and reduces noise, potentially enhancing model performance. Centering the matrix before PCA is optional due to the symmetric [-1,1] scaling.

4. **Dynamic Plane Representation:** The PCA-transformed data is used to generate a "dynamic plane snapshot" — a 2D representation suitable for CNN input. This further simplifies the input space and enhances feature extraction.

The model architecture processes these paired image sequences (raw and transformed) to generate trade signals, aiming to identify buy and sell opportunities. Five distinct examples of price and volume action (uptrend with rising volume, downtrend with volume spikes, reversal, sideways chop, and breakout spike followed by stabilization) are generated to demonstrate the effects of the transformation pipeline. For each example, both the pre- and post-transformation candlestick charts are generated, illustrating the impact of the transformations. These individual images, displayed separately, provide a qualitative understanding of the data preprocessing and its effect on the model input. Further details on the specific CNN architecture, layers, and hyperparameters will be provided in subsequent sections.

## Model Architecture

This section details the architecture of the models employed for the SCoVA project. The core architecture processes 5-day candlestick chart images using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to generate and evaluate trading signals.

**CNN-based Architecture:**

- **Input:** Sequences of five daily candlestick chart images. These images serve as the primary input for feature extraction and prediction. Volume information is incorporated into these images, the specific representation of which is detailed in a separate section. The generation and analysis of specific candlestick patterns, such as uptrends with rising volume, downtrends with volume spikes, reversals, sideways chop, and breakout/spike/stabilize patterns, are also described elsewhere. Of particular interest are the five distinct PCA-based examples generated for the breakout/spike/stabilize pattern and their visualization alongside original candlestick and volume data to understand the impact of varying volume profiles.
- **Signal Generation:** CNNs analyze the candlestick chart images and generate trading signals based on learned patterns indicative of potential price movements. The focus is on the top and bottom 10% of predictions to capture the most significant potential movements (buy/sell signals respectively).
- **Output:** The CNN outputs numerical predictions which are transformed into actionable trading signals. This transformation process is documented for clarity and transparency.
- **Hybrid Architectures:** The project explores combining CNNs with LSTMs or Transformers to capture both spatial patterns within candlestick images and temporal dependencies across image sequences.

**ViT-based Architecture:**

- **Input:** Sequences of daily candlestick chart images are used as input to the ViT model, enabling the capture of temporal dependencies in price and volume patterns.
- **Implementation:** ViTs are implemented using EfficientNet or patch embedding techniques to effectively process the image data.

**General Model Considerations:**

- **Trade Execution Logic:** Generated trade signals are evaluated against actual price data from t+1 to t+5 (the five days following the input window) for performance assessment in a simulated trading environment.
- **Input Data Format:** The format and size (_n_) of the input data are verified for compatibility with the model architecture.
- **Evaluation Metrics:** Model performance is evaluated using Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).
- **Data Exclusions:** Input data for the CNN excludes dates, real prices, and tickers, ensuring the model focuses solely on visual patterns.
- **Training Data:** The CNN is trained using the generated candlestick chart images and corresponding return labels.

This architecture aims to leverage the strengths of both CNNs and ViTs for effective analysis of candlestick chart data and generation of actionable trading signals. Further details on data pre-processing, pattern analysis, and volume integration are provided in dedicated sections of this document.

## A. Model Architecture

This section details the architecture of the models used for trade signal generation, focusing on visual pattern recognition within candlestick chart data. The core system uses a Vision Transformer (ViT) architecture processing normalized candlestick chart images. Normalization transforms Time, Price (using log-returns), and Volume (using log-transformation and robust scaling) to a uniform range of [-1, +1]. This preserves chronological order, ensures feature comparability, and prevents any single dimension from dominating the model. The `DynamicPlaneGenerator` module, developed based on established pseudocode, handles normalization, Principal Component Analysis (PCA), dynamic plane projection, re-centering, and image generation, preparing the data for the ViT. The ultimate goal is a fully deployable algorithmic trading bot. While a monitoring dashboard is planned, its development is outside the scope of this architectural discussion.

The model utilizes a dynamic frame construction process to prepare market data for the ViT and incorporates a self-correction mechanism based on prediction error feedback.

**1. Dynamic Frame Construction:** Within a recent time window, PCA is applied to the normalized Time, Price, and Volume data to identify the two primary axes of correlated movement (principal components PC1 and PC2). These components define a dynamic 2D plane that adapts to current market conditions.

**2. Rotation and Refocusing:** Recent market action is projected onto this dynamic 2D plane. The perspective is then re-centered, placing the most recent data point at the origin (0,0) to focus the model on the latest trends.

**3. Image Generation:** The transformed 2D plot of market flow is rendered as an image, potentially using Candlestick or Heiken-Ashi chart representations. This image serves as input to the ViT.

**4. Self-Correction Mechanism:** A feedback loop incorporates a Total Error signal composed of:

- **Vector Deviation Error:** Measures the distance and angular difference between predicted and actual market movement within the dynamic plane.
- **Frame Shift Error:** Quantifies the instability of the dynamic plane itself.

A "Wound Detection" system, using a dynamic threshold (e.g., a multiple of rolling standard deviation), identifies significant prediction errors. When triggered, a "Healing Phase" progressively restores full dynamism based on improvements in predictive accuracy, allowing the model to adapt to changing market dynamics and recover from periods of poor performance.

To incorporate multi-scale temporal information and after-market forces, the architecture will implement a multi-scale temporal model. This model integrates daily, weekly, monthly, quarterly, and yearly contexts into the prediction mechanism. These different timescales will be weighted and added to the existing contextual frame analysis. The design must determine how to assign weights to each timeframe for optimal prediction and address the handling of inherent cyclical patterns. Testing different weighting schemes for these periodicities will be crucial for optimizing predictive accuracy.

Two approaches for handling multi-timeframe data are being considered:

**1. Ensemble Model Approach:** Specialized models (potentially CNNs, LSTMs, or Transformers) will be trained for each timeframe (e.g., daily, weekly) and optimized for their respective scales. Predictions from each model will be combined to generate the final prediction. This approach allows for specialization and potentially improved performance on individual timeframes.

**2. Multi-Input Transformer Model:** This alternative approach uses a single Transformer model that directly ingests data from all timeframes simultaneously. This approach allows the model to learn the interdependencies between different time scales directly. Both approaches will be investigated and compared.

### A. Model Architecture (Specialization)

This section details the architecture of the predictive model, focusing on its multi-scale data processing, dynamic representation of market data, and self-correction mechanism.

The model leverages a **hierarchical attention mechanism** to process **multi-scale data**, incorporating intraday, daily, weekly, and monthly timeframes. This approach provides broader market context for generating short-term predictions. The hierarchical structure allows the model to dynamically query and incorporate information from longer timeframes based on the immediate intraday context, optimizing computational resources and enhancing interpretability.

Central to the model's architecture is the **Dynamic Rotating Plane**. This involves creating a dynamic 2D representation of market dynamics by applying Principal Component Analysis (PCA) to normalized Time, Price, and Volume data. This identifies the principal axes of local market flow and creates a plane oriented along these axes. The plane is dynamically re-centered on the latest data point, ensuring the representation remains relevant to current market conditions. This dynamic plane serves as input to a **Vision Transformer (ViT)**.

The ViT processes the dynamic planes generated from each timescale as a sequence of tokens, effectively learning the dependencies and relationships between different timescales. This allows the model to capture complex interdependencies between various timeframes for improved predictive power.

Furthermore, the architecture incorporates a **self-correction mechanism** using a performance-driven feedback loop. A "Total Error" signal, combining "Vector Deviation Error" and "Frame Shift Error", monitors prediction accuracy. When the Total Error exceeds a defined threshold, a "correction mode" dampens the rotation of the dynamic planes, stabilizing the model's interpretation of market dynamics. This dampening effect gradually diminishes as prediction accuracy improves and the Total Error decreases.

Finally, all input data—Time (fractional elapsed time), Price (log-returns), and Volume (log-transformed and robustly scaled)—are normalized to a uniform range of [-1, +1]. This ensures consistent input scaling and prevents features with larger magnitudes from dominating the model's learning process. The user can configure the input data by selecting the candlestick type (e.g., standard, Heiken-Ashi), defining the local window size for the dynamic plane, and choosing which features (price, time, volume) are included in the representation. Additionally, the user can specify the asset universe and date range for training and evaluation, with the specified date range automatically split into training and validation sets. A visualization of the standard candlestick chart alongside the dynamically rotated and re-centered 2D plane provides real-time insight into the model's interpretation of market data. The parameters of the self-correction system, including the wound detection threshold and healing trigger, are also configurable.

### A. Model Architecture (Specialization)

This section details the architectural considerations for the model, encompassing multi-scale context fusion, error handling, data handling, functional requirements, context awareness, transfer learning, and hyperparameter optimization strategies.

**Data Handling and Functional Requirements:**

The primary model input consists of 5-day candlestick chart images. However, the architecture incorporates several supporting functionalities:

- **Categorical Input Handling:** The model handles input based on stock categories, including market capitalization, sector, and share price bins. This categorization applies to both candlestick input generation and subsequent calculations.
- **Rally Time Calculation:** The model calculates rally times, considering the defined categories. The specific calculation method will be further defined.
- **Dynamic Capital Allocation:** A dynamic capital allocation strategy will be researched and implemented, considering starting capital and utilizing probability distributions to inform trades. Further research will determine the optimal approach for incorporating these distributions.
- **Error Detection and Healing:** Dynamic (non-time-based) error detection and healing strategies will be researched and implemented for responsive adaptation to unexpected events or model deviations. This research includes identifying suitable libraries for handling long-term intraday data.

**Multi-Scale Context Fusion:**

The model incorporates information from different time scales to enhance prediction accuracy using selectable fusion methods:

- **Attention-Based (ViT):** Leverages the attention mechanism within a Vision Transformer (ViT) architecture to weigh and combine information from different time scales.
- **Concatenation:** Directly concatenates features extracted from different time scales.
- **Weighted Average:** Calculates a weighted average of features from different time scales, allowing adjustable weights to prioritize specific timeframes.

**Error Metrics and Healing Triggers:**

Two error metrics and corresponding healing triggers are implemented:

- **Total Error Metric:** Combines _vector error_ and _frame shift error_, each with adjustable weights for customized application. This provides a nuanced understanding of model performance.
- **Performance-Based Healing Trigger:** Activates a healing mechanism if the Total Error metric drops below a user-defined threshold, enabling automated adjustments and retraining.
- **Time-Based Healing Trigger:** Triggers the healing mechanism if the Total Error metric remains above a specified threshold for a certain duration, addressing consistently high error situations.

**Context Awareness and Transfer Learning:**

- **Variable Input Length:** The model handles variable input lengths, requiring the determination of the optimal number of candlesticks per frame (allowing variability) and the total number of frames.
- **Transfer Learning:** The model's ability to transfer learned knowledge between different markets (US-US, US-India, and India-India) will be evaluated to assess generalizability and adaptability.
- **Context-Aware Periodicity:** Weighted predictions based on daily, weekly, monthly, quarterly, and yearly periodicities are incorporated. Weights are dynamically determined based on the number of frames, candlesticks per frame, and the specific stock category.

**PCA Analysis and Hyperparameter Optimization:**

- **PCA Analysis:** The modeling process will be repeated using a 2-dimensional dynamic plane derived from Principal Component Analysis (PCA) to evaluate potential performance improvements or dimensionality reduction benefits.
- **Hyperparameter Permutation Testing:** A comprehensive "try all permutations" approach will establish baseline performance values for each possible hyperparameter combination. This initial exploration will be conducted for a single epoch using a defined dataset (details to be provided later).

### A. Model Architecture (Specialization)

This section details the architecture of the Swaha project, incorporating the philosophical framework of the Bhagavad Gita's four paths to liberation and integrating the Zerodha KiteConnect API. The architecture is designed with principles of transparency, explainability, and ethical considerations derived from Dharmic principles.

**Zerodha KiteConnect API Integration:**

The model's functionality is deeply intertwined with the Zerodha KiteConnect API for seamless data acquisition and trading execution:

- **Data Acquisition:** Historical and live market data are retrieved via the KiteConnect API, enabling both backtesting and real-time trading.
- **Order and Portfolio Management:** Order generation, portfolio creation, and performance tracking are managed through the KiteConnect API, ensuring direct integration between the model's predictions and trading actions.

**Bhagavad Gita's Influence:**

The architecture and its user interface are shaped by the four paths to liberation as described in the Bhagavad Gita:

- **Gyaan (Gyaan Yoga - Knowledge):** The model prioritizes transparent and explainable AI principles, enabling users to understand the rationale behind trading decisions.
- **Bhakt (Bhakti Yoga - Devotion):** The UI/UX is designed to foster trust and confidence in the system while providing clear performance metrics.
- **Karam (Karma Yoga - Skilled Action):** The model is optimized for long-term sustainable performance and consistent returns rather than short-term gains.
- **Raaj (Raja Yoga - Meditation & Self-Control):** Robust risk management strategies and strict adherence to predefined trading rules ensure disciplined operation.

**Dharmic Principles & Ethical Considerations:**

Ethical considerations derived from Dharmic principles are embedded within the architecture through a set of inviolable rules:

- **Satya (Truthfulness):** Immutable logs maintain complete transparency and accountability for all trading activities.
- **Shaucha (Purity):** Rigorously cleaned and validated data ensures the integrity of the model's predictions and prevents biases.
- **Santosha (Contentment):** The system balances profit maximization with stability, avoiding excessive risk and promoting long-term sustainable growth.

This integration of philosophical and ethical principles is fundamental to the model's design, ensuring alignment with the project's core values. Further technical details regarding specific modules and their implementation will be provided in subsequent sections.

### Model Architecture

This section details the architecture of the models used for predicting stock market trends based on candlestick chart images. The primary model is a Convolutional Neural Network (CNN) designed to identify visual patterns in these charts. Supplementary models, including Long Short-Term Memory networks (LSTMs), Transformers, and Vision Transformers (ViTs), are also explored to capture temporal dependencies and further enhance pattern recognition.

**CNN Architecture:** The CNN processes 5-day candlestick chart images, specifically engineered to exclude explicit date, price, and ticker information, forcing the model to focus solely on visual patterns. The model outputs a numerical prediction, which is then transformed into buy/sell signals based on thresholds (e.g., top/bottom 10% of predicted returns). Performance is evaluated against actual subsequent market movements (t+1 to t+5 prices) using metrics like Mean Squared Error (MSE) or Root Mean Squared Error (RMSE).

**Hybrid Architectures:** To capture temporal dependencies and refine predictions, hybrid architectures combining CNNs with LSTMs or Transformers are considered. LSTMs process sequential candlestick data, while Transformers leverage attention mechanisms to identify crucial patterns. The input data format and size for these models are carefully defined.

**Vision Transformer (ViT) Architecture:** ViTs are also implemented for processing sequential candlestick windows. These implementations may leverage EfficientNet as a feature extractor or employ patch embedding. Feature engineering, specifically incorporating delta features, enhances model performance. The ViT architecture addresses challenges like handling variable-length input sequences through masking and padding, and efficiently processing multiple input images. Positional embeddings preserve sequential information.

**Overall Training Goal:** The overarching goal is to train models that accurately predict visual patterns in candlestick charts and, crucially, understand the causal relationships driving these patterns. Separate models are trained to predict both trend direction and reward magnitude. This multifaceted approach aims to move beyond simple pattern memorization towards a deeper understanding of market dynamics.
This section details the model architecture, focusing on its federated learning approach, which distributes the computational workload between a central server and client iPads. This architecture maximizes the utilization of iPad hardware for computationally intensive tasks while maintaining a lightweight and efficient server-side infrastructure.

**Client-Side Processing (iPad):**

The iPad performs the bulk of the processing, including:

- **Data Preprocessing and Image Generation:** The iPad fetches raw market data from the server and transforms it into candlestick chart images using JavaScript libraries and the Canvas API.
- **Model Training:** Leveraging TensorFlow.js and the device's GPU, the iPad trains a local model on the generated images. Web Workers ensure UI responsiveness during training.
- **Model Update Transmission:** After training, the iPad transmits updated model weights to the server, minimizing bandwidth usage through optimized data transfer techniques.

**Server-Side Orchestration:**

The server's primary responsibilities include:

- **Data Storage and Distribution:** The server acts as the central repository for raw market data, distributing it to connected iPads.
- **Global Model Management and Aggregation:** The server maintains the global model, a consolidated representation of learnings from all client devices, and performs federated averaging to incorporate client updates.
- **Training Coordination:** The server orchestrates the training process, distributing the initial model and instructions to the iPads, initiating client-side training cycles.

This distributed architecture addresses the computational demands of Vision Transformer (ViT) model training within the constraints of a Progressive Web App (PWA) environment. However, several challenges require careful consideration:

- **PWA Suitability for GPU-Intensive Tasks:** The feasibility and potential drawbacks of running computationally demanding ViT training within a PWA, including browser crashes and performance limitations, require thorough investigation.
- **Resource Limitations:** Specifically, the resource limitations of the iPad's PWA environment for ViT training, especially regarding long-term, uninterrupted operation, must be assessed.
- **Technical Challenges of GPU-Intensive Training in a PWA:** Research into TensorFlow.js performance and WebGPU support within the browser context is necessary to determine the viability of this approach.
- **PWA Crash Probability:** Assessing the likelihood of browser crashes under heavy GPU load within the PWA environment, considering factors like device capabilities and resource management, is crucial.
- **Alternative Solutions:** If a PWA proves unsuitable for reliable ViT training, alternative solutions, such as native applications or cloud-based training, must be explored.

## A. Model Architecture

This project employs a client-heavy architecture centered around a native iOS Swift application, leveraging Core ML for machine learning and Metal for GPU-accelerated computations. A minimal Python backend serves primarily as an orchestrator and data provider.

**Client-Side (iOS App):**

- **Model Training:** The iOS app facilitates both initial training and fine-tuning of machine learning models directly on the device using Core ML. This leverages the Apple Neural Engine (ANE) and GPU for efficient and responsive performance. Pre-trained models, originally developed in frameworks like PyTorch or TensorFlow, are converted to the `.mlmodel` format for Core ML compatibility.
- **Data Preprocessing and Image Generation:** Upon receiving raw numerical data (OHLCV, index data, etc.) from the backend, the app performs client-side candlestick image generation using the DynamicPlaneGenerator. This component leverages the Metal framework for GPU-accelerated processing of numerical data transformations, including Principal Component Analysis (PCA), rotations, and rendering. These generated images are then used as input for the Core ML models.
- **Real-time Inference:** The app performs real-time inference using the trained models, enabling immediate feedback and analysis.

**Server-Side (Python Backend):**

- **Data Serving:** The backend maintains the master database of raw numerical data and serves it to the iOS app upon request.
- **Authentication and API Management:** The backend handles secure connection and authentication with external APIs, such as the Zerodha Kite Connect API.

This client-heavy architecture was chosen after an initial exploration of a hybrid approach involving a Progressive Web App (PWA) frontend. The PWA architecture presented limitations related to memory constraints, long-running task stability, and limited GPU context access, hindering client-side processing. Migrating to a native iOS app allows for significantly improved performance, stability, and access to hardware resources, enabling crucial client-side functionalities like image generation and model training.

## Model Architecture

This section details the model architecture, encompassing its design, data flow, and technological considerations. The project employs a client-server architecture, distributing tasks between the client (frontend) and a Python backend. Initially, the client was a native iOS app performing compute-intensive operations like on-device image generation, model training/fine-tuning using Core ML, backtesting, and live inference. However, a shift to a cross-platform frontend technology like Flutter is under evaluation.

**Frontend Responsibilities (Current and Potential):**

Regardless of the chosen frontend technology (iOS native or Flutter), the client will be responsible for:

- **Image Processing:** Generating candlestick chart images from raw numerical data.
- **Model Inference:** Performing real-time predictions using the latest model version.
- **Potential On-Device Training:** While not a primary requirement, on-device training capabilities are being considered for future personalization and model adaptation.

**Backend Responsibilities:**

The Python backend serves several critical roles:

- **Model Management:** Storing and managing the master versions of trained models (Core ML or TensorFlow Lite, depending on the frontend). This centralized repository ensures consistency and facilitates efficient distribution of model updates.
- **Data Provisioning:** Providing the frontend with the necessary raw numerical data for image generation.
- **Model Update Aggregation:** Receiving and aggregating model updates generated by the client (if on-device training is implemented). This allows for continuous learning and improvement of the global model.
- **Experiment Management (if applicable):** If experimentation and research campaigns are conducted, the backend stores and manages experiment templates and results.
- **Authentication and API:** Provides secure authentication and a well-defined API for communication with the frontend.

**Frontend Technology Evaluation (Flutter):**

The potential transition to Flutter necessitates a thorough evaluation of its capabilities:

- **High-Performance ML Integration:** Assessing Flutter's integration with TensorFlow Lite for on-device inference and potentially training.
- **Custom GPU Graphics:** Evaluating Flutter's graphical capabilities, particularly for the DynamicPlaneGenerator component.
- **Background Processing Support:** Determining Flutter's suitability for background tasks.
- **Data Transfer Mechanisms & Performance:** Evaluating data transfer efficiency between the Flutter frontend and the Python backend.

**Model Details:**

The core model processes 5-day candlestick chart images to predict stock market trends. It primarily utilizes Convolutional Neural Networks (CNNs), potentially in conjunction with LSTMs or Transformers to capture sequential dependencies. Vision Transformers (ViT), possibly incorporating EfficientNet or patch embeddings, are also being considered for processing the candlestick image data.

- **Input Data:** 5-day candlestick chart images, explicitly excluding dates, real prices, and tickers to focus solely on visual patterns. The input format and size are rigorously verified for compatibility and efficiency.
- **Output Data:** Trade signals generated based on visual patterns, typically using top/bottom 10% buy/sell thresholds. These signals are evaluated against actual t+1 to t+5 prices to determine trade execution. Numerical outputs are clearly documented for transparency and reproducibility.
- **Evaluation Metrics:** Model performance is assessed using metrics such as Mean Squared Error (MSE) or Root Mean Squared Error (RMSE).
- **Training Process:** The CNN is trained using the generated candlestick images. Details of the training process will be documented once finalized.

This architecture promotes flexibility and scalability. The decoupled nature of the client and server allows for future expansion to other frontend platforms (web, Android) while maintaining a centralized backend for data and model management. The ongoing evaluation of Flutter will determine the final frontend technology choice, ensuring a robust, performant, and portable solution.

## A. Model Architecture (Specialization)

This section details the architecture of the machine learning model, focusing on its framework-agnostic design and platform-specific deployment. The model prioritizes performance, cross-platform compatibility, and a streamlined update mechanism.

A "Universal Source Model" will be implemented in either PyTorch or TensorFlow. This framework-agnostic approach ensures consistent architecture and weights across all platforms (iOS, Android, Web), simplifying development and maintenance. This source model serves as the single source of truth.

Deployment involves converting the Universal Source Model into platform-specific formats: `.mlmodel` for iOS (leveraging Core ML), `.tflite` for Android (using TensorFlow Lite), and a compatible format for TensorFlow.js on the web. This conversion is a key part of the deployment pipeline.

To optimize performance, iOS leverages Core ML directly. A dedicated Swift class (`CoreMLHandler.swift`) within the iOS project handles communication with the Flutter application via Platform Channels. This class receives image data, processes it through the Core ML model, and returns predictions to the Dart code. On Android and other platforms, TensorFlow Lite is employed via the `tflite_flutter` package, ensuring high-performance execution.

Model management is crucial, addressing storage (local, cloud), versioning, updates, and platform-specific selection. Updates are streamlined through a synchronization mechanism: after client-side training or fine-tuning (supported using platform-specific formats), weight deltas or updated weights are sent to the central server. These updates are aggregated and potentially incorporated into the Universal Source Model, ensuring consistent performance improvements across all platforms.

The feasibility of using a unified TensorFlow Lite solution across all platforms will be investigated alongside the Core ML/TensorFlow Lite hybrid approach. This investigation will consider performance trade-offs, development complexity, and long-term maintainability. Any encountered bugs within the Flutter TFLite implementation will be documented and addressed, potentially involving communication with the TFLite maintainers.
This section requires details about the model architecture for the SCoVA project, which are not present in the provided checklist chunk. The checklist focuses on various security aspects, including authentication, API security, data protection, remote wipe functionality, recovery features, multi-factor authentication, Zero Trust, social recovery mechanisms, password security, hardware wallet integration, and UI/UX elements related to security features. These topics are unrelated to the model's design and training. To accurately describe the model architecture, information about the model's inputs, layers, outputs, and overall structure are needed. This should include details regarding the use of CNNs, LSTMs, Transformers, and Vision Transformers (ViTs), as outlined in the project document. Please provide the relevant checklist information to populate this section.

## A. Model Architecture (Specialization)

This section details the model architecture, emphasizing on-device training, efficient testing strategies, and "shocker event" detection. The architecture prioritizes minimizing cloud computing costs during development and testing.

On-device training eliminates initial server-side training expenses, enabling complete testing and debugging cycles directly on the iOS device. Short, 1-epoch training sessions with minimal data ranges further reduce computational load and server costs during development. A small, computationally inexpensive "dummy" neural network, mirroring the input/output shape of the intended ViT model, facilitates rapid on-device smoke tests to verify core components like Core ML training setup, loss calculation, backpropagation, and weight updates.

A multi-stage testing approach further streamlines development and minimizes cloud costs:

1. **Unit/Integration Tests with Mock Data:** Unit and integration tests utilize mock data, including edge cases like spikes, flat periods, and data gaps (in CSV or JSON format), to thoroughly test individual components and their interactions without the expense of real or large-scale data.

2. **End-to-End Dry Run:** A "Dry Run" toggle within the Experiment Designer UI enables a complete system run-through without actual model training or real-world interactions (e.g., trading actions), verifying the entire pipeline's functionality and data flow.

3. **On-Device Smoke Tests with a Dummy Model:** A small "dummy" neural network serves as a smoke test for basic system functionality. The current implementation involves a single-epoch training during the live inference portion of the smoke test. This process is under review for potential optimization; directly using the available dummy model may be more efficient.

A key architectural component is the Cognitive Threat Analysis Module (CTAM), a separate module from the `DynamicPlaneGenerator`, designed for real-time visual detection and analysis of "shocker events" – unexpected market disruptions characterized by quantifiable metrics like volatility spikes, volume anomalies, and rapid price changes. Lightweight CNN models within the CTAM will balance computational cost and accuracy in detecting these patterns on financial charts.

Beyond reactive, historical analysis, the model architecture incorporates computer vision to proactively analyze equity charts for patterns and anomalies indicative of potential "shocker events." Research is underway to extend this computer vision approach to futures, options, and derivatives data, potentially requiring new data sources and processing methodologies for a more comprehensive view of market dynamics. This proactive approach complements existing historical data analysis. The single-epoch training with the dummy model validates the on-device training pipeline, including data flow from the `DynamicPlaneGenerator`, training loop execution, and weight extraction. Further investigation will clarify the integration of this single-epoch training with the broader model improvement process and how Core ML updates the universal model on the backend.

## A. Model Architecture (Specialization)

This section details the architecture of the dual-system model designed to handle both normal market behavior ("flow") and extreme market events ("shocks"). This architecture comprises a "Flow Engine" and a "Contextual Threat Assessment Module" (CTAM), also referred to as the "Threat Engine".

The existing Flow Engine (detailed elsewhere) handles normal market conditions, utilizing established error-correction mechanisms. Its focus on mean reversion provides stability and consistent performance within typical market fluctuations.

The CTAM complements the Flow Engine by focusing on "shock" events. It analyzes real-time market data from multiple asset classes to generate a Systemic Threat Level (STL) score, a quantitative representation of the overall perceived threat.

The CTAM utilizes specialized Convolutional Neural Networks (CNNs) as "threat detectors" for different asset classes:

- **Equities:** This CNN analyzes real-time snapshots of primary equity charts, identifying anomalous patterns like gaps and volume spikes that could indicate market instability.
- **Derivatives:** This CNN processes visualized options chain data (e.g., heatmaps) to detect anomalies suggestive of unusual options activity.

The outputs of these detectors are combined via a fusion mechanism (e.g., a fusion model or weighted average) to produce the STL score.

This STL score is then integrated into the trading system, influencing:

- **Proactive Pratyahara (Withdrawal):** If the STL exceeds a predefined threshold, the system's risk management module may trigger a market withdrawal.
- **Dynamic Plane Adjustment:** The STL modulates the smoothing factor of the dynamic plane, adapting the system's sensitivity to market fluctuations based on the perceived threat.
- **Final Prediction Augmentation:** The STL serves as a context token, enriching the final prediction model's input and enhancing its decision-making capabilities. This is achieved through the Adaptive Strategy Weighting (Seesaw Mechanism) described below.

A dedicated Shockwave Prediction Model (SPM) is activated when the CTAM detects a "shock" event. The SPM is trained exclusively on historical "shocker events" (e.g., flash crashes, earnings gaps, major news-driven spikes). Its objective is to predict the short-term (1-3 candlestick) direction and magnitude of price movements immediately following the shock's onset. Potential architectures for the SPM include a reactive CNN or a transformer-based model with a modified attention mechanism optimized for post-shock prediction.

The Adaptive Strategy Weighting (Seesaw Mechanism) blends predictions from the Flow Engine and the SPM based on the STL score:

`Final Prediction = (1 - STL) * FlowEngine_Prediction + (STL) * SPM_Prediction`

This dynamic weighting ensures the system primarily relies on the Flow Engine during low volatility (STL near 0) and shifts to the SPM during high volatility (STL near 1). This adaptable approach allows the system to leverage the Flow Engine's stability during normal market conditions while capitalizing on the SPM's specialized capabilities during periods of extreme volatility.

Finally, the CTAM's retraining process considers both "flow" and "shock" data to prevent desensitization to extreme events. This ensures the system remains sensitive to both subtle and significant market anomalies, promoting robust and adaptable performance.

## A. Model Architecture (Specialization)

This section details the architecture of the specialized model training and prediction pipeline, emphasizing the principle of clearly defined roles and responsibilities—specifically, the Specialization role within the four pillars: Continuity, Enforcement, Facilitation, and Specialization. This restructuring addresses the current workload imbalance by redistributing tasks from the overloaded Specialist role to the other three roles, streamlining the workflow and improving overall efficiency. This is not a superficial re-labeling; it involves a deep refactoring and reimagining of the original architecture, deconstructing existing components, rethinking their roles within the system, and redefining data flow and system topology.

The four pillars work as follows:

- **Continuity:** Oversees all processes before and after model execution. This includes understanding data preprocessing steps, the model's inputs and outputs, and the post-processing of predictions, ensuring a deep understanding of prerequisites and expected outcomes for each step in the model pipeline.

- **Enforcement:** Manages resource allocation for model training, monitors the execution process, and handles error detection and prevention. Responsibilities include tracking resource utilization (GPU, memory, etc.), implementing early stopping mechanisms, and ensuring data integrity.

- **Facilitation:** Manages all input/output operations for the model. Facilitators handle communication between internal model processes and external systems, including data loading, preprocessing, and the delivery of predictions, streamlining data access and management for the Specialist role.

- **Specialization:** Houses the core model components, focusing solely on performing their specialized tasks. Examples include the CNN for trade signal generation, the Vision Transformer (ViT), and modules for predicting trend direction and reward magnitude. These components have been deconstructed and their functionalities reassessed for optimal performance and integration within the four pillars framework. Heavy components like the DynamicPlaneGenerator and Multi-Scale Vision Transformer have been broken down into atomic functions, each assigned to the appropriate pillar. Stringent communication protocols prevent direct interaction between specialist components; all communication is routed through designated facilitators, ensuring controlled data flow and preventing unintended dependencies.

Following the principle of "Communication Restriction for Specialists," specialized models operate without direct interaction with external systems or other code components outside their defined interfaces. All inputs and outputs are mediated through facilitator components, enforcing a clear separation of concerns and promoting modularity. For example, a model receives its input (e.g., candlestick chart images, preprocessed data) from a facilitator and returns its output (e.g., trade signals, predictions) to another (or the same) facilitator.

This architecture is further supported by several key services:

- **Pre-flight Validation Service:** Verifies all necessary conditions (data integrity, environment configuration, data schema consistency) before major computational tasks (training, backtesting) to minimize errors.

- **Post-flight Analytics Service:** Analyzes results after training or backtesting, summarizing performance, updating the model registry, and identifying recurring patterns, facilitating iterative model development.

- **Real-time Process Monitor:** Continuously monitors the health and resource consumption of ongoing jobs, enabling proactive intervention and resource optimization.

- **Execution State Controller:** Manages the lifecycle of running processes, allowing for pausing, resuming, and terminating jobs, providing flexibility and control.

- **API Gateway & Orchestrator:** Serves as a single entry point for all frontend requests, orchestrating complex workflows and ensuring the correct execution of multi-stage processes.

These services, combined with the four pillars design principle, ensure the reliability, efficiency, and scalability of the model development and deployment pipeline, providing essential infrastructure for managing complex workflows and maximizing the value derived from the models. This revised architecture, driven by a detailed analysis of user needs, is optimized for the project's goals.

### A. Model Architecture (Specialization)

This section details the architectural design of the specialized model components, focusing on maintainability, scalability, and efficient resource utilization. The model architecture adheres to the project's broader commitment to a structured, inheritance-based design, with all services inheriting from one of four base classes: `ContinuityService`, `EnforcementService`, `FacilitationService`, and `SpecialistService`. Technical specifications and pseudocode for these base classes, along with their initial derived services, will be documented to ensure standardized development.

This architecture utilizes a microservice approach, categorizing components under the four pillars of Continuity, Enforcement, Facilitation, and Specialization. This model falls under the _Specialization_ pillar. This decomposition promotes modularity, enhances security, and improves testability. While the increased network hops inherent in a microservice architecture can introduce latency, mitigation strategies such as service discovery and load balancing will be implemented. API design will be optimized for minimal overhead.

**Key Architectural Principles:**

- **Isolation of Specialists:** Specialist components, like the CNN for trade signal generation and the ViT for candlestick data processing, are strictly isolated. Interactions are managed through well-defined interfaces and data exchange protocols to prevent unintended side effects and promote modularity.
- **Asynchronous Communication:** Asynchronous communication via a publisher/subscriber model enables concurrent operation of specialist components, mitigating latency and improving system responsiveness.
- **Resource Access via Enforcers:** `EnforcementService` components manage all access to resources (data sources, model parameters, hardware accelerators), ensuring security and preventing conflicts.
- **Workflow Orchestration via Facilitators:** `FacilitationService` components orchestrate model training and prediction workflows, including data flow management, task scheduling, and exception handling.
- **Inheritance for Code Structure:** Class inheritance enforces a strongly typed separation of concerns, improving code clarity, maintainability, and scalability, and ensures consistent core functionality across specialist components.

**Specific Specialist Services (Example):**

Several specialist services handle distinct stages of data processing and training:

- **`NormalizeWindow`**: Normalizes raw candlestick data within a time window. Inputs: raw data, configuration dictionary. Output: normalized array. This service is restricted from accessing external data sources (e.g., `google-cloud-storage`, `google-cloud-firestore`, `requests`).
- **`ComputePrincipalComponents`**: Performs Principal Component Analysis (PCA) on normalized candlestick data. Input: normalized array. Output: top two principal components. Restricted to essential libraries like `numpy`.
- **`ProjectToPlane`**: Projects candlestick data onto a 2D plane defined by the principal components. Inputs: original data, basis vectors. Output: 2D projected data.
- **`TrainOneEpoch`**: Executes one epoch of model training. Inputs: current model artifact (bytes), training data tensors, configuration dictionary. Output: updated model artifact (bytes). Agnostic to data source and destination.

The `ExperimentRunner` service orchestrates the training and backtesting process, interacting with specialist models through designated facilitators. Function naming conventions will adhere to a consistent project-wide standard (to be finalized), and the rationale for the chosen convention will be documented. As the definitions of Specialization, Continuity, Enforcement, and Facilitation are refined, this section will be updated.

### A. Model Architecture

This section details the architecture of the models used for stock price prediction. The architecture leverages Convolutional Neural Networks (CNNs), potentially combined with LSTMs, Transformers, or Vision Transformers (ViTs), to process both image-based inputs (candlestick charts) and sequential data. While initially implemented as a monolithic architecture for simplified development and debugging, the design considers potential future distribution.

**Initial Monolithic Implementation:** The models are initially developed within a single codebase to streamline debugging and facilitate a smoother transition to a distributed architecture if required later.

**Model Input and Processing:**

- **Input Data:** The primary input is 5-day candlestick chart images encapsulating open, high, low, close, and volume data. The models will also incorporate sequential candlestick data windows, enhanced with engineered delta features. The format and size _n_ of the expected input data will be explicitly defined and verified.
- **CNN for Trade Signal Generation:** CNNs process the candlestick images to generate trade signals, focusing on potential buy and sell opportunities identified within the top and bottom 10% of predicted returns. Date information, actual price values, and ticker symbols are excluded from the CNN input to focus solely on visual patterns.
- **Hybrid Architectures:** The architecture may incorporate LSTMs or Transformers, specifically Vision Transformers (ViTs) using EfficientNet or patch embedding for feature extraction, to capture temporal dependencies and improve predictive power.
- **Trade Execution Simulation:** Generated trade signals are evaluated against actual price data from t+1 to t+5 to simulate trade execution and measure performance. The process of converting raw CNN output into actionable buy/sell signals will be thoroughly documented.
- **Dual Prediction Models:** Two separate models predict trend direction (up or down) and the magnitude of potential profit/loss.

**Performance Monitoring and Logbook Integration:**

A crucial aspect of the architecture is a system-wide logbook for performance monitoring and optimization:

- **Comprehensive Tracking:** The logbook tracks function execution time (including pause/resume times for component interactions), network calls, and API calls across all services.
- **Visualization:** It visualizes asynchronous and parallel service calls, including branching threads and loopbacks, to aid in understanding complex interactions.
- **Bottleneck Identification:** The logbook helps identify performance bottlenecks, guiding refactoring efforts, particularly for complex components.
- **IDE Integration:** The feasibility of integrating logbook functionality within the IDE or leveraging existing IDE tools will be investigated.

**Model Evaluation and Training Goals:**

- **Evaluation Metrics:** Performance is evaluated using clear metrics like Mean Squared Error (MSE) or Root Mean Squared Error (RMSE).
- **Training Goal:** The goal is to accurately predict visual patterns within candlestick data, emphasizing understanding the _why_ behind predictions, ensuring causal grounding.

**Dynamic Rotating Plane and Dual-Engine Perception:**

Beyond the initial CNN-based architecture, two advanced concepts are being explored:

- **Dynamic Rotating Plane:** Market dynamics are analyzed within a 2D plane that dynamically re-centers and rotates based on Time, Price, and Volume data, providing a more dynamic perspective than static candlestick images.
- **Dual-Engine Perception:** This system uses two models: a _Flow Engine_ for stable market conditions and a _Shockwave Prediction Model (SPM)_ for volatile events. A weighted "seesaw" mechanism combines their predictions. The weighting mechanism will be detailed further.

### A. Model Architecture (Specialization)

This section details the specialized architecture of the models used within the SCoVA project. The system adheres to a Four Pillars System Architecture, with this component acting as a _Specialist_ responsible for model inference and training. Computationally expensive tasks, such as image generation and model training, are offloaded to client devices (Client-Side Heavy Lifting) via a native app frontend, minimizing server load and enabling rapid iteration and experimentation.

A Universal Model Hub, residing on a lightweight backend orchestrator, provides the source model. This model is converted into platform-specific formats and distributed to client devices for on-device training. Weight updates from these individual training sessions are then aggregated and sent back to the backend, informing updates to the Universal Model, facilitating continuous improvement.

Subsequent checklist chunks provided information related to error signals, performance-based recalibration, multi-scale temporal analysis, "rally time" prediction, explainability integration, feature store utilization, attribution methods, live market data integration, and a paper trading environment. However, these chunks lacked specific details about the core model architecture itself (e.g., model type, input features, layers, or training procedures). Therefore, a comprehensive description of the model architecture cannot be fully constructed based on the provided information.

The following architectural considerations _are_ informed by the checklist items, but require further details for complete specification:

- **Advanced Error Signal:** The model incorporates a Total Error signal composed of Vector Deviation Error and Frame Shift Error for performance monitoring.
- **Performance-Based Healing:** Model recalibration is triggered by performance degradation rather than a fixed schedule.
- **Multi-Scale Periodicity:** The model ingests and fuses intraday, daily, and weekly data for multi-scale temporal analysis.
- **"Rally Time" Prediction:** The model predicts the expected duration (in candlesticks) for predicted price movements.
- **Distributed Tracing:** OpenTelemetry is used for performance monitoring and debugging.
- **Explainability:** The architecture integrates an "explanation AI" component, leverages the Feature Store for narrative generation, and supports model-agnostic (LIME, SHAP) and model-specific attribution methods.
- **Live Market Data Integration:** A `Paper_Brokerage_Simulator`, using live tick data and a `DeriveOrderBookFeatures` service, provides realistic trade simulations.

To complete this section, please provide checklist items or additional information that specifically describe the core model's architecture, including:

- **Model Type:** (e.g., CNN, LSTM, Transformer, Hybrid)
- **Input Features:** (e.g., price, volume, technical indicators)
- **Architecture Details:** (e.g., number of layers, activation functions)
- **Output:** (e.g., price prediction, classification)
- **Training Procedures:** (e.g., optimizer, loss function)

## Model Architecture (Specialization)

This section details the specialized architecture of the predictive models, incorporating market depth, order book information, and dynamic plane images to enhance profitability and responsiveness to market conditions.

A core component is the `MarketDepthAnomalyDetector`, a Convolutional Neural Network (CNN) trained to identify anomalies within market depth heatmaps. These heatmaps visualize order quantity changes over time across five bid and five ask levels provided by the Zerodha API. The CNN is trained to recognize visual patterns indicative of market shocks, such as large orders, withdrawals, and one-sided depletion, generating a probability score, `P(OrderBookShock)`.

The `P(OrderBookShock)` score is integrated into the Cyber Threat Analysis Model (CTAM), combining with other threat indicators like `P(PriceShock)` and `P(DerivativesShock)` for a comprehensive threat assessment. Furthermore, the CTAM continuously monitors the ratio of Level 1 Bid Quantity to Level 1 Ask Quantity, termed the "Top-of-Book Pressure Gradient." Significant and rapid changes in this ratio, indicative of sudden shifts in market sentiment, trigger a spike in the Systemic Threat Level (STL), transferring predictive authority to the Shockwave Prediction Model for enhanced responsiveness to market shocks.

Order Book Imbalance (OBI), calculated by the `CalculateOrderBookImbalance` service, is a key feature representing the normalized difference between buy and sell pressure (-1.0 to +1.0). Recognizing the potential latency and lag associated with OBI, three integration strategies are being explored: using OBI directly for prediction, gauging the reliability of existing predictions based on OBI, and dynamically adjusting the model's reliance on flow and shock models based on observed imbalance. A brainstorming session is planned to further explore these and other potential integration strategies. Additionally, "Order Book Volatility," calculated as the standard deviation of OBI over a short time window, is used by the Self-Correction & Healing Controller (Enforcer) to dynamically adjust the `CorrectionFactor` for the `DynamicPlaneGenerator`, improving system robustness during periods of market instability.

The core prediction model leverages a Vision Transformer (ViT) augmented with contextual information. The `ComputeOrderBookState` service generates a feature vector encompassing OBI, Depth Asymmetry, and "Wall" Detection, provided as a context token to the ViT alongside the dynamic plane image (Time, Price, Volume). This enriches the model's input without modifying the core dynamic plane representation. Two additional features enhance the ViT's predictive capabilities: "Price Improvement Rate," tracked by the `CalculatePriceImprovementRate` service and incorporated as a context token to exploit correlations between price improvement and short-term market movements, and "Book Resilience Score," a component of the `ComputeOrderBookState` output representing the ratio of order book quantity at Level 1 compared to Levels 2-5, providing a nuanced input alongside existing flow engine patterns.

## A. Model Architecture (Specialization)

This section details the specialized architecture of the predictive models, addressing asymmetric market dynamics, incorporating market regime information, and managing risk.

A core architectural component is the implementation of **Asymmetric Prediction Models**: two distinct prediction engines, a _Bull_Flow_Engine_ optimized for bull markets and a _Bear_Flow_Engine_ optimized for bear markets. The _Bull_Flow_Engine_ is trained exclusively on historical data from uptrends, while the _Bear_Flow_Engine_ is trained exclusively on historical data from downtrends. A high-level regime-detection model dynamically selects the appropriate engine based on the current market regime.

The model further leverages asymmetric market behavior through **Asymmetric Feature Engineering**. A dedicated service, _CalculateAsymmetricFeatures_, calculates _Upside Volatility_ (standard deviation of positive returns) and _Downside Volatility_ (standard deviation of negative returns). The _Workflow_Broker_ calls this service, and the resulting vector (containing these volatility measures and potentially other asymmetric features) is provided as a "context token" to the Vision Transformer (ViT). This allows the ViT to explicitly learn from the differing characteristics of upward and downward price movements. Furthermore, the ViT architecture is modified to integrate market regime information directly via a Regime ID provided by the Asymmetric Regime Detection model.

The system also incorporates a meta-learning model, the "Anxiety Model," which analyzes real-time market depth data to generate an "Anxiety Level." This level reflects the potential for errors or necessary adjustments within the primary trading algorithm (DynamicPlane) and informs the system's Error Detector and Weight Shifter components. Two distinct trading modes, 'flow' and 'shock,' allow the system to adapt to different market regimes based on the Anxiety Model’s assessment of market conditions and potential for errors.

Finally, the system incorporates an execution quality feedback loop. The Self-Correction & Healing Controller monitors rolling average execution quality. Degraded execution quality, indicated by increased slippage and reduced price improvement, triggers an increase in the CorrectionFactor, making the DynamicPlane’s perception more conservative and mitigating potential prediction errors. This dynamic adjustment enhances the system's resilience and adaptability to changing market dynamics.

While the system leverages visual data processing through the Snapshot Computer Vision Algorithm (SCoVA), using candlestick chart images as primary input, further architectural deliberations are required to fully specify SCoVA’s integration and its exploration of "non-hierarchical asymmetric" properties. The potential of a graph-based perceptual model using a Graph Neural Network (GNN) to integrate different timeframes will be investigated. Details regarding SCoVA and its specific functionalities will be provided in dedicated documentation.

## Model Architecture

This section details the architecture of the Vision Transformer (ViT) model, incorporating specialized components designed to enhance its performance in financial markets. These enhancements include the integration of asymmetric market features, regime detection, and a novel "Dual-Token Context Injection" approach.

### Asymmetric Feature Integration

The model incorporates an _AsymmetricFeatureEngine_ service to enrich the ViT with contextual information regarding market dynamics. This engine leverages the inherent asymmetry in market behavior, recognizing that market rises and falls exhibit distinct characteristics. Specifically, rises are treated as organic market behavior, while falls represent corrections. The _AsymmetricFeatureEngine_ analyzes raw market data within a defined time window and outputs a feature vector, which serves as a context token input to the ViT.

This feature vector comprises:

- **Price and Volatility Asymmetry:** Upside and downside volatility (using semi-deviation), volatility skewness, and volatility kurtosis. These capture differences in risk profiles and the likelihood of extreme volatility events.
- **Volume and Participation Asymmetry:** Accumulation/Distribution Ratio and Order-to-Quantity Asymmetry. These provide insights into buying and selling pressure and conviction.
- **Correlation Asymmetry:** Price-Volume Correlation State for both positive and negative return candles. This helps distinguish between periods of market fear and greed.

By incorporating these asymmetric features as context, the ViT can better interpret visual information from candlestick charts and generate more informed trading signals.

### Contextualized Input and ViT Adaptation

The model pipeline operates as follows:

1. **Feature Vector Generation:** The _AsymmetricFeatureEngine_, called by the Workflow Broker, computes the asymmetric feature vector.
2. **Contextualized Input:** This vector, along with the dynamic plane image tensor, is passed to the Model Inference Service and subsequently to the ViT as a context token.
3. **ViT Processing:** The ViT's self-attention mechanism is designed to learn the relationships between the visual patterns in the dynamic plane image and the contextual information provided by the feature vector.

This design balances simplicity and potential for future enhancements. While more complex methods for understanding market asymmetry and volatility are being considered, the current architecture prioritizes maintainability and adaptability.

### Regime Detection and Integration

An unsupervised clustering model (e.g., Gaussian Mixture Model or Self-Organizing Map) will be trained offline on historical asymmetric feature vectors to detect distinct market regimes (4-8 regimes). In real-time operation, the 'IdentifyAsymmetricRegime' service will classify the current market regime based on the feature vector and provide a Regime ID (integer) as another context token to the ViT.

### Terrain-Based Input and Dual-Token Context Injection

Instead of directly inputting individual environmental metrics (e.g., humidity, temperature, AQI), a "terrain-based" approach will be explored, categorizing the current environment into predefined terrain categories. The resulting category ID will serve as an additional input to the ViT. This approach potentially enhances explainability and predictive accuracy but introduces a potential loss of granularity. Further analysis will determine the optimal balance between simplification, explainability, and information retention.

This "Dual-Token Context Injection" approach, using both the Regime ID and the terrain category as context tokens, aims to combine the explainability of discrete regime identification with the rich information provided by the asymmetric features. This hybrid approach allows the ViT to leverage both high-level market state and granular market dynamics for enhanced predictive performance.

## B. Training and Validation

This section details the training and validation process for the Vision Transformer (ViT) model, crucial for robust performance and generalization. The following steps outline this process:

1. **Training and Validation:** The ViT model will be trained using a designated training dataset and validated against a separate validation set (the last 30% of the data, from July 1, 2020, to December 31, 2021). A mean squared error (MSE) loss function will be used. Early stopping will be implemented to prevent overfitting by monitoring the validation loss and halting training if it doesn't improve for two consecutive epochs. The model with the lowest validation loss will be selected for testing.

2. **Learning Rate and Batch Size:** The optimal learning rate and batch size will be determined through experimentation and hyperparameter tuning during the training process. These values will be documented once finalized.

3. **Model Training Data Scale-Up:** A large-scale dataset of candlestick chart images, regime IDs, asymmetric feature vectors, and corresponding return labels will be prepared for training. The details of this data preparation and scaling process will be outlined in a subsequent section.

4. **Image Sequence Generator:** A sliding window image generator will be developed to create sequences of candlestick chart images from the raw OHLCV data. This generator will produce overlapping sequences of a defined length (e.g., 5-day sequences) for input to the ViT.

5. **Sample Dataset for Initial Testing:** A smaller sample dataset will be created for initial testing and validation of the model architecture, training pipeline, and data preprocessing steps. This smaller dataset will allow for faster experimentation and debugging before scaling up to the full dataset. This sample dataset will also include the regime IDs and asymmetric feature vectors.

## B. Training and Validation

This section details the training and validation process for the CNN model, focusing on creating a robust and generalizable model. This involves establishing key training parameters, preparing the training data, and utilizing appropriate validation techniques.

**1. Data Preparation and Input Verification:**

- **Candlestick Chart Generation and Verification:** The CNN model uses a 5-day candlestick chart as its primary input. No raw numerical data (OHLC, volume, etc.) is directly fed into the model. The input data, representing the candlestick information (Open, High, Low, Close prices) for the 5-day window, will be verified for correct formatting and size. Special attention will be paid to edge cases, particularly how the last 5 data points of each candlestick graph are handled to maintain data integrity. Visualizations of these charts, generated using `matplotlib`, will be used to aid in verification and debugging. This addresses a previous constraint where the `mplfinance` library was unavailable.
- **Image Sequence Generation:** A sliding window image generator will be developed to efficiently create sequences of candlestick images from the OHLCV data. This generator will be crucial for training, especially for future enhancements involving temporal models like LSTMs or Transformers. Initially, a smaller sample dataset of 3-image sequences will be created for rapid development, testing, and demonstration of the generator functionality before scaling up to the full dataset.
- **Model Training Data Scale-Up:** A large-scale dataset of candlestick images and their corresponding 5-day future return labels will be prepared. This large dataset will ensure the model has sufficient exposure to diverse market conditions and patterns, improving generalization.

**2. Model Training and Validation:**

- **Training Process:** The CNN model will be trained using the prepared dataset.
- **Validation and Early Stopping:** A separate validation set will be used to monitor performance during training and implement early stopping to prevent overfitting and select the best-performing model.
- **Hyperparameter Tuning:** Appropriate learning rate and batch size will be determined through experimentation and hyperparameter tuning to optimize model convergence and performance.

**3. Output Clarification:**

The CNN produces numerical outputs related to trade signals (entry, exit, and predicted return). Thorough documentation will detail the transformation process from the visual input to these numerical outputs, ensuring transparency and interpretability of the model's predictions. Each numerical value's meaning will be clearly defined.

## B. Training and Validation

This section details the procedures for training and validating the CNN and ViT models, ensuring robust performance and reliable predictions. This involves data preparation, hyperparameter tuning, and implementing a confidence-based filtering mechanism.

**Data Preparation and Training Pipeline:**

- **Image Sequence Generator:** A sliding window image generator will be developed to create sequences of candlestick images. This generator will handle both single images for the CNN and sequences of images (e.g., 3-image sequences) for the ViT, allowing for efficient data handling during training. A smaller sample dataset of 3-image sequences will be created for initial testing and debugging of the generator and models before scaling up to the full dataset.
- **Model Training Data Scale-Up:** A large-scale dataset of candlestick chart images and corresponding return labels will be prepared and processed. The scale of the data will be determined based on model capacity and computational resources, ensuring sufficient data to capture diverse market dynamics.
- **Training and Validation:** Standard training procedures will be followed, utilizing a designated validation set and employing early stopping to prevent overfitting. The dataset will be split into training and validation sets, and models will be trained on the training data while monitoring performance on the validation set to determine the optimal stopping point.
- **Learning Rate and Batch Size:** Appropriate learning rates and batch sizes will be determined through experimentation and hyperparameter tuning. The goal is to find values that facilitate stable and efficient training, leading to optimal model convergence. Validation set performance will guide the selection of optimal values.

**Hyperparameter Considerations and Justification:**

- **5-Candlestick Input Window Justification:** A literature review will be conducted to justify the choice of a 5-candlestick input window for the models. This research will provide a rationale for this specific window size and inform the model design.
- **Separate CNNs for Holding Periods:** Separate CNNs will be trained for each holding period (1 to 5 days). Performance metrics for each model will be documented.

**Confidence-Based Filtering (Historical Prediction Error Profiling - HPEP):**

Beyond basic training and validation, a confidence-based filtering mechanism using HPEP will be implemented to enhance trading strategy robustness.

- **Post-Training Confidence Profile:** After training, a confidence profile (HPEP map) will be generated by binning validation set predictions and calculating the accuracy for each bin. Bins can represent ranges of predicted returns (e.g., -5% to -3%, -3% to -1%, etc.). Optionally, average error magnitude or Sharpe ratio can be calculated for each bin. This map will be stored for use during backtesting.
- **Backtesting Trade Filtering:** During backtesting, trades will be filtered based on the HPEP map. Before executing a trade, the corresponding prediction bin will be referenced. The trade will only be executed if the historical accuracy for that bin exceeds a predefined threshold, enabling a more selective and potentially more profitable trading strategy.
- **Code Implementation:** The HPEP module will be integrated into the existing codebase within `test_model.py` (post-training) and `trade.py` (during filtering). Dummy data and a prototype script will be created for simulation and testing.

This rigorous training and validation process aims to produce robust and reliable models for informed predictions and more effective trading strategies.

## B. Training and Validation

This section details the training and validation procedures for the CNN model, emphasizing the transition to soft labeling for improved trading accuracy. It builds upon established best practices, including using a validation set, early stopping, learning rate scheduling, and batch size optimization (details to be documented separately). Data preparation steps, such as creating a sliding window image generator and a smaller sample dataset for testing, are also prerequisites for this phase. The core training and validation process encompasses the following:

1. **Input Window and Prediction Horizon:** The model uses a fixed input window of 5 candlesticks, supported by existing research (Jiang et al., 2023) demonstrating its effectiveness for pattern recognition. The output window (prediction horizon) is also set to 5 days, though this choice is based on practical considerations and warrants further investigation.

2. **Soft Labeling Implementation:** Transitioning from hard labels to soft labels is a key focus of this phase. Soft labels, representing a probability distribution over potential returns, are expected to improve trading accuracy, particularly during validation, by addressing the limitations of hard labels' precision when generalizing to unseen data.

3. **Return Space Discretization:** The range of possible returns will be discretized into bins (e.g., -5% to +5% with 0.5% increments). The CNN will predict the probability of the return falling within each bin.

4. **Model Output Modification:** The CNN's output layer will be replaced with a fully connected layer followed by a softmax activation function to generate the probability distribution over the return bins.

5. **Label Conversion:** Hard labels will be transformed into soft labels using a Gaussian kernel centered around the observed return. This smoothing accounts for inherent noise and uncertainty in the return data.

6. **Loss Function Adjustment:** Given the shift to probabilistic outputs, the loss function will be changed from Mean Squared Error (MSE) to a more appropriate metric for probability distributions, such as Categorical Cross-Entropy or KL-Divergence.

7. **Training and Validation Execution:** Standard training and validation procedures will be implemented with the modified architecture and loss function. Performance metrics, including validation loss, Sharpe ratio, alpha, and Mean Squared Error (MSE), will be tracked.

Addressing high trading costs, exploring uncertainty-aware architectures (e.g., Monte Carlo Dropout), and prototyping probabilistic trading strategies based on confidence thresholds are important considerations for improving model performance and practical application. However, these aspects fall under subsequent testing and prototyping phases and will be addressed later. Similarly, backtesting the probabilistic trading strategy is an essential evaluation step that follows the training and validation process described here.

## B. Training and Validation

This section details the training and validation procedures for the predictive models, incorporating the rally time prediction. The primary goal is to train a robust model capable of accurately predicting both the return and the time it takes to achieve that return (rally time).

Two primary modeling approaches will be investigated and compared:

**1. Dual-Headed CNN:**

This approach utilizes a Convolutional Neural Network (CNN) with two output heads:

- **Return Regression Head:** Predicts the scalar return of the trade. This head leverages features extracted from an EfficientNet backbone, followed by linear layers to produce the return prediction.
- **Rally Time Head:** Predicts the "rally time," defined as the number of candlesticks it takes to reach the target return. This head also uses EfficientNet features and subsequent linear layers.

The model will be trained using a weighted sum of Mean Squared Error (MSE) losses for each head. The weights for each loss component will be tunable hyperparameters, allowing for balancing the importance of return prediction versus rally time prediction. Standard training procedures will be employed, including the use of a dedicated validation set and early stopping to prevent overfitting. Learning rate and batch size will be carefully tuned to ensure optimal convergence.

**2. Survival Analysis Models:**

Alternative architectures based on survival analysis will also be explored. Models such as DeepSurv, DeepHit, and Weibull Time-To-Event models will be investigated. These models predict the probability distribution of time until a target price level is reached. This approach offers advantages in handling censored data (cases where the target price is not reached within the observation window) and provides a measure of uncertainty over the predicted rally time.

The validation process will involve comparing the performance of the survival analysis models against the dual-headed CNN approach. Metrics will include accuracy of return prediction, accuracy of rally time prediction, and overall profitability when incorporating the model's outputs into a trading strategy.

This rigorous validation process will ensure the selected model generalizes well to unseen data and provides reliable predictions for both return and rally time. Previous iterations highlighted inefficiencies related to trade implementation, specifically high turnover, uniform weighting, and inclusion of low-confidence predictions. These challenges will be addressed by incorporating a stop-loss mechanism, prediction confidence thresholding, and risk-based weighting into the trading strategy during the validation phase. Details of these enhancements are provided in Section C.

### B. Training and Validation

This section details the training and validation procedures, emphasizing strategies for enhancing model performance, handling sequential data, and learning from prediction errors. The process begins with foundational training and validation steps and progresses to more advanced techniques like sample re-weighting and meta-learning.

**Initial Training and Validation:**

1. **Dataset Preparation:** A large-scale dataset of candlestick images and corresponding labels will be prepared. Data augmentation strategies may be explored to enhance data diversity. A smaller sample dataset of 3-image sequences will be created for initial testing and debugging.
2. **Model Training:** CNN models will be trained using the prepared dataset. A validation set will be used to monitor performance, and early stopping will prevent overfitting.
3. **Hyperparameter Tuning:** Learning rates and batch sizes will be determined through experimentation and optimized for performance on the validation set.
4. **Image Sequence Generation:** A sliding window image generator will be developed to create sequences of candlestick images, especially for rally time prediction. This generator will handle the sequential nature of the input data.

**Advanced Training Techniques:**

1. **Error Map Generation:** During validation, predicted and actual returns will be tracked. Instances of "bad trades" (e.g., predicting long positions with negative returns) will be logged and analyzed for recurring patterns. This "error map" will inform subsequent training cycles, allowing the model to learn from its mistakes.
2. **Sample Re-weighting:** Higher loss weights will be assigned to misclassified trades, particularly those with significant losses. This cost-sensitive approach prioritizes avoiding high-impact errors.
3. **Bootstrapping of Hard Cases:** After each epoch, high-loss samples ("hard cases") will be collected into a dedicated dataset. A separate training cycle focused on these hard cases will improve performance on challenging examples.
4. **Meta-Model for Trade Review:** A secondary model (meta-learner) will predict the likelihood of a trade going wrong. This meta-model will be trained on the primary model's initial prediction, candlestick chart features, trade setup details, and the actual trade outcome, enhancing risk management.
5. **Reward Logic Enhancement:** The reward logic will consider the sequence of preceding candlesticks, enabling the model to learn temporal dependencies within chart patterns.

**Architectural Experiments and Enhancements:**

1. **Benchmark Comparison:** Three architectural approaches for handling temporal data (single static image, image pair, image sequence) will be compared to determine the most effective method.
2. **2-Image Paired Input Dataset Generator:** A data generator will create paired image inputs for the benchmark experiment, concatenating/stacking consecutive candlestick images.
3. **CNN + LSTM Hybrid Architecture:** A hybrid CNN-LSTM architecture will be implemented. The CNN will extract features from individual candlestick images, while the LSTM will model temporal dependencies.
4. **Vision Transformer (ViT) Implementation:** A ViT will process sequences of three consecutive 5-day candlestick chart images to capture temporal relationships for improved market prediction.

This multifaceted approach to training and validation aims to create robust models capable of handling the complexities of financial market prediction. While this section addresses core model training and error analysis, subsequent refinement stages will explore alternative exit strategies and further investigate learning from bad trades.

## B. Training and Validation

This section details the training and validation process for both the Convolutional Neural Network (CNN) and Vision Transformer (ViT) models. We will begin with a smaller dataset of 3-image sequences for rapid iteration and then scale up to a larger dataset for robust training. Performance will be evaluated using metrics appropriate for the chosen prediction target (scalar reward, class label (BUY/SELL/HOLD), or a combination using a multi-head setup).

### CNN Training and Validation

The CNN model will be trained using standard procedures. A dual-input CNN will also be explored, accepting both the current frame and the delta frame (difference between current and previous frames).

- **Data:** Initial training will use a sample dataset of 3-image sequences generated from stock chart data. This will be scaled up to a larger dataset as development progresses. The final size and characteristics of the training dataset will be documented.
- **Training Process:** Standard training procedures will be followed, including defining the learning rate and batch size through experimentation. The rationale for the chosen values and their impact on training will be documented.
- **Validation:** A validation set will be used to monitor performance and implement early stopping to prevent overfitting. Details on the data split and early stopping criteria will be documented.

### ViT Training and Validation

The ViT model will be trained using sequences of candlestick windows as input, incorporating delta embeddings. Both image subtraction and feature subtraction methods for generating delta features will be evaluated. The model will be designed to handle variable-length input sequences.

- **Data:** Similar to the CNN, training will begin with a sample dataset of 3-image sequences and then scale to a larger dataset. Additionally, experiments will be conducted with input sequence lengths of N=3, 4, and 5.
- **Delta Integration:** The ViT pipeline will incorporate delta embeddings, using either image subtraction or feature subtraction. Performance differences between these methods will be analyzed.
- **Sequence Generation:** A sliding window image generator will be developed to create sequences of candlestick chart images and their corresponding labels, enabling the model to learn temporal dependencies.
- **Variable-Length Input:** The ViT model will handle variable-length sequences using masking and padding. Shorter sequences will be padded with blank chart images up to the maximum sequence length (e.g., 5 charts), and a mask will inform the transformer which parts are padding. Positional embeddings will be implemented to ensure the model understands the order of images within the sequence.
- **Training Process:** Similar to the CNN, the learning rate and batch size will be determined through experimentation.
- **Validation:** A validation set will be used to monitor performance and implement early stopping to prevent overfitting. Details on the data split and early stopping criteria will be documented.

## B. Training and Validation

This section details the training and validation process, emphasizing data handling, alternative prediction methods, and rigorous performance evaluation. Standard practices like training CNNs with a validation set, early stopping, learning rate adjustments, and appropriate batch sizes will be employed. However, the following key explorations will be prioritized:

**1. Data Preparation and Experimental Design:**

- **Image Sequence Generator:** A robust sliding window image generator will be developed to create sequences of candlestick images for training and validation. This generator will support initial experiments with 3-image sequences and facilitate scaling to larger datasets.
- **Experimental Design for Validation:** A vision transformer (ViT) or U-Net architecture will be trained to generate images, from which predicted return values will be extracted. Performance will be rigorously evaluated against baseline models using metrics like Root Mean Squared Error (RMSE), Structural Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), and backtested profit performance.
- **Return Extraction from Predicted Images:** A system will be developed to extract open, high, low, and close values from the generated candlestick images to calculate returns. This might involve analyzing pixel locations or rendering the image data.

**2. Exploring Image-Based Prediction:**

- **Predicting Candlestick Images:** Instead of predicting numerical returns, the model will predict future candlestick images, framing the problem as visual sequence forecasting (image-to-image prediction). Returns will be extracted from these predicted images.
- **Prototype Design:** A prototype image-to-image candlestick forecaster will be developed using either a Convolutional Neural Network (CNN) decoder or a transformer-based image generator. This choice will inform subsequent training and validation procedures.
- **Conceptual Soundness:** The validity of this image-based approach will be assessed by analyzing how human traders interpret candlestick patterns and comparing this to the model's approach.
- **Theoretical Advantages:** A theoretical analysis will explore potential advantages of image-based prediction, including the richness of visual representations, improved uncertainty modeling, enhanced causal reasoning, impact on training supervision, interpretability gains, and potential for generative extensions.

**3. Risk Assessment and Comparative Analysis:**

- **Risk and Drawback Assessment:** Potential downsides of image-based prediction, such as less direct performance evaluation, compounding errors, loss of direct reward supervision, and ambiguity in translating visual predictions, will be carefully considered.
- **Comparison with Existing Models:** The chosen architecture will be compared with existing scalar regression and probabilistic return models, considering factors like output type, supervisory signal, trading strategy integration, learned structure richness, interpretability, ambiguity risk, data requirements, and modeling complexity. This comparison will provide a comprehensive understanding of the proposed model's strengths and weaknesses.
  Training and Validation of Image-Based Financial Prediction Models

This section details the robust training and validation procedures crucial for the image-based prediction models (CNNs and ViTs). Given the unique challenges of using images for financial prediction, special attention will be paid to the stability and reliability of the training process, addressing potential fragility in training and evaluation.

A key challenge is ensuring the robustness of training and evaluation methodologies. Several potential issues must be addressed: aligning ground truth labels with generated candlestick images; selecting appropriate loss functions that capture both image fidelity and trading relevance; preventing mode collapse (where the model generates only a limited set of outputs); avoiding overfitting to specific patterns in the training data; and establishing meaningful metrics for evaluating the trading relevance of generated images. Careful selection of loss functions, regularization techniques, and robust evaluation metrics will be employed to mitigate these risks.

While realistic chart generation is desirable, the primary focus is on extracting _trading value_. Training and validation will prioritize the model's ability to identify actionable trading opportunities, including clear entry and exit points, rather than simply replicating historical price patterns. Evaluation metrics will reflect this priority.

Furthermore, the model's predictions must be causally grounded. It's insufficient to learn only the appearance of patterns; the model must also understand _why_ they occur. This understanding is essential for generalization to unseen market conditions. While achieving causal understanding is complex, the training process will be designed to encourage the model to learn meaningful relationships between market dynamics and resulting price patterns, potentially through incorporating additional features or employing architectures capable of capturing causal dependencies.

A dual-module framework, comprising a chart generator and a separate trade evaluator, is being considered. This separation allows for specialized training and validation of each module. The chart generator can be trained for realistic chart generation, while the trade evaluator can be trained to identify profitable trading opportunities from the generated charts. This structure also facilitates more targeted evaluation, assessing the performance of each module independently. A robust experimental protocol will compare this dual-module, image-based prediction approach against traditional, scalar-based methods. This protocol will utilize appropriate metrics and consider performance under diverse market conditions.

Specific training and validation steps include:

1. **Training and Validation with Early Stopping:** Models will be trained using generated candlestick chart images and corresponding return labels. A validation set will be used to monitor performance during training and implement early stopping to prevent overfitting.

2. **Hyperparameter Tuning (Learning Rate and Batch Size):** Optimal learning rate and batch size will be determined through experimentation and hyperparameter tuning.

3. **Large-Scale Dataset Preparation:** A large-scale dataset of candlestick images and corresponding return labels will be prepared to ensure the model can learn complex patterns and generalize effectively.

4. **Sliding Window Image Sequence Generator:** A sliding window image generator will be developed to create sequences of candlestick chart images for models requiring sequential input, such as the Vision Transformer (ViT). This captures temporal relationships between consecutive candlestick patterns.

5. **Initial Prototyping Dataset:** A smaller sample dataset of 3-image sequences will be created to facilitate rapid prototyping and initial model development.

### B. Training and Validation (Enforcer)

This section details the training and validation process for the CNN and ViT models, emphasizing data preparation and parameter tuning within the dynamic projection system. Standard training and validation procedures will be employed, including splitting the data into training and validation sets, setting appropriate learning rates and batch sizes, and implementing early stopping to prevent overfitting.

A key aspect of this phase is preparing the data for the dynamic projection system. This involves understanding how individual candlesticks influence the re-centering and rotation of the feature space and the specific mathematical transformation used for this "rotation." This includes dynamically implementing PCA rotation within the Vision Transformer and CNN pipelines. Further clarification on the goal of this dynamic projection is needed to fully define the data preparation process.

A sliding window image generator will create sequences of candlestick images for the models, especially for the ViT. A smaller dataset of 3-image sequences will be used for initial testing and debugging of the ViT model and its input pipeline, allowing for rapid iteration and experimentation before scaling to the full dataset. This smaller dataset will also be used to validate the image sequence generator.

Visualizations of training progress and other relevant metrics will be created using `matplotlib` and `numpy`. The use of `seaborn` is not permitted.

## B. Training and Validation

This section details the training and validation procedures for the Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and potentially recurrent architectures like LSTMs or standard Transformers. It covers key aspects like data preparation, model training parameters, and performance evaluation. While the primary focus is training with static candlestick images, the process of generating dynamic snapshots for future model enhancement is also outlined due to its close coupling with the training pipeline.

**Static Image Training**

The following points outline the training and validation process using static candlestick images:

- **Training and Validation Split:** The dataset will be split into training and validation sets using an appropriate ratio (e.g., 80/20). Early stopping will be employed to prevent overfitting, monitored based on validation performance. Specific details regarding the split ratio and early stopping criteria (e.g., patience, metric) will be documented in the implementation.
- **Learning Rate and Batch Size:** Optimal learning rates and batch sizes will be determined through experimentation and hyperparameter tuning. The chosen values and their impact on model convergence and performance will be documented.
- **Data Scaling:** Appropriate data scaling techniques will be applied to the input data to ensure features are within a suitable range for model training. This might involve normalization or standardization, and the specific method used will be documented.
- **Dataset Scale-Up:** Strategies for scaling up the training dataset will be explored, potentially including data augmentation techniques. The final dataset size and characteristics will be detailed, along with considerations for storage and efficient data access during training.

**Dynamic Snapshot Generation for Future Training**

This subsection details the process of generating dynamic candlestick image snapshots, which can be used for future model training. This process involves rotating and re-centering the price data to create a locally optimized frame of reference.

1. **Image Sequence Generator:** A sliding window image generator will be implemented to create sequences of candlestick images, particularly important for models utilizing sequential input (e.g., LSTMs, Transformers, ViTs). This generator will extract fixed-length sequences from the data using a defined window size and stride.

2. **Sample Dataset:** A small sample dataset of 3-image sequences will be created for initial development and testing. This allows for rapid prototyping and verification of the training pipeline before scaling to the full dataset. The creation process for this representative sample dataset will be documented.

3. **Dynamic Snapshot Creation:** After rotating and re-centering the price data (detailed in the Dynamic Plane Implementation section), the transformed data will be rendered into candlestick images. Candlesticks will be drawn relative to the new, rotated axes (X' and Y').

4. **Rotation Artifact Handling:** Appropriate interpolation techniques (e.g., anti-aliasing) will be employed to minimize distortion and ensure the quality of the generated images.

5. **Volatility Jump Handling:** Strategies for handling sudden, large price movements (volatility jumps) will be implemented, such as smoothing or limiting rotation angle changes, to maintain consistent rotation behavior.

6. **Consistent Axis Scaling:** Consistent axis scaling (units per percentage move) will be maintained across all generated frames to ensure consistent model input and facilitate effective learning.

The goal of dynamic snapshot generation is to create a feature space focused on relative price movement rather than absolute position. This is achieved by dynamically rotating and re-centering the candlestick data before each prediction step, effectively creating a moving frame of reference that captures local price action. This approach aims to improve the model's ability to recognize patterns independent of absolute price levels and overall trend direction.

### B. Training and Validation (Enforcer)

This section details the training and validation process, emphasizing the preparation and use of the dynamic plane representation for the chosen CNN and Vision Transformer (ViT) models. This approach dynamically redraws the coordinate system (time, price, and volume) at each time step, creating a "locally true" 2D plane derived from principal component analysis (PCA) of recent market movements. This dynamic input is expected to enhance the model's ability to capture complex patterns.

**Data Preparation and Dynamic Plane Generation:**

1. **Dynamic Plane Generation using PCA:** A sliding window of recent candlestick data (e.g., 5-10 periods) will be used to calculate movement vectors in time-price-volume space. PCA will be applied to these vectors, and the top two principal components will define the axes of the dynamically redrawn 2D plane. This captures the primary movement trajectory and residual behavior within the local market context.

2. **Dynamic Plane Generator Algorithm:** A dedicated algorithm will generate the dynamic 2D plane. Detailed pseudocode will be provided, encompassing: data input, local window definition, movement vector calculation, PCA application, coordinate system rotation, and 2D plane reconstruction.

3. **Visualization of Dynamic Plane Evolution:** A conceptual diagram will illustrate the evolution and shifts of the dynamic 2D plane with each market movement, clearly demonstrating the dynamic coordinate system and its adaptation to market changes.

4. **Refined Angle Theta Calculation:** The calculation of angle theta, crucial for representing rotations within the dynamic plane, will be reevaluated and refined to integrate the principles of the dynamic representation. The initial simple price difference method will be replaced with a more sophisticated approach that considers the dynamic origin and rotational axes.

**Model Training and Validation:**

1. **Training and Validation Process:** Standard training and validation procedures will be followed for both CNN and ViT architectures. This includes using a designated validation set, implementing early stopping to mitigate overfitting, and tuning hyperparameters such as learning rate and batch size.

2. **Image Sequence Generation:** A sliding window image generator will create sequences of images from the dynamic plane output. This generator will be optimized for batch operation to efficiently process the entire time series and format the data appropriately for CNN/ViT input. A small sample dataset of 3-image sequences will be created for initial testing and debugging of the training pipeline.

3. **Data Scaling:** A large-scale dataset of candlestick data will be prepared to ensure robust model training. This data will be fed into the dynamic plane generator to create the training images.

These steps ensure a rigorous and precise data transformation process, providing the models with a dynamic and locally relevant perspective on market data. This is expected to enhance the models' ability to learn complex patterns and improve prediction accuracy.

## B. Training and Validation

This section details the training and validation of the CNN and ViT models, including data preparation, hyperparameter tuning, and addressing challenges specific to the dynamic plane implementation.

**Data Preparation and Training:**

1. **Dataset Generation:** Generate candlestick chart images and corresponding return labels for training the CNN. For the ViT, generate sequences of candlestick images using a sliding window generator. This generator will also be used to create dynamic candlestick snapshots, integrating the Rotating Dynamic Plane Generator into the ViT training pipeline to enable comparison between static and dynamic frame performance. A large-scale dataset incorporating diverse market conditions will be created to ensure model robustness and generalization. A smaller sample dataset of 3-image sequences will be used for initial testing, validation, and faster iteration during development.

2. **Data Augmentation:** The validation process will incorporate transformed candlestick images generated by the Rotating Dynamic Plane Generator to ensure models are robust to these transformations. This includes handling edge cases such as collinear data points, especially in early training frames, which can lead to instability in Principal Component Analysis (PCA) calculations.

3. **Hyperparameter Tuning:** Determine and optimize the learning rate and batch size for both CNN and ViT training.

**Dynamic Plane Implementation and Visualization:**

To ensure the stability and correctness of the dynamic plane implementation, a standalone animation simulator will be developed. This simulator will:

1. **Delayed Rotation:** Implement a delayed rotation mechanism, initiating plane rotation only after accumulating at least three stable data points to mitigate potential PCA instability and ensure robust rotation calculations.

2. **Rotation Smoothing:** Smooth the rotation matrices to prevent dimensional blowups and ensure visually appealing and smooth transitions in the animation.

3. **Visualization:** Clearly visualize the dynamic movement of the points and the live rotation and re-centering of the frame.

This simulator will allow thorough testing and validation of the dynamic plane mechanism, including handling edge cases such as single-point frames (by displaying a placeholder or empty canvas) and ensuring correct formatting of offsets to prevent dimension mismatches. The simulator will also address the requirement for visualizing the dynamic redrawing of the plane and generating example images of both original and transformed candlestick input. The technical constraint of needing at least two data points for the dynamic plane calculations will be enforced within the simulator and the image generation process. The simulator must be capable of rendering image representations of the dynamic 2D plane. This validation process will inform the final integration of the dynamic plane into the core model training pipeline.

### B. Training and Validation (Enforcer)

This section details the process of generating synthetic data representing various market regimes, visualizing these regimes on both standard and dynamic Heiken-Ashi charts, and analyzing the impact of the dynamic plane transformation. These visualizations and analyses are crucial for understanding the model's behavior under different market conditions and will be incorporated into the dissertation. While not directly part of the training loop, they inform model development and provide valuable context for evaluating performance.

The following market regimes will be visualized and analyzed:

- **Trend-Reversal-Recovery:** This regime captures a common market pattern of an initial trend followed by a reversal and subsequent recovery.
- **Choppy Sideways:** This regime simulates a period of high volatility and rapid price fluctuations within a relatively narrow trading range.
- **Strong Linear Uptrend:** This regime represents a sustained period of upward price movement, providing a contrasting scenario to the other two regimes.

For each regime, the following steps will be performed:

1. **Data Generation:** Synthetic candlestick data will be generated for each market regime using dedicated functions, including `generate_choppy_candlesticks(n=30)` for the choppy sideways regime.

2. **Standard Heiken-Ashi Chart Creation:** Standard Heiken-Ashi charts will be generated from the synthetic data using `generate_heiken_ashi` and `plot_heiken_ashi_candlestick`.

3. **Rotated Dynamic Heiken-Ashi Chart Creation:** The `dynamic_rotate_recenter_heiken` function will be applied to the Heiken-Ashi data, introducing rotation and recentering based on Principal Component Analysis (PCA). The resulting rotated dynamic Heiken-Ashi charts will be plotted using `plot_rotated_heiken`.

4. **Side-by-Side Comparison:** Standard and rotated dynamic Heiken-Ashi charts for each regime will be presented side-by-side in a subplot grid to facilitate direct visual comparison and avoid matplotlib figure stacking issues. This comparison highlights the impact of the dynamic plane transformation on the visual representation of each market regime.

5. **Combined Visualization:** For a cohesive presentation, the side-by-side comparisons for all three regimes will be combined into a single, comprehensive panel. This panel will serve as a key visual element in the dissertation, demonstrating the behavior of the dynamic plane under various market conditions.

6. **Analysis and Summary:** A detailed analysis of the dynamic plane's behavior across the different market regimes will be conducted, focusing on the implications for model learning and interpretation. The findings will be summarized and incorporated into the dissertation, providing valuable insights into the strengths and limitations of the dynamic plane approach.

This analysis, while preliminary to the formal training and validation of the predictive models, is crucial for understanding the transformed data and justifying design choices related to the dynamic coordinate system. Subsequent model training will leverage standard practices, including a validation set, early stopping, and careful tuning of the learning rate and batch size. The scale of the training dataset will also be a key consideration, with plans to utilize a large dataset to ensure model robustness and generalizability. These aspects of model training and validation will be addressed in more detail in a subsequent section.

## B. Training and Validation

This section details the training and validation process, emphasizing the importance of feedback mechanisms for a model reliant on accurate market movement representation. The process will leverage standard machine learning practices while incorporating unique elements to address the dynamic nature of the PCA-based input.

**Key Training and Validation Activities:**

- **Iterative Dataset Scaling:** Training will begin with a small sample dataset of 3-image sequences to validate the data pipeline and model architecture. A sliding window image generator will then be used to create progressively larger datasets, enabling robust training and evaluation of the model's ability to generalize to unseen market conditions.
- **Standard Training Procedures:** Standard practices, including the use of a validation set and early stopping to prevent overfitting, will be employed for all models (e.g., CNNs).
- **Hyperparameter Tuning:** Optimal learning rate and batch size will be determined through experimentation and validation set performance.
- **Sequence Generation:** A sliding window image generator will create sequences of candlestick images, incorporating PCA transformations, window size, and smoothing parameters. This generator will be optimized for efficiency as dataset sizes increase.

**Advanced Error Signal Integration (Forward Look):**

While the following error signal mechanisms will be fully implemented and evaluated in the Model Enhancement and Refinement section, their conceptual integration within the training process is crucial:

- **Dynamic Plane Error Signal:** The dynamic plane algorithm will incorporate an error signal mechanism to learn from prediction errors, adjusting its internal representation of market dynamics based on the difference between predicted and actual market behavior.
- **Feedback-Driven Refinements:** Concepts such as frame confidence correction, prediction error memory, feedback-driven frame smoothing, and dual-frame estimation will be integrated to enhance the model's responsiveness to market changes by leveraging feedback from past predictions. These mechanisms influence the dynamic plane's rotation and stability, enabling it to adapt to evolving market conditions. These will be detailed in the subsequent Model Enhancement and Refinement section.

## B. Training and Validation

This section details the training and validation process for the CNN and ViT models, including adaptive mechanisms for maintaining the stability and responsiveness of the dynamic plane generation.

**1. Model Training:**

- **Data Preparation:** A large-scale dataset of candlestick chart images will be prepared and processed. A smaller sample dataset of 3-image sequences will be used for initial testing and validation of the training pipeline before scaling to the full dataset. A sliding window image generator will be developed to create sequences for the ViT models.
- **Training Process:** CNNs will be trained using the prepared dataset, with a dedicated validation set for performance monitoring. Early stopping will be employed to prevent overfitting. Appropriate learning rates and batch sizes will be determined experimentally.
- **Hyperparameter Tuning:** Learning rates and batch sizes will be carefully tuned to optimize both the speed of convergence and the final model performance. This will incorporate feedback from prediction errors to ensure the model learns effectively.

**2. Dynamic Plane Stabilization:**

Given the dynamic nature of market data and the potential for drift in the Principal Component Analysis (PCA) used for dynamic plane generation, a robust frame correction mechanism, inspired by the biological process of wound healing, will be implemented:

- **Prediction Error Buffer:** A rolling buffer (e.g., 5-10 steps, determined experimentally) will store recent prediction errors (the difference between the signed prediction and the realized movement within the rotated frame).
- **Error Trend Detector:** This component will analyze the rolling mean and variance of errors within the buffer. A threshold (e.g., 1-2x the rolling standard deviation) will trigger corrective action.
- **Frame Correction Action:** When the error trend detector signals a significant deviation, a small rotation adjustment or damping will be applied to the PCA frame. This adjustment re-weights the principal axes, moving away from pure PCA towards a more stable representation. The magnitude of this adjustment is a critical parameter to be determined during training and validation.
- **Healing Phase:** After a correction, as the error magnitude returns to an acceptable range, the correction magnitude will be gradually reduced using an exponential decay function. The optimal decay rate will be tuned during validation.
- **Dynamic Plane Generator Integration:** The rolling frame correction algorithm will be integrated into the `DynamicPlaneGenerator` class as a modular function, enabling dynamic PCA frame adjustments during image generation.

**3. Monitoring and Evaluation:**

- **Error Visualization:** Visualizations illustrating the "error spike → correction → healing decay" process over simulated and real market sequences will aid parameter tuning and provide insights into system behavior.
- **Frame Intervention Metric:** A metric quantifying the amount of "frame intervention" over a typical trading year will be developed. This metric will assess the system's fluidity and adaptivity, balancing stability with responsiveness to changing market conditions and will be used during validation.

These integrated components will enhance the model's robustness and adaptability to changing market conditions. Specific parameters and configurations will be determined through rigorous experimentation and validation.

### B. Training and Validation (Enforcer)

This section details the training and validation process, focusing on enhancements to loss functions for improved error detection and correction within a dynamic 2D plane representation of price movements. Standard training practices like early stopping, learning rate and batch size tuning, and the use of a validation set will be employed (details outlined in the main checklist). This section specifically addresses the novel approach to handling both scalar (magnitude) and angular (directional) drift in predictions.

The core of this enhancement lies in the development of new loss functions and supporting error analysis tools:

- **New Loss Functions:** New loss functions will be designed to penalize both the magnitude and directional components of prediction errors. This dual focus encourages the model to learn not only the correct price prediction but also the correct trajectory of price movement within the 2D plane. These functions will incorporate the composite error score described below.

- **Composite Error Score:** A composite error score, integrating both distance (magnitude) and angle errors, will be used within the loss functions. Weighting factors (alpha and beta) will allow for adjustable prioritization of either distance or angular accuracy during training, offering flexibility in fine-tuning the model's behavior.

- **Enhanced Error Trend Detector:** Existing pseudocode for an enhanced Error Trend Detector will be implemented. This detector will leverage the composite error score and integrate with the current rolling window analysis to provide more nuanced error tracking and trend identification during training.

The error correction process itself focuses on the difference between predicted and realized displacement vectors within the dynamic 2D plane:

- **Distance Error:** Represents the magnitude difference between the predicted and realized displacement vectors, quantifying the accuracy of the _extent_ of movement.

- **Angle Error:** Represents the orientation difference between the predicted and realized direction vectors, quantifying the accuracy of the _direction_ of movement.

Critically, the initial frame creation rotation (based on PCA) is _not_ considered for error correction; the focus is solely on local vector misalignment.

To aid understanding and documentation, a diagram illustrating the two layers of rotation (global frame transformation via PCA and local vector misalignment) and how the prediction error relates to them will be created. This diagram will visually represent the relationship between predicted and realized vectors, highlighting both distance and angle errors within the rotated coordinate system. Furthermore, the number of rotational angles and distance vectors used will be clearly documented and ensured consistent with the implementation.

Further potential enhancements, subject to investigation and prioritization, include:

- **Dynamic Rolling Error Correction Module:** Expanding the Error Trend Detector into a dynamic module that actively adjusts predictions based on detected error trends.

- **Visual Simulation of Vectorial Misalignments:** Creating a visual simulation to demonstrate the cumulative effect of small vectorial misalignments, emphasizing their difference from simple price error accumulation. This will highlight the importance of addressing angular drift.

### B. Training and Validation (Enforcer)

This section details the training and validation procedures, emphasizing robust error calculations within the context of dynamic PCA planes. Standard training and validation steps (CNN training, validation set utilization, and early stopping) are augmented with error calculations that account for frame shifts. This involves measuring not only the vector deviation error within the dynamic local frame but also the error introduced by shifts in the PCA frame itself.

Effective training and validation rely on the following error calculations and management strategies:

- **Total Error Calculation:** This comprises two key components: Vector Deviation Error (error within the dynamic local frame) and Frame Shift Error (change between PCA axes). The Total Error is calculated as: `Total Error = Vector Deviation Error + Frame Shift Error`. Pseudocode for this calculation will be developed and implemented.

- **Frame Drift Error Measurement:** This measures the difference between consecutive frames (sets of basis vectors) using principal angles between two subspaces. In the 2D case, this involves calculating the angle between PCA1 at time _t_ and PCA1 at time _t+1_, and similarly for PCA2. The Frame Error is then calculated as a weighted sum of these angles: `Frame Error = α * Angle(PCA1_t, PCA1_{t+1}) + β * Angle(PCA2_t, PCA2_{t+1})`, where α and β are tunable weights.

- **Error Calculation with Shifted PCA Planes:** The error checking mechanism accounts for the dynamic nature of the PCA planes. Deviation errors between PCA1 and PCA2 are calculated for both real and predicted values, considering the frame shifts. Two methods will be explored for maintaining a consistent frame of reference:

  - **Freeze Frame:** The PCA rotation matrix ('R') is frozen at the prediction time. Both predicted and realized data points are projected using this same 'R' matrix.
  - **Reproject Realization:** The original rotation matrix 'R' (from the prediction time) is applied to the realized movement vector, projecting it back into the original prediction frame. Numerical examples and visualizations, along with pseudocode for a "Freeze and Correct" module, will illustrate these methods.

- **Weighted Error Calculation:** A weighted error calculation combines multiple error components for a comprehensive error assessment:

  - **Vector Deviation Error (d<sub>vec</sub>):** Represents the difference between predicted and actual vector movement within the current frame.
  - **Angular Error (θ<sub>vec</sub>):** Measures the angular difference between predicted and actual vector rotation.
  - **PCA1 Angle Error (θ<sub>PCA1</sub>):** Represents the drift in the primary principal component axis.
  - **PCA2 Angle Error (θ<sub>PCA2</sub>):** Represents the drift in the secondary principal component axis.

The weighted error formulas are as follows:

    * **Vector Error = α₁ * d<sub>vec</sub> + α₂ * θ<sub>vec</sub>**
    * **Frame Shift Error = β₁ * θ<sub>PCA1</sub> + β₂ * θ<sub>PCA2</sub>**
    * **Total Error = γ₁ * Vector Error + γ₂ * Frame Shift Error**

Where α₁, α₂, β₁, β₂, γ₁, and γ₂ are tunable weights. The potential of Frame Drift Error as a confidence indicator for trading decisions will also be investigated.

This robust frame management and comprehensive error analysis will be integrated with the existing training and validation pipeline, including CNN training, validation set utilization, appropriate learning rate and batch size selection, training data scaling, and development of a sliding window image sequence generator. A sample dataset for 3-image sequences will be created for initial testing and validation.

## B. Training and Validation

This section details the training and validation process, emphasizing robust performance and stability through standard procedures and a dynamic error detection and correction system.

**Standard Training and Validation Procedures:**

- **Dataset:** A large-scale dataset will be prepared to ensure models generalize well to unseen data. A smaller sample dataset of 3-image sequences will be created for initial testing and visualization of the sequence generation and training pipeline.
- **Data Preparation:** A sliding window image generator will create sequences of candlestick images for training, particularly for models handling sequential data.
- **Model Training:** Standard training and validation procedures will be implemented for the CNNs, including a validation set and early stopping to prevent overfitting. The learning rate and batch size will be optimized through experimentation.

**Dynamic Error Detection and Correction (Healing System):**

A dedicated module will implement a dynamic error detection and correction system ("Healing System") to manage training deviations. This system utilizes a rolling error buffer storing the total error over a defined number of steps (N, e.g., 5-10 training windows).

- **Rolling Error Tracking:** The mean and variance of the errors within the rolling buffer will be continuously calculated to assess training stability.
- **Adaptive Training Phases:**
  - **Wound Phase:** Triggered when the mean error exceeds k times the rolling standard deviation (e.g., k=2), indicating significant performance degradation. A correction factor will be applied to the training process, potentially adjusting PCA rotation, smoothing parameters, or other relevant training parameters.
  - **Healing Phase:** Initiated when the mean error falls below a healthy threshold (e.g., 1-1.5 times the rolling standard deviation), signifying performance recovery. The correction factor is then gradually removed or reduced through a decay mechanism (exponential with λ=0.95, or linear). The system can dynamically re-enter the Wound Phase if errors spike again during healing.

The hyperparameters N (buffer size), k (Wound Phase trigger multiple), the healthy threshold, and the correction factor's decay rate will be tuned experimentally to optimize training effectiveness. Initial values for thresholds and decay rates will be based on typical market behavior. Further research will explore optimal error metric aggregation, including potential normalization or adjustments for combining distance and angular errors, and the weighting scheme for vector deviation and frame shifts (using existing alpha and beta parameters or distinct weights).

## B. Training and Validation (Enforcer)

This section details the training and validation process for the Enforcer model, emphasizing performance-based adjustments to its healing mechanism. The process builds upon standard practices like using a validation set, early stopping, tuning the learning rate and batch size, and augmenting the training data with an image sequence generator applied to a sample dataset of 3-image sequences. Crucially, it incorporates the following enhancements to manage model recovery after disruptions ("wounds"):

**1. Prediction Correctness Tracking and Healing:**

Central to this enhanced training process is the tracking of prediction correctness. For each prediction, a binary score is assigned (+1 for correct, 0 otherwise), irrespective of framing issues. These scores are stored in a rolling buffer of length _N_ timesteps. The mean prediction correctness from this buffer drives the model's healing mechanism. Instead of relying solely on time-based decay, the correction factor, which compensates for past disruptions, is dynamically adjusted based on this mean prediction correctness. Improved predictive accuracy proportionally reduces the correction factor, preventing arbitrary exponential decay and promoting responsiveness to current market conditions.

**2. Dynamic Decay Rate Adjustment:**

The decay rate of the correction factor is also dynamically adjusted. Consistent correct predictions accelerate the decay, reducing the lag introduced to account for past disruptions. This adaptive behavior ensures the model remains sensitive to evolving market dynamics. The true prediction values are meticulously tracked and used as the basis for this dynamic decay rate adjustment.

**3. Healing Logic Implementation Details:**

The healing logic incorporates the mean prediction correctness as follows: a higher rolling prediction correctness (e.g., ≥ 80% over the last _N_ steps) proportionally reduces the correction factor. Conversely, deteriorating correctness maintains or even increases the correction factor. This logic is encapsulated within the `dynamic_decay_rate(mean_correctness)` function, which calculates the decay rate based on the mean prediction correctness. A proposed implementation is: `Decay Rate = 1 - (mean_correctness - healing_threshold)`. This formula ensures accelerated decay as correctness surpasses the healing_threshold.

**4. Formalized Procedures and Illustrative Example:**

To ensure clarity and facilitate implementation, the complete healing-by-correctness system, including prediction correctness tracking and dynamic decay rate functionality, is formally described using modular pseudocode (provided in Appendix A). A simulated toy example demonstrating the entire healing process (wound → correction → true healing through regained predictive accuracy) further illustrates the system's behavior (Appendix B).

**5. Initial Healing Thresholds:**

Initial healing thresholds, suggested within a realistic trading context, are set at 75-80% directional correctness. These thresholds serve as starting points for further optimization and refinement during the training and validation phases.

**Data Preprocessing for PCA:**

This section details the preprocessing steps for Price (P), Time (T), and Volume (V) data before applying Principal Component Analysis (PCA). This ensures the model receives meaningful, appropriately scaled input.

**1. Z-score Normalization:**

Each feature (P, T, and V) within a rolling window of _N_ data points is independently standardized using z-score normalization:

```
μ_feature = (1/N) * Σ feature_i
σ_feature = sqrt((1/N) * Σ (feature_i - μ_feature)^2)
feature_scaled = (feature_i - μ_feature) / σ_feature
```

This centers the data around zero mean and scales to unit variance, ensuring equal feature contribution to PCA.

**2. Time Handling:**

Two approaches are available for handling timestamps:

- **Relative Time Index:** For consistent _N_-candle windows, use integers (1, 2, ..., _N_) representing relative time, applying z-score normalization before PCA.
- **Absolute Clock Time:** To capture intraday patterns, calculate normalized time deltas: `Δt_i = (timestamp_i - μ_t) / σ_t`. Be mindful of large timestamp gaps inflating σ_t.

**3. Volume Transformation:**

Address volume's heavy-tailed distribution using either:

- **Log Transformation:** `v_i' = log(1 + v_i)`
- **Robust Scaling:** Subtract the median volume and divide by the interquartile range (IQR).

**4. PCA Implementation:**

After preprocessing, apply PCA to the scaled data matrix using Singular Value Decomposition (SVD):

```python
u, s, vh = np.linalg.svd(X_scaled, full_matrices=False)
axes = vh[:2]   # Extract the first two principal components
```

The first two principal components are then used for subsequent analysis.

## B. Training and Validation

This section details the data transformations and preprocessing steps required to prepare the data for training and validation. These steps ensure consistent and reliable model performance by creating robust and meaningful input features.

**Data Preprocessing and Transformation:**

1. **Price Transformation (Window-Relative Returns):** Raw price values are transformed into window-relative returns. This is calculated as the percentage change or log return of each price relative to the first price in the input window. This anchoring normalizes price data across different stocks and time periods, reduces the influence of absolute price magnitudes, mitigates the impact of extreme price spikes, and provides a consistent starting point for comparison within each window.

2. **Volume Transformation (Robust Scaling):** To handle potential outliers and spikes in trading volume, a log transformation is applied, followed by robust scaling using the median and interquartile range (IQR). This approach mitigates the impact of extreme values and ensures the volume data is on a suitable scale for the model.

3. **Time Transformation (Fractional Elapsed Time):** Timestamps are converted into fractional elapsed time within each window. This is calculated by subtracting the minimum timestamp from each timestamp and dividing by the total time elapsed within the window. This normalized representation (ranging from 0 to 1) preserves chronological order, handles irregular time spacing, and addresses the challenges of using live Last Traded Prices (LTPs) by creating a self-contained time representation within each window.

**Feature Engineering and Dimensionality Reduction:**

To prepare the data for model input, the following steps are performed:

1. **Data Matrix Construction:** A 3-dimensional matrix is constructed with columns representing: (1) fractional elapsed time, (2) window-relative return (or log return) of the price, and (3) the robustly scaled volume.

2. **Principal Component Analysis (PCA):** PCA is applied to the 3-dimensional matrix to reduce dimensionality while retaining essential information. The resulting principal components, representing the underlying correlations between time, return, and volume, serve as the input features for the model.

**Input Normalization for Model Training:**

Before training, the transformed features (derived from Price, Volume, and Time) are further normalized to a range of -1 to +1. This normalization stabilizes training and prevents features with larger magnitudes from dominating the learning process.

- **Price (P):** The window-relative returns (or log returns) are normalized. Either clamping within a specific range or scaling (e.g., dividing by the maximum absolute value or min-max scaling to the [-1, 1] range) is employed, depending on the data characteristics and desired model behavior.

- **Volume (V):** The already robustly scaled volume data is further normalized to the [-1, 1] range. This ensures consistency with the other input features.

- **Time (T):** The fractional elapsed time (`time_frac`) is linearly transformed to the [-1, 1] range using the formula: `(2 * time_frac) - 1`.

These preprocessing, transformation, and normalization steps ensure that the input data is consistent, robust, and appropriately scaled for effective model training and validation.

**Dataset Generation:**

- **Large-Scale Dataset:** A comprehensive dataset will be created to capture diverse market conditions and ensure model generalization. This dataset incorporates all the transformations and preprocessing steps described above.

- **Sliding Window Image Generator:** A sliding window image generator will create sequences of candlestick chart images for model input, dynamically generating training and validation datasets.

- **Sample Dataset (3-Image Sequences):** A smaller sample dataset of 3-image sequences will be created for initial testing and development, facilitating rapid iteration and validation before scaling up to the full dataset.
  Data Transformation and Model Training

This section details the data transformation pipeline and the subsequent training and validation procedures for the CNN model. Visualizations of both pre- and post-transformed data are crucial for understanding the effects of the dynamic plane implementation and ensuring data integrity.

**A. Data Transformation Pipeline**

The following steps outline the data transformation process:

1. **Log Transformation and Percentile Clipping:** Apply a logarithmic transformation (`np.log1p`) to the volume data to mitigate the impact of extreme values. Subsequently, clip outliers at the 5th and 95th percentiles to further reduce the influence of extreme values and prepare the data for normalization. Similarly, clip extreme outliers in the log return of price at the 5th and 95th percentiles.

2. **Min-Max Scaling:** After log transformation and percentile clipping, min-max scale the `time_frac`, log return of price, and log volume data to the [-1, +1] range. Crucially, use the minimum and maximum values _across the entire dataset_ for each feature to ensure consistent scaling and prevent data leakage during training and validation. This consistent scaling is particularly important for the subsequent PCA step.

3. **Principal Component Analysis (PCA):** Combine the normalized `time_frac`, log return of price, and log volume data into a single matrix. Perform PCA on this matrix. Centering the matrix before PCA is optional, as it is often already zero-mean due to the symmetric [-1, +1] scaling. This dimensionality reduction technique improves model training efficiency and potentially enhances generalization performance.

4. **Visual Validation:** Create five distinct examples showcasing price and volume action to validate the data transformation pipeline. Each example should include pre-transformation candlestick images and post-transformation images (normalized to [-1, 1]). These examples should be based on a single-day chart with 10-minute intervals and displayed individually for clear comparison.

**B. Training and Validation**

This section outlines the training and validation process, emphasizing the use of transformed dynamic plane snapshots. The training pipeline utilizes a dataset of generated image pairs, each representing a distinct market scenario: an uptrend with rising volume, a downtrend with volume spikes, a reversal, sideways chop, and a breakout spike followed by stabilization.

1. **Data Preparation:** Generate pairs of images for each scenario. The first image in each pair is a standard candlestick chart with volume, displaying open, high, low, close prices, and volume. The second image is the corresponding transformed dynamic plane snapshot, created by:

   - Normalizing the time, log return of price, and log volume data to [-1, 1].
   - Applying PCA to the normalized data to reduce dimensionality and capture key variations.

2. **Model Training:** Train the CNN using the generated image pairs. Display the transformed dynamic plane snapshots sequentially during training to visualize the model's interpretation of each market pattern.

3. **Investigating Volume Representation:** A key consideration is whether the transformed data sufficiently captures volume information. Analyze the "Breakout Spike then Stabilize" pattern with varying data volumes and apply PCA. Visualize the resulting PCA patterns alongside their corresponding original candlestick and volume charts to determine if explicit volume bars are necessary in the transformed snapshots. Use 10-minute intervals for these charts.

### B. Training and Validation (Enforcer)

This section details the training and validation process for the models, emphasizing robust performance and generalization capabilities. The process incorporates multi-timeframe analysis and addresses the influence of after-market forces.

1. **Multi-Scale Temporal Model Implementation:** A multi-scale temporal model will be trained, integrating predictions from daily, weekly, monthly, quarterly, and yearly timeframes using a weighted sum approach. This approach aims to capture both short-term fluctuations and long-term market trends.

2. **Architectural Design for Multi-Scale Timeframes:** The model architecture will be carefully designed to handle the weighted integration of different timeframes and address cyclical patterns in the data. The weighting scheme applied to each timeframe (daily, weekly, monthly, quarterly, yearly) will be optimized for predictive accuracy.

3. **Periodicity Weight Optimization:** The model will incorporate periodicity weights to reflect the influence of daily, weekly, monthly, quarterly, and yearly contexts. Various weighting combinations will be tested during validation to determine the optimal balance for maximizing predictive accuracy.

4. **Incorporating After-Market Forces:** The model will be trained to consider the impact of after-market forces, which, while not directly reflected in intraday candlestick data, can significantly influence subsequent trading sessions. The weighted periodicity approach helps capture this implicit information.

5. **Training and Validation Methodology:** Convolutional Neural Networks (CNNs) will be trained using a robust validation set and early stopping to prevent overfitting. For sequence-based models like Vision Transformers (ViTs), a sliding window image generator will create sequences of candlestick images. A smaller sample dataset of 3-image sequences will be used for rapid prototyping, debugging, and initial experimentation before scaling up to the full dataset. Hyperparameters, such as learning rate and batch size, will be carefully tuned, and the training dataset will be scaled up to ensure robust generalization to unseen data. This may involve data augmentation or synthetic data generation.

## B. Training and Validation

This section details the training and validation procedures for the models, emphasizing the use of multi-scale data and a hierarchical attention mechanism.

**Data Preparation and Preprocessing:**

- **Multi-Scale Data Generation:** Datasets will be generated for various timeframes (intraday, daily, weekly, monthly) using the Dynamic Rotating Plane method. This creates a comprehensive set of context images for each prediction point, enabling the model to learn from diverse temporal perspectives. This addresses the requirement for processing market data across multiple timeframes to contextualize short-term predictions.
- **Data Scaling:** A large-scale dataset will be prepared for training across these multiple timeframes and for the varied input sequence lengths required by the models. This involves generating image sequences for the different time scales to ensure sufficient data for robust training.
- **Image Sequence Generation:** A sliding window image generator will be developed to efficiently create these sequences for the specified timeframes (intraday, daily, weekly, monthly).
- **Sample Dataset for Initial Testing:** A smaller sample dataset consisting of 3-image sequences will be created to facilitate initial testing and development of the hierarchical attention model. This allows for rapid iteration and validation of the model architecture and training process before scaling up to the full dataset.

**Model Training and Validation:**

- **Non-Hierarchical Attention Model:** The hierarchical attention model, designed to dynamically query longer timeframes based on intraday context, will be trained and validated using the multi-scale datasets. The training will incorporate a validation set and early stopping to prevent overfitting and ensure optimal performance. This hierarchical approach aims for computational efficiency and interpretability by focusing on relevant longer-term information.
- **CNN and Transformer Training:** Both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) will be trained and validated using standard procedures. This includes utilizing a validation set and early stopping to prevent overfitting.
- **Hyperparameter Tuning:** Appropriate learning rates and batch sizes will be determined through experimentation and hyperparameter tuning for both the CNN, ViT, and the hierarchical attention model.

**Model Evaluation and Analysis:**

- **Performance Benchmarking:** The performance of the multi-scale model will be rigorously evaluated against a baseline intraday model.
- **Attribution Analysis:** Techniques like Grad-CAM or SHAP will be employed to understand the model's reliance on different timeframes.
- **Robustness Testing:** Model performance during major market events will be analyzed to assess its robustness and stability under various market conditions.

This comprehensive training and validation process aims to develop robust, accurate models capable of leveraging multi-timeframe information for effective trading signal generation.

### B. Training and Validation (Enforcer)

This section details the training and validation process for both CNN and ViT models, emphasizing practical implementation, robust execution, and performance optimization. A configurable lookahead period allows training for predictions over various time horizons, supporting diverse trading strategies.

1. **Training and Validation:** Both CNN and ViT models are trained and validated using a dedicated validation set to prevent overfitting and monitor performance. Early stopping is implemented to halt training when validation performance plateaus or declines. The data used for training and validation is configurable, allowing users to specify the asset universe and date range. The data is automatically split into training and validation sets based on this configuration.

2. **Learning Rate and Batch Size:** Optimal learning rate and batch size are crucial hyperparameters and are determined through careful experimentation. These, along with optimizer settings and loss function selection, are configurable through the Model & Learning Architecture section, providing comprehensive control over the training process.

3. **Model Training Data Scale-Up:** Training robust deep learning models requires a substantial dataset. The Dynamic Plane Configuration facilitates generating diverse training examples by adjusting parameters like candlestick type, local window size, and included features, effectively expanding the dataset. The sliding window image sequence generator further enhances this by creating sequences of candlestick data, especially beneficial for the ViT model. The Self-Correction System Configuration, while focused on model self-improvement, indirectly contributes to effective training data size by mitigating the impact of noisy or misleading examples.

4. **Image Sequence Generator:** A sliding window image sequence generator provides sequential candlestick data to the models, particularly the ViT. Its functionality is directly linked to the Dynamic Plane Configuration, ensuring alignment between the generated sequences and the model's input representation. The Live Visualization of Perception tool aids in developing and debugging this generator, verifying the consistency between the standard chart view and the model's input.

5. **Sample Dataset for Initial Testing:** A smaller sample dataset of 3-image sequences is used for initial testing and validation of the sequence handling logic, especially for the ViT. This facilitates rapid prototyping and experimentation with different architectures and hyperparameters before scaling to the full dataset. This smaller dataset also simplifies visualization and analysis of model behavior using tools like the Live Visualization of Perception.

### B. Training and Validation (Enforcer)

This phase focuses on robust training and validation procedures to optimize model performance and generalization. The following functionalities will be implemented:

- **Performance-Based Healing Trigger:** A performance-based healing trigger will activate when model prediction accuracy falls below a user-defined threshold, enabling a swift response to performance degradation.

- **Time-Based Healing Trigger:** A time-based healing trigger will activate if the model's error remains above a predefined threshold for a specified, configurable duration, addressing persistent performance issues. This complements the performance-based trigger.

- **Total Error Metric:** A comprehensive "Total Error" metric, combining weighted vector error and frame shift error, will be used by both healing triggers. The weights will be adjustable to tailor the metric to specific applications and datasets.

- **Multi-Scale Context Fusion:** To improve the model's ability to learn from various temporal contexts, multi-scale context fusion will be implemented. Users can select from Attention-Based (ViT), Concatenation, or Weighted Average fusion methods.

### C. Training and Validation (Enforcer - Hyperparameter Tuning and Market Adaptation)

This phase focuses on refining model performance through hyperparameter tuning and evaluating its adaptability across different market contexts.

- **Hyperparameter Permutation Testing:** Initial baseline performance will be established through comprehensive permutation testing. This involves training the model for a single epoch across all combinations of specified hyperparameters using defined training and testing date ranges.

- **Context Awareness:** The training will incorporate context awareness by dynamically determining the optimal number of candlesticks per frame and the total number of input frames.

- **Transfer Learning Evaluation:** The effectiveness of transfer learning will be assessed by training models on US market data and testing them on both US and Indian markets, and training models on Indian market data and testing them on Indian market data.

- **Context-Aware Periodicity:** Weighted predictions based on daily, weekly, monthly, quarterly, and yearly periodicities will be generated. Optimal weights will be determined based on the number of frames, candles per frame, and stock characteristics (market capitalization, sector, and share price).

- **PCA Analysis:** The entire training and validation process will be repeated using the first two principal components (PCA axes) derived from the input data to evaluate the impact of dimensionality reduction.

### D. Training and Validation (Enforcer - Data Integration and Real-time Considerations)

This phase addresses the practical aspects of training and validation, specifically focusing on data acquisition, real-time data handling, and integration with the Zerodha KiteConnect API.

- **KiteConnect Data Integration:** Training and validation data will be sourced via the Zerodha KiteConnect API. The training pipeline will be integrated with the API to fetch historical data, and potentially real-time data via websocket, while accounting for API limitations.

- **Websocket Connection Handling:** Validation strategies will address the challenges of using the KiteConnect websocket for live, dynamic data. This may involve online validation or simulated real-time environments.

- **Order and Portfolio Management Integration:** Validation will include simulated trading scenarios using historical data and the KiteConnect API to assess the model's impact on portfolio performance and risk management.

## B. Training and Validation (Enforcer)

This section details the training and validation procedures for the models, ensuring robust performance and generalizability.

1. **Data Splitting:** The dataset will be split into training, validation, and test sets. The validation set will be used to monitor performance during training and prevent overfitting, while the test set will be held out for final evaluation. The specific split ratios will be documented in the experimental setup section.

2. **Model Training:** Models (CNNs, ViT, etc.) will be trained using the prepared training dataset. Early stopping will be employed to halt training when validation performance plateaus or starts to degrade, mitigating overfitting.

3. **Hyperparameter Tuning:** Appropriate learning rate and batch size will be determined through experimentation and hyperparameter tuning, optimizing model performance. The chosen optimization algorithm and its configuration will also be documented.

4. **Data Augmentation and Scaling:** Data augmentation techniques may be employed to increase the effective size of the training dataset and improve model robustness. Appropriate scaling or normalization techniques will be applied to the input data as needed.

5. **Image Sequence Generation (ViT):** For the ViT model, a sliding window image generator will be implemented to create sequences of candlestick images, handling the sequential nature of the data. The sequence length and other generator parameters will be defined in the model configuration.

6. **Performance Evaluation:** Model performance will be evaluated using appropriate metrics on the validation and test sets. These metrics will be clearly defined and reported in the results section.

## B. Training and Validation

This section details the strategy for efficiently training and validating the CNN and ViT models while minimizing computational costs. Given the potential for high computational overhead from image generation and processing, a federated learning approach using iPads as primary training devices will be implemented. This distributed strategy leverages the iPads' processing power for computationally intensive tasks, while a central server orchestrates the overall training process and aggregates model updates.

The key components of this federated learning system are:

- **Federated Learning Framework:** A robust framework will be developed, encompassing:

  - Efficient on-device image generation on iPads, leveraging their hardware capabilities.
  - Model training using TensorFlow.js on iPads.
  - Secure and efficient data transfer mechanisms between iPads and the server for model updates and synchronization.
  - Aggregation of model updates from multiple iPads on the server to create a global model.
  - Optimization of iPad resource utilization and server interaction for a streamlined training process.

- **Client-Side Processing:** iPads will perform the computationally intensive tasks of image generation and model training. The server will primarily coordinate the federated learning process, aggregating updates and distributing the updated global model.

- **Federated Training Workflow:** The workflow consists of the following steps:

  1. **Image Generation:** iPads generate candlestick chart image sequences.
  2. **Local Model Training:** iPads train a local model instance using TensorFlow.js based on the generated images.
  3. **Model Update Transmission:** iPads send model updates (e.g., gradients) to the server.
  4. **Server Aggregation and Distribution:** The server aggregates client updates, creates an updated global model, and distributes it back to the iPads for the next training round.

- **Implementation Details:** The federated learning implementation will:

  - Leverage iPad GPUs for optimized on-device image generation and model training.
  - Optimize data transfer between iPad and server to minimize bandwidth consumption.
  - Implement robust aggregation algorithms on the server to handle updates from multiple iPads.
  - Culminate in a functional prototype demonstrating the feasibility and performance of the federated learning approach.

- **Data and Model Update Handling:** Efficient data and model update handling will be implemented through:
  - Optimized data structures and communication protocols for efficient transfer of incremental updates between iPads and the server.
  - Caching mechanisms on iPads to reduce redundant computations and data transfer.
  - Use of Web Workers on iPads to ensure a responsive user experience during training.

This approach addresses the computational challenges by distributing the workload across multiple devices, making the training process more efficient and scalable while reducing the burden on the central server. The user interface for managing and monitoring the training process, including hyperparameter adjustments (learning rate, batch size, etc.), will be retained within the main application. Further details on hyperparameter selection and dataset scaling will be provided in subsequent sections.

## B. Training and Validation (Enforcer)

This section details the training and validation process for the Enforcer, employing a hybrid approach that leverages both server-side and client-side (iPad) resources. This strategy balances the computational demands of training with the responsiveness and data accessibility of the client-side environment while addressing potential limitations of a Progressive Web App (PWA) environment, particularly on resource-constrained devices.

A key concern is the feasibility of resource-intensive Vision Transformer training within a PWA on an iPad. Research into TensorFlow.js performance and WebGPU capabilities is crucial to determine whether the desired architecture is feasible. This research will consider factors such as browser stability under heavy GPU load, performance bottlenecks, and resource management. Potential drawbacks like browser crashes due to resource constraints will be thoroughly investigated.

Should the PWA environment prove unsuitable for such computationally intensive tasks, alternative solutions like native iOS applications or cloud-based training will be explored.

The chosen hybrid approach mitigates these risks by dividing the training process into two phases:

**1. Server-Side Training (Heavy Lifting):**

- Initial model training and large-scale, multi-permutation campaigns will be executed on a more powerful server environment (e.g., Google Cloud Run with GPU acceleration, such as NVIDIA T4).
- The PWA acts as an interface to initiate, monitor, and retrieve the results of these server-side jobs. This offloads the most computationally intensive tasks, minimizing the risk of client-side instability.

**2. Client-Side Fine-tuning and Inference:**

- The iPad focuses on:
  - **Model Fine-tuning (Delta Training):** Utilizing recent, localized data, the iPad performs fine-tuning, generating weight updates (deltas) that are sent back to the server to enhance the global model.
  - **Interactive Backtesting:** Shorter-period backtesting is performed locally, leveraging the iPad's resources and providing immediate feedback.
  - **Live Inference:** Real-time inference is performed using WebSockets and the DynamicPlaneGenerator, benefiting from local data access and responsiveness.

This hybrid approach enables efficient use of both server and client resources. The server handles computationally demanding training, while the client focuses on fine-tuning, localized learning, and responsive interactions, minimizing the impact of potential PWA limitations.

Within both server-side and client-side training, best practices will be employed:

- **Training and Validation:** A validation set and early stopping will prevent overfitting and maximize performance.
- **Hyperparameter Tuning:** Learning rate and batch size will be carefully tuned based on observed performance.
- **Data Handling:** Efficient data handling strategies will address potential resource constraints, especially on the client-side.
- **Image Sequence Generation:** A sliding window image generator will create the necessary input sequences, with careful attention paid to memory management within the PWA context.

By using a hybrid approach and focusing client-side activities on less resource-intensive tasks, this strategy addresses the user concerns regarding PWA stability and performance on iPads while maximizing the benefits of local data and interactivity.

### B. Training and Validation

This section details the training and validation process for the CNN and ViT models, focusing on achieving robust performance and generalization.

1. **Training and Validation with Early Stopping:** Models will be trained using Core ML, leveraging the Apple Neural Engine (ANE) and GPU for hardware acceleration. A validation set will be used to monitor performance and prevent overfitting. Early stopping will be implemented to halt training when validation performance plateaus or degrades.

2. **Hyperparameter Tuning (Learning Rate and Batch Size):** Optimal learning rates and batch sizes will be determined through experimentation and hyperparameter tuning within the Core ML training environment.

3. **Training Data Scale-Up:** A large-scale dataset derived from raw OHLCV data, stored locally using Core Data or Realm, will be used for training. This local storage allows for efficient data retrieval and offline training.

4. **Image Sequence Generation (Sliding Window Approach):** A `DynamicPlaneGenerator`, implemented in Swift and utilizing Metal for GPU acceleration, will generate image sequences via a sliding window approach. This component handles PCA, rotations, and rendering of numerical data into images compatible with Core ML.

5. **Sample Dataset for Initial Validation:** A smaller sample dataset of 3-image sequences, generated by the `DynamicPlaneGenerator`, will be used for initial validation of the training pipeline and data compatibility with the CNN and ViT architectures.

6. **Error Rate Monitoring and Re-tuning:** Post-deployment, model performance will be continuously monitored. If the error rate exceeds a predefined threshold, a re-tuning process will be triggered. Details of this re-tuning process, especially within the PWA and Native Android App contexts, are addressed in later sections.

7. **Decoupled Architecture for Deployment:** The project utilizes a decoupled architecture with a Python backend and separate frontends (PWA and Android). The backend serves raw numerical data, provides the latest trained model, and aggregates model updates from the distributed frontends. This facilitates efficient client-side computation and simplifies porting to different frontend platforms.

### B. Training and Validation

This section details the training and validation procedures for the CNN models used for trade signal generation. The following steps ensure a robust training process, leading to a well-generalized model capable of accurately predicting future stock price movements.

1. **Training and Validation:** The CNN models will be trained using prepared candlestick image data and corresponding return labels. A dedicated validation set will be used to monitor model performance during training and prevent overfitting. Early stopping will be implemented to halt training when validation performance plateaus or begins to degrade.

2. **Learning Rate and Batch Size:** Appropriate learning rate and batch size parameters will be determined through experimentation and hyperparameter tuning to optimize training efficiency and stability.

3. **Model Training Data Scale-Up:** To enhance model robustness and generalization, the training dataset will be scaled up. This may involve acquiring additional historical data or employing data augmentation techniques.

4. **Image Sequence Generator:** A sliding window image generator will be implemented to create sequences of candlestick images. This is particularly important for models requiring temporal context, such as LSTMs, Transformers, or the Vision Transformer (ViT). The generator will create overlapping sequences of a defined length from the existing candlestick images.

5. **Sample Dataset for 3-Image Sequences:** A smaller sample dataset consisting of 3-image sequences will be created for initial testing, debugging, and validation of models designed to process image sequences. This smaller dataset allows for quicker iteration and debugging before scaling up to the full dataset, which is especially beneficial for computationally intensive models.

This structured approach ensures that the resulting models are robust, reliable, and capable of generalizing to unseen market data. Further details regarding specific training parameters, validation set construction, and data augmentation techniques will be provided as they become available.
Training and Validation (Enforcer)

This section details the training and validation process for the CNN and ViT models, emphasizing robust performance and generalization to unseen data. While the ultimate goal is a universal source model deployable to various platforms (iOS, Android, Web), this section focuses specifically on the training and validation procedures. The choice of framework (likely PyTorch or TensorFlow, given their flexibility and conversion capabilities) and the potential for on-device training will inform decisions regarding model architecture and size to accommodate resource constraints.

The following steps outline the training and validation process:

1. **Training and Validation:** The CNN models will be trained using a prepared dataset of candlestick images and corresponding return labels. A validation set will be held out to monitor performance during training and prevent overfitting. Early stopping will be implemented to prevent overtraining and ensure optimal generalization performance.

2. **Hyperparameter Tuning:** Optimal learning rates and batch sizes will be determined through rigorous experimentation. These hyperparameters significantly influence training dynamics and final model performance.

3. **Data Scaling:** To enhance model generalization, a large-scale dataset of candlestick images and return labels will be curated and prepared.

4. **Image Sequence Generation (ViT):** A sliding window image generator will be developed to create sequences of candlestick images for training the ViT model. This generator will create overlapping windows of a specified length from the larger dataset, enabling the ViT model to learn temporal dependencies in the data.

5. **Sample Dataset for Initial Testing (ViT):** A smaller sample dataset of 3-image sequences will be created for initial testing and debugging of the ViT model and its input pipeline. This smaller dataset facilitates rapid iteration and verification of the model architecture and training process before scaling up to the full dataset.

### B. Training and Validation (Enforcer)

This section details a cost-effective and robust training and validation process designed to minimize expenses while ensuring model integrity. A multi-stage testing approach, combined with resource-conscious training practices, achieves this goal.

**Cost-Effective Training:**

Initial training runs will be limited to one epoch using a minimal data range. This drastically reduces computational load and associated server costs, allowing for frequent testing and debugging cycles without depleting the budget. Leveraging on-device training (iOS) for initial sessions further minimizes reliance on server resources, enabling quicker debugging cycles and reducing cloud compute costs during development. The focus will be on functionality and stability verification on the device before scaling up to full datasets and longer training runs on cloud servers.

**Multi-Stage Testing:**

A three-stage testing approach optimizes resource utilization during training and validation:

1. **Unit/Integration Tests with Mock Data:** This stage verifies individual components and their interactions using mock data in CSV or JSON format, including edge cases like price spikes, flat periods, and data gaps. This ensures robustness without the cost of large datasets. Specifically, tests will cover the `DynamicPlaneGenerator`, data normalization functions (`time_frac`, `log_return`, `volume_scaling`), and the data loader.

2. **End-to-End Dry Run:** A full end-to-end dry run simulates the entire training and validation pipeline using the mock data. This comprehensive system test avoids actual training costs. A "Dry Run" toggle in the Experiment Designer UI will facilitate this process.

3. **On-Device Smoke Tests with a Dummy Model:** On-device smoke tests utilize a small, pre-trained "dummy" neural network mirroring the input/output shape of the actual ViT model. This verifies deployment and inference on the target hardware with minimal resource consumption. An investigation will determine whether the current practice of one-epoch training during this stage is necessary or if the existing dummy model can be used directly for further optimization. This will clarify the optimal smoke test procedure.

## B. Training and Validation (Enforcer)

This section details the training and validation procedures for the on-device model training pipeline, encompassing standard training practices, integration of a Cognitive Threat Analysis Module (CTAM) for "shocker event" responsiveness, and rigorous testing methodologies.

**Standard Training and Validation Procedures:**

The primary model (CNN, as described in previous sections) will be trained using established best practices:

1. **Dataset Generation:** A sliding window image generator will create training sequences of images. A large-scale dataset generated using this method is crucial for robust training and generalization to unseen market conditions. A smaller, 3-image sequence sample dataset will be created for initial testing and validation.

2. **Training Methodology:** Standard training will utilize a designated validation set and early stopping to prevent overfitting. Optimal learning rate and batch size will be determined through experimentation and validation set performance monitoring.

3. **On-Device Validation:** A crucial smoke test using a dummy model and a single training epoch will be performed on-device. This validates the entire training pipeline, including data loading, training loop execution, and weight extraction. The necessity and sufficiency of a single epoch, its performance and resource implications, and the relationship between this on-device training and the broader model improvement process (including Core ML's backend universal model updates) will be further evaluated. The seamless connection between the `DynamicPlaneGenerator`, the transformed image tensors, and the Core ML training session will be technically verified, ensuring a complete training step (forward pass, loss calculation, and backpropagation) executes without errors. The application's access to updated weights after this dummy training step, their packaging and transmission to the Python backend, and the subsequent update of the universal model will also be confirmed.

**Cognitive Threat Analysis Module (CTAM) Integration:**

The CTAM enhances model responsiveness to unexpected market occurrences ("shocker events"):

4. **"Shocker Event" Definition:** "Shocker events" will be rigorously defined using quantifiable metrics such as volatility spikes, unusual trading volume anomalies, and rapid price changes. This definition forms the basis for event identification and simulation within the training data.

5. **CTAM Development:** Lightweight CNN models will be developed for the CTAM to analyze financial charts for "shocker event" patterns, prioritizing computational efficiency for real-time anomaly detection.

6. **CTAM Integration and Influence:** The CTAM will be integrated into the training framework, carefully managing its interaction with the `DynamicPlaneGenerator`. The CTAM's "Threat Level" score, reflecting the likelihood of a "shocker event," will influence the `DynamicPlaneGenerator`, potentially adjusting the smoothing function or the main model's learning rate for dynamic adaptation to market volatility.

7. **"Shocker Event" Implementation:** Defined "shocker events" will be introduced into the training data, enabling the model and CTAM to learn proactive responses to unexpected market fluctuations, improving robustness and real-world applicability.

**Testing Strategy:**

A three-stage testing strategy will be implemented and documented:

1. **Unit & Integration Testing:** Individual components and their interactions will be tested using local mocks to isolate and verify functionality.

2. **End-to-End Pipeline Simulation (Dry Run):** The entire pipeline will be simulated with a small data subset to verify data flow, transformations, and the training process without saving the trained model.

3. **On-Device Smoke Test (Dummy Model):** A simplified model will be trained on-device with a small dataset to validate weight updates, synchronization with the backend, and the entire process, including update packaging, transmission, reception, and universal model update. This serves as a prioritized, low-cost method for pipeline troubleshooting. A comprehensive technical document will detail these testing and training procedures, including the "smoke test" debugging strategy.

## B. Training and Validation (Enforcer)

This section details the training and validation process for the dual-system model (Flow Engine and Threat Engine), addressing both architectural considerations and practical training procedures. Given the Enforcer's role in managing resources and ensuring robust execution, the training and validation process will adhere to the principles of Continuity, Enforcement, Facilitation, and Specialization.

**Architectural Considerations:**

The current model, reliant on an error-correction mechanism, exhibits a mean reversion bias that limits its effectiveness during periods of high volatility. To address this, a dual-system architecture is implemented:

- **Flow Engine:** Handles normal market behavior using the existing error-correction mechanism for stability within typical market conditions.
- **Threat Engine (CTAM):** Acts as a contextual override, designed to detect and react to outlier "shock" events, mitigating the Flow Engine's mean reversion bias during volatile periods. The Threat Engine utilizes specialized Convolutional Neural Networks (CNNs) for anomaly detection.

**CNN Training and Validation:**

Separate CNNs will be trained for different financial instruments:

- **Equities:** Detects anomalies in equity charts (e.g., gaps, volume spikes) using candlestick chart snapshots and corresponding volume data.
- **Derivatives:** Detects anomalies in derivative markets by analyzing visualized options chain data in heatmap format.

The training and validation process for these CNNs involves:

- **Model Retraining Strategy:** A dynamic retraining process considers both gradual market flow and sudden market shocks to prevent desensitization to significant market movements.

**Practical Training Procedures:**

The Enforcer manages the following aspects of the training process:

1. **Resource Allocation:** Manages GPU allocation, memory management, and workload distribution for efficient resource utilization.
2. **Process Monitoring and Execution:** Oversees the training process, tracks key metrics (e.g., loss, accuracy), and intervenes to address issues.
3. **Error Detection and Prevention:** Implements checks and safeguards (e.g., resource utilization monitoring, validation metric tracking) to prevent training failures.

The following steps, guided by the four architectural principles, further detail the process:

1. **Atomic Functions and Data Flow:** The training and validation process is decomposed into atomic functions with clear inputs and outputs for a manageable data flow. Existing components (e.g., image sequence generator, dataset creation) will be reviewed and potentially restructured.
2. **Categorization based on Four Pillars:** Each atomic function is categorized according to Continuity, Enforcement, Facilitation, and Specialization to clarify its role and ensure alignment with the Enforcer's focus.
3. **Stringent Communication Protocols:** Clear communication protocols are established between the training process and other system components (e.g., data acquisition, model architecture) for seamless data flow.

Specific training parameters (learning rate, batch size, dataset scale-up) and the implementation of a sliding window image generator for image sequences will be detailed in subsequent sections.
Training and Validation (Enforcer)

This section details the robust and controlled execution of the training and validation processes. The Enforcer role emphasizes strict management of resources and reproducible results through the following key principles and implemented services:

- **Controlled Resource Access:** All access to training and validation data, including model hyperparameters (learning rate, batch size, etc.), is managed through dedicated Enforcer components. This ensures data integrity and prevents unintended modifications. This access control extends to the image sequence generator and sample dataset creation, guaranteeing a consistent and predictable data flow to the training pipeline.

- **Isolated Training Environments:** Training and validation procedures for each model (CNNs, ViT) are isolated to prevent interference and enable focused optimization. For example, the CNN's training process using candlestick images is separate from the ViT's training using image sequences, including independent management of hyperparameters.

- **Modular Code Structure through Inheritance:** A modular and maintainable codebase is achieved through class inheritance. Base classes define common functionalities (e.g., data loading, preprocessing, metric calculation). Specialized subclasses (e.g., `CNNTrainer`, `ViTTrainer`) inherit these functionalities and implement model-specific training loops and hyperparameter management, ensuring consistent practices across different training processes.

- **`NormalizeWindow` Service:** This service normalizes the range of input data (e.g., candlestick data) within a sliding window to a predefined range. This normalization ensures consistent input distributions for the models, improving training stability and performance.

- **Real-time Process Monitoring:** A dedicated service continuously monitors the health and resource consumption (CPU, memory, GPU utilization) of training and validation jobs. This service detects potential issues like training stagnation, alerting users and enabling early intervention to prevent wasted resources.

- **Execution State Control:** This service manages the state of active training and validation processes, providing functionalities for pausing, resuming, and gracefully terminating jobs. This control is crucial for managing long-running training sessions, enabling interruptions, and ensuring proper resource release.

## B. Training and Validation

This section details the training and validation process, emphasizing secure resource management and robust model development. It outlines the interaction with the `Resource_Enforcer` service and the implementation of a dedicated `EnforcementService` for ensuring training integrity and performance.

### Resource Management with `Resource_Enforcer`

The `Resource_Enforcer` service plays a critical role in maintaining data integrity and controlling access to Google Cloud Storage (GCS) during training and validation. It serves as the exclusive gateway for all GCS interactions, preventing unauthorized access and ensuring a consistent data pipeline. Specifically, the `Resource_Enforcer` handles:

- **Reading Training Data:** The training process requests data (e.g., candlestick images, labels) through the `Resource_Enforcer`, which retrieves the necessary files from GCS.
- **Writing Validation Results:** Validation results (e.g., model performance metrics, validation loss) are stored in GCS via the `Resource_Enforcer`.
- **Managing Checkpoints:** Model checkpoints are managed by the `Resource_Enforcer`, enabling secure storage, retrieval, and resumption of training from specific points.

This centralized approach to data access ensures data integrity, simplifies auditing and monitoring, and strengthens the security posture of the training and validation procedures.

### Training and Validation Procedure using `EnforcementService`

To ensure consistent and maintainable training and validation procedures across different models, an inheritance-based architecture is employed. An abstract `EnforcementService` base class encapsulates core functionalities like logging, metric tracking, and checkpoint management. Specific training and validation services, such as `CNNTrainingValidationService`, inherit from this base class and extend its functionality for particular models.

**`EnforcementService` Base Class:** This class provides common functionalities:

- **Logging:** Comprehensive logging of training parameters, performance metrics, and any relevant events.
- **Metric Tracking:** Tracks key metrics like loss, accuracy, and validation performance.
- **Checkpointing:** Manages saving and loading model checkpoints at defined intervals.

**`CNNTrainingValidationService` (Example):** This service, inheriting from `EnforcementService`, implements the specific training and validation procedures for CNN models, including:

- **Dataset Management:** Handles dataset splitting, data loading (including image sequence generation for 3-image sequences, as outlined in the project requirements), and data scaling.
- **Model Training:** Defines the model architecture, optimizer (with specified learning rate and adjustments), loss function, and training loop. Incorporates early stopping to prevent overfitting. Addresses batch size configuration as per the project outline.
- **Model Validation:** Evaluates the model performance on the validation set using appropriate metrics.
- **Interaction with Other Services:** Interacts with data provision services (via the `Resource_Enforcer`) for data access and model saving services for storing trained models and checkpoints. This interaction adheres to the microservice architecture defined in the project.

This structure ensures a robust and verifiable training and validation process, enforcing the reliability and performance of the final model while maintaining flexibility for different model types and training configurations. The specific details regarding learning rate adjustments, batch size settings, data scaling methodologies, and the sample dataset for 3-image sequences will be documented within the `CNNTrainingValidationService` implementation details.

### B. Training and Validation (Enforcer)

This section details the training and validation procedures for the Enforcer models within the Four Pillars system architecture, emphasizing a client-side heavy lifting approach. This distributed strategy leverages user device resources (CPU, GPU, memory) for model training, minimizing server load and enabling rapid iteration. The lightweight backend orchestrator primarily serves training data (images and labels) to the client application, while the Universal Model Hub receives updated model weights from the on-device training performed via the native app frontend. This approach necessitates careful client-side resource management, a core responsibility of the Enforcer component.

Given this distributed training context, the following key activities are implemented:

1. **Data Delivery and Preparation:** The backend orchestrator efficiently delivers the necessary training data (candlestick chart images and corresponding labels) to the client application. Asynchronous communication and optimized data transfer mechanisms are employed to minimize latency and ensure smooth data flow.

2. **Client-Side Training Execution:** The Enforcer component manages the training process on the user's device, utilizing available resources effectively. This includes handling the complexities of varying device capabilities and ensuring a consistent training experience across different platforms.

3. **Advanced Error Signal and Performance-Based Healing:** Model training incorporates a composite Total Error signal, composed of Vector Deviation Error and Frame Shift Error, providing a nuanced performance assessment. This allows for targeted interventions and a dynamic, performance-based "healing" or corrective process, optimizing resource allocation and responsiveness to market changes. This contrasts with traditional, fixed-schedule retraining.

4. **Multi-Scale Periodicity:** Training data incorporates candlestick chart images derived from multiple timeframes (intraday, daily, and weekly) to enhance the model's understanding of cyclical patterns. This multi-scale approach improves the model's ability to generalize across varying market conditions.

5. **Model Validation and Monitoring:** Robust validation techniques, such as k-fold cross-validation and early stopping, are implemented to monitor performance during training and prevent overfitting. This ensures the trained models generalize well to unseen market data. Post-training, continuous monitoring of prediction accuracy and error signals informs ongoing model refinement and potential retraining.

6. **Model Upload and Synchronization:** Trained model weights are efficiently uploaded from the client device to the Universal Model Hub. Mechanisms are in place to ensure secure and reliable transfer, as well as version control and synchronization across devices.

This client-side focused training and validation process, coupled with advanced error analysis and performance-based adaptation, ensures robust, performant, and adaptable Enforcer models.

### B. Training and Validation

This section details the training and validation process for the CNN-based trading models, encompassing both performance optimization and explainability. The training process includes standard practices such as using a validation set, early stopping to prevent overfitting, and hyperparameter tuning for the learning rate and batch size. The training data will be scaled up to improve model robustness. A sliding window image generator will create image sequences for models requiring temporal context, initially tested with a sample dataset of 3-image sequences.

To ensure the models' decisions are transparent and justifiable, explainability is a key focus. This involves:

- **Attribution Methods:** Model-agnostic methods like LIME and SHAP, along with model-specific methods such as attention maps, will identify influential features in trading decisions, providing insights into the _why_ behind predictions.

- **Narrative Generation Service:** A dedicated `Narrative_Generation_Service` will generate human-readable explanations for each trade. This service leverages the attribution methods and integrates with a Feature Store to access versioned input features and system state information, ensuring accurate and context-rich explanations with full traceability and reproducibility.

- **Explanation AI Integration (Exploration):** The potential of integrating an "Explanation AI" will be explored to further enhance explainability beyond feature attribution.

### B. Training and Validation (Enforcer)

This section describes the Enforcer's role in training and validation, specifically focusing on risk mitigation through a simulated trading environment. This involves transitioning the model from backtesting to a paper trading environment before live deployment.

A `Paper_Brokerage_Simulator` emulates the Zerodha Kite Connect API, allowing realistic order execution and portfolio management testing with simulated funds and live market data. The simulator's state, including cash balances, open positions, and order statuses, is maintained in a dedicated Firestore collection named `paper_portfolio`.

The `Live_Execution_Enforcer` supports two modes: `LIVE` and `PAPER`, controlled via a toggle in the Live Trading Dashboard UI. In `LIVE` mode, trade requests are routed to the Zerodha Kite Connect API; in `PAPER` mode, they are directed to the `Paper_Brokerage_Simulator`. This dual-mode operation allows for safe evaluation and refinement of strategies and model parameters before live trading. The "Paper Trading" mode provides users with a transparent view of their simulated portfolio performance, mimicking real-world trading without financial risk.

### B. Training and Validation

This section details the training and validation procedures for the CNN and ViT models, along with the development of supporting data pipelines.

1. **Dataset Preparation and Augmentation:** A large-scale dataset is crucial for robust model training. This will involve acquiring additional historical data if necessary and employing data augmentation techniques to enhance the variability of the training set and improve model generalization.

2. **Image Sequence Generation (ViT):** A sliding window image generator will be developed to create sequences of candlestick chart images for the ViT model. This generator will efficiently handle large datasets by creating overlapping sequences and ensuring proper data alignment, enabling dynamic data loading during training.

3. **Sample Dataset for Initial Testing:** A smaller sample dataset of 3-image sequences will be created for initial testing and debugging of the ViT input pipeline and training process. This allows for rapid iteration and validation before scaling to the full dataset.

4. **Training and Validation Process (CNN and ViT):** Both the CNN and ViT models will be trained using the prepared datasets. A dedicated validation set will be used to monitor performance during training, and early stopping will be implemented to prevent overfitting. The training process will involve tuning hyperparameters such as learning rate and batch size through experimentation to optimize model performance.

### C. Anxiety Model Training

This section details the training process for the Anxiety Model, a meta-learning model designed to predict the behavior of the primary trading algorithm (DynamicPlane) and output an "Anxiety Level" used to dynamically adjust the system's response to market conditions. The Anxiety Model will be trained _post-hoc_ using backtest data and a comprehensive set of high-frequency market depth features.

The following features will be used for training the Anxiety Model:

- **Order-to-Quantity Ratio:** The ratio of the number of orders to the total quantity of shares being bid or offered.
- **Rate of Change of Order Book Imbalance (OBI):** Measures the speed and direction of changes in order book imbalance.
- **Level 1 Dominance:** Represents the proportion of buy or sell orders at the best bid/ask price.
- **Book "Flicker" Rate:** Quantifies the frequency of changes in the order book, potentially indicating market instability.

The target variable for training will be the Total Error from the DynamicPlane algorithm, encompassing both Vector Deviation and Frame Shift. This allows the Anxiety Model to learn the relationship between market conditions (represented by the input features) and the performance of the primary trading algorithm. The "Price Improvement Rate," calculated by a dedicated service, will be incorporated as a feature, potentially as a context token for the ViT model, to enhance profitability prediction and dynamic plane perception.
Training and Validation

This section details the training and validation procedures for the CNN models within the SCoVA (Snapshot Computer Vision Algorithm) project, emphasizing considerations for risk management, asymmetry, and dynamic market conditions. The training process leverages discrete, dynamically generated visual snapshots of market data as primary inputs.

**Core Training and Validation Steps:**

1. **Dataset Preparation:** A large-scale dataset will be prepared, incorporating techniques to ensure sufficient diversity and representation of various market scenarios, including bull and bear market trends. Separate training and validation datasets will be created for each market regime. Additionally, the CalculateAsymmetricFeatures service will enrich the training data with features like Upside Volatility and Downside Volatility, incorporated as context tokens for the Vision Transformer.

2. **Image Sequence Generation:** A sliding window image generator will create sequences of candlestick chart images. This captures the dynamic changes in market conditions over time and provides temporal context for the models. A smaller sample dataset of 3-image sequences will be used for initial testing and rapid prototyping.

3. **Model Training (CNNs and Vision Transformer):** The CNN models, and the Vision Transformer incorporating asymmetric features, will be trained using the prepared datasets. Early stopping criteria, based on validation set performance, will prevent overfitting and optimize generalization.

4. **Hyperparameter Tuning:** Appropriate learning rate and batch size values will be determined through experimentation and validation, ensuring optimal model convergence and performance.

5. **Asymmetric Prediction Models:** Separate models, Bull_Flow_Engine and Bear_Flow_Engine, will be trained for bull and bear markets, respectively, using their corresponding datasets. This specialization allows each model to focus on the unique characteristics of each market trend.

6. **Loss Function Optimization:** Specialized loss functions will be implemented to enhance risk management. A risk-averse loss function will prioritize minimizing large losses by penalizing underestimation more heavily. An asymmetric loss function will further refine this by applying varying penalties depending on the magnitude and direction of prediction error, with higher penalties for larger-than-predicted losses.

7. **Integration with System Components:** While the Portfolio_Risk_Manager and HealingController fall outside the direct scope of this section, the training process will be informed by their requirements. Model selection and hyperparameter tuning will consider the asymmetric risk management parameters and the "prospect theory" approach of prioritizing robustness to negative fluctuations.

8. **Validation:** Model performance will be evaluated on the separate validation sets for each market regime. Validation metrics will consider the impact of the asymmetric features and loss functions, ensuring the models effectively address the project's risk management objectives.

The interplay between these training techniques and asymmetry-focused adjustments is crucial for creating robust and effective models capable of navigating dynamic market conditions and managing risk effectively. This integrated approach ensures alignment with the core principles of using discrete visual snapshots and leveraging temporal context for accurate predictions.

## B. Training and Validation

This section details the training and validation procedures for the enhanced Vision Transformer (ViT) model, incorporating both the state-aware attention mechanism, regime ID integration, and an asymmetric feature vector as a context token. The process focuses on establishing a robust and scalable training pipeline.

1. **Data Preparation and Feature Generation:** A sliding window approach will generate sequences of candlestick images for the ViT input. Crucially, for each generated image sequence, the corresponding asymmetric feature vector will be computed by the `AsymmetricFeatureEngine` service and provided as a context token. This service calculates features capturing asymmetries in recent price action and volume, including:

   - **Price & Volatility Asymmetry:** Upside/Downside Volatility, Volatility Skewness, and Volatility Kurtosis.
   - **Volume & Participation Asymmetry:** Accumulation/Distribution Ratio and Order-to-Quantity Asymmetry.
   - **Correlation Asymmetry:** Price-Volume Correlation State for positive and negative returns.

   This combined dataset of image sequences and context vectors will be split into training and validation sets. A smaller sample dataset with 3-image sequences will be created for initial testing and rapid experimentation. This allows for efficient validation of the context token integration and other architectural modifications before scaling up to the full dataset.

2. **Model Training with Context Token Integration:** The ViT architecture has been adapted to accept both the image sequence tensor and the asymmetric feature vector as a context token. The training process involves feeding both inputs to the ViT. The `Workflow_Broker` retrieves the feature vector from the `AsymmetricFeatureEngine` and provides it to the `Model_Inference_Service` alongside the image tensor.

3. **Regime ID Integration:** In addition to the asymmetric feature vector, the current market regime, represented by a "Regime ID" from an external Asymmetric Regime Detection model, is also integrated as a context token. This provides the ViT with structured information about the prevailing market conditions.

4. **State-Aware Attention Mechanism:** The ViT incorporates a state-dependent attention mechanism that dynamically adjusts its focus based on market conditions, such as volatility or a calculated "threat level," improving adaptability to changing market dynamics.

5. **Hyperparameter Tuning and Model Evaluation:** The learning rate and batch size will be carefully tuned, considering the inclusion of both context tokens. Experiments will determine optimal settings for effective learning of the relationships between visual patterns, asymmetric features, and regime information. Validation will be performed using the designated validation set, potentially incorporating early stopping to prevent overfitting. Model performance will be closely monitored throughout the training and validation process.

6. **Scaling Up Training Data:** Preparation of a large-scale dataset, including image data and corresponding asymmetric feature vectors and Regime IDs, is crucial for robust model training. The established pipeline is designed for scalability to accommodate this larger dataset.

While further enhancements are encouraged, a focus on simplicity and avoiding over-engineering is paramount. Future modifications should demonstrate clear benefits and minimize disruption to the established pipeline for long-term maintainability and adaptability.

### B. Training and Validation

This section details the training and validation procedures, focusing on incorporating contextual information into the model using a "Dual-Token Context Injection" method within the Vision Transformer (ViT).

Two distinct tokens will be injected into the ViT input:

1. **Regime ID:** This token represents the current market regime classification, providing high-level context about overall market conditions.

2. **Raw Asymmetric Vector:** This token contains the raw asymmetric feature vector, providing granular detail about specific market characteristics, potentially revealing insights beyond the broader regime classification.

The ViT will be trained to leverage both tokens. This dual-token approach allows the ViT flexibility to prioritize or combine information from both sources. The approach is also non-destructive; if the raw asymmetric vector provides no additional value, its inclusion will not negatively impact the model, and its computation can be easily removed without significant architectural changes. This facilitates experimentation and refinement of the contextual information provided during training and validation.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation for candlestick data. This involves transforming the data to generate dynamic snapshots that capture local price movements. The following steps outline the process:

1. **Dynamic Rotation Implementation:** The coordinate system representing price, volume, and time will be dynamically rotated based on recent price movement. This rotation aims to align the plane with the dominant trend, providing a clearer view of local fluctuations.

2. **Local Movement Vector Calculation:** A local movement vector will be calculated, representing the direction and magnitude of recent price changes. This vector will serve as the basis for calculating the rotation angle.

3. **Rotation Matrix Calculation:** The rotation angle will be calculated from the local movement vector and used to construct a rotation matrix for transforming the data.

4. **Dynamic Origin Shift:** The origin of the coordinate system will be dynamically shifted to center the view on the most recent data point, facilitating analysis of current market activity.

5. **Dynamic Snapshot Generation:** After rotation and translation, a snapshot of the transformed data will be generated. This snapshot will visually represent the dynamic plane, showing the rotated and shifted data.

6. **Rotation Artifacts Handling:** Potential artifacts from rotation (e.g., aliasing or blurring) will be mitigated through interpolation and anti-aliasing techniques.

7. **Volatility Jump Handling:** Sudden, large price movements (volatility jumps) will be handled by scaling the rotation angle or applying a smoothing function.

8. **Consistent Axis Scaling:** Axis scaling will be maintained consistently throughout the transformations, preserving the relative relationships between price, volume, and time.

9. **Pseudocode Pipeline:** Pseudocode for the dynamic snapshot generation pipeline will be provided for clarity and implementation.

10. **Rotating Snapshot Generator Module:** A dedicated module will encapsulate the logic for rotation, translation, and image generation.

11. **Data Normalization:** Price (P), Volume (V), and Time (T) data will be normalized to the range [-1, 1] before transformations to ensure consistent scaling and improve calculation stability.

12. **Price Clipping:** Percentile clipping will be applied to price data to mitigate the impact of outliers on rotation and translation calculations.

This section details the implementation of a dynamic plane transformation applied to candlestick chart images before they are fed into the CNN model. This transformation aims to represent price movements in a way that is more readily learned by the CNN, potentially highlighting subtle patterns correlated with future price action.

1. **Dynamic Rotation Implementation:** The coordinate system of the candlestick chart is dynamically rotated based on the overall price movement within a defined time window (e.g., 5 days). The dominant direction of price change (upwards if the close is significantly higher than the open, downwards if the reverse is true) determines the rotation angle.

2. **Local Movement Vector Calculation:** A local movement vector is calculated for each day within the time window, representing the change in price from open to close. This vector contributes to the overall trend analysis used for rotation.

3. **Rotation Matrix Calculation:** Based on the dominant price movement direction, a corresponding rotation matrix is calculated using standard trigonometric functions. This matrix is then applied to transform the coordinates of the candlestick chart.

4. **Dynamic Origin Shift:** After rotation, the origin of the coordinate system is dynamically shifted based on the price movement. This shift can center the chart around a point of significant change or the average price, keeping the focus on relevant price action.

5. **Dynamic Snapshot Generation:** After the rotation and translation, a new candlestick chart image, the "dynamic snapshot," is generated. This transformed image serves as the input to the CNN.

6. **Rotation Artifact Handling:** The rotation process can introduce artifacts, particularly near the edges of the image. Mitigation strategies, such as padding or interpolation, will be implemented to maintain image quality and prevent the introduction of spurious patterns.

7. **Volatility Jump Handling:** Sudden, large price movements (volatility jumps) can skew the rotation and translation, leading to misrepresentations of the underlying trend. Mechanisms such as dynamic thresholding or scaling adjustments will be employed to handle these jumps gracefully.

8. **Consistent Axis Scaling:** Consistent axis scaling is maintained throughout the transformation process to preserve the relative magnitudes of price and volume changes and ensure comparability between snapshots.

9. **Pseudocode Pipeline:** Detailed pseudocode for the entire dynamic snapshot generation pipeline, from initial data input to final image output, will be provided to ensure clarity, reproducibility, and facilitate implementation.

10. **Rotating Snapshot Generator Module:** A dedicated software module will encapsulate the dynamic snapshot generation functionality. This modular design promotes code organization, reusability, and maintainability.

11. **Data Normalization:** Price (P), Volume (V), and Time (T) data are normalized to a consistent range (e.g., -1 to +1) before any transformations. This normalization ensures consistent input scaling and prevents potential issues arising from large value ranges.

12. **Percentile Clipping for Price:** Extreme price outliers are clipped using a percentile-based approach to prevent them from disproportionately influencing the dynamic plane calculations and causing distortions. This ensures the transformation remains robust in the presence of anomalous price data.
    This section details the implementation of a dynamic plane representation for financial time series data, specifically candlestick charts. This dynamic approach transforms the chart's coordinate system based on recent price action, aiming to capture relative price and volume changes for improved model input.

13. **Dynamic Rotation Implementation:** The candlestick chart's coordinate system is dynamically rotated based on the dominant direction of price movement. This aligns the primary axis of price action with the horizontal axis, potentially highlighting subtle patterns.

14. **Local Movement Vector Calculation:** A local movement vector represents the direction and magnitude of recent price change within a defined time window (e.g., 5 days). This vector is calculated using price differences within the window.

15. **Rotation Matrix Calculation:** The rotation angle is derived from the local movement vector and used to construct a rotation matrix applied to the candlestick data points.

16. **Dynamic Origin Shift:** After rotation, the coordinate system's origin is dynamically shifted to center the transformed candlestick data, maintaining a consistent visual representation.

17. **Dynamic Snapshot Generation:** A "dynamic snapshot," a transformed candlestick chart image, is generated after rotation and translation, serving as input to the CNN model.

18. **Rotation Artifact Handling:** Potential artifacts (e.g., distortions, clipping) introduced by rotation are addressed through appropriate mitigation techniques.

19. **Volatility Jump Handling:** Mechanisms are implemented to handle sudden, large price movements (volatility jumps) that could destabilize the dynamic plane calculations, ensuring robustness.

20. **Consistent Axis Scaling:** Consistent scaling is maintained across both axes during transformations, preventing visual distortions and ensuring comparability between dynamic snapshots.

21. **Percentile Clipping for Price:** Outlier price values are managed through percentile clipping to mitigate their impact on transformations and improve snapshot robustness.

22. **Data Normalization:** Price (P), Volume (V), and Time (T) data are normalized to a consistent range (e.g., -1 to +1) to ensure equal feature contribution during model training.

23. **Rotating Snapshot Generator Module:** A dedicated software module encapsulates the dynamic snapshot generation process (rotation, translation, image generation) for code reusability and maintainability.

24. **Pseudocode Pipeline:** Detailed pseudocode documents the entire dynamic snapshot generation pipeline for clarity and future modification/debugging.

This dynamic plane implementation provides a robust and informative visual representation of price action, enabling the CNN model to potentially learn patterns and predict future returns more effectively by focusing on relative changes rather than absolute price levels.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation of market data. This approach captures the relative movements of price, volume, and time by normalizing these elements onto a consistent scale for the model. The following steps outline the process:

1. **Local Movement Vector Calculation:** A local movement vector is calculated, representing the change in price, volume, and time over a given period.

2. **Rotation Angle and Matrix Calculation:** The angle of rotation is derived from the local movement vector. A rotation matrix is then calculated using this angle.

3. **Dynamic Rotation Implementation:** The coordinate system is dynamically rotated based on the calculated rotation matrix. This allows the model to focus on relative price changes rather than absolute values.

4. **Dynamic Origin Shift:** The origin of the coordinate system is dynamically shifted after the rotation. This combined transformation centers the current price action, further emphasizing relative movements.

5. **Dynamic Snapshot Generation:** After the rotation and translation, a "dynamic snapshot" (a candlestick chart image) is generated. This transformed image serves as input to the model.

6. **Rotation Artifact Mitigation:** Rotating images can introduce artifacts, particularly near the edges. Mitigation strategies are implemented to ensure data quality.

7. **Volatility Jump Handling:** Mechanisms are implemented to handle sudden, large price movements (volatility jumps) gracefully, preventing extreme rotations and potential information distortion.

8. **Consistent Axis Scaling:** Consistent scaling of the price, volume, and time axes is maintained throughout the transformation process. This preserves the relative relationships between these features, preventing misinterpretations by the model.

9. **Data Normalization:** Price (P), Volume (V), and Time (T) data are normalized to the range of -1 to +1. This ensures comparable scales for all input features, preventing any single feature from dominating the model's learning process.

10. **Percentile Clipping for Price:** Outlier price values are clipped using a percentile-based approach. This mitigates the impact of extreme price movements on the dynamic plane calculations and prevents distortions in the generated snapshots.

11. **Rotating Snapshot Generator Module:** A dedicated module encapsulates the dynamic snapshot generation process, promoting code reusability and maintainability.

12. **Pseudocode Pipeline:** Clear and concise pseudocode is provided for the entire dynamic snapshot generation process to aid understanding and implementation.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of a dynamic plane representation for candlestick data, focusing on the transformations required to create dynamic snapshots. This approach aims to create a dynamic perspective of price action by rotating and translating the coordinate system based on the movement of price, volume, and time, potentially revealing patterns not readily apparent in a static representation.

The implementation involves the following steps:

1. **Local Movement Vector Calculation:** A local movement vector will be defined, representing the change in price, volume, and time over a short period. The specific calculation method (e.g., using price only, a combination of price and volume, or incorporating time) requires further definition.

2. **Dynamic Rotation Implementation:** The coordinate system will be rotated based on the local movement vector. This rotation aims to align the x-axis with the dominant direction of price action, potentially highlighting trends and reversals. The corresponding rotation angle and rotation matrix will be calculated and applied to the candlestick data points.

3. **Dynamic Origin Shift:** The origin of the coordinate system will be dynamically shifted, potentially relative to the current price or other relevant indicators. This centers the view on the most recent price action. The specific translation logic requires further definition.

4. **Dynamic Snapshot Generation:** After rotation and translation, a snapshot of the transformed candlestick data will be generated. These snapshots will serve as input for the model.

5. **Data Normalization:** Price (P), Volume (V), and Time (T) data will be normalized to the range of -1 to +1. This ensures that these features contribute equally to the transformations, preventing any single feature from dominating due to scale differences.

6. **Percentile Clipping for Price:** Percentile clipping will be applied to price data to mitigate the impact of outliers on the rotation and translation calculations, ensuring more robust transformations.

7. **Rotation Artifacts Handling:** Strategies will be developed and implemented to mitigate potential artifacts introduced by rotating the discrete data of candlestick charts.

8. **Volatility Jump Handling:** Mechanisms will be incorporated to gracefully handle sudden, large price movements (volatility jumps) that could destabilize the generated snapshots by drastically altering the rotation and translation.

9. **Consistent Axis Scaling:** The implementation will maintain consistent scaling across the x and y axes for meaningful visual representation and model training.

10. **Pseudocode Pipeline:** Clear pseudocode will document the entire dynamic snapshot generation process to aid understanding and verification.

11. **Rotating Snapshot Generator Module:** A dedicated module will encapsulate the rotating snapshot generator functionality for code reusability and maintainability.

The dynamic plane output needs to be interpretable by downstream modules, including risk management and trade filtering mechanisms. Specifically:

- **Dynamic Plane Output and Stop-Loss:** Stop-loss logic must be defined in relation to the transformed data representation. This might involve mapping the stop-loss percentage to corresponding thresholds within the transformed coordinate system.

- **Confidence Thresholding and Trade Filtering:** Integrating a confidence metric derived from the model alongside the transformed data will enable trade filtering based on prediction confidence. Trades with confidence below a specified threshold will be prevented from executing.

- **Risk-Based Weighting and Dynamic Plane:** Features derived from the transformed data, such as volatility or trend stability metrics within the dynamic plane, can inform risk-based weighting calculations. This allows dynamic position sizing based on the observed characteristics within the transformed data space.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation of financial time series data, specifically designed to enhance feature extraction and model training for a Vision Transformer (ViT). This involves transforming candlestick data into a dynamic coordinate system that adapts to price movements, enabling the model to capture relative price changes and volatility more effectively.

The dynamic plane implementation follows this pipeline:

1. **Local Movement Vector Calculation:** A local movement vector, representing the direction and magnitude of price change within a given time window, is calculated. This vector serves as the basis for the dynamic rotation. This could incorporate volume data as well.

2. **Dynamic Rotation Implementation:** The coordinate system is dynamically rotated based on the calculated movement vector. This aligns the primary axis of the plane with the dominant price trend, highlighting local price fluctuations relative to this trend.

3. **Rotation Matrix Calculation:** Using the local movement vector, the rotation angle is determined. A rotation matrix is then calculated to transform the candlestick data into the rotated coordinate system.

4. **Dynamic Origin Shift:** The origin of the coordinate system is dynamically shifted to the starting point of the current time window. This centers the rotated plane on the most recent data, emphasizing recent price action.

5. **Dynamic Snapshot Generation:** After applying the rotation and translation, a snapshot of the transformed candlestick data is generated. This snapshot represents the dynamic plane at a specific point in time and serves as input for the ViT model.

6. **Rotation Artifacts Handling:** Strategies are implemented to mitigate potential artifacts introduced by the rotation, such as distortions in candlestick shapes or misrepresentations of price relationships.

7. **Volatility Jump Handling:** Mechanisms are incorporated to handle sudden, large price movements (volatility jumps). These mechanisms prevent extreme rotations and ensure the stability of the dynamic plane representation.

8. **Consistent Axis Scaling:** A consistent scaling factor is maintained across both axes of the dynamic plane. This prevents distortions introduced by unequal scaling and ensures accurate representation of price movements.

9. **Data Normalization:** Price (P), Volume (V), and Time (T) data are normalized to the range of -1 to +1. This ensures consistent input ranges for the model and can improve training stability.

10. **Percentile Clipping for Price:** Outliers in price data are clipped using a percentile-based approach. This reduces the influence of extreme values and improves the robustness of the dynamic plane representation.

11. **Rotating Snapshot Generator Module:** A dedicated module encapsulates the functionality of the rotating snapshot generator. This modular design promotes code reusability and maintainability.

12. **Pseudocode Pipeline:** Detailed pseudocode will be provided for the dynamic snapshot generation process, outlining the specific transformation steps and facilitating implementation.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation for financial time series data, transforming candlestick data into a dynamic coordinate system. This approach emphasizes relative price movements rather than absolute values, potentially revealing patterns more readily discernible by vision-based models. While this section doesn't directly address predicting candlestick images instead of returns, these transformations could be adapted for such an approach.

The following steps outline the dynamic plane creation process:

1. **Local Movement Vector Calculation:** A local movement vector, capturing the direction and magnitude of price change within a defined time window, will be calculated. This vector serves as the basis for the dynamic rotation, effectively representing the local trend. The specific calculation method will consider factors like window size and weighting schemes.

2. **Rotation Matrix Calculation:** Based on the local movement vector, a rotation angle will be calculated and used to construct a rotation matrix. This matrix will transform the candlestick data, aligning the coordinate system with the identified local trend.

3. **Dynamic Rotation Implementation:** The candlestick chart's coordinate system will be dynamically rotated using the calculated rotation matrix. This rotation emphasizes relative price changes, simplifying the visual representation for subsequent analysis.

4. **Dynamic Origin Shift:** The coordinate system's origin will be dynamically shifted relative to a reference point, such as the current candlestick's open price. This provides a consistent reference frame despite price fluctuations.

5. **Dynamic Snapshot Generation:** After rotation and translation, a snapshot of the transformed data will be generated, representing the data in the new, dynamic coordinate system. This snapshot can then be used as input for image-based prediction models.

6. **Rotation Artifacts Handling:** Potential artifacts introduced by rotation (e.g., distortions or aliasing) will be mitigated through appropriate techniques.

7. **Volatility Jump Handling:** Mechanisms will be implemented to handle sudden, large price movements (volatility jumps), potentially through scaling the rotation angle or smoothing the movement vector. This prevents extreme rotations that could misrepresent market behavior.

8. **Consistent Axis Scaling:** Maintaining consistent axis scaling across all transformed snapshots is crucial for meaningful comparisons and consistent model input. This will be carefully managed throughout the transformation process.

9. **Data Normalization:** Price (P), Volume (V), and Time (T) data will be normalized to the range of -1 to +1. This ensures consistent feature contribution during model training and prevents scaling-related issues.

10. **Percentile Clipping for Price:** Outlier price values will be clipped using a percentile-based approach to prevent extreme price fluctuations from disproportionately influencing the transformations and model predictions.

11. **Pseudocode Pipeline:** Clear and concise pseudocode will be provided for the entire dynamic snapshot generation pipeline to facilitate understanding and implementation.

12. **Rotating Snapshot Generator Module:** A dedicated, reusable module will be implemented to encapsulate the rotating snapshot generator functionality, promoting code organization and maintainability.

Addressing variable-length input sequences for the ViT model will also be explored, experimenting with different numbers of candlestick images (N=3, 4, and 5). Positional embeddings will retain sequential information, while masking and padding will accommodate varying sequence lengths during model training.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane transformation applied to candlestick data, creating dynamic snapshots for model input. This approach aims to capture price movements relative to a shifting coordinate system, enabling the model to focus on relative price changes rather than absolute values. This implementation prioritizes visual clarity to enhance the model's understanding of price action, ultimately aiming to improve the clarity and actionability of trading decisions.

This section focuses solely on the mechanics of the dynamic plane implementation. It does not address comparisons to other forecasting methodologies, risks and drawbacks of image-based prediction, model validation, or return extraction from predicted images. These topics are handled elsewhere.

The dynamic snapshot generation process follows these steps:

1. **Local Movement Vector Calculation:** A local movement vector is calculated based on the price difference between the opening and closing prices within the candlestick window. This vector represents the primary direction of price action.

2. **Rotation Matrix Calculation:** The angle between the local movement vector and a reference axis (e.g., the x-axis) is calculated. This angle is then used to construct a rotation matrix.

3. **Dynamic Rotation Implementation:** The coordinate system is rotated based on the calculated rotation matrix. This rotation aligns the x-axis with the dominant price movement.

4. **Dynamic Origin Shift:** The origin of the coordinate system is dynamically shifted to the opening price of the current candlestick. This centers the price movements around the starting point of the window.

5. **Dynamic Snapshot Generation:** Candlestick images are generated after applying the rotation and translation transformations. These transformed images, representing dynamic snapshots, are fed to the model.

6. **Rotation Artifacts Handling:** Strategies are implemented to mitigate potential artifacts introduced by the rotation, such as aliasing or blurring.

7. **Volatility Jump Handling:** Sudden, large price movements (volatility jumps) are handled to prevent distortions in the transformed images. Specific mitigation techniques will be detailed in [section reference or future documentation].

8. **Consistent Axis Scaling:** Consistent scaling is maintained across both axes to prevent unintended biases in the model.

9. **Normalize P, V, T Data:** Price (P), Volume (V), and Time (T) data are normalized to the range of -1 to +1. This normalization ensures consistent input scaling and prevents any single feature from dominating.

10. **Percentile Clipping for Price:** Outlier price values are clipped using a percentile-based approach to enhance robustness and prevent extreme values from distorting the transformation.

11. **Pseudocode Pipeline:** Pseudocode for the dynamic snapshot generator is provided for clarity and implementation guidance in [section reference or appendix].

12. **Rotating Snapshot Generator Module:** A dedicated, reusable module encapsulates the logic for generating the rotated and translated candlestick images, promoting modularity and maintainability.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane for transforming candlestick data, enhancing the model's ability to capture relevant price action patterns. By rotating and translating the coordinate system based on recent price movements, the model focuses on local changes, aiming to understand _why_ certain patterns occur, not just _what_ they look like. This approach aligns with the architectural requirement for causally grounded predictions. Furthermore, it prioritizes visually plausible sequences reflecting real-life candlestick patterns, supporting the goal of accurately predicting visual sequences. The relationship between this visual accuracy and financial performance will be a key area of investigation, along with the clarity and actionability of the resulting trading decisions.

1. **Local Movement Vector Calculation:** A local movement vector, representing recent price changes, will be calculated. This vector forms the basis for the dynamic rotation and translation.

2. **Dynamic Rotation Implementation:** The candlestick chart's coordinate system will be dynamically rotated based on the local movement vector. This rotation aligns the primary axis with the dominant trend, potentially highlighting subtle momentum shifts.

3. **Rotation Matrix Calculation:** The rotation angle, derived from the local movement vector, will be used to calculate the rotation matrix for the coordinate transformation.

4. **Dynamic Origin Shift:** The coordinate system's origin will be dynamically shifted to center on the most recent price action, focusing the model on local patterns.

5. **Dynamic Snapshot Generation:** After rotation and translation, "dynamic snapshots" of the transformed candlestick data will be generated and used as model input.

6. **Rotation Artifacts Handling:** Mitigation strategies will address potential artifacts introduced by the rotation, ensuring the transformed data remains accurate and representative.

7. **Volatility Jump Handling:** Sudden, large price movements (volatility jumps) will be handled specifically to prevent distortions in the dynamic snapshots.

8. **Consistent Axis Scaling:** Consistent axis scaling will be maintained throughout the transformations, ensuring comparability between different snapshots.

9. **Normalize P, V, T Data:** Price (P), Volume (V), and Time (T) data will be normalized to the range of -1 to +1 for consistent input scaling, improving training robustness and addressing potential fragilities related to ground truth alignment and pattern overfitting.

10. **Percentile Clipping for Price:** Outliers in price data will be clipped using percentile clipping to prevent extreme values from distorting the normalization and subsequent transformations.

11. **Rotating Snapshot Generator Module:** A dedicated module will encapsulate the rotating snapshot generator functionality for maintainability and reuse.

12. **Pseudocode Pipeline:** Pseudocode will detail the complete dynamic snapshot generation process.

## Dynamic Plane Implementation

This section details the implementation of a dynamic 2D plane for representing market data, replacing the static 3D Cartesian coordinate system. This dynamic plane, conceptually similar to a Frenet frame or tangent plane, utilizes rotational axes and a shifting origin to adapt to price movements. Two approaches will be explored: a prototype demonstrating the visualization of parabolic trajectories through axis rotation and origin refocusing, and a theoretical framework for dynamic plane implementation within the SCoVA project.

### Prototype Development

A prototype will be developed to visualize motion within this 2D framework, demonstrating how axis rotation and dynamic origin refocusing can effectively represent a parabolic trajectory. This prototype will serve as a proof of concept for the framework and inform the subsequent theoretical development.

### Theoretical Framework

This dynamic plane concept draws parallels to several established mathematical and physical frameworks, including parallel transport, affine connections, Frenet-Serret frames, the SE(3) group, and local inertial frames in general relativity. Exploring these connections will provide a richer theoretical context and potentially inspire novel solutions. A concise algebraic summary of the dynamic 2D plane concept, using mathematical notation and equations, will be essential for clear communication and future development.

The core concept involves abandoning the fixed 3D frame and transitioning to a dynamic 2D plane with rotational axes and a dynamic origin that adjusts based on price action. This approach allows for a more nuanced representation of market dynamics by focusing on the local movement and orientation of the data. Specifically, the implementation involves the following steps:

1. **Local Movement Vector Calculation:** A local movement vector will be defined to capture the direction and magnitude of recent price changes. This vector will serve as the basis for calculating the rotation of the dynamic plane.
2. **Dynamic Rotation Implementation:** The coordinate system of the 2D plane will be dynamically rotated based on the local movement vector. This rotation reorients the plane to align with the current price trajectory. A specific rotation law governing these adjustments needs to be formulated.
3. **Rotation Matrix Calculation:** Based on the local movement vector, the rotation angle will be calculated, and the corresponding rotation matrix derived. This matrix transforms the data point coordinates.
4. **Dynamic Origin Shift:** The origin of the 2D plane will be dynamically shifted to track the latest price point. This translation ensures the plane remains centered on the most recent data, emphasizing local changes.
5. **Dynamic Snapshot Generation:** After applying the rotation and translation, snapshots of the transformed data will be generated, representing the market data in the context of the dynamic plane. A dedicated module, the Rotating Snapshot Generator Module, will encapsulate this functionality to promote modularity and code reusability. A pseudocode pipeline will be provided outlining the complete snapshot generation process.

6. **Data Normalization and Preprocessing:** Price (P), Volume (V), and Time (T) data will be normalized to the range of -1 to +1 to ensure equal contribution to the analysis. Percentile clipping will manage price outliers, preventing extreme movements from unduly influencing the dynamic plane.
7. **Artifact and Volatility Handling:** Potential rotation artifacts (distortions or discontinuities) will be addressed. Mechanisms will handle sudden, large price movements (volatility jumps), ensuring the dynamic plane remains stable during periods of high market volatility. Consistent axis scaling will be maintained throughout the transformations.

### Challenges and Future Directions

The practical application of this 2D dynamic frame within the SCoVA project requires careful consideration of its potential limitations, including path dependence, singularities, and computational overhead. These limitations must be addressed for effective implementation and interpretation of results.

Furthermore, investigating curvature invariants will help quantify the dynamics of the market representation. Generalizing the concept beyond a 2D plane to encompass more complex market surfaces is another avenue for future development.

Finally, we will investigate how this 2D dynamic frame maps to the complexities of the financial market, conceptually similar to a three-body problem. This involves representing multiple interacting entities (e.g., different assets or market factors) within the 2D framework. Potential approaches include pairwise relative coordinates, a barycentric frame, or a shape space representation. Recent advances in the three-body problem, such as machine learning approximations and newly discovered periodic solutions, will be explored for potential relevance and integration. This will help understand how the simplified 2D representation captures the complex interplay of various market forces.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation for capturing the evolving nature of financial time series data. This approach transforms the coordinate system based on price movements, creating a dynamic perspective. Inspired by biological navigation systems, the implementation represents price movement (analogous to 3D movement) within a dynamic 2D plane whose orientation adjusts based on the direction of movement, leveraging egocentric and allocentric frames of reference.

The dynamic plane implementation incorporates the following key components:

- **Local Movement Vector Calculation:** This vector represents the instantaneous direction and magnitude of price change and serves as the basis for calculating the rotation matrix. Its precise definition is crucial for the accuracy and stability of the dynamic rotation. It may also incorporate other relevant indicators like volume and trading activity. The time window for calculating this vector will be defined (e.g., the past 5 days).

- **Rotation Matrix Calculation:** Based on the local movement vector, the rotation angle is calculated and used to construct the 2D rotation matrix. This matrix transforms the price data coordinates onto the rotated plane. The numerical conditioning of this transformation, particularly compared to using global coordinates, requires careful analysis to ensure stability and reliability.

- **Dynamic Rotation Implementation:** The 2D coordinate system rotates dynamically, aligning with the direction of price movement. This rotation aims to capture the local context of price changes more meaningfully. The stability of this rotation during rapid price fluctuations requires careful consideration. Numerical stability can be enhanced using angular-momentum conserving integrators (Lie-group methods). Simpler integration approaches like Euler and RK4 will be compared and their trade-offs evaluated.

- **Dynamic Origin Shift:** The origin of the 2D plane dynamically shifts to center the current candlestick data. This centering ensures that the most recent price action remains the focus, allowing the model to prioritize relative changes around the current market state.

- **Dynamic Snapshot Generation:** After applying rotation and translation transformations, a snapshot of the transformed candlestick chart data is generated. These snapshots serve as input for subsequent model processing steps.

- **Rotation Artifacts Handling:** Rotation can introduce artifacts, particularly with discrete data. Mitigation strategies, such as smoothing or interpolation techniques, will be implemented to address these distortions.

- **Volatility Jump Handling:** Sudden, large price movements (volatility jumps) can drastically alter the rotation angle, potentially destabilizing the dynamic plane. Mitigation mechanisms, such as limiting the maximum rotation angle or employing adaptive smoothing parameters, will be implemented.

- **Consistent Axis Scaling:** Maintaining consistent axis scaling is crucial for preventing data distortions and accurately representing price movements in the dynamic plane, regardless of the rotation angle.

- **Data Normalization:** Price (P), Volume (V), and Time (T) data will be normalized to the range of -1 to +1 to improve model stability and performance, ensuring features contribute equally to the analysis.

- **Percentile Clipping for Price:** Outlier price values can skew the rotation and translation calculations. Percentile clipping will cap extreme values to mitigate their impact, improving the robustness of the dynamic plane implementation.

- **Pseudocode Pipeline:** Clear and concise pseudocode will illustrate the entire dynamic snapshot generation process, serving as a blueprint for implementation and aiding workflow understanding.

- **Rotating Snapshot Generator Module:** A dedicated module will encapsulate the rotating snapshot generator functionality, promoting code reusability and maintainability within the overall model pipeline.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane that visually represents price movement as an upward spiral using a 2D chart with rotating axes. This approach avoids the complexity of 3D visualization while capturing the evolving nature of price trajectories.

The implementation simulates movement along a helix projected onto a 2D plane. The helix parameters (radius and pitch) control the spiral's shape and relate directly to the visualization's curvature and torsion. These parameters require careful tuning to reflect the characteristics of the price data.

The dynamic rotation calculates a local movement vector based on price changes. This vector informs the rotation matrix, determining the axis rotation's angle and direction. This process simulates the upward spiral by dynamically rotating the chart's coordinate system according to the local price trend. A dynamic origin shift translates the coordinates to maintain visual focus on the current price point. Price (P), Volume (V), and Time (T) data are normalized to a -1 to +1 range to ensure consistent scaling and prevent distortions. Percentile clipping is applied to the price data to manage outliers and prevent them from unduly influencing the scaling and rotation calculations.

The system generates dynamic snapshots of the chart after each transformation, representing the evolving price movement as a rotating 2D spiral. Several critical aspects must be addressed:

- **Artifact Handling:** Rotating the coordinate system can introduce artifacts, especially during rapid price movements. Mitigation strategies are required.
- **Volatility Jump Handling:** Sudden, large price movements can destabilize the dynamic plane. Robust mechanisms are needed to handle these jumps.
- **Consistent Axis Scaling:** Maintaining consistent axis scaling is crucial for accurate visual representation and prevents misinterpretations.

Data storage for the planar coordinates (u, v) and the moving frame's orientation is essential. A simple arc length counter can be used for the planar coordinates. The orientation should be stored using a method that allows reconstruction, such as quaternions or rotation vectors. This data enables recreation of the spiral path. Memory and storage limitations must be considered, ensuring compatibility with visualization processes.

Visualizing the planar trace within the moving chart requires careful consideration:

- **Intuitive Representation:** Explore alternative visual representations to enhance understanding of the trace's meaning and movement.
- **Tooltips/Annotations:** Implement tooltips or annotations providing contextual information about specific points or segments of the trace.
- **Help/Tutorial:** A dedicated help section or tutorial should explain the planar trace concept and interpretation.

Technical constraints dictate that the 3D helix and 2D trace visualizations be generated as separate figures, not subplots. Matplotlib is suggested for the 3D helix, while the 2D trace plotting library is flexible. The implementation will utilize `matplotlib` and `numpy`, with `seaborn` explicitly prohibited. Each chart generation call must produce a single, cohesive chart. A key challenge is visualizing the relationship between a straight line trace on the price movement helix and its corresponding 2D plane representation aligned with the moving frame of reference.

This dynamic plane implementation draws inspiration from biological navigation systems, specifically egocentric and allocentric mapping. Research into these reference frames within the brain, including head-direction cells and parallax, can inform the design and optimization of the dynamic plane representation. The goal is to create a robust and informative visualization that captures both local and global price movement context. A detailed pseudocode implementation will be provided. This will form the basis for a dedicated "Rotating Snapshot Generator" module, promoting code reusability and maintainability.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic 2D plane for representing price movements, replacing a traditional 3D framework. This simplified 2D representation utilizes two rotational axes and a dynamic origin to focus on relative price changes rather than absolute values. The plane's coordinate system dynamically rotates and translates with each new candlestick or group of candlesticks, influencing the feature space before the next prediction. This dynamic adjustment, potentially implemented using affine transformations, PCA rotation, or a learned rotation matrix, prioritizes local movement over absolute position.

The following steps outline the implementation process:

1. **Local Movement Vector Calculation:** A local movement vector, calculated from the difference between current and previous candlestick data points (or a group thereof), quantifies the direction and magnitude of recent price changes. This vector serves as the basis for the dynamic rotation.

2. **Rotation Matrix Calculation:** The rotation angle is derived from the local movement vector and used to construct a rotation matrix. This could involve PCA to determine the principal axes of movement and align the coordinate system accordingly. Dynamic integration of PCA into the Vision Transformer or CNN pipeline requires careful design for efficient recalculation at each time step.

3. **Dynamic Rotation Implementation:** The 2D plane's coordinate system is rotated according to the calculated rotation matrix. This aligns the axes with the dominant direction of recent price change, providing a clearer view of local trends.

4. **Dynamic Origin Shift (Translation):** The origin of the 2D plane is dynamically shifted to center the most recent data point. This further emphasizes local movement by maintaining focus on the latest price action.

5. **Dynamic Snapshot Generation:** After rotation and translation, a snapshot of the transformed 2D plane is generated. This snapshot, representing the visualized price movement, is then used as input for the prediction model.

6. **Rotation Artifacts Handling:** Potential artifacts introduced by the rotation, such as distortions or aliasing, will be addressed and mitigated to ensure accurate and clear visualization.

7. **Volatility Jump Handling:** Mechanisms will be implemented to handle sudden, large price movements (volatility jumps) to prevent them from disproportionately influencing the rotation and translation. This might involve adaptive scaling or filtering.

8. **Consistent Axis Scaling:** Consistent axis scaling will be maintained throughout the dynamic transformations to preserve the relative magnitudes of price and volume changes.

9. **Pseudocode Pipeline:** A pseudocode pipeline will be provided for the dynamic snapshot generator, outlining the sequence of operations.

10. **Rotating Snapshot Generator Module:** A dedicated, modular "Rotating Snapshot Generator" module will encapsulate this functionality for maintainability.

11. **Data Normalization:** Price (P), Volume (V), and Time (T) data will be normalized to a range of -1 to +1 to ensure equal contribution to calculations and visualizations.

12. **Percentile Clipping for Price:** Outliers in price data will be clipped using a percentile-based approach to mitigate the impact of extreme price movements on the dynamic transformations.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation of price movements. This approach dynamically rotates and translates the coordinate system of the input candlestick data based on local price changes, providing a consistent, locally-centered perspective of price action. This aims to enhance the model's ability to recognize patterns irrespective of the overall market trend.

1. **Dynamic Rotation Implementation:** The coordinate system will be dynamically rotated based on recent price movement. This involves calculating a rotation angle from a local movement vector (derived from recent price changes) and applying it to the data. Potential challenges of this dynamic approach, such as overfitting to noise in small windows, computational overhead, and loss of the absolute reference frame, will be addressed. A baseline comparison against no rotation and static rotation will be performed to quantify the benefits.

2. **Local Movement Vector Calculation:** The local movement vector will be calculated from recent price changes (e.g., the difference between the current and previous closing prices). This vector determines the direction and magnitude of the rotation.

3. **Rotation Matrix Calculation:** The rotation matrix will be constructed using the calculated rotation angle. This matrix is then applied to transform the input data.

4. **Dynamic Origin Shift:** After rotation, the coordinate system's origin will be dynamically shifted (translated) to center the data around the current price point. The specific translation method will be detailed.

5. **Dynamic Snapshot Generation:** Following rotation and translation, a "snapshot" of the transformed data will be generated. This snapshot serves as input to the subsequent stages of the pipeline. A detailed pseudocode implementation and module design are provided below.

6. **Rotation Artifacts Handling:** Mitigation strategies will be implemented to address potential artifacts introduced by the rotation, such as interpolation errors or boundary effects. These strategies may include appropriate interpolation techniques or boundary padding.

7. **Volatility Jump Handling:** The system will incorporate mechanisms, such as smoothing or adaptive scaling, to handle sudden, large price movements (volatility jumps) and prevent erratic rotations.

8. **Consistent Axis Scaling:** A consistent scaling factor, or a dynamic scaling mechanism based on recent volatility, will be employed to maintain consistent axis scaling and avoid data distortion throughout the transformations.

9. **Pseudocode Pipeline:** Detailed pseudocode for the entire dynamic snapshot generation process will be provided in a later section.

10. **Rotating Snapshot Generator Module:** A dedicated module will encapsulate the functionality of the dynamic snapshot generator, ensuring modularity and maintainability.

11. **Data Normalization:** Price (P), Volume (V), and Time (T) data will be normalized to the range of -1 to +1 to ensure consistent input scaling and prevent features with larger magnitudes from dominating the model.

12. **Percentile Clipping for Price:** Outlier price values will be clipped using a percentile-based approach to enhance robustness against extreme price fluctuations and prevent these outliers from skewing the dynamic plane calculations.

The computational cost of these transformations, particularly the rotation, will be carefully analyzed. Optimizations or alternative dimensionality reduction techniques will be explored if necessary. Critically, the entire process will operate on image inputs, such as candlestick images or Heiken Ashi charts.
This section details the implementation of a dynamic plane, a crucial component for representing market data in a locally relevant context. This technique transforms price data into a two-dimensional representation that captures local trends and oscillations, facilitating more effective analysis by convolutional neural networks (CNNs) or Vision Transformers. The core concept involves dynamically adjusting the coordinate system at each time step to align with recent price movements.

The process begins by calculating a local movement vector. For each new candlestick, or at a defined interval, this vector (**v**) is calculated as the difference between the current price (P<sub>current</sub>) and the previous price (P<sub>previous</sub>):

```
v = P<sub>current</sub> - P<sub>previous</sub>
```

Where P<sub>current</sub> and P<sub>previous</sub> represent the price and time (x, y) coordinates of the latest and previous data points, respectively.

Next, a rotation matrix is constructed. The rotation angle (θ) is determined using the arctangent of the local movement vector components:

```
θ = arctan(v<sub>y</sub> / v<sub>x</sub>)
```

This angle is used to create the rotation matrix **R**(θ):

```
R(θ) = [[cos(θ), sin(θ)], [-sin(θ), cos(θ)]]
```

This rotation matrix is then applied to a small window of recent candlestick data (e.g., the last 5-10 candlesticks). Following the rotation, the coordinates are translated so that the current price point is located at the origin (0,0). This centers the movement spatially, effectively leveling the local trend horizontally. Oscillations around the trend are then represented as deviations around 0 along the transformed y-axis. This normalized representation provides a consistent frame of reference for the model, regardless of the direction or magnitude of the overall price trend. These transformed snapshots serve as input to the CNN or Vision Transformer.

Several key implementation details require careful consideration:

- **Dynamic Snapshot Generation:** Candlestick data within the rotated and re-centered window must be redrawn relative to the transformed axes. The time axis will no longer be strictly horizontal in these dynamic snapshots.

- **Rotation Artifact Handling:** Interpolation techniques, such as anti-aliasing, should be used during the redrawing process to mitigate potential visual artifacts introduced by rotation.

- **Volatility Jump Handling:** Smoothing or limiting the delta between consecutive rotation angles can mitigate the impact of abrupt rotations caused by sudden, large price movements.

- **Consistent Axis Scaling:** A uniform scaling mechanism (units per percentage move) must be enforced across all generated frames to prevent unintended biases.

A detailed pseudocode implementation outlining the steps involved in the dynamic rotating snapshot generator is required. This pseudocode should encompass the calculation of the local movement vector, rotation matrix, dynamic origin shift, and the final image generation. This will serve as a blueprint for the actual code implementation and facilitate clear understanding. A conceptual diagram illustrating this process will also be developed to aid in visualization and communication.

## C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation of candlestick data, designed for enhanced feature extraction by a Vision Transformer (ViT) or Convolutional Neural Network (CNN). The core component is the Rotating Dynamic Plane Generator, which transforms candlestick data into dynamic snapshots suitable for ViT/CNN input.

The dynamic plane leverages price and volume information, applying rotation and translation to create a representation that emphasizes recent market trends. This approach aims to provide a more informative input for the chosen model (ViT or CNN), potentially improving performance in tasks such as trend prediction. The generator will incorporate controllable parameters for window size and optionally include volume as a third dimension. Further experimentation will compare the performance of models trained on static versus dynamic frames.

The key steps of the dynamic plane generation process are:

1. **Data Normalization:** Price (P), Volume (V), and Time (T) data are normalized to the range of -1 to +1. This ensures consistent scaling across all features and prevents any single feature from dominating due to magnitude differences.

2. **Outlier Clipping:** Outlier price values are clipped using a percentile-based approach. This mitigates the impact of extreme price movements, improving the robustness of the dynamic plane representation.

3. **Local Movement Vector Calculation:** A local movement vector is calculated, representing the dominant direction and magnitude of recent price and, optionally, volume changes. This vector serves as the basis for the dynamic rotation and translation of the plane.

4. **Dynamic Rotation Implementation:** The coordinate system of the plane rotates dynamically, aligning itself with the prevailing direction indicated by the local movement vector. This allows the model to focus on relative price and volume changes within the context of the current trend.

5. **Rotation Matrix Calculation:** Based on the local movement vector, the appropriate rotation angle is determined, and the corresponding rotation matrix is calculated. This matrix is then applied to the data points on the plane.

6. **Dynamic Origin Shift:** The origin of the coordinate system is dynamically shifted to maintain focus on the most recent price action. This prevents the plane from drifting too far from the relevant price and volume range.

7. **Dynamic Snapshot Generation:** After rotation and translation, a snapshot of the transformed plane is generated. This snapshot, representing the current state of the market in the rotated and translated frame, is then used as input for the ViT or CNN. The output format will be compatible with the chosen model’s input requirements, either as an image (for ViT) or a set of numerical features (for CNN).

8. **Rotation Artifact Handling:** Measures are implemented to mitigate potential artifacts introduced by the rotation process, ensuring the integrity of the generated snapshots.

9. **Volatility Jump Handling:** The system is designed to handle sudden, large price movements (volatility jumps) gracefully, preventing erratic rotations and ensuring the stability of the dynamic plane. This stability is further enhanced by the outlier clipping performed earlier.

10. **Consistent Axis Scaling:** The scaling of the axes is maintained consistently throughout the rotation and translation process. This ensures that relative relationships between data points are preserved.

11. **Rotating Snapshot Generator Module:** A dedicated module encapsulates the functionality of the rotating snapshot generator, promoting code reusability and maintainability.

12. **Pseudocode Pipeline:** Detailed pseudocode will be provided, outlining the entire process from data input and window selection to movement calculation, dynamic frame construction, rotation, translation, and snapshot generation. This pseudocode will serve as a blueprint for the Python implementation.

### C. Dynamic Plane Implementation

This section details the implementation of the dynamic plane, a core component of the visualization and data transformation process. This novel approach represents market dynamics in a way potentially revealing patterns recognizable by CNNs and ViTs.

The dynamic plane operates by rotating the coordinate system based on price and volume changes, providing a consistent frame of reference for the machine learning models. A local movement vector, calculated from the change in price and volume between consecutive timesteps, determines the rotation angle. This angle is then used to calculate a rotation matrix, which transforms the data points into the rotated coordinate system. The origin of the plane dynamically shifts to keep the most recent data centered, emphasizing current market behavior.

After transformation, a snapshot of the dynamic plane is rendered as an image, serving as input for the CNN/ViT models. This process generates the datasets for training these models.

Key implementation details and considerations are as follows:

- **Data Normalization and Clipping:** Price (P), Volume (V), and Time (T) data are normalized to the range of -1 to +1 for consistent input scaling. Percentile clipping mitigates the influence of extreme price outliers on the dynamic plane calculations. Smoothing techniques like Heikin-Ashi may be explored for further refinement.

- **Rotation Artifact and Volatility Jump Handling:** Strategies are implemented to mitigate potential artifacts from rotation and ensure accurate representation. The system robustly handles volatility jumps, preventing them from unduly influencing the rotation and visualization.

- **Consistent Axis Scaling:** Consistent axis scaling is maintained across different timesteps and market conditions for accurate representation and comparison.

- **Rotating Snapshot Generator Module:** A dedicated module encapsulates the rotating snapshot generator functionality, promoting code organization and reusability.

- **Pseudocode:** Detailed pseudocode will be provided to clarify the implementation steps and logic of the dynamic snapshot generation.

- **Handling Limited Data Points:** The system gracefully handles scenarios with limited data points. Rotation and plotting are delayed until at least two data points are available. Single-point frames are handled by displaying a placeholder. Offsets are carefully formatted to avoid dimension mismatches, especially during the initial stages of data accumulation.

- **Mitigating PCA Instability:** Principal Component Analysis (PCA), used for dimensionality reduction, can be unstable with collinear data points, particularly in early frames with limited data. Strategies to mitigate this instability, while not detailed here, are crucial for the robustness of the implementation.

The following visualizations will further illustrate the dynamic plane's functionality:

- **Example Images:** Images demonstrating both the raw candlestick input and the transformed output after applying the dynamic plane transformations will be provided.

- **Dynamic Animation:** An animation visualizing the dynamic redrawing of the plane as new data points are added will showcase the plane's evolution and the impact of transformations. This animation will handle both long data sequences and edge cases such as initialization with minimal data.

A technical constraint to note is the requirement of at least two data points to compute the dynamic plane.

## C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation of Heiken-Ashi candlestick data. This representation aims to improve model performance and provide a more insightful visualization of market dynamics, especially during volatile or choppy conditions. The implementation involves transforming the data through rotation, translation, and normalization, and generating visualizations of the transformed data.

### C.1. Core Transformation and Visualization

The core process transforms Heiken-Ashi data through dynamic rotation and translation, generating snapshots of the data on a 2D plane.

1. **Data Transformation:**

   - **Dynamic Rotation:** Heiken-Ashi candlestick data is dynamically rotated. The midpoint between the open and close values of each candle serves as the basis for rotation, with the specific rotation method derived from the referenced code example (Chat1.json message #98).
   - **Local Movement Vector Calculation:** The local movement vector, defined by the change in midpoint between consecutive Heiken-Ashi candles, informs the rotation and translation calculations.
   - **Rotation Matrix Calculation:** The rotation angle, calculated from the local movement vector, is used to construct the rotation matrix for data transformation.
   - **Dynamic Origin Shift:** After rotation, the origin is dynamically shifted based on the calculated midpoint of the current Heiken-Ashi candle, effectively recentering the data.

2. **Visualization:**

   - **Dynamic Snapshot Generation:** Following transformation, snapshots (images) of the data are generated, visualizing the data on a 2D plane.
   - **Artifact Handling:** Potential rotation artifacts (e.g., distortion, aliasing) are mitigated.
   - **Volatility Jump Handling:** Mechanisms are implemented to handle sudden, large price movements and ensure stable visualization.
   - **Consistent Axis Scaling:** Consistent axis scaling is maintained throughout for accurate and comparable data representation.

3. **Data Normalization and Preprocessing:**

   - **Normalization:** Price (P), Volume (V), and Time (T) data are normalized to the range of -1 to +1 for consistent scaling and improved model performance.
   - **Percentile Clipping:** Percentile clipping mitigates the influence of price outliers.

4. **Pseudocode and Modularization:**
   - **Pseudocode Pipeline:** Pseudocode will be provided to illustrate the workflow.
   - **Rotating Snapshot Generator Module:** A dedicated module will be created for the rotating snapshot generator for enhanced code organization and reusability.

### C.2. Initial Validation with Static Examples and Simulated Data

The initial phase focuses on validating the core transformation pipeline using static examples and simulated choppy market data, deferring animation until later stages.

1. **Static Examples:** Example images will illustrate the transformation process:

   - **Heiken Ashi Input (Before Processing):** An example image of the raw Heiken Ashi candlestick data will serve as a baseline.
   - **Transformed Heiken Ashi Input:** An example image will show the transformed Heiken Ashi data, demonstrating the effect of recentering and projection onto the rotating axes.

2. **Choppy Market Simulation:**
   - **Data Generation:** The `generate_choppy_candlesticks(n=30)` function will generate synthetic candlestick data representing choppy, sideways market behavior.
   - **Standard Heiken-Ashi Visualization:** A standard Heiken-Ashi chart will be generated as a baseline and saved as `/mnt/data/standard_heiken_ashi_choppy.png`.
   - **Dynamic Plane Visualization:** The dynamic plane implementation will be visualized using `dynamic_rotate_recenter_heiken` and `plot_rotated_heiken` functions, with the output saved as `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`. Comparison of these visualizations will demonstrate the dynamic plane's transformation of choppy market data.

### C.3. Future Development: Dynamic Point Movement and Animation

While not the current focus, future development will include dynamic point movement and animation:

- **Dynamic Point Movement:** The visualization will animate the movement of data points within the 3D space to illustrate data changes over time.
- **Live Frame Rotation and Recentering:** The visualization will feature live frame rotation and recentering around the moving points, providing insights into data point relationships as the market evolves.
- **Standalone Animation Simulator:** A separate simulator will be developed incorporating delayed rotation (initiated after three stable data points) for PCA stability, smooth initial plane formation, and step-by-step visualization.
- **Smooth Rotation Matrices:** Rotation matrices will be smoothed to prevent dimensional blowups and ensure smooth animation transitions. The minimum of three data points before rotation initiation ensures stable PCA calculations and prevents erratic visualization behavior.

## C. Dynamic Plane Implementation

This section details several approaches for implementing a dynamic plane representation of market data. The overarching goal is to transform the data into a relative coordinate system, emphasizing the relational dynamics of market movements rather than absolute price levels. This facilitates a shift in model learning from absolute feature perception to relational perception. Three distinct implementations are explored:

**C.1. Rotation-Based Dynamic Plane**

This implementation dynamically rotates the coordinate system based on the dominant direction of recent price action.

1. **Local Movement Vector Calculation:** A local movement vector is calculated based on recent price changes within a defined time window. This vector represents the dominant direction of price action. The specific calculation method will influence the responsiveness of the dynamic rotation.

2. **Rotation Matrix Calculation:** Using the local movement vector, a corresponding rotation angle is calculated, and a rotation matrix is generated. This matrix transforms data points into the rotated coordinate system.

3. **Dynamic Origin Shift:** The origin of the coordinate system is dynamically shifted, potentially centering the data around the most recent price point to further emphasize relative movement. The specific translation strategy needs to be defined.

4. **Dynamic Snapshot Generation:** A snapshot of the transformed data is generated, representing the market data in the dynamic plane and serving as input to the model.

5. **Artifact and Volatility Handling:** Strategies for mitigating rotation artifacts (e.g., using PCA Rotation Stabilization Techniques) and handling volatility jumps (e.g., adaptive window sizes or dampening mechanisms) are crucial and require further development.

6. **Consistent Axis Scaling:** Maintaining consistent scaling across the axes is essential to ensure comparability of movements and prevent representational distortions.

7. **Pseudocode Pipeline:** Clear pseudocode for the entire process will be provided.

**C.2. PCA-Based Dynamic Plane**

This implementation leverages Principal Component Analysis (PCA) on a rolling window of price, volume, and time data to create a dynamic coordinate system.

1. **PCA Window Size and Smoothing:** The rolling window size is a critical parameter. Smaller windows increase sensitivity to recent market changes, while larger windows provide greater stability. Smoothing techniques or stability thresholds are necessary to prevent noise-driven rotations and overfitting.

2. **Relational Model Design:** The model should learn relationships and patterns within the PCA-defined coordinate system, focusing on geometric shapes and flows rather than fixed semantic interpretations of price, volume, and time.

3. **Geometric Pattern Recognition:** The model architecture should prioritize the recognition of emergent geometric patterns and trends formed by the transformed data points.

4. **Impact on Feature Representation:** A thorough investigation of PCA's impact on feature representation is required. This analysis should determine which features (price, volume, time, or combinations thereof) are primarily represented and how these representations change with different input patterns.

5. **Interpretability Projection:** Projecting the model's focus back into the original Time-Price-Volume space is crucial for interpretability and understanding model decisions. This will provide human-interpretable insights.

**C.3. Heiken-Ashi with Dynamic Plane Analysis**

This implementation analyzes the impact of the dynamic plane transformations on standard Heiken-Ashi charts.

1. **Market Regime Simulation and Visualization:** Three market regimes (Trend-Reversal-Recovery, Choppy Sideways, and Strong Linear Uptrend) are simulated and visualized. Standard Heiken-Ashi charts are compared side-by-side with their rotated dynamic plane counterparts in a comprehensive panel for clear visual comparison.

2. **Dynamic Plane Analysis and Summary:** The behavior of each regime within the dynamic plane is analyzed, focusing on how rotation affects the visual representation of trends, reversals, and sideways movements. The implications for model learning and prediction are summarized. This summary will be suitable for inclusion in dissertation chapter drafts. This analysis will help evaluate the robustness and usefulness of the dynamic plane approach in complex and volatile market scenarios, potentially revealing underlying trends or patterns obscured in standard Heiken-Ashi charts.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane, a core component of the model. This dynamic plane provides a unique perspective on price action by rotating and translating the coordinate system based on market movements, aiming to capture the relative relationships between price, volume, and time more effectively. It adapts to evolving market conditions by incorporating error signals and feedback mechanisms to ensure accurate and stable predictions.

The implementation involves the following steps:

1. **Data Normalization and Outlier Clipping:** Price (P), Volume (V), and Time (T) data are normalized to the range of -1 to +1 to ensure consistent scaling and improve model training stability. Outliers in price data are clipped using a percentile-based approach to mitigate the impact of extreme price movements.

2. **Dynamic Rotation Implementation:** The coordinate system is dynamically rotated based on the current price movement vector. This rotation aligns the plane with the dominant trend, providing a clearer representation of price action relative to the trend.

3. **Local Movement Vector Calculation:** A local movement vector is calculated to represent the recent price change. This vector serves as the basis for determining the rotation angle.

4. **Rotation Matrix Calculation:** The appropriate rotation angle is calculated based on the local movement vector, and the corresponding rotation matrix is generated. This matrix is then applied to transform the data points.

5. **Dynamic Origin Shift (Error-Driven):** The origin of the coordinate system is dynamically shifted based on prediction error, centering the view on the most relevant price action. This shift, inspired by biological spatial distinctions (peripersonal vs. extrapersonal), requires further refinement and algorithmic sketching to fully specify its relationship to prediction errors.

6. **Rotation Artifacts Handling:** Potential artifacts introduced by the rotation, such as distortions or aliasing, are mitigated.

7. **Volatility Jump Handling:** Sudden, large price movements (volatility jumps) are handled gracefully by introducing a lag to the rotation, allowing the system to smoothly adjust to the new price regime. A strategy for deactivating this lag and returning to normal rotation needs to be defined.

8. **Dynamic Snapshot Generation:** After rotation and translation, a snapshot of the transformed data is generated for subsequent processing. A dedicated module, the Rotating Snapshot Generator, will encapsulate this process. Pseudocode outlining the pipeline for this generator should be developed.

9. **Dual-Frame Estimation:** Two overlapping local frames are maintained: an "optimistic" frame reflecting immediate rotations and a "stable" frame based on slow smoothing. Predictions are dynamically weighted between these frames based on observed market consistency, combining responsiveness with stability.

10. **Error Signal Integration and Feedback Mechanisms:** An error signal mechanism, inspired by biological systems, allows the model to adjust its market structure perception based on the difference between predicted and actual market movements. This feedback loop includes:

    - **Frame Confidence Correction:** Discrepancies between predicted and actual market movements trigger corrections in rotational frame assumptions.
    - **Prediction Error Memory:** A rolling memory of recent prediction errors influences the rotation's weight, adapting the dynamic plane's stability based on its historical success.
    - **Feedback-Driven Frame Smoothing:** High prediction errors trigger temporary slowdown in rotation (increased smoothing) for robustness during volatile periods.
    - **Lightweight Prediction-Error Feedback:** Prediction errors are integrated back into model training, potentially through auxiliary loss functions or frame stability monitoring, to improve predictive accuracy. The previously used static PCA error value has been removed to enhance performance.

This dynamic plane implementation aims to provide a robust and adaptive representation of market dynamics for subsequent analysis and prediction.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane, focusing on a rolling frame correction algorithm within a `DynamicPlaneGenerator` class to enhance stability and adaptability. This algorithm dynamically adjusts the plane's orientation based on detected error trends, complementing the rotation based on price movement.

1. **Dynamic Rotation Implementation:** The coordinate system rotates dynamically based on prevailing price movement. Furthermore, a rolling frame correction algorithm dynamically adjusts the plane's orientation based on detected error trends. The specifics of this adjustment, including magnitude and method (e.g., reweighting principal axes, discounting minor eigenvectors), require further investigation.

2. **Local Movement Vector Calculation:** The local movement vector calculation incorporates both price movements and adjustments from the rolling frame correction algorithm. This interaction requires careful consideration.

3. **Rotation Matrix Calculation:** Rotation angle and matrix calculations integrate adjustments based on error trends, potentially improving stability and reducing volatility compared to using price data alone.

4. **Dynamic Origin Shift (Coordinate Translation):** Coordinate translation considers both price movement and adjustments from the frame correction algorithm.

5. **Dynamic Snapshot Generation:** Snapshots capture the dynamic plane's state after both price-based transformations and error-based adjustments, reflecting the corrected plane orientation.

6. **Rotation Artifacts Handling:** Artifact handling addresses potential artifacts from both price-based rotations and the rolling frame correction algorithm.

7. **Volatility Jump Handling:** This addresses the impact of sudden price movements on both the initial plane transformation and the subsequent frame corrections, considering the interaction between these processes.

8. **Consistent Axis Scaling:** Maintaining consistent axis scaling is critical throughout the dynamic transformation, including during frame correction actions, ensuring robustness against both price fluctuations and correction adjustments.

9. **Rolling Frame Correction Algorithm:** This algorithm, central to the dynamic plane's adaptive nature, operates as follows:

   - **Error Detection:** A rolling buffer (e.g., 5-10 steps, determined experimentally) stores recent prediction errors, calculated as the difference between the predicted and realized movement within the rotated frame. An error trend detector analyzes this buffer, calculating the rolling mean and variance. A threshold, potentially based on a multiple (e.g., 1-2x) of the rolling standard deviation, triggers the frame correction action.
   - **Frame Correction Action:** When the error threshold is exceeded, small rotation adjustments or damping are applied to the PCA frame.
   - **Healing Phase:** Applied corrections gradually diminish as prediction errors subside, allowing the system to return to its normal state. This "wound-healing" approach enhances stability and responsiveness.

10. **Pseudocode Pipeline:** Pseudocode for the dynamic snapshot generator will include the rolling frame correction algorithm logic (error detection, correction application, and healing phase).

11. **Rotating Snapshot Generator Module:** The `DynamicPlaneGenerator` class houses the rotating snapshot generator module and will incorporate the modular rolling frame correction function.

12. **Data Normalization:** Price (P), volume (V), and time (T) data are normalized to the -1 to +1 range.

13. **Percentile Clipping for Price:** Percentile clipping mitigates the impact of extreme price movements on the dynamic plane transformations and the rolling frame correction algorithm.

This enhanced dynamic plane implementation aims to improve stability and adaptability through its error-driven correction mechanism. Further research and development are needed to finalize implementation details and evaluate its effectiveness. Visualization of the error-correction-healing process and a metric quantifying frame intervention will be valuable assessment tools.

## C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane, a crucial component for capturing the nuances of price and volume movements in financial time series data. The core concept involves rotating and translating the coordinate system to align with market dynamics, generating transformed snapshots for model input. Error correction mechanisms are also integrated to refine the dynamic representation.

### Dynamic Transformations and Snapshot Generation

The following steps outline the process of generating dynamic snapshots:

1. **Data Normalization:** Price (P), Volume (V), and Time (T) data are normalized to the range of -1 to +1. This ensures consistent input scaling for the model and prevents features with larger magnitudes from dominating. Percentile clipping is applied to the price data to mitigate the impact of extreme values.

2. **Local Movement Vector Calculation:** A local movement vector is calculated, representing the change in price and volume within the current time window. This vector serves as the basis for calculating the rotation angle and subsequent transformations.

3. **Dynamic Rotation Implementation:** The coordinate system is dynamically rotated based on the local movement vector. This involves calculating a rotation angle derived from the market's trajectory.

4. **Rotation Matrix Calculation:** The rotation angle is used to calculate the rotation matrix. This matrix is then applied to the data points to rotate the coordinate system.

5. **Dynamic Origin Shift (Translate Coordinates):** The origin of the coordinate system is dynamically shifted to maintain focus on the current market state. This translation ensures the transformed snapshots are centered around the relevant price and volume data.

6. **Dynamic Snapshot Generation:** After rotation and translation, dynamic snapshots are generated. These transformed images capture the market dynamics and serve as input to the model.

7. **Rotation Artifacts Handling:** Potential artifacts introduced by rotation, such as interpolation errors or boundary effects, are mitigated to ensure accurate representation of market movements.

8. **Volatility Jump Handling:** Mechanisms are implemented to handle sudden, large price movements (volatility jumps) without disrupting the dynamic plane's stability. This might involve scaling adjustments or alternative transformation methods.

9. **Consistent Axis Scaling:** Consistent scaling is maintained throughout the transformations to preserve the relative relationships between price, volume, and time.

10. **Rotating Snapshot Generator Module:** A dedicated module encapsulates the functionality of the rotating snapshot generator, promoting code reusability and maintainability. This module will contain the logic for steps 2 through 9.

11. **Pseudocode Pipeline:** Detailed pseudocode will be provided for the dynamic snapshot generation pipeline (steps 2 through 9) to clarify the implementation steps and facilitate understanding.

### Error Correction and Validation

To ensure the accuracy of the dynamic plane representation, an error correction mechanism is implemented. This involves calculating two error values:

- **Distance Error:** The magnitude difference between the predicted displacement vector and the realized displacement vector within the dynamic 2D plane.

- **Angle Error:** The orientation difference between the predicted direction vector and the realized direction vector within the dynamic 2D plane.

These error values are used to refine the dynamic plane transformations and improve the accuracy of the generated snapshots. A diagram illustrating the rotations, transformations, and error calculations is needed to further clarify this process. The number of rotational angles and distance vectors needs to be explicitly defined and documented, ensuring alignment between the implementation and the intended behavior. The initial frame creation rotation is specifically excluded from the error correction calculations.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane for analyzing price movements and ensuring consistent comparison of predicted and realized market movements. The core concept involves maintaining a consistent short-term frame of reference (1-5 candlesticks) for predictions, assuming negligible structural market drift within this timeframe. This avoids recalculating the dynamic frame for every price fluctuation, except in high-frequency trading scenarios. The dynamic plane is managed using Principal Component Analysis (PCA), creating a local coordinate system defined by PCA1 and PCA2.

Two primary methods, "Freeze Frame" and "Reproject Realization," address the challenge of shifting PCA frames:

**Freeze Frame:** This method "freezes" the PCA rotation matrix (R) calculated at time 't' for short-term predictions (e.g., 1-3 candlesticks). Both predicted and realized data points are projected using this frozen matrix. This ensures comparisons occur within a consistent frame of reference, mitigating the impact of potential shifts in the underlying data distribution.

**Reproject Realization:** The realized movement at 't+1' is reprojected back into the PCA frame established at time 't'. This utilizes the original PCA basis (rotation matrix) to express the realized movement vector in the initial prediction frame. This method, like Freeze Frame, ensures consistency despite potential market shifts, but differs by reprojecting only the realized data, while Freeze Frame reprojects both.

These methods differ from traditional distance and angular error calculations, which assume a constant PCA plane. Freeze Frame and Reproject Realization explicitly address potential PCA plane shifts, providing more accurate comparisons in dynamic market conditions.

The following steps will solidify the implementation:

1. **Pseudocode Pipeline (Freeze and Compare):** Pseudocode will be developed for the "Freeze & Compare in Dynamic PCA Frame" technique, outlining the algorithmic steps for transforming and comparing price movements within the dynamic plane. This will serve as the foundation for the Rotating Snapshot Generator Module.

2. **Rotating Snapshot Generator Module:** This module will generate dynamic snapshots of market data, encapsulating the logic for coordinate system rotation, translation, and snapshot generation. It will also manage storing the PCA basis (rotation matrix) for each window in a lightweight data structure for efficient access during reprojection.

3. **Visual Simulation of Frame Alignment:** A visual simulation will illustrate the alignment process between predicted and realized price movements using the dynamic plane, verifying the correct implementation of the "Freeze & Compare" method.

4. **Visualizations of Freeze Frame and Projection:** Visual examples will showcase the "freeze frame" and "reprojection" functionalities, clarifying their distinction from existing distance/angular error calculations based on static PCA planes and highlighting how reprojection enables comparison regardless of subsequent market movements.

5. **Error Analysis and Correction:** Numerical examples with predicted vs. realized movement values will be provided, including calculations of angle/distance error within the frozen frame and plots illustrating prediction and reality paths in both frozen and shifted frames. Furthermore, lightweight frame adjustments will be explored to address accumulated angular errors exceeding a defined threshold. This ensures long-term stability and accuracy. The consistency of the PCA plane between prediction and reality matrices, considering potential differences in price and volume values, will be investigated to determine if a transformation or alignment step is necessary. Finally, a robust pseudocode prototype for a "Freeze and Correct" module will be developed, encapsulating the logic for freezing the PCA frame, handling data projection, and implementing the error correction strategies. This pseudocode will be the basis for subsequent code implementation.
   Dynamic Plane Implementation

This section details the implementation of a dynamic plane, crucial for capturing the evolving relationships between price, volume, and time and adapting to market fluctuations. This dynamic approach enhances the model's robustness and responsiveness to changing market conditions by adjusting the coordinate system (through rotation and translation) based on market movements and prediction errors. This creates dynamic snapshots that better reflect current market dynamics.

**Error Calculation and Correction:**

A robust error calculation mechanism is essential for this dynamic implementation. Traditional static error checking is insufficient. Instead, this implementation uses a two-component error calculation:

1. **Vector Deviation Error:** Quantifies the difference between predicted and actual data points projected onto the current, dynamic PCA plane.

2. **Frame Shift Error:** Quantifies the change in the PCA axes (frame drift) between consecutive time steps (_t_ and _t+1_). This is calculated as a weighted sum of the angles between the respective PCA vectors: `Frame Error = α * Angle(PCA1_t, PCA1_{t+1}) + β * Angle(PCA2_t, PCA2_{t+1})`, where _α_ and _β_ are tunable weights.

The **Total Error** is the sum of these two components. Pseudocode will be developed to guide the implementation of this calculation.

This error calculation drives a self-correcting mechanism. A rolling buffer stores the Total Error over a defined number of time steps (e.g., 5-10 candlestick windows). The rolling mean and standard deviation of these errors are continuously calculated.

A "Wound Phase" is triggered when the mean error exceeds a threshold (e.g., twice the rolling standard deviation). This indicates consistently high prediction errors. During the Wound Phase, a correction factor is applied to the data transformations used in constructing the model's input, potentially adjusting PCA rotation or smoothing parameters. The specific implementation of this correction factor requires further investigation and will be tailored to the observed errors.

A "Healing Phase" begins when the mean error falls below a lower threshold (e.g., 1.5 times the rolling standard deviation), indicating a return to acceptable performance. The correction factor is then gradually reduced or removed, allowing the system to return to normal operation. If errors spike again during the Healing Phase, the system re-enters the Wound Phase.

Further details on the specific dynamic plane mechanisms (rotation, translation, snapshot generation) and the data transformations used in input image construction will be elaborated in subsequent sections. This includes exploring the relationship between Frame Drift Error and model confidence, potentially using it as an indicator for trading decisions. The normalization of distance and angular errors will also be addressed to ensure accurate aggregation within the weighted error calculation. Finally, a small-scale simulation will be developed to demonstrate vector deviation and PCA frame drift within the dynamic plane implementation.

## Mitigating Issues and Ensuring Data Integrity in Dynamic Snapshots

This section details the error detection and healing system implemented to address discrepancies and maintain the reliability of dynamically generated snapshots. This system ensures the stability and accuracy of the model in the face of market fluctuations.

**Error Detection and Healing System:**

The system incorporates the following components:

- **Correction Factor Decay Mechanism:** Following the detection and correction of an anomaly (e.g., a volatility jump affecting plane stability), a correction factor is applied. This factor gradually decays over time, allowing the system to return to its normal operating parameters as market conditions stabilize. The decay mechanism will be either exponential (e.g., with a tunable decay rate λ=0.95) or linear.

- **Dynamic Correction Re-entry:** The system remains vigilant even during the "Healing Phase" (the period after a correction where the correction factor is decaying). If errors resurge, indicating renewed market instability, the system dynamically re-enters "Correction Mode," ensuring continuous adaptation to market dynamics.

- **Modular Implementation:** The error detection and healing logic resides within a dedicated module. This modular design enhances code clarity, maintainability, and testability, centralizing the handling of anomaly detection, correction application, correction factor decay, and re-entry into correction mode.

- **Parameter Tuning:** The system's effectiveness relies on appropriately tuned parameters. Initial values for thresholds (triggering correction mode entry) and decay rates (governing the healing phase) will be determined based on analysis of common market regimes and subsequently refined through optimization.

- **Healing Phase Visualization:** The healing phase will be visualized to ensure proper functionality and facilitate understanding. Simulations of various market conditions will trigger healing phases, with the decay of the correction factor and potential re-entry into correction mode visually represented. This visualization will be crucial for debugging and validation.

## Dynamic Plane Implementation

This section details the implementation of dynamic plane transformations, which are crucial for representing price movements in a normalized and consistent manner. The process involves normalizing price (P), volume (V), and time (T) data, handling potential outliers and volatility jumps, and generating dynamic snapshots for model input. A performance-based healing mechanism further enhances the dynamic plane's adaptability and robustness.

**Data Preprocessing and Transformation:**

1. **Normalize P, V, T Data:** P, V, and T data within a rolling window are normalized to the range of -1 to +1. The specific normalization technique used will be documented.
2. **Percentile Clipping for Price:** Outliers in price data are clipped using a percentile-based approach. The specific percentile thresholds will be documented.
3. **Dynamic Rotation Implementation:** The coordinate system is rotated based on price movement. Further details on the rotation implementation are required.
4. **Local Movement Vector Calculation:** A local movement vector, which serves as input for the rotation calculations, is defined. The precise calculation method requires further specification.
5. **Rotation Matrix Calculation:** The rotation angle and corresponding rotation matrix are calculated based on the local movement vector. The specific algorithm requires clarification.
6. **Dynamic Origin Shift:** Coordinates are translated after rotation. The logic for determining the translation vector requires further detail.
7. **Dynamic Snapshot Generation:** Snapshots are generated after coordinate transformations. Details regarding image format, resolution, and the capture process need further definition.
8. **Rotation Artifacts Handling:** Strategies for mitigating potential artifacts introduced by the rotation will be documented.
9. **Volatility Jump Handling:** Sudden, significant price movements are handled to prevent destabilization of the dynamic plane. The specific approach requires further explanation.
10. **Consistent Axis Scaling:** Consistent scaling is maintained across the dynamic plane to prevent distortions from price or volatility changes. The scaling method will be documented.
11. **Pseudocode Pipeline:** Pseudocode for the dynamic snapshot generator will clarify the overall process.
12. **Rotating Snapshot Generator Module:** A modular component for the snapshot generator will promote code reusability and maintainability. Interface details and dependencies require further elaboration.

**Performance-Based Healing:**

The dynamic plane's responsiveness is further refined through a performance-based healing mechanism:

- **Prediction Correctness Tracking:** A mechanism tracks the accuracy of predictions (direction and magnitude of movement), using a simple scoring system (+1 for correct, 0 for incorrect).
- **Rolling Prediction Correctness Buffer:** A rolling buffer stores the correctness scores for the last N timesteps (N determined empirically). The buffer's average represents the mean prediction correctness.
- **True Prediction Value Tracking:** The raw prediction values are tracked and used in conjunction with the correctness tracking.
- **Dynamic Decay Rate Adjustment:** The decay rate for dynamic plane adjustments (rotation, translation) is dynamically adjusted based on the mean prediction correctness. Higher correctness results in a lower decay rate (faster adaptation), while lower correctness increases the decay rate (greater stability). This adjustment is directly proportional to predictive recovery, as indicated by the rolling buffer's average, avoiding arbitrary decay functions.

## C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane for data representation. This involves transforming price, volume, and time data, then applying Principal Component Analysis (PCA) to create a robust and informative representation of market dynamics.

**1. Data Transformation:**

The raw price, volume, and time data undergo the following transformations to prepare them for PCA:

- **Time (T):** Time is represented as the fractional elapsed time within the candlestick window. This is calculated by subtracting the minimum timestamp from each timestamp and dividing by the total time span within the window (the difference between the maximum and minimum timestamps). This approach normalizes time to the range [0, 1], handles irregular time intervals, and captures the rate of data arrival. This fractional time is then further transformed to the range [-1, 1] using: `(2 * time_frac) - 1`.

- **Price (P):** Price is transformed into window-relative returns, calculated as the log return relative to the first price in the window. This anchors the price series to zero at the beginning of the window, enabling comparisons across different stocks and time periods while mitigating the influence of absolute price values on the PCA. Subsequently, these log returns are normalized to the range [-1, 1]. This can be achieved through either clipping (dividing by the maximum absolute log return observed within the window) or min-max scaling based on the minimum and maximum log returns within the window. This process mitigates the effect of extreme price movements.

- **Volume (V):** To handle potential spikes and outliers, volume data undergoes a logarithmic transformation: `v_i' = log(1 + v_i)`. Following this, robust scaling is applied using the median and interquartile range (IQR). Specifically, the median volume is subtracted from each volume data point, and the result is divided by the IQR. This robust scaling ensures that the volume data contributes meaningfully to the PCA without being unduly influenced by extreme values.

**2. 3-Dimensional Matrix Construction for PCA:**

A 3-dimensional matrix is constructed using the transformed data. The columns of this matrix represent fractional elapsed time (T), window-relative returns (P), and scaled volume (V).

**3. Principal Component Analysis:**

PCA is performed on the constructed 3D matrix. This identifies the principal components that capture the most significant variance and correlations within the data, effectively reducing dimensionality while preserving essential information. These principal components then serve as input for further analysis and modeling.

This approach addresses several key challenges:

- **Scaling and Normalization:** Normalizing all features to the range [-1, 1] ensures they contribute equally to the PCA and prevents features with larger magnitudes from dominating.
- **Time Representation:** Fractional elapsed time handles irregular timestamps and captures the rate of data arrival.
- **Outlier Handling:** Logarithmic transformation and robust scaling of volume, along with the relative return calculation for price, mitigate the impact of extreme values.
- **Dynamic Data Representation:** The resulting principal components capture the dynamic interplay between price, volume, and time, providing a robust and concise representation of market movements.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation of candlestick and volume data, transforming it into a format suitable for CNN input. This transformation involves logarithmic returns, Principal Component Analysis (PCA) rotation, and normalization, creating a "dynamic snapshot" of market data.

Two distinct dynamic transformations are employed:

**1. Rotation-Based Dynamic Transformation:**

This transformation dynamically rotates the candlestick chart based on price movement, aligning the primary axis of change with the image plane.

- **Local Movement Vector Calculation:** A vector representing the direction and magnitude of price change within a 5-day window is calculated.
- **Rotation Matrix Calculation:** The vector determines the rotation angle used to construct the rotation matrix applied to the price and volume data.
- **Dynamic Origin Shift:** The origin is shifted to center the transformed data within the image plane.
- **Dynamic Snapshot Generation:** A new candlestick image, the "dynamic snapshot," is generated after rotation and translation, serving as CNN input. Five sequential image pairs (original and transformed) will be generated.
- **Artifact and Volatility Handling:** Strategies mitigate rotation artifacts and handle sudden large price movements (volatility jumps).
- **Consistent Axis Scaling:** Axis scaling remains consistent throughout the transformation to preserve relative magnitudes.

**2. PCA-Based Dynamic Transformation:**

This transformation uses PCA to reduce dimensionality while retaining crucial information, creating a 2D representation suitable for CNN input.

- **Data Normalization:** Time, log return, and log volume data are normalized to the range [-1, 1] to ensure equal feature contribution during PCA.
- **PCA Rotation:** PCA is applied to the normalized data to identify principal components and create a 2D representation.
- **Dynamic Snapshot Generation:** This transformed data forms the dynamic plane snapshot used as CNN input.

**Data Preprocessing for both Transformations:**

Prior to both transformations, the following preprocessing steps are applied:

- **Normalization:** Price (P), Volume (V), and Time (T, if applicable) are normalized to [-1, 1]. Price normalization utilizes log returns, while volume normalization involves a log transformation (`np.log1p(volume)`). Min-max scaling is then applied to achieve the desired range.
- **Percentile Clipping:** Outliers in both price (log returns) and volume (log-transformed) are clipped at the 5th and 95th percentiles before min-max scaling. This mitigates the influence of extreme values. The specific details of the log transformation for volume are detailed elsewhere in the document.

**Validation:**

To demonstrate the effectiveness of these dynamic plane representations, five example image pairs (original candlestick chart and corresponding transformed snapshot) will be generated, illustrating: an uptrend with rising volume, a downtrend with volume spikes, a reversal, sideways chop, and a breakout.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of the `DynamicPlaneGenerator` module, which transforms market data into a dynamic 2D representation for the predictive model. This module handles data normalization, principal component analysis (PCA), coordinate transformation, and image generation.

1. **Dynamic Rotation Implementation:** The `DynamicPlaneGenerator` dynamically rotates the coordinate system based on recent price movements using PCA on normalized market data. This aligns the primary axis with the dominant price action, effectively capturing market trends.

2. **Local Movement Vector Calculation:** A local movement vector, representing recent trends in price and volume, is calculated to determine the rotation angle for data transformation.

3. **Rotation Matrix Calculation:** The rotation angle, derived from the local movement vector, is used to calculate the rotation matrix for transforming the coordinate system.

4. **Dynamic Origin Shift:** After rotation, the origin is dynamically shifted to center the current market state within the generated image, providing a consistent perspective for the model.

5. **Dynamic Snapshot Generation:** A snapshot of the transformed data is generated as an image, representing the dynamic 2D plane, and serves as input to the predictive model.

6. **Rotation Artifact Handling:** Strategies are implemented to mitigate potential artifacts introduced by rotation, particularly during volatile price movements, ensuring image quality.

7. **Volatility Jump Handling:** Mechanisms are in place to handle sudden, large price movements (volatility jumps), preventing them from disproportionately influencing the transformation and maintaining stability.

8. **Consistent Axis Scaling:** Consistent scaling is maintained across both axes to prevent distortions and preserve the relative importance of price and volume.

9. **Pseudocode Pipeline:** A clearly defined pseudocode pipeline guides the implementation, ensuring a structured and reproducible development process. This pipeline will be documented separately.

10. **Rotating Snapshot Generator Module:** The `DynamicPlaneGenerator` is implemented as a modular and reusable component.

11. **Normalization of Price, Volume, and Time Data:** Price, Volume, and Time data are normalized to a uniform [-1, +1] range. Price is normalized using log-returns, while volume is log-transformed and robustly scaled. This ensures feature comparability and prevents any single dimension from dominating the PCA calculation.

12. **Percentile Clipping for Price:** Extreme price outliers are clipped using a percentile-based approach to mitigate the impact of extreme market events and ensure the stability of the dynamic plane generation.

## Dynamic Plane Implementation

This section details the implementation of a dynamic plane transformation applied to price, volume, and time data to create a visual representation of market dynamics for input to a predictive model (e.g., a Vision Transformer). This transformation aims to capture the most relevant market movements and provide a consistent input representation across different time steps.

1. **Data Normalization and Clipping:** Price, Volume, and Time data are normalized to a range of -1 to +1. Extreme price outliers are clipped using a percentile-based approach to prevent them from unduly influencing the transformation. This normalization and clipping ensure that each variable contributes proportionally to the analysis and enhances robustness against extreme market events.

2. **Principal Component Analysis (PCA):** PCA is performed on the normalized data within a recent time window. The two principal components (PC1 and PC2) define the axes of a dynamic 2D plane, aligning it with the most significant correlated market movements.

3. **Dynamic Origin Shift:** The origin of the 2D plane is dynamically shifted to the most recent data point in the time series. This re-centering focuses the model on relative movements rather than absolute values.

4. **Rotation Matrix Calculation:** The rotation matrix, derived from the eigenvectors obtained during PCA, captures the transformation from the standard coordinate system to the PCA-defined plane.

5. **Local Movement Vector Calculation:** After projecting the recent market history onto the 2D plane, the last data point serves as the new origin (0,0). Subsequent movements are calculated relative to this dynamically updated origin. This vector can be used for further analysis or incorporated into the model's input.

6. **Dynamic Snapshot Generation:** The transformed 2D market flow is rendered as an image, potentially using a candlestick or Heiken-Ashi representation. This image serves as input to the predictive model.

7. **Artifact and Volatility Handling:** Strategies will be implemented to mitigate potential artifacts from the rotation and projection, such as data sparsity or distortions. Mechanisms will also address sudden, large price movements (volatility jumps) that might skew the PCA calculation.

8. **Consistent Axis Scaling:** The axes of the 2D plane maintain consistent scaling across different time steps, ensuring consistent model input.

9. **Rotating Snapshot Generator Module:** A dedicated module will encapsulate the functionality of the dynamic snapshot generator, handling data transformation, plane rotation, and image generation.

10. **Pseudocode Pipeline:** Pseudocode will be provided, outlining the entire process from data normalization and PCA to image generation.

This dynamic plane transformation is intended to be applied consistently across various timeframes (e.g., 10-minute, daily, weekly, monthly). Further research and experimentation will be conducted to optimize its impact on the model's predictive power.
This section details the implementation of the dynamic plane, a core component for visualizing market data and preparing it for a Vision Transformer (ViT). This plane dynamically transforms price, volume, and time data into a 2D coordinate system, generating a sequence of images that capture market dynamics.

**1. Data Normalization and Outlier Handling:** Price (P), Volume (V), and Time (T) data are normalized to a [-1, +1] range to ensure consistent input scaling for the model and prevent features with larger magnitudes from dominating. Outliers in price data are mitigated using percentile clipping to further stabilize the dynamic plane generation and prevent extreme rotations caused by large price fluctuations. The specific percentile thresholds will be determined through data analysis.

**2. Local Movement Vector Calculation:** A local movement vector, representing the change in price and volume over a given time step, is calculated. This vector informs both the rotation angle and the translation of the dynamic plane's origin.

**3. Dynamic Rotation Implementation:** The coordinate system of the dynamic plane rotates based on the calculated local movement vector. The rotation angle is derived from the direction and magnitude of the price and volume changes, allowing the model to visually capture trends and momentum.

**4. Rotation Matrix Calculation:** The rotation angle is used to calculate the rotation matrix. This matrix is then applied to the data points, effectively rotating the coordinate system.

**5. Dynamic Origin Shift:** The origin of the dynamic plane is dynamically shifted based on the local movement vector. This translation keeps the current data point centered, allowing the model to focus on relative changes in price and volume.

**6. Dynamic Snapshot Generation:** Following the rotation and translation, a snapshot of the transformed data is generated as an image. This process is repeated for each time step, creating a sequence of images representing the dynamic evolution of market data.

**7. Artifact and Volatility Handling:** Strategies are implemented to mitigate potential artifacts introduced by the rotation process, ensuring the visual representation remains accurate. Mechanisms are also in place to handle sudden, large price movements (volatility jumps) and prevent extreme rotations or distortions in the dynamic plane.

**8. Consistent Axis Scaling:** Consistent scaling of the Price, Volume, and Time axes is maintained throughout the dynamic transformations, preserving the relationships between these variables for model interpretability.

**9. Pseudocode and Module Implementation:** Clear and detailed pseudocode will be provided for the dynamic snapshot generator, outlining the sequence of operations performed at each time step. This functionality will be encapsulated within a dedicated, reusable module to promote code organization and maintainability.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of the dynamic plane, a core component of the model's perception system. The dynamic plane transforms the representation of market data, creating a dynamic, locally-centered perspective for the model by rotating and translating the coordinate system based on recent price action. This allows the model to focus on current market dynamics and adapt to changing trends.

1. **Dynamic Rotation Implementation:** The coordinate system is dynamically rotated to align its primary axis with the dominant price trend. This rotation is based on a local movement vector calculated from recent price fluctuations. A smoothing mechanism, potentially using an exponential moving average applied to the rotation matrix, mitigates jerky transitions, particularly during periods of high volatility. A user-adjustable smoothing factor provides control over the degree of smoothing.

2. **Local Movement Vector Calculation:** A local movement vector, representing the direction and magnitude of recent price changes within a defined local window, is calculated. This vector is a key input for determining the rotation angle and dynamic plane adjustments.

3. **Rotation Matrix Calculation:** The rotation angle is derived from the local movement vector, and the corresponding rotation matrix is calculated. This matrix is applied to transform the coordinates of the data points within the local window.

4. **Dynamic Origin Shift:** The coordinate system's origin is dynamically shifted to maintain a local perspective, centering the plane on the most recent data. This emphasizes the current market context for the model.

5. **Dynamic Snapshot Generation:** Following the rotation and translation transformations, a snapshot of the transformed data is generated. This snapshot, representing the market data from the dynamic plane's perspective, is then fed as an image input to the model.

6. **Rotation Artifact Handling:** Strategies are implemented to mitigate potential artifacts, such as distortions or aliasing, introduced by the rotation process, ensuring data integrity.

7. **Volatility Jump Handling:** Mechanisms, such as adaptive scaling or smoothing techniques, are implemented to handle sudden, large price movements (volatility jumps) and prevent extreme distortions of the dynamic plane.

8. **Consistent Axis Scaling:** Consistent scaling is maintained across both axes of the dynamic plane, ensuring the relative importance of price, time, and volume remains consistent despite transformations.

9. **Data Normalization:** Price (P), Volume (V), and Time (T) data are normalized to the range of -1 to +1 to improve model training stability and performance.

10. **Percentile Clipping for Price:** Outlier price values are clipped using a percentile-based approach to mitigate the impact of extreme price fluctuations on the dynamic plane transformations and model training.

11. **Rotating Snapshot Generator Module:** A dedicated module encapsulates the rotating snapshot generation process, promoting code organization, reusability, and maintainability.

12. **Pseudocode Pipeline:** Pseudocode documenting the entire snapshot generation pipeline, from data input to image output, is provided for clarity and to facilitate further development and review.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of a dynamic plane representation for candlestick data, enabling context-aware analysis of price movements.

1. **Dynamic Rotation Implementation:** The coordinate system of the candlestick data is dynamically rotated based on the overall price movement. This allows the model to focus on relative price changes rather than absolute values.

2. **Local Movement Vector Calculation:** A local movement vector represents the short-term price trend. This vector determines the rotation angle for the dynamic plane.

3. **Rotation Matrix Calculation:** The rotation angle, derived from the local movement vector, is used to calculate the rotation matrix. This matrix is applied to the candlestick data to perform the rotation.

4. **Dynamic Origin Shift:** The origin of the coordinate system is dynamically shifted to maintain the current price action within the center of the frame. This preserves the local context of the price movement.

5. **Dynamic Snapshot Generation:** Images of the transformed candlestick data are generated after applying the dynamic rotation and origin shift. These dynamic snapshots serve as input to the model.

6. **Rotation Artifact Handling:** Potential artifacts introduced by rotation, such as distortions or aliasing, are mitigated through appropriate strategies.

7. **Volatility Jump Handling:** Mechanisms are implemented to handle sudden, large price movements (volatility jumps), ensuring the dynamic plane adapts smoothly without information loss.

8. **Consistent Axis Scaling:** Consistent scaling is maintained across both axes of the dynamic plane, ensuring proportional representation of price movements regardless of the rotation angle.

9. **Pseudocode Pipeline:** Pseudocode is provided for the dynamic snapshot generator, outlining the transformation process. This serves as a blueprint for implementation and clarifies the underlying logic.

10. **Rotating Snapshot Generator Module:** A dedicated module encapsulates the logic for dynamic rotation, origin shifting, and snapshot generation, promoting code organization and maintainability.

11. **Normalization of Price, Volume, and Time Data:** Price (P), Volume (V), and Time (T) data are normalized to the range of -1 to +1 to ensure consistent input scaling and prevent features with larger magnitudes from dominating the learning process.

12. **Percentile Clipping for Price:** Outlier price values are clipped using a percentile-based approach to handle extreme price movements and prevent them from skewing the dynamic plane representation.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of a dynamic plane transformation applied to candlestick data. This transformation aims to enhance the model's ability to recognize patterns irrespective of absolute price levels. Given the computational intensity of image generation and processing, cost optimization strategies, such as caching and efficient pseudocode design, are crucial.

The dynamic plane implementation involves rotating and translating the coordinate system based on price movements, creating a dynamic visual representation of the data. This process is detailed below:

1. **Dynamic Rotation Implementation:** The coordinate system is dynamically rotated based on observed price movement. The rotation angle is calculated relative to a reference point (e.g., the starting price). The specific methodology for determining the reference point and calculating the angle will be detailed in subsequent documentation.

2. **Local Movement Vector Calculation:** A local movement vector represents the change in price over a specific period. This vector is derived from the price difference between two consecutive data points. The specific calculation and the time window used will be detailed in the accompanying technical documentation.

3. **Rotation Matrix Calculation:** A 2D rotation matrix is constructed using the calculated rotation angle and standard trigonometric functions (sine and cosine). The precise matrix construction and its application to the data points will be documented.

4. **Dynamic Origin Shift:** The origin of the coordinate system is dynamically shifted to ensure relevant price movements remain within the viewport. The specific translation method will be elaborated on in further technical specifications.

5. **Dynamic Snapshot Generation:** After applying the rotation and translation, a snapshot of the transformed data is generated for further analysis. The format and specifics of this snapshot will be thoroughly documented.

6. **Rotation Artifacts Handling:** Potential artifacts introduced by rotation, such as aliasing or distortion, will be addressed through mitigation strategies like interpolation or anti-aliasing techniques. The chosen method and its implementation will be detailed.

7. **Volatility Jump Handling:** A mechanism will handle sudden, large price movements (volatility jumps) to ensure the visualization remains informative and avoids visual discontinuities. Details of this mechanism, including any thresholds or smoothing techniques, will be provided.

8. **Consistent Axis Scaling:** A scaling mechanism will maintain consistent axis scaling while accommodating the dynamic nature of the plane and preserving relative proportions. The specifics of the scaling algorithm will be documented.

9. **Data Normalization:** Price (P), Volume (V), and Time (T) data will be normalized to the range of -1 to +1. The specific normalization technique for each data type will be detailed.

10. **Percentile Clipping for Price:** Outliers in price data will be handled using percentile clipping. The chosen percentile thresholds will be specified and justified in the accompanying documentation.

11. **Pseudocode Pipeline:** Detailed pseudocode for the dynamic snapshot generation pipeline will outline the step-by-step process, including calculations and transformations. This pseudocode will guide efficient implementation and architecture design for image generation and processing, incorporating caching mechanisms to avoid redundant computations.

12. **Rotating Snapshot Generator Module:** A dedicated software module will encapsulate the dynamic snapshot generator functionality. Its interface, dependencies, and internal workings, including caching strategies, will be documented.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of the `DynamicPlaneGenerator`, a crucial component responsible for transforming numerical candlestick data into a visual representation suitable for input to the Core ML model. This component leverages Apple's Metal framework for GPU-accelerated performance. The `DynamicPlaneGenerator` encapsulates the transformation process, promoting modularity and enabling independent optimization.

**Conceptual Overview**

The `DynamicPlaneGenerator` manipulates the coordinate system of the candlestick data based on price and volume movements. By dynamically rotating and translating the data, the generated images emphasize relative changes rather than absolute price levels, potentially enhancing the model's ability to recognize patterns and trends.

**Implementation Details**

The `DynamicPlaneGenerator`, implemented as a native Swift module, utilizes Metal for the following key operations:

1. **Local Movement Vector Calculation:** Computes the local movement vector based on price and volume changes. This vector informs the subsequent rotation calculations.

2. **Rotation Matrix Calculation:** Calculates the rotation angle and corresponding rotation matrix based on the local movement vector.

3. **Dynamic Rotation Implementation:** Rotates the candlestick data's coordinate system using the calculated rotation matrix.

4. **Dynamic Origin Shift:** Translates the coordinate system's origin.

5. **Dynamic Snapshot Generation:** Renders the transformed data into images or tensors formatted for Core ML compatibility.

6. **Rotation Artifact Handling:** Mitigates potential visual artifacts introduced by the rotation process. Details on the specific mitigation techniques will be provided in subsequent documentation.

**Data Normalization and Performance**

Prior to transformation, the price, volume, and time data are normalized to a consistent range (e.g., -1 to +1) to ensure uniform scaling and prevent distortions. Leveraging Metal for GPU acceleration ensures efficient processing of these transformations.

### C. Dynamic Plane Implementation

This section details the implementation of the Dynamic Plane Generator, a core component for transforming financial time series data (price, volume, etc.) into a visual representation suitable for CNN analysis. This implementation uses a decoupled architecture, allowing for deployment across various platforms, including Progressive Web Apps (PWAs).

**Dynamic Plane Generation Process**

The Dynamic Plane Generator transforms raw numerical data into a dynamic visual representation through the following steps:

1. **Data Normalization:** Price, volume, and time data are normalized to a range of -1 to +1 to ensure consistent scaling.
2. **Percentile Clipping (Price):** Outliers in price data are clipped using a percentile-based approach to mitigate the impact of extreme price movements.
3. **Dynamic Rotation Implementation:** The plane's coordinate system is dynamically rotated based on underlying price movement. This involves:
   - **Local Movement Vector Calculation:** Calculating a vector representing the immediate change in price and volume.
   - **Rotation Matrix Calculation:** Determining the rotation angle from the movement vector and constructing the corresponding rotation matrix.
4. **Dynamic Origin Shift:** The coordinate system's origin is dynamically shifted to maintain focus on the most recent data.
5. **Dynamic Snapshot Generation:** Images of the transformed plane are generated after rotation and translation. This process handles potential rotation artifacts and volatility jumps.
6. **Rotation Artifacts Handling:** Strategies are implemented to mitigate artifacts introduced by the rotation, ensuring data accuracy.
7. **Volatility Jump Handling:** Mechanisms are in place to handle sudden, large price movements, preventing visualization distortions.
8. **Consistent Axis Scaling:** Consistent axis scaling is maintained throughout the transformation process, preserving relative magnitudes of price and volume changes.

**Decoupled Architecture and Platform Portability**

A decoupled architecture is employed: a Python backend serves the raw numerical data and the latest model, while the frontend (PWA or native mobile app) handles the Dynamic Plane Generator's computations. This approach ensures flexibility and portability.

**PWA Implementation Details:**

For PWA deployment, the Dynamic Plane Generator is re-implemented in TypeScript/JavaScript using libraries like `ndarray` or a lightweight matrix math library (e.g., for PCA). The pseudocode for the Dynamic Plane Generator will be documented, and a modular `Rotating Snapshot Generator` module will be designed for easy integration and maintenance.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of a dynamic plane representation of market data, crucial for generating insightful visualizations and potentially serving as input to machine learning models. This involves transforming and rendering data points onto a 2D plane that dynamically adjusts based on market movements.

- **Data Normalization:** Price (P), Volume (V), and any other relevant time-series data (T) will be normalized to a range of -1 to +1. This ensures consistent input scaling for transformations and prevents features with larger magnitudes from unduly influencing the visualization. Outlier price values will be clipped using a percentile-based approach to further enhance robustness.

- **Local Movement Vector Calculation:** A local movement vector will be calculated to quantify the change in price and potentially other relevant indicators (e.g., volume, volatility) over a given time window. This vector serves as the basis for determining the dynamic rotation angle.

- **Dynamic Rotation Implementation:** The coordinate system of the plane will be rotated based on the direction and magnitude of the local movement vector. This allows for a visually intuitive representation of trends, emphasizing the dominant direction of market activity. The rotation will be implemented using a rotation matrix calculated from the determined angle.

- **Dynamic Origin Shift:** The origin of the coordinate system will be dynamically shifted to keep the most recent data point centered. This maintains focus on the latest market activity while preserving a visual history of past movements.

- **Consistent Axis Scaling:** Consistent scaling of the axes will be maintained across different time periods and market conditions, despite the dynamic rotations and translations. This ensures accurate interpretation of the visualization.

- **Artifact Mitigation:** Potential artifacts introduced by the rotation, such as distortion or aliasing, will be addressed through appropriate mitigation techniques. Strategies to handle sudden, large price movements (volatility jumps) will also be incorporated, potentially involving scaling adjustments or alternative transformation strategies.

- **Dynamic Snapshot Generation:** After applying the transformations, snapshots of the dynamic plane will be generated and rendered as images. These snapshots form the basis for visual analysis and potential input to machine learning models.

- **Rotating Snapshot Generator Module:** A dedicated `RotatingSnapshotGenerator` module will encapsulate the transformation and rendering logic, promoting code organization and reusability. Pseudocode for this module will be provided for clarity.

- **Cross-Platform Implementation (Dart, TensorFlow Lite, Core ML):** The dynamic plane generation and user interface will be developed in Dart for cross-platform compatibility. Platform-specific model execution will be handled through a `MethodChannel` calling a `runPrediction` method. This method will interact with TensorFlow Lite on Android (managed by a Kotlin `TFLiteHandler` class) and Core ML on iOS. Model management strategies, including storage, updates, and version control for both `.tflite` and `.mlmodel` formats, will be defined. The portability of Core ML models to other platforms will be investigated, and solutions for cross-platform compatibility (e.g., model conversion, cross-platform training frameworks) will be explored.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of the dynamic plane, a crucial component for transforming candlestick data into a format suitable for the model. This dynamic approach enhances the representation of price action by adjusting the coordinate system in response to market movements. Crucially, this implementation must be compatible with on-device training within the Core ML framework.

To ensure proper functionality, several checks and verifications are required. Prior to full-scale training, a "smoke test" using a dummy model and a single training epoch is essential. This test is not intended for drawing inferences but serves as a critical validation step, confirming the integrity of the entire training and update pipeline. This smoke test will verify key components of the dynamic plane implementation.

## C. Dynamic Plane Implementation (Specialization)

This section details the implementation of a dynamic plane, augmented by a Cognitive Threat Analysis Module (CTAM) to handle "shocker events"—unexpected occurrences in financial data requiring a proactive response. The dynamic plane transforms the data representation based on price movements, while the CTAM provides an additional layer of analysis to identify and react to anomalous patterns.

The core dynamic plane implementation encompasses the following:

- **Data Normalization:** Normalizes price (P), volume (V), and time (T) data to the -1 to +1 range to improve model stability and performance. Price outliers are clipped using percentile clipping to mitigate the influence of extreme values on the dynamic plane transformations.
- **Local Movement Vector Calculation:** Calculates a local movement vector that quantifies the direction and magnitude of recent price changes. This vector serves as the basis for determining the rotation angle.
- **Rotation Matrix Calculation:** Calculates the rotation angle and corresponding rotation matrix using the local movement vector.
- **Dynamic Rotation:** Rotates the coordinate system of the data representation using the calculated rotation matrix. This rotation aims to align the plane with the primary direction of price action. Strategies are implemented to mitigate potential rotation artifacts, such as distortions or loss of information, and to handle sudden, large price movements (volatility jumps) gracefully.
- **Dynamic Origin Shift:** Translates the origin of the coordinate system dynamically, potentially centering it around the current price or another relevant point. This translation ensures that the focal point of analysis remains centered. Consistent axis scaling is maintained throughout these transformations.
- **Dynamic Snapshot Generation:** Generates images of the transformed data after rotation and translation. These images serve as input for subsequent model processing. A modular and reusable Rotating Snapshot Generator module encapsulates this functionality. Pseudocode outlining the entire process from input data to transformed image output will be provided in [section/appendix reference].

The CTAM enhances the dynamic plane's responsiveness to unexpected market events as follows:

- **"Shocker" Event Definition:** Defines and quantifies "shocker events" based on characteristics such as volatility spikes, volume anomalies, and rapid price changes. Specific metrics and thresholds are established to identify these events.
- **CTAM Development:** A Cognitive Threat Analysis Module (CTAM) is developed. Operating separately from the DynamicPlaneGenerator, the CTAM focuses on visual detection and analysis of unexpected events within the generated snapshots. Lightweight CNN models are employed for real-time anomaly detection, balancing accuracy and computational efficiency.
- **CTAM Integration:** The CTAM is integrated into the overall framework. The DynamicPlaneGenerator's response to the CTAM's "Threat Level" score is determined. This response might involve modifications to the smoothing function or learning rate used in the dynamic plane transformations. This integration aims to make the system more resilient to extreme market fluctuations.

Regarding the initial points about data pipeline connection and on-device training loop execution, these appear misplaced in this section focusing on the dynamic plane and CTAM implementation. They likely belong in a section discussing model training and validation. The concern about the necessity of on-device training with a dummy model should also be addressed in a more appropriate section discussing training strategy and resource utilization. Please relocate these points accordingly.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation of market data, using specialized services for numerical transformations. This representation transforms and normalizes price, volume, and time data, then rotates the coordinate system based on recent market movements to generate dynamic snapshots for model input.

1. **Local Movement Vector Calculation:** The local movement vector, representing the primary direction of market movement, is calculated based on the change in price over a defined time window (e.g., 5 days).

2. **Rotation Matrix Calculation:** Using the local movement vector, the corresponding rotation angle is calculated. This angle is then used to generate the rotation matrix.

3. **Dynamic Rotation Implementation:** The `Coordinate_Rotation_Service` applies the calculated rotation matrix to the price, volume, and time data, dynamically aligning the plane with the direction of market change.

4. **Dynamic Origin Shift:** After rotation, the coordinate system's origin is dynamically shifted to center the current data point. This provides a consistent reference frame for the model.

5. **Normalize P, V, T Data:** The `Normalization_Service` normalizes the price (P), volume (V), and time (T) data to a consistent range (-1 to +1). This ensures that the differing magnitudes of these variables do not unduly influence the rotation and subsequent analysis.

6. **Percentile Clipping for Price:** To handle extreme price outliers, percentile clipping is applied to the price data before normalization. This prevents extreme values from skewing the normalization process and impacting the dynamic plane representation.

7. **Dynamic Snapshot Generation:** After rotation, translation, and normalization, a snapshot of the transformed data is generated. This snapshot serves as input for the model.

8. **Rotation Artifacts Handling:** Potential artifacts introduced by the rotation, such as distortions or wrapping, are mitigated through strategies like interpolation or alternative projection methods.

9. **Volatility Jump Handling:** Sudden, large price movements (volatility jumps) are handled to prevent inaccuracies. These jumps can significantly impact the local movement vector and rotation, requiring specialized handling.

10. **Consistent Axis Scaling:** Consistent scaling of the price, volume, and time axes is maintained throughout the transformations to preserve the relative relationships between these variables.

11. **Rotating Snapshot Generator Module:** The `Rotating_Snapshot_Generator` module encapsulates the entire dynamic plane implementation, integrating the `Normalization_Service`, and `Coordinate_Rotation_Service`. This modular design promotes code reusability and maintainability.

12. **Pseudocode Pipeline:** Detailed pseudocode is provided, outlining the entire dynamic snapshot generation process, including data normalization, rotation, translation, and snapshot creation. This documentation aids in understanding and implementation.

## C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation of market data, crucial for capturing evolving relationships between price, volume, and time. This representation involves transforming candlestick data through rotation and translation of the coordinate system, orchestrated by the `Workflow_Broker` facilitator service. While the desired dynamic rotation and translation functionalities are currently unavailable, this section outlines the intended design and implementation plan, along with the core dynamic plane services. Addressing the unimplemented rotation and translation steps (outlined below) requires further research and development or alternative approaches.

### C.1 Core Dynamic Plane Services

These services create the initial dynamic plane representation from raw market data:

1. **`NormalizeWindow`**: This service normalizes the raw numerical input array representing market data within a specific time window. It accepts the raw data and a configuration dictionary as input and returns the normalized array. This normalization ensures consistent scaling across different time windows and prevents features with larger values from dominating the analysis. This service avoids dependencies on `google-cloud-storage`, `google-cloud-firestore`, and `requests` to maintain a clean and focused functionality.

2. **`ComputePrincipalComponents`**: After normalization, this service identifies the primary axes of variation within the data. It takes the normalized array as input and computes the top two principal component vectors using `numpy`. These vectors represent the directions of greatest variance and form the basis of the dynamic plane.

3. **`ProjectToPlane`**: This service projects the original market data onto the dynamic plane defined by the principal components. It takes the original data and the calculated basis vectors (from `ComputePrincipalComponents`) as input and returns the 2D projected data. This projection simplifies the data representation while preserving the most significant variations, making it suitable for visualization and subsequent model input.

4. **`TrainOneEpoch`**: This service performs model training on the projected data. It receives the current model artifact (as bytes), training data tensors derived from the projected data, and a configuration dictionary. It returns updated model artifact bytes after a single epoch of training. This modular design isolates the training process, promoting code reusability and simplified testing.

### C.2 Dynamic Transformation Workflow (Planned)

The following steps, managed by the `Workflow_Broker`, describe the intended dynamic transformation process. These steps require further implementation:

1. **Data Preprocessing:** Normalize Price (P), Volume (V), and Time (T) data to the range of -1 to +1 and apply percentile clipping to the price data to mitigate outlier influence.

2. **Dynamic Rotation:** Rotate the candlestick data's coordinate system based on recent price movement using a calculated local movement vector and rotation matrix. This aims to align the plane with the dominant trend. Implement strategies to handle rotation artifacts.

3. **Dynamic Origin Shift:** Translate the coordinate system's origin to center the data after rotation, emphasizing relative price fluctuations.

4. **Dynamic Snapshot Generation:** Generate new candlestick images representing the transformed data. Address potential distortions caused by volatility jumps. Maintain consistent axis scaling throughout the transformation process.

5. **Documentation and Modularization:** Provide clear pseudocode for the entire pipeline and encapsulate the rotating snapshot generator functionality within a dedicated module.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of the dynamic plane, a core component of the model's specialization that captures the evolving relationships between price, volume, and time. This client-side implementation leverages device resources for image generation and transformation, reducing server load and enabling rapid iteration. This aligns with the architectural principle of client-side heavy lifting. The dynamic plane interacts with a lightweight backend orchestrator (facilitator) for data retrieval and sends updated model weights back to the universal model hub after on-device training. The native app frontend interacts directly with this module.

The dynamic plane operates by interpreting market data on a 2D plane whose orientation and center continuously adjust based on changes in time, price, and volume. Instead of static candlestick images, the model receives dynamic snapshots of this transformed plane. The key functionalities are:

1. **Dynamic Rotation Implementation:** The 2D plane's coordinate system rotates based on the dominant direction of price movement, aligning the model's perspective with the current market trend.

2. **Local Movement Vector Calculation:** A local movement vector, representing the immediate change in price and volume over a specific time interval, informs the plane's rotation and translation.

3. **Rotation Matrix Calculation:** The local movement vector determines the rotation angle, and a corresponding rotation matrix transforms the plane's coordinates.

4. **Dynamic Origin Shift:** The 2D plane's origin dynamically shifts to maintain focus on recent market activity, based on the local movement vector and potentially other factors like volatility.

5. **Dynamic Snapshot Generation:** After rotation and translation, a snapshot of the transformed 2D plane is generated as input for the model, capturing the dynamic relationship between time, price, and volume.

6. **Rotation Artifacts Handling:** Strategies mitigate potential artifacts introduced by rotation, ensuring a clean and accurate market data representation.

7. **Volatility Jump Handling:** Mechanisms handle sudden, large price movements (volatility jumps), preventing overreaction to transient market fluctuations.

8. **Consistent Axis Scaling:** Consistent scaling of the time, price, and volume axes ensures stable model performance and prevents distortions due to magnitude changes.

9. **Pseudocode Pipeline:** Clear pseudocode will be provided for the dynamic snapshot generation process, clarifying the sequence of operations.

10. **Rotating Snapshot Generator Module:** A dedicated module encapsulates the rotating snapshot generator functionality, promoting modularity and code reusability.

11. **Data Normalization:** Price (P), Volume (V), and Time (T) data are normalized to a range of -1 to +1, ensuring equal contribution to the model's interpretation and preventing scale-related dominance.

12. **Percentile Clipping for Price:** Percentile-based clipping of price outliers prevents extreme price movements from unduly influencing the rotation and translation.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of the dynamic plane, a crucial component for visualizing and interpreting market dynamics. While the current implementation focuses on price and volume, future iterations will integrate order book features and real-time tick data for enhanced accuracy and insight.

1. **Dynamic Rotation Implementation:** The coordinate system is rotated based on price movement to align the plane with the dominant trend, facilitating visualization of price action relative to this trend. Future integration of derived order book features could further refine the rotation logic.

2. **Local Movement Vector Calculation:** The local movement vector, representing the immediate direction and magnitude of price action, is calculated based on recent price changes. Incorporating real-time tick data will improve the accuracy of this vector.

3. **Rotation Matrix Calculation:** The rotation angle and corresponding rotation matrix are calculated based on the local movement vector. This matrix transforms the plane's coordinates.

4. **Dynamic Origin Shift:** The plane's coordinates are dynamically translated, potentially centering the origin on the current price or another relevant point.

5. **Dynamic Snapshot Generation:** Images of the transformed plane are generated after rotation and translation, capturing the dynamic evolution of the market.

6. **Rotation Artifacts Handling:** Measures are implemented to mitigate potential artifacts introduced by rotation, such as distortions or aliasing.

7. **Volatility Jump Handling:** Strategies are implemented to handle sudden price movements or volatility jumps, ensuring the dynamic plane remains stable and informative. Real-time data feeds will enable quicker reactions to market fluctuations.

8. **Consistent Axis Scaling:** Consistent axis scaling is maintained to prevent misleading visualizations.

9. **Pseudocode Pipeline:** Pseudocode will be provided for the dynamic snapshot generator, outlining the sequence of operations. This will eventually incorporate order book feature derivation.

10. **Rotating Snapshot Generator Module:** A modular and reusable component for the dynamic snapshot generator will be designed to accommodate future integration with order book feature derivation.

11. **Normalization of P, V, T Data:** Price (P), volume (V), and time (T) data are normalized to a -1 to +1 range for consistent processing and visualization. Normalized order book features will complement this normalization in the future.

12. **Percentile Clipping for Price:** Percentile clipping handles price outliers, preventing them from distorting the visualization.

The integration of order book features and real-time tick data, as outlined in the project checklist, will significantly enhance the Dynamic Plane implementation, providing a more comprehensive and accurate representation of market conditions. This enhanced representation will ultimately enable more informed decision-making.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of a dynamic plane incorporating order book data beyond traditional price and volume. Due to limitations in the available market depth data, initial spread-based calculations were discarded. Instead, the implementation leverages order quantity and count at different price levels to enrich the dynamic plane representation. Specifically, a new dimension derived from order book imbalance is incorporated. This is achieved through… (Continue with the explanation of the order book imbalance implementation).

### Order Book Feature Integration and Dynamic Plane Implementation

This section details the integration of order book features and the implementation of a dynamic plane, crucial components for representing market dynamics and applying computer vision techniques.

**Order Book Feature Extraction:**

1. **Order Book Imbalance (OBI) Calculation:** A new service, `CalculateOrderBookImbalance`, will be implemented to calculate the OBI. This service takes the market depth dictionary as input and outputs a normalized float between -1.0 (selling pressure) and +1.0 (buying pressure).

2. **Market Depth Heatmap Visualization:** The `GenerateDepthQuantityHeatmap` service will generate a heatmap visualization of the order book. This heatmap will consist of 10 rows (5 bid levels and 5 ask levels) and N columns representing the last N timesteps. Color intensity will correspond to the order quantity at a given price level and time.

3. **Market Depth Anomaly Detection:** A Convolutional Neural Network (CNN), `MarketDepthAnomalyDetector`, will process the generated heatmaps to identify visual patterns indicative of market shocks or unusual activity.

**Dynamic Plane Implementation:**

The dynamic plane represents price, volume, and time data, incorporating the calculated OBI as a fourth dimension. This representation enhances the model's understanding of market dynamics. The following steps outline the implementation:

1. **Data Normalization:** Price (P), Volume (V), Time (T), and OBI data will be normalized to the range of -1.0 to +1.0. This ensures consistent input scaling and prevents any single feature from dominating. Outlier price values will be clipped using a percentile-based approach to further manage extreme price movements.

2. **Local Movement Vector Calculation:** A local movement vector, representing short-term price trajectory, will be calculated from recent price changes.

3. **Dynamic Rotation Implementation:** The plane's coordinate system will be dynamically rotated based on the local movement vector, aligning the primary axis with the dominant price action. This highlights trends and reversals.

4. **Rotation Matrix Calculation:** The rotation angle, derived from the local movement vector, will be used to construct a rotation matrix applied to the data points.

5. **Dynamic Origin Shift:** The coordinate system's origin will be dynamically shifted to center the most recent data point, maintaining focus on the latest price action.

6. **Dynamic Snapshot Generation:** After rotation and translation, a snapshot of the transformed data will be generated as input to the model.

7. **Artifact and Volatility Handling:** Measures will be implemented to mitigate rotation artifacts (distortions or blurring) and handle volatility jumps, ensuring accurate representation of market behavior.

8. **Consistent Axis Scaling:** Consistent scaling will be maintained across all axes (Time, Price, Volume, and OBI) to preserve the relative importance of each dimension.

9. **Rotating Snapshot Generator Module:** A dedicated module, `RotatingSnapshotGenerator`, will encapsulate the snapshot generation logic, ensuring modularity and maintainability. This module will provide a pseudocode representation of the entire pipeline for clarity and reproducibility.

### C. Dynamic Plane Implementation (Specialization)

This section details the implementation of the dynamic plane generation, a core component of the SCoVA (Snapshot Computer Vision Algorithm). SCoVA utilizes discrete, dynamically generated visual snapshots of the market rather than a continuous time series. These snapshots incorporate asymmetric feature engineering to enhance the model's understanding of market dynamics, differentiating between periods of organic growth and decline. The following steps outline the process:

1. **Asymmetric Feature Integration:** The `CalculateAsymmetricFeatures` specialist service provides Upside and Downside Volatility measures. These measures influence the dynamic plane transformations, allowing the model to distinguish between periods of increased upward or downward price movement.

2. **Dynamic Rotation Implementation:** The coordinate system is dynamically rotated based on observed price movement and modulated by the asymmetry features. For instance, higher Upside Volatility might lead to a smaller clockwise rotation, while higher Downside Volatility could result in a larger counter-clockwise rotation.

3. **Local Movement Vector Calculation:** A local movement vector quantifies the direction and magnitude of price changes within the snapshot window. This vector is weighted by the asymmetry features, emphasizing downward pressure during periods of high Downside Volatility.

4. **Rotation Matrix Calculation:** The rotation angle, calculated from the weighted local movement vector, is used to construct the rotation matrix applied to the snapshot data points.

5. **Dynamic Origin Shift:** The coordinate system's origin is dynamically shifted to maintain focus on relative changes. This translation, while not directly influenced by asymmetry features, ensures the snapshot remains centered on relevant market conditions.

6. **Dynamic Snapshot Generation:** The final dynamic snapshot, representing the transformed market data and incorporating the influence of asymmetric features, is generated. This snapshot serves as input to the computer vision model, along with the asymmetry features provided as context.

7. **Rotation Artifacts Handling:** Potential artifacts from rotation (aliasing or distortion) are mitigated to ensure visual integrity.

8. **Volatility Jump Handling:** Mechanisms handle sudden, large price movements to prevent extreme rotations and maintain dynamic plane stability.

9. **Consistent Axis Scaling:** Consistent scaling across all axes ensures accurate visual representation and prevents features with larger magnitudes from dominating the learning process. Price (P), Volume (V), and potentially Time (T) data are normalized to a range of -1 to +1. Outlier price values are managed using percentile clipping.

10. **Pseudocode Pipeline:** Pseudocode will be provided to outline the complete dynamic snapshot generation process.

11. **Rotating Snapshot Generator Module:** A dedicated module encapsulates the dynamic snapshot generator functionality for code organization and reusability.
    The Dynamic Plane Implementation enhances the Vision Transformer (ViT) with contextual market information, crucial for interpreting candlestick patterns within broader market dynamics. This implementation uses a "Dual-Token Context Injection" approach, providing the ViT with both high-level regime understanding and granular feature data.

Two specialized context tokens augment the dynamic plane image input:

- **Regime ID Token:** A categorical token representing the identified market regime based on asymmetric feature analysis. This discrete identifier is generated by the enhanced `IdentifyAsymmetricRegime` module.
- **Asymmetric Vector Token:** A continuous vector comprising raw asymmetric features like upside/downside volatility and skewness, computed by the modified `AsymmetricFeatureEngine`.

The ViT model is modified to effectively utilize these context tokens alongside the dynamic plane image. Its self-attention mechanism learns relationships between the image data and the injected regime and feature information, enabling a more nuanced interpretation of market dynamics.

The dynamic plane implementation involves these steps:

1. **Feature Vector Calculation:** The `AsymmetricFeatureEngine` computes a vector of features representing market asymmetry and volatility. This vector serves as the Asymmetric Vector Token and informs the Regime ID Token.

2. **Regime Identification:** The `IdentifyAsymmetricRegime` module analyzes the feature vector to categorize the current market regime, generating the Regime ID Token.

3. **Context Token Integration:** The `Workflow_Broker` retrieves both the Regime ID Token and the Asymmetric Vector Token. These tokens, along with the dynamic plane image tensor, are then passed to the `Model_Inference_Service` for input to the ViT.

4. **Dynamic Rotation:** The dynamic plane's coordinate system rotates based on recent price movement, aligning the plane with the dominant trend and highlighting local price fluctuations. This rotation is calculated using a local movement vector.

5. **Dynamic Origin Shift:** Following rotation, the dynamic plane's coordinates are translated to keep current price action centered in the visual representation.

6. **Dynamic Snapshot Generation:** The final image representation of the dynamic plane is generated, incorporating price, volume, and time data after rotation and translation.

7. **Rotation Artifact Handling:** The implementation addresses and mitigates potential visual artifacts resulting from rotation.

This approach enhances the model's predictive capabilities by incorporating information about underlying market forces while balancing complexity and practicality. Further exploration of enhancements for asymmetry understanding is encouraged, provided they demonstrably improve the model's performance without undue complexity.

### C. Dynamic Plane Implementation

This section details the implementation of a dynamic plane representation of market data, enriching the Vision Transformer (ViT) input with contextual information via a dual-token approach. This approach enhances the model's understanding of both prevailing market regimes and specific nuances within them.

Two distinct tokens inject context into the ViT:

1. **Regime ID Token:** A discrete identifier representing the current market regime, providing the model with high-level contextual awareness.

2. **Raw Asymmetric Feature Vector Token:** A raw, unprocessed vector of asymmetric features, offering granular detail that complements the regime classification and potentially reveals intra-regime nuances.

The ViT learns to leverage both tokens for informed predictions. The feature vector's influence is designed to be conditional; if it doesn't offer additional explanatory power beyond the regime ID, its computation won't create a performance bottleneck and can be removed if deemed unnecessary.

This "Dual-Token Context Injection" method offers several advantages:

- **Flexibility:** Adaptable and modifiable as the model evolves.
- **Non-Destructive:** Preserves original data, allowing alternative context injection strategies.
- **Granular Detail:** Provides potentially valuable nuanced information beyond the broader regime classification.

This dual-token approach provides a robust and adaptable mechanism for contextualizing the ViT's input, enhancing its ability to learn and predict market behavior. Subsequent sections will detail the specific implementation steps and evaluation.

### D. Model Enhancement and Refinement

This section details enhancements and refinements to the core model architecture, focusing on improving predictive accuracy and trading performance. Key areas of focus include confidence-based trade filtering, leveraging historical prediction errors, optimizing the input window, refining the CNN's labeling approach, and incorporating additional data sources.

- **Confidence-Based Trade Filtering:** Moving beyond a simple decile-based strategy, trades are filtered based on the magnitude of the predicted return, executing only those exceeding a predefined threshold. This focuses on high-conviction predictions and avoids trades with low expected returns. This approach is further refined through Historical Prediction Error Profiling (HPEP).

- **Historical Prediction Error Profiling (HPEP):** HPEP leverages historical prediction errors as an optimizable hyperparameter. This involves constructing a map of historical errors to filter trades during backtesting, identifying and avoiding scenarios prone to prediction errors. This incorporates a dedicated HPEP module, a post-training confidence profile, an `accuracy_threshold` hyperparameter for tuning filter sensitivity, and integration into the backtesting framework.

- **Input Window Optimization:** The rationale for the 5-day input window for the CNN is thoroughly investigated and justified, referencing existing literature. The possibility of training separate CNNs for different holding periods is explored, and the impact of input window size on performance is analyzed.

- **Soft Labeling for CNN:** The potential benefits of soft labeling for the CNN are explored. If adopted, the CNN will be modified to predict a probability distribution instead of a point estimate, requiring a suitable loss function for probabilistic predictions.

- **Incorporating External Data:** Fundamental and macroeconomic data are integrated to enrich the model's input, potentially improving its predictive capabilities.

## D. Model Enhancement and Refinement

This section details enhancements and refinements to the core model, focusing on improving prediction accuracy, robustness, and practical applicability. These improvements encompass specialized techniques such as prediction filtering, accuracy-based trading, historical error profiling, input window analysis, specialized models for different holding periods, soft labeling, incorporating external data sources, memory management optimization, and error signal integration.

**Prediction Magnitude Filtering:** This enhancement involves establishing a threshold for trade execution. Only predictions with a magnitude exceeding this threshold will trigger a trade, aiming to reduce the number of trades based on weak signals and potentially improve overall performance. This approach capitalizes on the rationale that higher-magnitude predictions are likely to be more reliable and offer better trading opportunities. Various threshold values will be explored during backtesting to optimize the trade-off between trade frequency and average return per trade.

**Accuracy-Based Trade Selection:** This strategy prioritizes trades where the model exhibits the highest historical prediction accuracy. This involves analyzing periods of high model accuracy and concentrating trades within those periods to capitalize on stronger predictive capability.

**Historical Prediction Error Profiling (HPEP):** HPEP will be implemented as an optimizable hyperparameter to refine trade selection and mitigate risk. This involves profiling the model's historical prediction errors to identify patterns or conditions under which the model performs poorly. This information can then be used to filter trades or adjust the model's behavior during similar future conditions. The HPEP implementation includes:

- **Code Implementation:** Develop the HPEP module.
- **Post-Training Confidence Profile Generation:** Build and store the HPEP map.
- **Backtesting Trade Filtering:** Integrate the HPEP map into the backtesting framework to filter trades.
- **Accuracy Threshold Hyperparameter (`confidence_threshold`):** Introduce and tune a `confidence_threshold` parameter in conjunction with the HPEP map to determine trade execution.

**Input Window Analysis and Specialized CNNs:** The current model uses a 5-day candlestick window as input. A literature review will be conducted to justify this choice and explore potential alternatives. Additionally, separate CNN models will be trained for different holding periods, allowing for specialized models tailored to various investment horizons. Clarifying how the CNN transforms the 5-day candlestick chart images into numerical trade signals (including entry points, exit points, and predicted returns) is crucial for interpretability. The input data format will be rigorously verified, ensuring the correct representation of OHLC prices and consistent handling of edge cases, particularly for the last _n_ data points of each candlestick graph. The size _n_ of the input data points will be clearly defined.

**Soft Labeling for CNN Predictions:** The CNN will be modified to output a probability distribution over potential returns rather than a single point estimate. This requires adapting the loss function (e.g., using cross-entropy loss) to accommodate the change in output format.

**Incorporating External Data:** Fundamental and macroeconomic data will be incorporated to enrich the model's input and potentially improve its predictive power.

**Memory Management Optimization:** Options for memory management (A, B, and C) will be evaluated and experimentally compared to determine the most efficient approach for handling large datasets and complex models.

**Error Signal Integration:** An error signal will be integrated into the market movement algorithm, implementing an error correction mechanism to adjust the model based on past prediction errors.

## D. Model Enhancement and Refinement

This section details strategies to refine the model's performance and robustness, encompassing architectural adjustments, hyperparameter tuning, alternative trading strategies, and leveraging domain-specific knowledge.

**Input Window and Prediction Horizon:**

The model utilizes a 5-candlestick input window, consistent with findings from Jiang et al. (2023) supporting its effectiveness for financial time series pattern recognition. While the prediction horizon is also currently set to 5 days for practical reasons, future work will explore optimizing this parameter.

**Soft Labeling:**

To address limitations of hard labels observed during validation, the current hard labeling approach will be replaced with soft labeling. Instead of predicting a single deterministic value, the CNN will output a probability distribution over a range of potential returns. This allows the model to express uncertainty, potentially improving trading accuracy, particularly during validation. This transition necessitates identifying a suitable loss function for training with soft labels, modifying the CNN architecture to output probability distributions, and developing methods for generating these distributions.

**CNN Architectures and Training:**

Separate CNNs will be trained for each prediction horizon (1 to 5 days), enabling specialized models tailored to different timeframes. Performance will be evaluated using metrics including validation loss, Sharpe ratio, alpha, and Mean Squared Error (MSE) to facilitate model comparison and selection.

**Trading Strategy Enhancements:**

Beyond decile ranking, trading strategies based on maximum prediction accuracy will be explored. This involves defining how this percentage is calculated and employed as a trade selection criterion. A comparative performance analysis against the existing decile-based method will be conducted.

**Historical Prediction Error Profiling (HPEP):**

HPEP will be implemented as a tunable hyperparameter to optimize backtesting performance. This involves calculating historical prediction error, incorporating it into the optimization process, and defining the permissible range for this hyperparameter. An `accuracy_threshold` hyperparameter will be introduced and tuned in conjunction with HPEP. This will enable filtering trades based on historical error profiles, potentially improving overall performance. Code will be implemented within `test_model.py` (post-training analysis) and `trade.py` (real-time trade filtering) to build, store, and utilize an HPEP map (profile). A working prototype with dummy data will be developed for simulation and testing before integration. The HPEP map will be constructed post-training by binning validation set predictions and calculating the accuracy (and optionally, average error magnitude or Sharpe ratio) within each bin. During backtesting, trades will be executed only if the historical accuracy associated with the predicted return's corresponding bin exceeds the predefined `accuracy_threshold`.

**Literature Review and Justification:**

A thorough literature review will be conducted to further justify the chosen 5-candlestick input window, referencing existing research and establishing a robust theoretical foundation for this design decision. This review will provide context and support for the model's architectural choices.

## D. Model Enhancement and Refinement

This section details enhancements to the model's predictive capabilities, trading strategy, and risk management to improve overall performance and alpha generation. The current strategy, while demonstrating positive alpha before transaction costs in 6 out of 8 portfolios, suffers from inefficiencies primarily stemming from high turnover (trading every 5 days), uniform portfolio weighting, and a lack of intelligent trade filtering.

**Addressing these limitations requires a multi-faceted approach focusing on:**

1. **Soft Labeling for Probabilistic Predictions:** The CNN will be modified to predict a probability distribution over potential returns rather than a single point estimate. This shift allows for a more nuanced understanding of the model's confidence and facilitates probabilistic trading strategies. The output layer will be replaced with a fully connected layer followed by a softmax activation, generating a probability distribution over discretized return bins. Existing labels will be converted to soft labels using a Gaussian kernel centered around the actual return. The loss function will be changed from Mean Squared Error (MSE) to a more appropriate function like Categorical Cross-Entropy or Kullback-Leibler Divergence to accommodate the probabilistic output.

2. **Selective Trade Execution:** Instead of acting on all predictions, several filtering mechanisms will be implemented to prioritize higher-confidence and potentially more profitable trades:

   - **Prediction Magnitude Filtering:** Trades will only be executed if the predicted return magnitude exceeds a predefined threshold. This filters out low-confidence predictions and reduces trading frequency, potentially lowering transaction costs.

   - **Historical Prediction Error Profiling (HPEP):** HPEP will be implemented as an optimizable hyperparameter. This involves profiling the historical accuracy of predictions and using this profile to inform future trade decisions. A post-training confidence profile (HPEP map) will be built and stored, then used during backtesting to filter trades based on a tunable `confidence_threshold` hyperparameter.

   - **Trading Based on Maximum Prediction Accuracy (Further Research):** This approach, requiring further investigation, will explore prioritizing trades associated with the highest prediction accuracy, assuming a correlation between higher accuracy and greater potential returns.

3. **Risk-Based Portfolio Management:** A risk-weighted portfolio management strategy will allocate capital based on factors such as prediction confidence, the inverse of historical volatility, and the signal-to-noise ratio of predicted vs. actual returns during validation. This approach optimizes capital allocation by prioritizing higher-confidence, lower-risk trades.

4. **Addressing Short-Selling Constraints and Performance Analysis:** A thorough performance analysis will be conducted, including:

   - Investigating the breakdown of alpha generation between long and short positions, particularly within the small-cap segment, to understand the impact of potential short-selling constraints.

   - Analyzing per-index Sharpe ratios, both before and after transaction costs, to pinpoint specific areas for improvement.

   - Reviewing the handling of unsuccessful trades to inform potential modifications, such as incorporating a stop-loss mechanism.

These enhancements aim to refine the model's predictive capabilities, reduce unnecessary trading, manage risk effectively, and ultimately improve the overall trading performance and alpha generation of the system.

## D. Model Enhancement and Refinement

This section details enhancements to the model architecture, training process, and trading logic, focusing on predicting and utilizing "rally time"—the time it takes for a stock price to reach a target return—and learning from past trading performance.

**Predicting Rally Time:**

To enhance temporal awareness, the model will predict rally time—the number of periods (e.g., candlesticks) required to reach a target return. For each time step _t_, we will scan forward up to _N_ periods and identify the smallest _k_ where `Close[t+k] >= Close[t] + predicted_return`. This _k_ value becomes the target label for rally time. If the target return is not reached within _N_ periods, we set _k = N+1_. Model performance on this task will be evaluated using appropriate metrics.

**Enhanced Model Architecture:**

A multi-head neural network architecture will be implemented to predict both return and rally time. This architecture will utilize shared EfficientNet features, branching into two heads:

- **Return Regression Head:** Predicts the scalar reward (return).
- **Rally Time Head:** Predicts the rally time (_k_).

The loss function will be a weighted sum of the Mean Squared Error (MSE) losses for each head. Survival analysis models (e.g., DeepSurv, DeepHit, Weibull Time-To-Event models) will also be explored. These models are well-suited for handling censored data (cases where the target return isn't reached within _N_ periods) and expressing uncertainty over time.

**Enhanced Trading Logic:**

The trading logic will incorporate predicted rally time, enabling:

- **Dynamic Capital Allocation:** Optimizing investment based on the predicted time horizon.
- **Position Sizing Based on Time Premium:** Adjusting position size based on expected rally duration.
- **Early Exits:** Exiting positions if the target return is not reached within the predicted rally time window.

**Advanced Exit Strategies:**

Beyond a simple stop-loss, the following advanced exit strategies will be implemented:

- **Volatility-Aware Exits:** Dynamic exit thresholds based on the Average True Range (ATR) will be used to adapt to changing market volatility.
- **Time-Based Confidence Decay:** Positions will be exited if the predicted return does not materialize within a predefined timeframe.
- **Prediction Divergence:** Trades will be exited if new model predictions diverge significantly from previous predictions, indicating a shift in market dynamics.
- **Portfolio Contextual Exits:** Underperforming trades relative to similar trades within the portfolio will be exited.

**Learning from Mistakes:**

Several techniques will be employed to learn from losing trades:

- **Sample Re-weighting:** Misclassified trades with high losses will be assigned higher weights during retraining.
- **Bootstrapping of Hard Cases:** Difficult-to-classify examples will be collected and used for specialized training cycles.
- **Meta-Model for Trade Review:** A secondary model will be trained to predict the likelihood of trade failure before execution, based on the initial trade prediction and other relevant features. This meta-learner will help identify potentially problematic trades and inform decision-making. Analysis of misclassified trades will focus on identifying patterns and refining the model accordingly. This information will also be used to inform feature engineering and potentially filter or re-weight training samples.

### D. Model Enhancement and Refinement

This section details enhancements and refinements to the model architecture, focusing on improved prediction accuracy, incorporating temporal context, and practical applicability.

**Temporal Context Integration:**

Three approaches for incorporating temporal context will be compared through a benchmark experiment:

1. **Single Static Image:** Using a single candlestick image as input (baseline).
2. **Paired Images:** Using two consecutive candlestick images as input. A dedicated data generator will create these paired inputs, representing the transition between trading periods.
3. **Image Sequence:** Using a sequence of candlestick images as input. This will require modifying the model's input layer.

**Advanced Architectures:**

Two advanced architectures will be explored for modeling temporal dependencies:

- **CNN-LSTM Hybrid:** A CNN will extract features from individual candlestick images, and an LSTM will process the sequence of these extracted features to capture temporal relationships.
- **Vision Transformer (ViT):** A ViT will process sequences of three consecutive 5-day candlestick charts. Two input pathways will be evaluated: (1) encoding each image with EfficientNet before inputting the feature vectors to the ViT and (2) dividing each image into patches, flattening them, applying positional encoding, and directly inputting them into the ViT. Additionally, the effectiveness of image subtraction versus feature subtraction for generating delta features within the ViT will be compared.

**Prediction Refinement and Risk Management:**

Several strategies will be implemented to refine predictions and manage risk:

- **Prediction Magnitude Filtering:** Trades will be executed only when the predicted return exceeds a predefined threshold, focusing on high-conviction predictions.
- **Prediction Accuracy Filtering:** Trades will be selected based on the model's prediction accuracy, leveraging periods of higher confidence.
- **Historical Prediction Error Profiling (HPEP):** HPEP will be implemented and used to dynamically adjust trading behavior based on past prediction errors. An HPEP map will be generated and stored, and a tunable `confidence_threshold` hyperparameter will filter trades during backtesting.

**Input and Output Enhancements:**

The following enhancements will be applied to the model's input and output:

- **Input Window Justification and Specialized CNNs:** A literature review will justify the choice of the 5-day candlestick input window. Separate CNNs will be trained for different holding periods to specialize predictions across various time horizons.
- **Soft Labeling:** The CNN output will be modified to produce a probability distribution over potential returns instead of a single point estimate. This allows for a more nuanced representation of uncertainty and can improve performance with an appropriate loss function.

**Feature Engineering and Dataset Design:**

- **Delta Features:** Delta features, calculated via either image subtraction or feature subtraction, will be incorporated to capture changes between consecutive candlestick windows.
- **Sliding Window Dataset:** A dataset of 3-day candlestick chart sequences (t-2, t-1, t) will be created using a sliding window approach. Each sequence will be labeled with the corresponding return, rally time, and a categorical signal (buy, sell, hold).

These enhancements aim to bridge the gap between theoretical model performance and practical trading strategies by incorporating temporal context, managing risk, and refining predictions.

### D. Model Enhancement and Refinement

This section details enhancements and refinements to the ViT model architecture and training process, focusing on handling variable-length input sequences, optimizing performance, and shifting to an image-based prediction approach.

**Variable-Length Input and Positional Embeddings:**

To accommodate varying context lengths, the ViT model will be adapted to process variable-length sequences of candlestick images. Implementing positional embeddings will allow the model to understand the temporal relationships within the candlestick data by capturing the sequential order of the input images. This addresses the limitation of a fixed input length for the ViT. Instead of being restricted to a fixed number of images (e.g., 3), the model will handle a dynamic number of inputs, providing greater flexibility.

**Masking and Padding:**

To maintain a consistent input shape for the ViT, shorter sequences will be padded with blank chart images up to a defined maximum sequence length (e.g., 5 charts). A mask will inform the transformer which input elements represent actual data and which are padding, allowing the model to ignore padded elements during processing. This approach ensures consistent handling of variable-length sequences without introducing bias.

**Training with Variable-Length Sequences and Memory Management:**

The ViT will be trained explicitly with variable-length sequences, leveraging the implemented masking and dynamic positional encodings. This enables the model to utilize different amounts of historical context, potentially improving its ability to capture relevant patterns. Experiments with different input sequence lengths (N=3, 4, and 5 candlestick images, corresponding to 15, 20, and 25 trading days, respectively) will determine the optimal input length for maximizing performance. Furthermore, a structured experiment will evaluate three distinct memory management options (A, B, and C) using a consistent dataset to determine the optimal strategy for handling variable-length sequences while maintaining performance. The evaluation will consider key performance indicators, including Sharpe Ratio, directional accuracy, Mean Squared Error (MSE), and rally-time prediction accuracy.

**Shifting to Candlestick Image Prediction:**

Instead of predicting numerical returns directly, the model will be adapted to predict the subsequent candlestick pattern as an image. This predicted image will then be interpreted to derive a return value. This shift to visual sequence forecasting (image-to-image prediction) requires careful evaluation. Two methods for extracting OHLC values from the predicted images will be explored: pixel location analysis and rendering predicted image data.

**Conceptual and Theoretical Evaluation of Image-Based Prediction:**

This new approach necessitates a thorough evaluation of its conceptual soundness. This involves analyzing how human traders interpret candlestick patterns and comparing this to the model's image-based approach. The potential theoretical advantages of image-based prediction, including representation richness, improved uncertainty modeling, potential for causal reasoning, enhanced training supervision, increased interpretability, and generative flexibility, will be explored. This analysis will provide a foundation for understanding the potential benefits and drawbacks of this architectural shift. The added complexity of coding and maintenance associated with this approach will also be considered.

## D. Model Enhancement and Refinement

This section details strategies for refining the model to generate actionable trading decisions based on its predictions. The primary focus is on bridging the gap between visual pattern prediction accuracy and financial performance.

**Connecting Visual Accuracy to Trading Decisions:** While accurate candlestick sequence prediction is the foundation (as outlined in the Model Training Goal), the ultimate objective is profitable trading. The model must not only identify patterns but also assess the trade viability based on those patterns. Simply predicting a pattern's presence doesn't guarantee a successful trade. Therefore, we will investigate the relationship between visual prediction accuracy and actual financial performance, analyzing how well the former translates into profitable trading outcomes. Crucially, model evaluation will prioritize the actionable nature of the predictions for trading decisions, not just visual fidelity. This necessitates a metric that reflects the practical utility of predictions in a trading context.

**Enhanced Evaluation Metrics:** Model evaluation will encompass both financial returns and the generation of visually plausible candlestick sequences aligned with real-world patterns. This dual approach ensures that predictions are both visually grounded and financially relevant.

**Key Refinement Strategies:**

- **Trading Value Focus:** While realistic chart generation is important, the ultimate goal is identifying profitable trading opportunities. Model performance will be evaluated based on its ability to generate charts that provide clear entry and exit points, prioritizing trading value over mere visual fidelity.

- **Causal Grounding of Predictions:** The model should not just replicate visual patterns but also understand the underlying market dynamics driving them. Causally grounded predictions, based on an understanding of _why_ patterns occur, improve generalization and robustness in new market situations.

- **Robust Training and Evaluation:** Addressing the potential fragility of image-based prediction training and evaluation is crucial. This includes ensuring accurate ground truth alignment, developing appropriate image fidelity loss functions, mitigating mode collapse and pattern overfitting, and establishing robust metrics for evaluating the trading relevance of generated images.

- **Dual-Module Framework (Chart Generator + Trade Evaluator):** A hybrid architecture incorporating a dedicated trade evaluator module alongside the chart generator is under consideration. This module will analyze generated charts, identify potential trading opportunities, and provide concrete entry and exit signals. This decoupled approach allows for specialized optimization of each component and facilitates clearer interpretation of trading signals.

- **Prediction Magnitude Filtering:** Trades will be filtered based on the magnitude of predicted returns. A threshold will be established, executing trades only when the predicted return exceeds it. This approach aims to reduce trades based on less confident predictions, potentially improving overall performance.

- **Trading Based on Maximum Prediction Accuracy:** Strategies based on trading only when the model exhibits peak prediction confidence will be explored. This involves identifying periods where the model demonstrates the highest accuracy and concentrating trades during those times.

- **Leveraging Historical Error Profiles & Soft Labeling:** Analyzing historical prediction errors can reveal systematic biases or patterns in misclassifications. This information can be used to refine predictions or incorporated via soft labeling during training, enhancing the model's ability to handle uncertainty and improve calibration.

- **Incorporating External Data Sources:** The potential of integrating external data sources, such as news sentiment or economic indicators, will be investigated to provide additional context and potentially improve predictive accuracy.

## D. Model Enhancement and Refinement

This section details enhancements to the core predictive model, focusing on refining the Convolutional Neural Network (CNN) architecture and trading strategy. These enhancements are grounded in established research and will be supported by relevant literature to ensure academic rigor. This section also addresses the innovative approach of representing market data on a dynamic 2D plane, moving beyond a traditional 3D Cartesian framework.

### D.1 CNN and Trading Strategy Enhancements

The following enhancements focus on improving the CNN's predictive power and refining the trading strategy based on the model's outputs:

- **Prediction Magnitude Filtering:** Implement a mechanism to filter trades based on the magnitude of predicted price movements. Only trades where the predicted movement exceeds a predefined threshold will be executed, minimizing trades based on weak signals.

- **Confidence-Based Trading:** Implement a strategy that selects trades based on the model's confidence in its predictions. This will involve tracking and utilizing a confidence metric, potentially derived from the model's historical accuracy, and executing trades only when the confidence level surpasses a predefined threshold.

- **Historical Prediction Error Profiling (HPEP):** Integrate HPEP as an optimizable hyperparameter. This involves profiling the model's historical prediction errors to understand its performance across various market conditions. This profile, stored as an HPEP map, will be used to filter trades during backtesting. An `accuracy_threshold` hyperparameter will be introduced and tuned to control the filtering process. This requires implementing the HPEP module, generating the post-training HPEP map, and integrating it into the backtesting framework.

- **5-Candlestick Window Justification:** A literature review will be conducted to justify the choice of a 5-candlestick input window for the CNN, providing a theoretical basis for this design decision.

- **Holding Period-Specific CNNs:** Train separate CNNs for different holding periods (e.g., 1 day, 2 days, ..., 5 days) to optimize each model for its specific timeframe.

- **Soft Labeling for CNN Predictions:** Implement soft labeling for the CNN, modifying the architecture to output a probability distribution over possible price movements rather than a single deterministic value. This will require adapting the loss function, likely using cross-entropy loss.

- **Fundamental and Macroeconomic Data Integration:** Explore incorporating fundamental (e.g., company earnings, financial ratios) and macroeconomic (e.g., interest rates, inflation) data into the model to enrich its input with potentially influential information.

### D.2 Dynamic 2D Plane Representation

This section details a novel approach to representing market data using a dynamic 2D plane, moving away from a fixed 3D Cartesian representation. This innovative approach leverages the concept of a moving frame, akin to a Frenet frame or tangent plane, on a 1-dimensional differentiable manifold.

The core idea involves replacing the fixed 3D Cartesian frame with a dynamic 2D plane possessing rotational axes and a dynamic origin. The data will be projected onto this dynamic plane, and rotations of the axes within the plane will be analyzed for their impact on the data representation and model predictions. This necessitates an investigation into coordinate transformations, manifolds, and the concept of "bending of space" within the context of this dynamic representation.

Mathematically, this involves attaching an orthonormal frame at each point on the 1-dimensional manifold and rotating it along the path traced by the market data. The research will explore how a curve, such as a parabola along the z-axis in a conventional 3D frame, can be represented within this dynamic 2D plane through rotations of the axes.

A crucial aspect of this approach is analyzing the information balance and degrees of freedom within the dynamic 2D representation. The two coordinates (u,v) defining points on the plane and the three Euler angles (or rotation matrix) describing the frame's orientation will be carefully considered. This analysis will ensure the dynamic 2D representation effectively captures the information from the original data without redundancy or information loss.

### D. Model Enhancement and Refinement

This section details enhancements to the predictive model, focusing on memory management and error signal integration. Additionally, it explores potential future refinements inspired by concepts of dimensionality reduction and narrative generation.

**Memory Management Optimization:** Different memory management strategies (Options A, B, and C—details to be defined) will be evaluated experimentally to determine the most efficient approach for handling the increased data load from incorporating external data sources. This analysis will consider the trade-offs between storing 2D coordinates with orientation data versus storing full 3D positions, focusing on the memory usage and computational costs associated with maintaining and updating orientation information (potentially using quaternions or rotation matrices). This relates to the analysis of memory and computation costs associated with using moving frames, a concept explored in parallel research.

**Error Signal Integration:** An error signal, derived from the difference between predicted and actual market movements, will be integrated into the market movement prediction algorithm. This feedback loop aims to improve the model's adaptability to changing market conditions. This relates to the broader research on understanding the benefits and limitations of different motion abstractions and ensuring the model can effectively traverse the entire prediction space.

**Future Refinements:** While not currently implemented, concepts from related research offer promising avenues for future model enhancement. Mapping 3D space into a 2D representation, using techniques like moving frames (Frenet-Serret frames) or stereographic projections, could offer a more compact and informative representation of candlestick data than raw candlestick images. This could lead to more efficient models and potentially improve prediction accuracy. Furthermore, enriching the Narrative_Generation_Service with market regime information could provide valuable context for interpreting model predictions, particularly regarding the magnitude of predicted price movements, aligning with the goals of filtering predictions based on magnitude and accuracy. This contextual information could improve the model's ability to identify and react to different market regimes.

### D. Model Enhancement and Refinement (Specialization)

This section details enhancements to refine the model's predictive capabilities, focusing on incorporating a dynamic projection system based on Principal Component Analysis (PCA) to shift the model's focus from absolute price positions to relative local movements.

**Dynamic PCA Implementation:** A dynamic PCA rotation will be implemented within the model pipeline. This involves the following steps at each time step:

1. **Window Selection:** The relevant data window for PCA calculation is selected.
2. **Feature Preparation:** Features are prepared for PCA, including any necessary scaling or normalization.
3. **PCA Computation:** Principal components are calculated for the selected window.
4. **Principal Direction Identification:** The principal directions of variance are identified.
5. **Feature Space Rotation:** The feature space is rotated based on the principal components.
6. **Data Re-centering:** Data is re-centered after rotation.
7. **Model Input Preparation:** The transformed data is prepared as input for the model.

**Integration and Technical Considerations:**

Several key aspects of the dynamic PCA integration will be addressed:

- **Implementation Details:** Specific implementation details will be provided, encompassing feature extraction, dynamic PCA calculation, and how the transformed data is fed into the model. This will clarify how each new candlestick or group of candlesticks affects the re-centering and rotation of the feature space before the next prediction.

- **Placement within the Network Pipeline:** Different options for integrating PCA into the network pipeline will be explored:

  - **Pre-processing on Raw Data:** Applying PCA directly to the raw input data.
  - **Applying PCA to Learned Image Embeddings:** Performing PCA on the intermediate image embeddings.
  - **PCA as a Fixed Layer:** Integrating PCA as a fixed layer within the network architecture.
  - **Data Augmentation:** Utilizing image-level rotation based on PCA as a data augmentation technique.

- **Backpropagation through Dynamic PCA:** The technical challenges associated with backpropagation through dynamic PCA transforms will be addressed:

  - **Non-differentiability:** Handling the non-differentiable nature of PCA calculations.
  - **Gradient Flow:** Ensuring proper gradient flow back to the original features.
  - **Processing Strategy:** Evaluating the trade-offs between batch and sequential processing.

- **Clarifying the "Rotation" Mechanism:** The exact mathematical transformation representing the "rotation" will be explicitly defined (e.g., affine transformation, PCA rotation, learned rotation). This impacts the model's learning behavior and its ability to capture relative movement.

- **Clarifying the Goal of Dynamic Projection:** The objective of the dynamic projection will be precisely stated: enabling the model to focus on relative local movement rather than absolute position, aiming to achieve prediction invariance to previous trend direction. This will guide the implementation and evaluation process.

### D. Model Enhancement and Refinement

This section details techniques to refine the model's performance, focusing on filtering predictions, managing computational costs, incorporating external data, and implementing dynamic transformations for improved predictive accuracy.

**Filtering and Optimizing Predictions:**

- **Prediction Magnitude Filtering:** To focus on higher-confidence predictions, trades will be filtered based on the magnitude of the model's predictions. A threshold will be established above which trading signals are considered actionable, aiming to reduce the number of trades executed.

- **Historical Prediction Error Profiling (HPEP):** HPEP will be implemented as an optimizable hyperparameter. This involves profiling the historical error of predictions and using this profile to inform future trade decisions. A dedicated HPEP module will be implemented to build and store the HPEP map post-training. Backtested trades will be filtered based on this map using a tunable accuracy threshold hyperparameter (`confidence_threshold`).

**Input and Output Refinements:**

- **Input Window Justification and Alternatives:** The current model utilizes a 5-candlestick input window. This choice will be justified through a literature review. Additionally, training separate CNNs for different holding periods will be explored.

- **Soft Labeling for CNN Predictions:** The CNN will be modified to output a probability distribution instead of a single classification. This will involve implementing a suitable loss function for probability distributions.

**Data Integration and Computational Efficiency:**

- **Incorporating External Data:** Fundamental and macro data will be investigated as potential additional input features to enhance model performance.

- **Computational Cost Management:** Experiments will be conducted to evaluate different memory management options (Options A, B, and C) to address the potential computational overhead introduced by enhancements like HPEP and external data integration.

**Dynamic Transformations for Enhanced Accuracy:**

The core model uses a fixed window of candlestick data. To capture relative price movements more effectively, dynamic transformations, specifically rotation and origin shifting, will be implemented.

1. **Local Movement Vector Calculation:** The local movement vector, `v`, representing the change in price (both X and Y coordinates, incorporating time), is calculated as:

   ```
   v = P_current - P_previous
   ```

2. **Rotation Matrix Calculation:** The rotation angle, θ (theta), is calculated from the local movement vector:

   ```
   θ = arctan(v_y / v_x)
   ```

   This angle is used to construct the rotation matrix, `R(θ)`:

   ```
   R(θ) = [[cos(θ), sin(θ)], [-sin(θ), cos(θ)]]
   ```

   This matrix is applied to a small window of candlestick data (e.g., the last 5-10 candlesticks).

3. **Dynamic Origin Shift:** The origin of the coordinate system is shifted to the current price point, centering the price action around (0,0). This normalizes the price movement, emphasizing relative movements over absolute values.

**Dynamic Snapshot Generation and Handling Rotation:**

Dynamically generated, rotated candlestick images will be fed into the CNN or Vision Transformer. This process requires careful handling of potential issues:

- **Rotation Artifacts Handling:** Interpolation techniques, such as anti-aliasing, will be employed to mitigate potential distortions arising from rotation during image redrawing.

- **Volatility Jump Handling:** Strategies like smoothing or limiting rotation angle changes will address potential issues caused by abrupt frame rotations due to sudden large price movements.

- **Consistent Axis Scaling:** Maintaining uniform axis scaling (units per % move) across all generated frames is crucial for consistent model training.

A detailed pseudocode pipeline will be developed to outline the step-by-step process of generating these rotated images, ensuring clear and reproducible implementation. This pipeline will encompass the entire process from initial data input to final image output, including rotation, re-centering, and artifact mitigation.

**Error Signal Integration:**

An error signal feedback mechanism will be integrated within the market movement algorithm. This will enable dynamic correction and adaptation to changing market conditions, refining the rotational axis paradigm implementation. This mechanism will address the integration of rotating axes and define their critical behavior. Potential risks and challenges like overfitting, computational overhead, and loss of absolute reference frame will be considered. A baseline comparison strategy (no transformation, static transformations, simpler alternatives) will quantify the benefits of these dynamic adjustments. The requirement for image-based input (candlestick images or other suitable visual representations) will be maintained.

## Model Enhancement and Refinement (Specialization)

This section details enhancements to the model's predictive capabilities by incorporating a dynamic plane representation of market data. This approach aims to capture the dominant trajectory and oscillatory behavior of the market by reconstructing the measurement plane at each time step and using the resulting dynamic snapshots as input for a Vision Transformer (ViT).

### Dynamic Plane Generation

The core of this enhancement is the Dynamic Plane Generator. This module dynamically redraws the coordinate system used to represent market data (time, price, and optionally volume). Instead of simply rotating based on trend or applying Principal Component Analysis (PCA) to price variance, this method reconstructs the entire 2D plane at each step. This allows the model to operate within a locally true, reconstructed 2D world, potentially improving its ability to discern relevant patterns.

The generator operates as follows:

1. **Local Window Definition:** At each time step, a local window of previous candlesticks (e.g., 5-10) is analyzed. This window size is a configurable parameter.

2. **Movement Vector Calculation:** Movement vectors are calculated within the time-price-volume space for each candlestick in the window.

3. **PCA Application:** PCA is applied to these movement vectors to identify the top two principal components. These components form the axes of the dynamically redrawn 2D plane.

4. **Coordinate System Rotation and Refocusing:** The coordinate system is rotated and refocused such that the origin lies at the most recent candlestick and the axes align with the principal components.

5. **2D Plane Reconstruction and Snapshot Rendering:** The candlestick data is projected onto this new 2D plane, creating a dynamic candlestick snapshot. This snapshot is then rendered as an image for input to the ViT. Optionally, volume information can be encoded into the snapshot.

### ViT Integration and Experimentation

The Dynamic Plane Generator is integrated directly into the ViT training pipeline. The generated dynamic candlestick snapshots are fed into the ViT model during training. A key experiment will compare model performance when trained on static candlestick images versus these dynamic snapshots. This will quantify the impact of the dynamic representation on predictive accuracy.

### Visualization and Understanding

Visualizations are crucial for understanding the effects of the dynamic plane transformations. The following visualizations will be generated:

- **Example Images of Raw Candlestick Input:** Baseline images of the raw candlestick data before any transformations.
- **Example Images of Transformed Candlestick Input:** Images of the dynamic candlestick snapshots after the dynamic plane transformations.
- **Animated Visualization of Dynamic Redrawing:** An animation demonstrating the evolution of the dynamic plane as new data points are added. This animation will clearly depict the effects of the window size and volume inclusion parameters, and provide insights into the behavior of the dynamic coordinate system.

### Implementation Details

The Dynamic Plane Generator will be implemented in Python using libraries like NumPy, Matplotlib, and PIL. The implementation will be optimized for batch operations to handle larger datasets efficiently. A detailed pseudocode implementation will be developed prior to the full implementation. This pseudocode will encompass data input, local window definition, movement vector calculation, PCA application, coordinate system rotation and refocusing, and 2D plane reconstruction and snapshot rendering.

## D. Model Enhancement and Refinement

This section details enhancements and refinements to the model, focusing on dynamic plane visualization, data generation, and addressing technical constraints related to animation and rotation.

**Data Generation and Visualization:**

- **Dynamic Plane Dataset Generation:** The dynamic plane principle will be used to generate datasets for training both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). This involves capturing the evolving relationships between price, volume, and time by generating images of the dynamic plane at different time points. Efficient batch generation will be implemented for this process.

- **Smoothing Techniques:** Smoothing techniques, such as Heikin-Ashi, will be explored to potentially improve model training or interpretability. This involves evaluating the impact of smoothing input data before generating the dynamic plane representation and may require modifying the existing implementation.

- **Animated Dynamic Plane Evolution:** An animation will visualize the step-by-step changes to the dynamic plane as new data points are introduced. This visualization aid will be crucial for debugging and understanding the plane's behavior. Robust handling of edge cases, specifically scenarios with no or only one initial data point, will be implemented to prevent errors.

**Addressing Technical Constraints:**

- **Minimum Points for Animation and Rotation:** The animation and dynamic rotation calculations, particularly those involving PCA/SVD after rotation, require a minimum of two data points. Rotation and plotting will be deferred until sufficient data is available to prevent errors. Furthermore, PCA stability requires at least three stable data points to prevent instability and ensure robust rotation calculations.

- **Handling Single-Point Frames:** When only a single data point is present in early animation frames, an empty canvas or placeholder visual will be displayed to maintain a smooth user experience and prevent errors.

- **Format Offsets:** Proper formatting of offsets, especially with limited data points, is crucial to prevent dimension mismatches and subsequent calculation errors during animation.

**Visualization Enhancements:**

- **Smooth Rotation Matrices:** Smoothing or stabilization techniques will be applied to the rotation matrices to prevent dimensional blowups and ensure visually smooth transitions during the animation.

- **Standalone Animation Simulator:** A standalone simulator will be developed to provide a detailed, step-by-step visualization of the dynamic plane, incorporating delayed rotation (after 3+ points) and smoothing of early plane formation.

- **Dynamic Point Movement and Live Frame Adjustments:** The animation will showcase dynamic point movement and real-time frame rotation and recentering, providing a more intuitive understanding of how the coordinate system adjusts to changing data.

**Heikin-Ashi Integration and Dynamic Rotation:**

The model will be further refined by incorporating Heikin-Ashi candles and a dynamic rotation technique. This involves:

1. **Heikin-Ashi Generation:** Generating Heikin-Ashi candles from standard OHLC data to smooth price action.

2. **Visualization:** Plotting Heikin-Ashi candlesticks, using green for upward trends (close >= open) and red for downward trends (close < open).

3. **Dynamic Rotation and Recentering:** Calculating the midpoint between the open and close values of each Heikin-Ashi candle and using this midpoint to dynamically rotate and recenter the data. This transformed data will then be visualized on a 2D plane.

4. **Image Saving:** Saving both the standard Heikin-Ashi charts and the dynamically rotated charts as PNG images for further analysis and potential use in model training.

These enhancements aim to improve the visualization, stability, and analytical power of the dynamic plane representation, ultimately contributing to a more robust and insightful model.

## Model Enhancement and Refinement

This section details enhancements to the model's robustness and performance by incorporating realistic market scenarios, analyzing market regimes within the dynamic plane, leveraging the PCA-defined coordinate system, and refining the model architecture and learning approach.

### Realistic Market Simulation and Dynamic Plane Evaluation

To enhance robustness, the model will be evaluated under complex market conditions using both standard and dynamic Heiken-Ashi charts. This involves:

1. **Complex Price Simulation:** Simulating realistic price patterns, including rallies, drops, and recovery phases.
2. **Choppy Market Simulation:** Simulating chaotic, sideways markets using the `generate_choppy_candlesticks(n=30)` function.
3. **Standard Heiken-Ashi Chart Generation:** Generating a standard Heiken-Ashi chart from the choppy data using `generate_heiken_ashi` and `plot_heiken_ashi_candlestick`, saved as `/mnt/data/standard_heiken_ashi_choppy.png`.
4. **Rotated Dynamic Heiken-Ashi Chart Generation:** Applying rotation and recentering to the choppy Heiken-Ashi data using `dynamic_rotate_recenter_heiken` and plotting the result with `plot_rotated_heiken`, saved as `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`. This allows direct comparison with the standard chart to understand the dynamic plane's impact on chaotic data. Similar baseline charts for standard market conditions will be generated and saved as `/mnt/data/heiken_ashi.png` and `/mnt/data/rotated_dynamic_plane_heiken_ashi.png`.

### Market Regime Analysis and Visualization

This section focuses on visualizing and analyzing how market regimes (trend, reversal, sideways) are represented in both standard and dynamic plane charts. The process includes:

1. **Visualizing Market Regimes:** Creating visualizations of trend, reversal, and sideways regimes within the dynamic plane.
2. **Simulating an Additional Regime:** Simulating and visualizing a third regime, such as a strong linear uptrend or a sharp V-shaped recovery, to enhance the comparative analysis.
3. **Side-by-Side Comparison:** Visualizing standard Heiken-Ashi and rotated Dynamic Plane charts side-by-side for all three regimes to highlight their strengths and weaknesses.
4. **Combined Visualization:** Combining all visualizations into a single panel for comprehensive comparison.
5. **Analysis and Documentation:** Analyzing and summarizing the observed behaviors and their implications for model learning, documenting the findings for inclusion in dissertation drafts.

### Leveraging the PCA-Defined Coordinate System

This section focuses on enhancing the model's ability to recognize patterns within the dynamic coordinate system generated by PCA applied to price, time, and volume data.

- **Relational Model Design:** Prioritize learning relationships within the PCA space, moving away from traditional technical indicators reliant on fixed price/volume relationships.
- **Geometric Pattern Recognition:** Train the model to identify geometric shapes and flows within the normalized PCA space.
- **Interpretability Projection:** Implement a projection mechanism to translate the model's focus back into the original Time-Price-Volume space for interpretability.
- **Window Size and Smoothing:** Carefully consider the PCA window size and implement smoothing techniques to mitigate noise-driven rotations and potential overfitting. Investigate the relationship between window size and market noise.

### Model Architecture and Learning Refinements

This section outlines refinements to the model's architecture, loss function, and learning approach.

- **PCA Rotation Stabilization:** Explore and implement stabilization techniques to mitigate "jittery" rotations caused by noise or small window sizes, ensuring robust feature extraction.
- **Loss Function Design:** Develop specialized loss functions tailored to the dynamic PCA space, focusing on capturing relational dynamics rather than absolute feature values.
- **Relational Learning Paradigm:** Shift the model's learning from absolute feature recognition to a nuanced relational understanding of market movements.

### D. Model Enhancement and Refinement

This section details enhancements to the model's predictive capabilities, focusing on error signal integration, adaptive mechanisms, and dynamic frame optimization. These improvements aim to enhance trading performance and robustness across varying market conditions.

**Error Signal Integration and Adaptive Learning:**

Inspired by biological error correction mechanisms, an error signal component will be integrated into the market movement algorithm. This component will account for increasing uncertainty in longer-term predictions, enabling dynamic adjustments based on past performance. Specifically, the model will compare predicted market movements with actual outcomes and adjust its internal representation of market structure accordingly. This mimics the human brain's ability to use error signals for movement correction. This will be achieved through the following enhancements:

- **Error Signal in Dynamic Plane Algorithm:** The error signal will be integrated into the dynamic rotating plane algorithm. This integration will allow the model to refine its projections by comparing predicted and actual market movements, and adjusting its internal representation of the market structure. A detailed sketch will be created to clarify the interaction between the error signal and the dynamic plane transformations.

- **Prediction-Error Feedback Integration:** A lightweight prediction-error feedback mechanism will be implemented during model training, potentially through auxiliary loss functions or by monitoring frame stability. This will allow the model to learn from its errors and adapt more effectively to changing market conditions.

**Dynamic Frame Optimization and Stability:**

Several mechanisms will be implemented to optimize the dynamic rotating plane and enhance its stability:

- **Adaptive Frame Correction (Frame Confidence Correction):** A feedback loop will adjust the rotational frame based on the comparison between predicted and actual market movements. This will allow the model to learn the stability of its projections and adapt future rotations accordingly.

- **Feedback-Driven Frame Smoothing:** The rotation speed of the dynamic plane will be dynamically adjusted based on the magnitude of prediction errors. High errors will trigger a temporary slowdown, effectively smoothing the dynamic frames and promoting a more conservative approach during volatile periods.

- **Dual-Frame Estimation:** The model will utilize two overlapping local frames: a fast-updating "optimistic" frame and a slower, more stable frame. Predictions will be dynamically weighted between these frames based on observed market consistency, combining short-term responsiveness with long-term stability.

- **Lagging Rotation Deactivation Strategy:** A strategy will be developed to deactivate the lagging rotation mechanism when appropriate. This will ensure responsiveness to market changes while mitigating potential negative effects of excessive lag. The previously used static error value in PCA calculations, which represented prediction error memory, will be removed due to its negative impact on performance.

**Rolling Frame Correction Algorithm:**

A rolling frame correction algorithm, inspired by biological wound healing, will further enhance frame stability and address prediction error. This algorithm comprises three phases:

1. **Prediction Error Buffer:** A rolling buffer stores recent prediction errors (difference between predicted and realized movement). The buffer size is configurable, affecting the mechanism's sensitivity.

2. **Error Trend Detector:** This component analyzes the buffered errors, calculating rolling mean and variance to identify consistent trends. Exceeding a threshold (multiple of the rolling standard deviation) triggers frame correction.

3. **Frame Correction and Healing:** Upon triggering correction, small rotational adjustments or damping are applied to the dynamic PCA frame. A healing phase gradually removes the correction as errors subside, ensuring smooth transitions and adaptation to evolving market dynamics. This addresses potential stagnation from static weights and implicitly performs frame coincidence correction through continuous evaluation and adjustment. It also mitigates the "plateau in dual records" issue by actively adapting to shifting market conditions. The "peripersonal vs. extrapersonal" gap concept, inspired by biological perception, will be explored to differentiate between near-term, high-confidence predictions and longer-term, less certain predictions within the backtesting environment. This distinction can inform trade selection and risk management.

## D. Model Enhancement and Refinement

This section details enhancements to the model, focusing on error detection, correction, and training improvements to refine predictive capabilities and improve trading performance. These enhancements center around the Dynamic Plane Generator and its interaction with market data.

**Dynamic Plane Adjustment and Stabilization:**

A rolling frame correction algorithm will be integrated into the `DynamicPlaneGenerator` class to enhance stability and responsiveness to market shifts. This algorithm dynamically adjusts the coordinate frame used for generating the dynamic plane. When error trends are detected, a correction action, consisting of a small rotation adjustment or damping applied to the Principal Component Analysis (PCA) frame, will be triggered. This reweights principal axes, shifting away from pure PCA towards a more stable representation. A healing phase, implemented using an exponential decay function, will gradually reduce the correction magnitude as the error returns to an acceptable range, smoothly transitioning back to the PCA-based frame. The magnitude of the adjustment and the decay rate are critical parameters that will be determined through optimization. A visualization tool will be developed to illustrate the "error spikes → correction → healing decay" cycle over simulated market data, facilitating parameter tuning and providing insights into the algorithm's behavior. A "frame intervention" metric will be developed to quantitatively assess the frequency and significance of correction algorithm interventions over a typical trading year, providing insight into the system's overall stability and responsiveness.

**Enhanced Error Detection and Analysis:**

A sophisticated Error Trend Detector will be developed, moving beyond simple rolling means of errors. This detector will analyze deviations from expected relational movement within the dynamic 2D frame, considering both distance and angular errors, as well as the influence of price, time, and volume. Deviation vectors will be monitored in the dynamic 2D plane, calculating the vector difference between predicted and realized movement vectors. Angular error tracking will be implemented, calculating the angle between predicted and realized vectors within the rotated 2D plane using the arccosine of the dot product of the vectors, divided by the product of their magnitudes. This provides a measure of directional alignment between predicted and actual market movements.

**Error Integration and Model Training:**

A composite error score, combining distance and angular error components, will be developed. Weighting factors (alpha and beta) will be tuned to adjust the relative importance of each error type. This nuanced error understanding will be incorporated into a rolling window analysis within the enhanced Error Trend Detector (reference: `997a23e9-bc2c-4b89-9c42-f3a60ff421b2`), potentially leading to a dynamic rolling error correction module (reference: `8c63a08a-2821-4acb-b2af-63fe3f28bf59`). A visual simulation demonstrating the cumulative effect of small vectorial misalignments compared to simple price errors may be developed (reference: `d17e77cc-24e0-42a6-b2b0-2e7e0630495c`). New loss functions will be explored to incorporate this enhanced error handling into model training, penalizing both scalar (price difference) and angular drift to improve prediction accuracy in both magnitude and direction (reference: `1e6d304d-0ba4-49c3-8deb-e1318278eec3`). These calculated distance and angle error signals will be integrated into the market movement algorithm to create an error correction feedback loop.

**Rotational Frame Clarification:**

A pending investigation will confirm the correct implementation of rotational angles and distance vectors within the dynamic plane, clarifying whether two of each exist as previously suggested. This will be documented and confirmed with the user, with any necessary adjustments made to the application logic or documentation. A diagram illustrating the global frame transformation (based on PCA) and the local vector misalignment, along with a visual representation of the prediction error, will be created to aid understanding and analysis.

## D. Model Enhancement and Refinement

This section details enhancements to the model's predictive capabilities by addressing the challenge of maintaining consistency between predicted and realized market movements within a dynamic PCA frame. This involves refining the handling of short-term predictions, implementing a robust framework for comparison, and visualizing the process for clarity.

### Dynamic PCA Frame Management

The core of this enhancement lies in managing the dynamic PCA frame, specifically for short-term predictions (e.g., 1-5 candlesticks). Given this short time horizon, structural market drift is assumed negligible compared to potential prediction errors. Therefore, the dynamic frame will not be recalculated for every price fluctuation, except in scenarios involving extremely rapid, high-frequency trading. This simplification reduces computational overhead while maintaining sufficient accuracy. Two key methods facilitate consistent comparisons within this dynamic frame:

- **Freeze Frame:** This method "freezes" the PCA rotation matrix (R) at the time of prediction (t). Both the predicted and realized data points at t+1 are projected using this same matrix. This maintains a consistent frame of reference, even if the underlying data distribution and, consequently, the PCA plane, shifts between prediction and realization.

- **Reproject Realization:** This method reprojects the realized movement vector at t+1 back into the PCA frame established at t using the original rotation matrix (R). This effectively transforms the realized data back into the prediction's frame of reference, ensuring a consistent basis for comparison despite potential market shifts.

These methods differ from traditional distance and angular error calculations that assume a static PCA plane, which can be misleading when the PCA plane itself shifts.

### Implementation Details

To ensure robust implementation and clear understanding, the following components will be developed:

- **Error Correction and Frame Consistency:** Minor adjustments will be implemented to correct accumulated angular errors in the dynamic frame when a predefined threshold is exceeded. A functional prototype based on existing pseudocode will be developed to test and refine the error correction logic.

- **Freeze & Compare Implementation:** Pseudocode will detail the "Freeze & Compare" technique within the dynamic PCA frame, serving as a blueprint for implementation. A visual simulation will demonstrate how prediction and realization are aligned using this method.

- **Data Structures and Visualization:** A lightweight data structure will store the PCA basis (rotation matrix) for each window, enabling efficient reprojection for comparison. Visualizations of the "Freeze Frame" and "Reprojection" processes will further clarify the dynamic approach and its advantages over static methods. They will also illustrate how the 2D plane derived from PCA maintains consistency, given potential differences in price and volume values that could influence the PCA axes. This investigation will determine if any transformation or alignment is required for accurate comparisons.

- **Illustrative Example and Pseudocode:** A numerical example with visualizations will demonstrate angle/distance error calculation within the frozen frame. Plots will visualize the prediction and reality paths in both the frozen and shifted frames, highlighting the impact of the chosen method. Robust pseudocode for a "Freeze and Correct" module will encapsulate the logic for both "Freeze Frame" and "Reproject Realization" methods, providing a blueprint for consistent code implementation. This module will handle freezing the PCA frame and projecting both predicted and realized data points for comparison.

## D. Model Enhancement and Refinement

This section details enhancements to the model's predictive power, decision-making capabilities, and robustness by addressing error analysis, dynamic adaptation to market conditions, and confidence estimation.

### Error Analysis and Dynamic Adaptation

The model's error calculation will be refined to incorporate both vector deviation and PCA frame drift, accounting for dynamic market conditions.

**Total Error Calculation:**

The total error is a weighted sum of the vector deviation error and the frame shift error:

- **Vector Deviation Error:** This measures the difference between predicted and actual values within the current PCA frame. It combines distance and angular differences:

  - `Vector Error = α₁ * d<sub>vec</sub> + α₂ * θ<sub>vec</sub>`
  - where `d<sub>vec</sub>` is the Euclidean distance, `θ<sub>vec</sub>` is the angular difference, and `α₁` and `α₂` are weighting factors.

- **Frame Shift Error:** This quantifies the change in the PCA frame between successive time steps by calculating the weighted sum of the angles between corresponding PCA axes:

  - `Frame Shift Error = β₁ * θ<sub>PCA1</sub> + β₂ * θ<sub>PCA2</sub>`
  - where `θ<sub>PCA1</sub>` and `θ<sub>PCA2</sub>` represent the angles between PCA1 and PCA2 vectors at time _t_ and _t+1_ respectively, and `β₁` and `β₂` are weighting factors.

- **Total Error:** This combines the vector and frame shift errors:

  - `Total Error = γ₁ * Vector Error + γ₂ * Frame Shift Error`
  - where `γ₁` and `γ₂` control the relative importance of prediction error and frame instability.

All angles within the error calculations will be normalized (e.g., mapping degrees to the range [0, 1]) to ensure consistent scaling. Reasonable default values for the weights (α, β, and γ) will be determined based on trading practices and principles of physics, and a numerical example will be provided to illustrate the calculation. Formal pseudocode will also be included for clarity and implementation.

**Rolling Error Tracking and Dynamic Correction:**

To adapt to changing market conditions, a rolling error buffer storing the prediction error over a recent window of _N_ steps (e.g., 5-10 candlestick windows) will be implemented. The mean and variance of the errors in this buffer will be used to trigger dynamic model adjustments.

- **Wound Phase:** If the mean error exceeds _k_ times the rolling standard deviation (e.g., _k_ = 2), the model enters a "Wound Phase." This signifies potentially degraded prediction accuracy and triggers the application of a correction factor. This correction factor might involve adjustments to model parameters or modifications to input data processing, such as adjusting the PCA rotation or smoothing operations during frame construction.

- **Healing Phase:** The model transitions to the "Healing Phase" when the mean error drops below a lower threshold (e.g., 1-1.5 times the rolling standard deviation). This indicates a recovery in prediction accuracy. The correction factor is gradually reduced as the errors decrease, allowing the model to smoothly transition back to normal operation. If errors spike again during the Healing Phase, the model re-enters the Wound Phase, restarting the correction process.

This dynamic approach aims to create a self-regulating system capable of adapting to evolving market conditions and maintaining robust performance.

### Confidence Metrics and Decision Making

**Frame Drift as Confidence Indicator:** The potential of using frame drift error as a confidence indicator will be investigated. Higher frame drift might suggest increased market instability and lower confidence in predictions, informing decisions on trade execution or position holding.

**Simulation and Error Component Weighting:** A small-scale simulation will be created to visualize the effects of vector deviation and PCA frame drift, aiding in understanding these errors and informing mitigation strategies. The effectiveness of the weighting parameters within the error calculations will also be evaluated to optimize their contribution to the overall error metric. This includes assessing the normalization of distance and angular errors to ensure accurate aggregation.

## D. Model Enhancement and Refinement

This section details several enhancements to the model, focusing on improving prediction accuracy, responsiveness to market dynamics, and efficient handling of input data. These enhancements include a performance-based healing mechanism, dynamic correction adjustments, and preprocessing techniques for dimensionality reduction.

### Performance-Based Healing and Dynamic Decay Rate Adjustment

The current model uses a fixed decay rate to account for market disruptions. However, a dynamic decay rate, adjusting based on real-time predictive accuracy, offers improved responsiveness. This enhancement aims to minimize the lag introduced by the decay factor when the model consistently predicts correctly, allowing quicker adaptation to market trends. This involves:

- **Tracking True Prediction Values:** Meticulous logging of the actual predicted values generated by the algorithm. This data is crucial for dynamic decay rate adjustment.
- **Prediction Correctness Tracking:** A scoring mechanism (+1 for correct, 0 for incorrect) will assess prediction accuracy, focusing on directional or magnitude correctness, independent of framing or windowing.
- **Rolling Prediction Correctness Buffer:** A rolling buffer storing the prediction correctness scores for the last N timesteps, providing a stable measure of recent performance.
- **Dynamic Decay Rate Adjustment:** The decay rate will be dynamically adjusted based on the rolling buffer's mean prediction correctness. Consistent accuracy reduces the decay rate and its associated lag, enabling faster adaptation to current market conditions.
- **Performance-Based Healing Implementation:** The correction factor will be reduced proportionally to the true predictive recovery, as measured by the rolling prediction correctness buffer, replacing the fixed, time-based decay with a more responsive and context-aware correction mechanism.

### Dynamic Correction and Healing Logic

This enhancement refines the healing logic by dynamically adjusting the correction factor based on recent prediction accuracy. This accelerates recovery from periods of poor performance ("wounds") and promotes stable, accurate predictions. The following steps will implement this dynamic healing system:

1. **Updated Healing Logic:** The healing logic will be modified to incorporate the mean prediction correctness over the last N steps. Higher rolling correctness reduces the correction factor, while lower correctness maintains or increases it.
2. **`dynamic_decay_rate` Function:** A function, `dynamic_decay_rate(mean_correctness)`, will calculate the correction factor's decay rate. The proposed formula, `Decay Rate = 1 - (mean_correctness - healing_threshold)`, accelerates decay as mean correctness surpasses the defined threshold.
3. **Formal Modular Pseudocode:** Formal, modular pseudocode will be written for the complete Healing-by-Correctness system, encompassing prediction correctness tracking and dynamic decay rate calculation.
4. **Toy Example Simulation:** A toy example will simulate the entire healing process, from the initial "wound" to correction application and the restoration of accurate predictions ("true healing"), serving as a proof of concept.
5. **Initial Healing Thresholds:** Initial healing thresholds, potentially around 75-80% directional correctness, will be proposed based on a realistic trading context and subject to further refinement.

### Data Preprocessing with Principal Component Analysis (PCA)

This enhancement focuses on refining input data handling through preprocessing techniques for dimensionality reduction using Principal Component Analysis (PCA). This aims to improve input data quality and potentially boost model performance. Details on the specific application of PCA to price, time, and volume data will be provided in the subsequent sections.
Before applying Principal Component Analysis (PCA), the price (P), volume (V), and time (T) data within a rolling window of _N_ data points undergo preprocessing. This involves transformations and normalization to ensure features contribute equally to the PCA and to enhance the model's robustness and predictive accuracy.

**Data Preprocessing:**

1. **Volume (V) Transformation and Scaling:** Due to the often heavy-tailed distribution of trading volume, a log transformation is applied: `v_i' = log(1 + v_i)`. This mitigates the impact of extreme values. Subsequently, robust scaling using the median and interquartile range (IQR) is performed. This method is less sensitive to outliers than standard z-score normalization.

2. **Time (T) Transformation:** Time is represented as fractional elapsed time within each window. This is calculated by subtracting the minimum timestamp from each timestamp and dividing by the total time elapsed within the window. This fractional representation (ranging from 0 to 1) is then normalized to the range [-1, 1] using the formula: `(2 * time_frac) - 1`. This approach handles irregular timestamps and ensures consistent representation across different trading periods.

3. **Price (P) Transformation:** Raw price values are transformed into log returns relative to the first price in the window. This anchors the price series, mitigating the impact of extreme price spikes and allowing the model to focus on relative price changes. Like time, these log returns are then normalized to the range [-1, 1].

**Principal Component Analysis (PCA):**

Following these transformations, a 3-dimensional matrix is constructed with columns representing fractional elapsed time, normalized log returns, and robustly scaled volume. PCA is applied to this matrix using Singular Value Decomposition (SVD). The first two or three principal components, which capture the most significant variations and correlations within the data, are then used as input features for the model. This dimensionality reduction can improve model performance and efficiency. The SVD implementation can leverage libraries like NumPy: `u, s, vh = np.linalg.svd(X_scaled, full_matrices=False); axes = vh[:2]`. (Adjust the number of components as needed.)
Data Preprocessing and Transformation

This section details the preprocessing steps applied to the price and volume data to improve model performance. These transformations aim to normalize the data, handle outliers, and prepare the data for subsequent Principal Component Analysis (PCA).

First, the volume data is log-transformed using `np.log1p(volume)` to address potential skewness. Outliers are then mitigated by clipping extreme values at the 5th and 95th percentiles. Finally, the volume data is min-max scaled to the [-1, 1] range.

Price data undergoes a similar process. Log returns are calculated, and extreme values are clipped at the 5th and 95th percentiles. The resulting log returns are then min-max scaled to the [-1, 1] range. The time fraction (`time_frac`) is also min-max scaled to the same range. This consistent scaling across time, price, and volume ensures comparable ranges and prepares the data for PCA.

The normalized time, price, and volume features are combined into a single matrix for PCA. Centering the matrix before PCA is optional, as the min-max scaling often results in data already centered around zero.

To visualize the impact of these transformations, five examples illustrating distinct price and volume patterns (uptrend with rising volume, downtrend with volume spikes, reversal, sideways chop, and breakout spike followed by stabilization) will be presented. Each example will include pre-transformation candlestick charts, post-transformation images showing the normalized data, a single-day chart, and a chart with 10-minute intervals.

Model Enhancement and Refinement

This section outlines strategies to enhance the model's predictive accuracy and trading performance.

One approach involves filtering trades based on the magnitude of predictions, acting only on those exceeding a defined threshold. This aims to reduce noise and prioritize high-confidence signals. The relationship between prediction accuracy and trading outcomes will be analyzed to determine if prioritizing high-accuracy trades improves overall results.

Historical Prediction Error Profiling (HPEP) will be implemented to further refine trade selection. HPEP analyzes past prediction errors to create a confidence profile, which is then used as an optimizable hyperparameter (`confidence_threshold`) to filter trades during backtesting. A Post-Training Confidence Profile storing the HPEP data will be generated to facilitate this filtering, enabling past performance to inform future trade decisions and potentially mitigate risk.

Dynamic Plane Representation

This section details the creation of a dynamic plane representation from candlestick and volume data for use as input to the Convolutional Neural Network (CNN).

1. **Data Normalization:** Time, log return, and log volume are normalized to [-1, 1] for consistent scaling.

2. **PCA Rotation:** PCA is applied to the normalized data to reduce dimensionality to two principal components, creating a 2D representation.

3. **Transformation Method:** The combined transformations of log returns, PCA rotation, and normalization generate the final dynamic plane snapshot used as CNN input.

Five illustrative examples (uptrend with rising volume, downtrend with volume spikes, reversal, sideways chop, and breakout spike followed by stabilization) will be visualized as both original candlestick charts with volume and their corresponding transformed dynamic plane snapshots. These paired images will be displayed separately for clear comparison.

Model Refinement and Exploration

This section focuses on further refining the model. While smoothing techniques like bivariate spline interpolation were considered, they were deemed unsuitable due to the model's reliance on discrete data changes. Further model refinements and enhancements are currently under exploration and will be detailed in subsequent updates.

### D. Model Enhancement and Refinement

This section details enhancements and refinements to the core predictive models, aiming to improve trading performance and create a robust, deployable system. These enhancements focus on incorporating multi-scale temporal information, dynamic market context, and self-correction mechanisms.

**Dynamic Frame of Reference and Self-Correction:**

Instead of using a static representation of market data, a dynamic frame of reference is constructed using Principal Component Analysis (PCA). Normalized time, price, and volume data from a recent window are analyzed to determine the two primary axes of correlated movement (PC1 and PC2). These axes define a dynamic 2D plane onto which recent market action is projected, with the most recent data point centered at the origin (0,0). This transformed 2D representation, potentially visualized using candlestick or Heiken-Ashi charts, is rendered as an image and serves as input for the predictive model (e.g., a Vision Transformer).

The model predicts a 2D movement vector (Δx', Δy') within this dynamic plane, representing the anticipated future trajectory of market movement, and "Rally Time," the estimated duration for this movement. A self-correction mechanism, based on a Total Error signal incorporating Vector Deviation Error (distance and angular error) and Frame Shift Error (discrepancy between predicted and actual dynamic plane), triggers a "Healing Phase" when errors exceed a dynamic threshold. This phase progressively restores full dynamism based on improving predictive accuracy, enabling the model to adapt and recover from periods of poor performance.

**Multi-Scale Temporal Modeling and After-Market Forces:**

To capture cyclical patterns and broader market influences, the model incorporates multi-scale temporal information. Predictions from different timeframes (daily, weekly, monthly, quarterly, and yearly) are integrated, likely through a weighted sum approach. The architectural design will address optimal weight assignment and account for inherent cyclical patterns within each timeframe. This incorporation of periodicity aims to capture the implicit influence of after-market forces not directly reflected in intraday data. Systematic experimentation with various weighting schemes will be crucial for maximizing predictive accuracy.

**Addressing Data Pipeline Complexity and Exploring Advanced Architectures:**

Integrating multi-timeframe data using the Dynamic Rotating Plane method introduces data pipeline complexity. Efficient data flow and processing will require careful dataset preparation for each timeframe within this framework. Two advanced model architectures will be explored:

- **Ensemble Model Approach:** Specialist models trained for each timeframe (e.g., 10-minute, daily, weekly) will have their predictions combined for a potentially more robust overall prediction.
- **Multi-Input Transformer Model:** This architecture would receive multi-timeframe data as input, potentially offering a more integrated and efficient approach compared to the ensemble method. Further investigation will determine the most suitable architecture.

### D. Model Enhancement and Refinement

This section details enhancements and refinements to the core model architecture, data handling, and prediction strategy.

**Multi-Scale Data Integration and Non-Hierarchical Attention:** To capture both short-term fluctuations and long-term trends, market data will be processed across multiple timeframes: intraday, daily, weekly, and monthly. Datasets for each timeframe will be generated using the Dynamic Rotating Plane method, providing a richer context for predictions. A hierarchical attention mechanism will dynamically weigh information from different timeframes based on the intraday context, improving both computational efficiency and interpretability. This mechanism allows the model to focus on the most relevant timescales for a given prediction.

**Dynamic Rotating Plane:** To capture the local dynamics of market data, a dynamic 2D plane will be implemented. This plane, derived from principal component analysis (PCA) applied to normalized Time, Price, and Volume data, will be dynamically re-centered on the latest data point, ensuring its continued relevance to current market conditions. This approach provides a concise representation of market movements, which is used as input to the model.

**Ensemble Methods and Multi-Input Transformers for Multi-Timeframe Data:** Ensemble methods and multi-input transformer models will be explored to leverage data from all timeframes concurrently. The transformer's attention mechanism will dynamically weight the importance of information from each timeframe, capturing complex inter-timeframe relationships. Various weight assignment strategies, including static and learned weights, will be investigated to optimize the model's predictive power.

**Historical Prediction Error Profiling (HPEP):** To improve prediction reliability, a Historical Prediction Error Profile (HPEP) will be implemented. This module tracks and profiles prediction errors over time, creating a map of confidence levels based on past performance in similar market conditions. A `confidence_threshold` hyperparameter will be tuned to optimize the trade selection process, allowing the system to execute trades selectively when the model exhibits high confidence.

**Soft Labeling and Loss Function Refinement:** Instead of hard labels, the CNN model will be modified to predict a probability distribution over possible outcomes (soft labeling). This allows for the use of more nuanced loss functions during training, improving the model's ability to express uncertainty and potentially leading to better calibration.

**Incorporating Fundamental and Macroeconomic Data:** Fundamental and macroeconomic data will be integrated to enhance predictive power. This integration requires careful consideration of data handling strategies, including addressing potential challenges such as missing data and differing data frequencies. Different memory management strategies will be evaluated to handle the increased data load efficiently.

**Error Signal Integration and Self-Correction:** A self-correcting mechanism will be implemented by integrating an error signal into the market movement algorithm. This allows the model to learn from its mistakes and adapt to changing market conditions. The mechanism dampens frame rotation when the combined "Vector Deviation Error" and "Frame Shift Error" (Total Error) spikes, allowing the system to gradually self-correct as prediction accuracy improves. Data normalization of Time, Price (log-returns), and Volume (log-transformed, robustly scaled) into a uniform [-1, +1] range is crucial for this process.

**Model Evaluation and Analysis:** The enhanced multi-scale model will be rigorously evaluated against a baseline intraday model to quantify the impact of incorporating multi-scale information. Attribution analysis will be performed to understand the model's reliance on different timeframes, providing insights into its decision-making process. The model's performance will be analyzed during major market events to assess its robustness and adaptability to changing market dynamics.

## D. Model Enhancement and Refinement

This section details enhancements and refinements to the model, focusing on improving prediction accuracy, robustness, and adaptability to various market conditions and trading strategies.

**Expanding Model & Learning Architecture Options:**

The current model and learning architecture will be expanded to offer greater flexibility and cater to a wider range of use cases. This includes:

- **Separate CNNs for Different Holding Periods:** Training specialized CNNs for various holding periods (e.g., 1, 3, 5, 10 candles) will allow for tailoring models to specific trading horizons. This addresses the current limitation of predicting only the next immediate candle.
- **Soft Label Approach for CNN:** Investigating and potentially implementing a soft label approach for CNN predictions. This involves modifying the CNN to output a probability distribution over possible outcomes instead of a single deterministic prediction. This requires modifying the CNN architecture and utilizing an appropriate loss function, such as cross-entropy loss.
- **Multi-Scale Context Fusion:** Implementing a multi-scale context fusion mechanism to incorporate information from different time scales (short-term and long-term trends). Users will be able to select from several fusion methods: Attention-Based (ViT), Concatenation, and Weighted Average.

**Improving Prediction Accuracy and Robustness:**

Several enhancements are proposed to improve the model's predictive capabilities and overall robustness:

- **Implement Lookahead Period in Training:** Enhancing the model to predict price movements over a variable lookahead period, allowing for exploration of trading strategies with different holding periods.
- **Prediction Magnitude Filtering:** Filtering trades based on the magnitude of the predicted return, using a user-defined threshold to reduce false positives and focus on higher-conviction trades.
- **Trading Based on Maximum Prediction Accuracy:** Exploring methods for estimating prediction confidence or reliability to select trades with the highest predicted accuracy.
- **Historical Prediction Error Profiling (HPEP):** Implementing HPEP as an optimizable hyperparameter. This involves profiling historical prediction errors based on factors like market conditions and prediction magnitude. This profile will then be used to adjust trade selection or position sizing. This requires code implementation, building a post-training confidence profile, backtesting trade filtering, and introducing a tunable `confidence_threshold` parameter.
- **5 Candlestick Window Justification:** Conducting a literature review to justify the choice of a 5-candlestick input window and explore potential alternatives.

**Incorporating Additional Data and Improving System Performance:**

The following enhancements aim to improve the model's performance and resource utilization:

- **Incorporating Fundamental and Macro Data:** Enhancing the model by incorporating fundamental and macroeconomic data to provide additional context and potentially improve predictive power.
- **Memory Management Optimization:** Evaluating different memory management strategies (Options A, B, and C) through experiments to ensure efficient resource utilization.
- **Error Signal Integration:** Adding an error signal to the market movement algorithm to enable continuous learning and adaptation.

**Enhancing User Interface and Transparency:**

The current UI for displaying the Model & Learning Architecture will be reviewed and improved to provide users with more comprehensive information and facilitate informed decision-making. This will enhance usability and transparency, ensuring users have a clear understanding of the model's configuration and can effectively adjust parameters.

**Performance Monitoring and Healing:**

To maintain consistent performance, a healing mechanism will be implemented, triggered by either:

- **Performance-Based Healing Trigger:** Activated if the model's performance falls below a user-defined threshold.
- **Time-Based Healing Trigger:** Activated if the total error (composed of Vector Error and Frame Shift Error, each with adjustable weights) remains above a predefined threshold for a specified duration. Both the threshold and duration will be configurable parameters.

These enhancements will create a more robust, adaptable, and user-friendly model capable of handling diverse market conditions and achieving consistent trading performance.

### D. Model Enhancement and Refinement

This section details enhancements and refinements implemented to improve the model's performance. Each enhancement is described with technical specifics to ensure reproducibility and minimize ambiguity.

- **Context Awareness:** The model dynamically adjusts the input data's granularity by optimizing the number of candlesticks per frame and the total number of frames. This adaptation to varying market conditions allows the model to capture patterns across different timescales. Further research will explore optimal adaptation strategies.

- **Transfer Learning:** We investigated the effectiveness of transfer learning between US and Indian markets. This involved training the model on one market and evaluating its performance on the other, both within and across regions. Results of this investigation and their implications for model generalization are documented separately.

- **Context-Aware Periodicity Weighting:** Predictions are weighted based on different time periodicities (daily, weekly, monthly, quarterly, and yearly). The optimal weighting configuration considers factors such as the number of frames, candlesticks per frame, and stock characteristics (market capitalization, sector, and share price). This aims to leverage periodic patterns in market behavior. The specific weighting scheme and its optimization process are documented in detail.

- **Dimensionality Reduction with PCA:** The modeling process was repeated using a dynamic plane derived from the two principal components of the input data. This explored the potential of dimensionality reduction for identifying more robust predictive features. A detailed analysis of the results and the impact of PCA on model performance is documented separately.

- **Hyperparameter Permutation Testing:** To establish baseline performance and inform subsequent hyperparameter tuning, an exhaustive search of all hyperparameter permutations was conducted for a single training epoch. This analysis provided insights into the impact of various hyperparameter combinations on the given training and testing data. The complete results of this permutation testing are documented separately.

- **Prediction Magnitude Filtering:** A filter based on the magnitude of predicted returns was implemented. Trades are executed only when the predicted return exceeds a predefined threshold, reducing the impact of low-confidence predictions. The threshold value, its justification, and its impact on performance metrics (e.g., Sharpe Ratio, maximum drawdown) are documented in detail.

- **Trading Based on Maximum Prediction Accuracy:** This enhancement incorporates historical prediction accuracy into trading decisions. A mapping between predicted return magnitude and historical accuracy was created. Trades are selected based on this mapping, prioritizing predictions with higher historical accuracy. The methodology for calculating and applying this accuracy-based filter, including the data used, timeframe, and selection logic, is thoroughly documented, supplemented with charts and diagrams illustrating the relationship between prediction magnitude and historical accuracy.

- **Historical Prediction Error Profiling (HPEP) as an Optimizable Hyperparameter:** Historical prediction error profiling (HPEP) was implemented as an optimizable hyperparameter. This involves analyzing historical prediction errors based on factors such as market conditions, time of day, and asset characteristics. The specific implementation details and the impact of HPEP on model performance are documented separately.

### D. Model Enhancement and Refinement

This section details enhancements and optimizations to the model and its supporting infrastructure, addressing computational costs associated with image processing and exploring advanced training methodologies.

**Historical Performance Error Profile (HPEP):** A Historical Performance Error Profile (HPEP) is employed to dynamically adjust model predictions and trading decisions. This profile captures the model's historical prediction accuracy under various market conditions. The implementation details of HPEP, including profile generation, storage, and utilization within the trading strategy, are fully documented. Specific examples demonstrate how HPEP influences trading decisions across diverse market scenarios. Key components include:

- **Code Implementation and Backtesting:** The HPEP module's code is thoroughly documented, explaining each segment's purpose and functionality. The construction and storage of the HPEP map, including data structures and algorithms, are detailed. The process of using the HPEP map for trade filtering during backtesting is explained, along with the specific evaluation metrics employed.
- **Confidence Threshold:** The `confidence_threshold` hyperparameter controls the trading strategy's sensitivity to prediction confidence. The tuning process, the explored range of values, and the final optimal value are documented. The impact of this hyperparameter on performance metrics is analyzed and visualized.
- **Input Window Justification:** The rationale for the 5-candlestick input window is thoroughly justified, citing prior literature and empirical evidence. Alternative window sizes are evaluated and compared.
- **Advanced Model Configurations (If Applicable):** The following enhancements are documented if implemented:
  - **Separate CNNs for Holding Periods:** Training separate CNNs for different holding periods is detailed, including architecture and training parameters for each model.
  - **Soft Label Approach:** The exploration and implementation of a soft label approach for CNN predictions are documented, including architectural changes and loss function modifications.
  - **Fundamental and Macro Data Integration:** If incorporated, the specific data sources, features, and integration methods are meticulously documented.
- **Memory Management and Error Signal Integration:** The evaluation of different memory management options (A, B, and C) is documented, along with the results. The implementation of an error signal into the market movement algorithm, its specific form, and its impact on performance are analyzed and presented with supporting data.

**Optimizing Image Processing and Model Training:**

To mitigate computational costs and enhance efficiency, the following strategies are proposed for optimizing image generation and the computer vision pipeline:

- **Offline Image Processing:** Image processing will be moved offline to reduce the load on the main application server. Different infrastructure options will be explored to minimize cloud computing costs without compromising user experience.
- **Optimized Image Generation and Computer Vision:**
  - **Caching:** Processed images will be cached to reduce redundant computations.
  - **Lightweight Architectures:** Less resource-intensive architectures for image generation and computer vision algorithms will be investigated.
- **iPad-Based Processing (Exploration):** Offloading image generation and model training to the user's iPad will be explored as a longer-term optimization. This investigation will consider memory constraints, data access, and the potential for federated learning or hybrid approaches. Key aspects include:
  - **Federated Learning/Hybrid Approach:** This approach, distributing the training process and reducing reliance on centralized servers, requires further research and experimentation.
- **Maintaining UI Functionality:** All optimizations will prioritize preserving full user control over model training and retraining through a seamless user interface, abstracting away the underlying infrastructure complexities.

### D. Model Enhancement and Refinement

This section details enhancements and refinements to the model training process, addressing performance, stability, and resource constraints, particularly on iPads. A hybrid architecture distributing tasks between a server and the iPad client balances computational load and enables efficient use of local data.

**Hybrid Training Architecture:** To mitigate potential resource limitations on iPads, particularly during GPU-intensive tasks like Vision Transformer training, a hybrid approach is employed. Initial model training and large-scale multi-permutation campaigns will be executed on a server equipped with robust GPU capabilities (e.g., Google Cloud Run jobs utilizing NVIDIA T4 GPUs). Subsequent fine-tuning and personalized model adaptation will occur on the iPad, leveraging smaller batches processed via Web Workers to minimize browser crash risks and memory constraints.

**PWA Stability and Performance:** A thorough analysis of PWA feasibility for this application will be conducted, specifically addressing stability concerns related to resource-intensive computations on iPads. This includes investigating Vision Transformer training stability within the PWA environment, considering resource limitations, potential cross-platform inconsistencies, and browser-specific behavior. Optimization methods and alternative approaches (e.g., native applications, cloud-based solutions) will be explored if necessary. Data on TensorFlow.js and WebGPU performance and stability on iPadOS will be collected. Research will focus on current browser memory limits for PWAs (including the 50MB cache limit), potential crash rates for GPU-intensive workloads, and access to iPad GPU resources within the PWA browser environment. Different web standards will be investigated to overcome these limitations and ensure a stable and optimized application.

**Client-Server Interaction and Federated Learning:** The server acts as a central hub, storing and serving the raw market data, maintaining the global master model (a consolidated version trained across multiple iPads), and orchestrating the training process. The server distributes the global model and instructions to each connected iPad and implements federated averaging to combine individual client model updates into a refined global model. The server provides API endpoints for data retrieval, model distribution, and receiving model updates. On the client-side, the iPad fetches raw market data, generates candlestick chart images using JavaScript libraries (e.g., Canvas API), and performs on-device model training with TensorFlow.js. After local training, the iPad sends updated model weights back to the server. Web Workers handle background processing of image generation and model training to maintain UI responsiveness.

**UI Adaptation:** The iPad application's user interface will reflect the hybrid training process. The Campaign Runner and Experiment Designer will display status indicators for data downloading, image generation, local training, and uploading model updates. The UI will also incorporate local resource monitoring (CPU, memory usage) to provide feedback to the user.

### D. Model Enhancement and Refinement (Specialization)

This section details strategies for enhancing the model's predictive performance and robustness. These enhancements encompass both client-side refinements and backend support.

**Client-Side Refinements:**

- **Prediction Filtering and Confidence Thresholds:** The model's effectiveness can be improved by filtering predictions based on a tunable `confidence_threshold`. This ensures that only high-confidence predictions, exceeding the defined threshold, trigger trading actions. Further investigation will explore trading strategies optimized for maximum prediction accuracy.
- **Historical Prediction Error Profiling (HPEP):** HPEP will be implemented to leverage historical prediction errors. This involves creating and maintaining an HPEP map, generated post-training. During backtesting and live trading, this map will be used to identify and prioritize trades where the model historically exhibits higher confidence, potentially improving trade selection and overall performance. The HPEP map itself will be treated as an optimizable hyperparameter.
- **On-Device Model Fine-tuning:** The model will be designed to support on-device fine-tuning (delta training) using Core ML on the iPad. This allows the model to adapt to recent market dynamics using locally available data, without requiring the full dataset to be transmitted to the server. Resulting weight updates (deltas) will be sent back to the server to update the primary model.

**Backend Support and Integration:**

The Python backend plays a crucial supporting role, focusing on data management, API interaction, and model versioning:

- **Data Serving:** The backend maintains the master database of OHLCV and other relevant numerical data. It serves this data to the iOS app on demand, ensuring the app has access to the latest information for image generation, model training, and backtesting.
- **API Management and Authentication:** Secure connection and authentication with external APIs, such as the Zerodha Kite Connect API, are handled by the backend. This allows the app to execute trades based on its predictions while maintaining security and controlling access.
- **Model Versioning and Aggregation:** While the client performs on-device training and generates model updates (deltas), the backend manages the storage and serving of different model versions. This facilitates potential future enhancements like collaborative learning or centralized model improvement strategies.

This hybrid architecture leverages the strengths of both client and server. The iOS app, utilizing Core ML and potentially Metal for image generation within the `DynamicPlaneGenerator`, handles computationally intensive tasks like on-device training, backtesting, and live inference. The backend manages data, API access, and model versioning, minimizing its complexity and resource requirements. This division of labor allows for a scalable and efficient system. Further development will explore migrating the frontend to a native iOS Swift application to further optimize performance and user experience.

### D. Model Enhancement and Refinement

This section details refinements to the model's architecture, implementation, and performance monitoring for optimal cross-platform deployment.

**Decoupled Architecture and Client-Side Processing:** The system employs a decoupled architecture. A Python backend serves the raw numerical data, provides the latest master model file, and aggregates model updates from client applications. Client-side processing of computationally intensive tasks—data processing, image generation, and prediction execution—leverages the power of client devices and reduces server load. This decoupling allows flexibility in porting the frontend to different platforms, including Progressive Web Apps (PWAs) and native mobile applications.

**Platform-Specific Optimizations:** To maximize performance, the application leverages platform-specific machine learning frameworks. Core ML is utilized on iOS, while TensorFlow Lite (via the `tflite_flutter` package) is used for Android and other platforms. This approach requires converting the universal PyTorch/TensorFlow model into the respective `.mlmodel` and `.tflite` formats. A model management strategy will address model loading, updates, and versioning across these formats.

**Platform Channels for Native Integration:** Communication between the shared Dart code and the native machine learning implementations (Swift for iOS and Kotlin for Android) is facilitated by Platform Channels. This allows seamless data exchange (image input and model predictions) between the Flutter application and the native components. On iOS, a dedicated `CoreMLHandler.swift` class interacts directly with Core ML for optimized performance.

**Error Rate Monitoring and Re-tuning:** Continuous error rate monitoring is crucial for long-term model stability. The system will track prediction accuracy and trigger a re-tuning process when the error rate exceeds a predefined threshold. This automated re-tuning ensures the model remains effective and adapts to evolving market dynamics. The specifics of error rate calculation and the re-tuning process will be detailed in subsequent documentation.

**TensorFlow Lite Bug Evaluation and Cross-Platform Compatibility:** Known bugs within the Flutter TFLite implementation will be thoroughly investigated, documenting reproduction steps and potentially reporting them to the TFLite maintainers. Furthermore, cross-platform compatibility between TensorFlow Lite and Core ML will be explored, considering conditional compilation or runtime checks to dynamically switch between frameworks on iOS for potential performance gains.

**On-Device Training and Personalized Model Adaptation:** The system will implement on-device training, allowing users to personalize the model based on their individual needs and preferences. Further details on the implementation of on-device training will be provided in subsequent documentation.
Model Enhancement and Refinement

This section details the strategies employed to enhance and refine the SCoVA model's predictive accuracy and adapt its performance to evolving market conditions. Further details regarding specific techniques will be provided upon receiving the relevant checklist items. A placeholder for this information is included below.

**Awaiting Checklist Information for Model Enhancement and Refinement.** This section will be populated with specific details on model enhancement and refinement techniques once the relevant checklist items are provided. Anticipated topics include, but are not limited to:

- Prediction Magnitude Filtering: How small predictions are handled and potentially disregarded.
- Historical Prediction Error Profiling: Analyzing past predictions to identify systematic biases or weaknesses.
- Soft Labeling: Leveraging probabilistic labels for improved model calibration.
- Hyperparameter Tuning and Optimization: Strategies for finding optimal model parameters.
- Backtesting Procedures: Evaluating model performance on historical data.
- Ensemble Methods: Combining multiple models for increased robustness.
- On-Device Training and Federated Learning: Adapting the model to individual user data while preserving privacy. (This item was addressed in a previous misplaced checklist chunk.) Users will be able to fine-tune the platform-specific model on their own devices using frameworks like Core ML on iOS. A synchronization mechanism will extract updated weights or weight deltas from these client-side models after on-device training and transmit them back to the Python backend to be incorporated into the universal source model. This creates a feedback loop, allowing the universal model to learn and improve based on aggregated user data while respecting user privacy.

This placeholder ensures the document structure remains intact while awaiting the correct information. Providing the relevant checklist chunk will enable the completion of this crucial section.

### D. Model Enhancement and Refinement (Specialization)

This section details enhancements and refinements to the core model, aiming for increased prediction accuracy and profitability while remaining mindful of resource constraints. The refinement process emphasizes efficient experimentation to minimize computational costs.

**Cost-Effective Model Development:**

Model development and refinement will prioritize resource efficiency. Strategies to minimize server costs include:

- **Limited Training Runs:** Initial testing and debugging will utilize limited training epochs and smaller data ranges.
- **Offloading to iOS:** Whenever possible, processing, including complete app runs and stability checks, will be shifted to the less resource-intensive iOS device environment, reducing reliance on Google Cloud resources.
- **Code Optimization:** Code will be optimized for efficiency to minimize the computational load and associated costs, especially during the iterative debugging process.

**Multi-Stage Testing and Validation:**

A multi-stage testing approach will further enhance cost-effectiveness and enable early error detection:

1. **Unit/Integration Tests (Mock Data):** Isolated testing of individual components and their interactions will be conducted using mock data (CSV or JSON format) representing various market conditions, including edge cases like spikes, flat periods, and gaps. This avoids the expense of using real market data during early testing phases.

2. **End-to-End Dry Runs:** A full end-to-end dry run of the system, simulating the entire data flow and processing pipeline without actual model training or live market data, will verify complete system functionality and data integrity before utilizing cloud resources. A "Dry Run" toggle in the Experiment Designer UI will facilitate this process.

3. **On-Device Smoke Tests (Dummy Model):** On-device smoke tests using a small, readily available dummy neural network will verify basic functionality and deployment integrity on the target hardware. The necessity of the current 1-epoch training of the dummy model during these tests will be investigated to potentially further optimize speed and efficiency.

**On-Device Training Validation:**

Beyond basic functionality, on-device training will undergo rigorous validation to ensure the integrity of the entire training pipeline:

- **Training Loop Execution:** Validate complete execution of a training step (forward pass, loss calculation, backpropagation) within the Core ML framework on-device.
- **Data Pipeline Connection:** Verify the seamless flow and correct format of transformed image tensors from the `DynamicPlaneGenerator` to the Core ML training session on the iPad.
- **Model Update Mechanism:** Clarify how Core ML updates the universal model on the backend during on-device training.
- **Data Loading and Weight Extraction:** Validate data loading procedures and the ability to extract weights after the training step.

**Advanced Enhancements (Future Exploration):**

Looking ahead, the following enhancements will be explored to further improve predictive capabilities:

- **Real-time Event Detection:** Develop a system for real-time detection of events that could impact market performance, providing a more proactive approach to risk management.
- **Expanded Computer Vision Application:** Extend the application of computer vision techniques to a wider range of financial data beyond candlestick charts.

This comprehensive approach to model enhancement and refinement aims to maximize performance gains while minimizing resource consumption and ensuring a robust and reliable on-device training pipeline.

## D. Model Enhancement and Refinement (Specialization)

This section details enhancements designed to improve the model's predictive capabilities, particularly its handling of “shocker events”—unexpected market fluctuations characterized by metrics such as volatility spikes, anomalous trading volume, and rapid price changes. These enhancements aim to shift the model from a reactive to a proactive approach, anticipating and capitalizing on market shifts.

Initially, a dual-system architecture—comprising "exploitation" and "exploration" components—is proposed to address the current model's limitations in handling extreme market volatility. The "exploitation" system focuses on maintaining stability and performance during normal market conditions using the existing error-correction mechanism. Conversely, the "exploration" system identifies and capitalizes on high-alpha opportunities presented by market extremes. This dual functionality is further refined by implementing a "Flow Engine" and a "Threat Engine." The Flow Engine manages normal market behavior, while the Threat Engine detects and reacts to "shocker events." Upon detecting such an event, the Threat Engine overrides the Flow Engine, shifting the model from "exploitation" to "exploration" mode.

This dual-system approach is complemented by specialized Convolutional Neural Networks (CNNs) trained to act as "threat detectors." One CNN focuses on equity market anomalies like gaps and volume spikes, while another analyzes visualized options chain data (heatmaps) for derivatives market anomalies. The outputs of these specialized CNNs are combined to produce a Systemic Threat Level (STL) score, representing the overall market anomaly level. This STL integrates into the system in three ways: (1) triggering proactive trading withdrawal (Pratyahara) during periods of high instability; (2) dynamically adjusting the Dynamic Plane's smoothing factor to adapt to market volatility; and (3) serving as a context token for the final prediction process, improving prediction accuracy.

Furthermore, a Shockwave Prediction Model (SPM) is introduced, specifically trained on historical "shocker events" to predict short-term price movements following a market shock. A dynamic weighting mechanism, called the "seesaw," balances the predictions of the SPM and the Flow Engine based on the STL. This allows the model to leverage the strengths of both predictive approaches, maximizing performance during both normal and volatile market conditions. Finally, the model's retraining strategy differentiates between regular market flow and shock events, ensuring the model remains sensitive to both gradual trends and sudden market shifts, preventing performance plateaus. This enhanced architecture aims to improve the model's ability to perform consistently across a wider range of market conditions and capitalize on high-alpha opportunities presented by "shocker events."

### D. Model Enhancement and Refinement

This section details enhancements and refinements to the core model, focusing on specialized functionalities and improvements to prediction accuracy. These enhancements are designed to be implemented modularly, minimizing inter-module communication and promoting maintainability.

**Specialized Prediction Refinement Techniques:**

- **Prediction Magnitude Filtering:** Implement a mechanism to filter trades based on the predicted price movement magnitude. Only execute trades exceeding a predefined threshold, improving the risk-reward ratio by focusing on higher-conviction predictions.

- **Confidence-Based Trade Selection:** Explore prioritizing trades based on the model's confidence in its prediction. This might involve favoring trades where the model has historically demonstrated high accuracy for similar patterns.

- **Historical Prediction Error Profiling (HPEP):** Integrate HPEP as an optimizable hyperparameter. This involves profiling the model's historical prediction errors and using this profile to inform trading decisions. A post-training HPEP map will be constructed and used to filter trades during backtesting. An `accuracy_threshold` hyperparameter will be tuned to optimize this filtering process.

- **Input Window Optimization:** Conduct a literature review to justify the current 5-day candlestick window and explore alternatives. Train separate CNNs with various window sizes and holding periods to determine the optimal input window for accurate predictions.

- **Probabilistic Predictions (Soft Labeling):** Investigate and implement soft labeling for the CNN. Modify the network to output a probability distribution over possible price movements instead of a single deterministic prediction, necessitating a suitable loss function for probability distributions.

**Specialized Data Transformation and Feature Engineering:**

- **Normalization Service:** Develop a service to normalize numerical input arrays, ensuring consistent input to the model and potentially improving training stability and prediction accuracy.

- **Principal Component Analysis (PCA) Service:** Create a service to perform PCA on numerical arrays for dimensionality reduction, feature extraction, and noise reduction, potentially improving model performance and reducing computational complexity.

- **Coordinate Rotation Service:** Implement a service to rotate numerical arrays using provided basis vectors. This enables data transformation into different coordinate systems, potentially revealing hidden patterns and facilitating analyses like the Dynamic Plane Implementation.

These specialized components will contribute to a more robust and adaptable trading system. Any necessary coordination between these components will be managed through a central facilitator module, adhering to the overall system architecture.

## D. Model Enhancement and Refinement (Specialization)

This section details the implementation of specialized services designed to enhance and refine the model's predictive capabilities. These services adhere to architectural principles of modularity and maintainability, with a focus on clear separation of concerns and controlled interactions. This approach facilitates future enhancements and improves the system's overall robustness and scalability.

This refinement process encompasses several key aspects: specialized component development, asynchronous communication where applicable, controlled resource access, orchestrated workflows, and a structured codebase. Specifically, enhancements focus on prediction magnitude filtering, exploring trades based on maximum prediction accuracy, and historical prediction error profiling (HPEP). These enhancements are implemented as isolated components to minimize unintended side effects and ensure clear boundaries between functionalities.

Furthermore, the enhancements are integrated into the `ExperimentRunner` service, which orchestrates training and backtesting. The `ExperimentRunner` interacts with the specialized components through designated Facilitators, ensuring controlled execution and data flow. This mediated communication pattern enhances maintainability and prevents direct coupling between components.

The following specialized services support these enhancements:

- **NormalizeWindow:** This service normalizes raw numerical array data according to a provided configuration dictionary, preparing the data for subsequent processing stages like principal component analysis. It operates under strict dependency constraints, prohibiting the import of `google-cloud-storage`, `google-cloud-firestore`, and `requests` libraries to maintain a focused functionality.

- **ComputePrincipalComponents:** This service performs dimensionality reduction by taking a normalized array and computing the top two principal component vectors. Its implementation is restricted to using only the `numpy` library, ensuring efficient and deterministic computations.

- **ProjectToPlane:** Using the output of `ComputePrincipalComponents`, this service projects the original data onto a 2D plane defined by the computed basis vectors, simplifying data representation for potential visualization and subsequent model training.

- **TrainOneEpoch:** This service encapsulates a single epoch of model training. It receives model artifact bytes, training data tensors, and a configuration as input, returning updated model artifact bytes. Its design deliberately avoids awareness of data origin or destination, enforcing a strong separation of concerns and promoting reusability.

The implementation also leverages asynchronous communication where appropriate, using a publish/subscribe model for components operating independently or with varying latencies. Resource access is strictly controlled through Enforcer components that validate and authorize access requests, ensuring data security and preventing unauthorized modifications. Facilitator components manage workflows, coordinate the execution of isolated components, and manage data flow, simplifying the addition of future enhancements. Finally, class inheritance structures the codebase, promoting code consistency, clarity, and long-term maintainability. Consistent function naming conventions and adherence to the four functional pillars (Continuity, Enforcement, Facilitation, and Specialization) further contribute to a robust and maintainable implementation.
Centralizing data management, the `State_Enforcer` streamlines updating and retrieving training data, model parameters, and evaluation results.

- **`Resource_Enforcer`:** This service maintains exclusive control over Google Cloud Storage, safeguarding model artifacts, datasets, and other essential resources. Consistent and secure management of experiment results, training logs, and model checkpoints contributes to reproducible results and facilitates efficient model refinement.

- **`Live_Execution_Enforcer`:** This service secures Zerodha API interaction, ensuring reliable and secure trade executions during backtesting and live trading. This enables accurate performance assessment under realistic market conditions and supports further refinement based on observed trading behavior.

- **`Workflow_Broker`:** Orchestrating complex tasks, such as building the dynamic plane or running experiments, is crucial for model refinement. The `Workflow_Broker` coordinates the execution of specialized services, facilitating structured experimentation and analysis for more efficient model enhancement.

- **`Pre-flight_Validation_Service`:** Working in conjunction with the `Resource_Enforcer`, this service validates data, ensuring its integrity and supporting model refinement. This robust mechanism verifies data consistency and completeness, which is fundamental for reliable model training and evaluation.

These services establish a foundation for efficient and robust model enhancement and refinement. While this section focuses on the supporting infrastructure for data integrity, resource management, and secure workflow execution, the following section details specific refinement techniques.

### D. Model Enhancement and Refinement

This section details enhancements and refinements to the core model, focusing on specialized techniques to improve predictive accuracy and trading performance. The following principles and tasks will guide development:

- **Inheritance-Based Design:** All enhancements and refinements will adhere to an inheritance-based approach, maintaining design consistency and seamless integration with the existing system.

- **Four Base Classes:** All new services will inherit from one of four base classes: `ContinuityService`, `EnforcementService`, `FacilitationService`, or `SpecialistService`. This reinforces architectural consistency and provides a clear organizational structure.

- **Base Class Specifications and Pseudocode:** Before implementation, detailed technical specifications and pseudocode will be drafted for the four base classes and any inheriting services. This pre-implementation planning ensures clarity, maintainability, adherence to the architectural vision, and early identification of potential challenges. This documentation will serve as a blueprint for all new features.

This disciplined approach promotes code reusability, maintainability, scalability, and facilitates clear communication and collaboration within the development team. Specifically, the following refinements will be implemented:

1. **Prediction Magnitude Filtering:** Similar to a confidence threshold, this technique filters trades based on the magnitude of the predicted return. Only trades exceeding a predefined threshold (positive or negative) will be executed.

2. **Trading Based on Maximum Prediction Accuracy:** This involves analyzing the model's historical prediction accuracy across various market conditions and prioritizing trades where the model has historically performed best.

3. **Historical Prediction Error Profiling (HPEP):** HPEP profiles the model's historical prediction errors under different scenarios (e.g., volatility levels, market trends). This "error map," stored as an optimizable hyperparameter, allows trading strategies to adapt based on the expected error for a given market context.

4. **HPEP Module Implementation:** This module generates, stores, and retrieves the historical error profile.

5. **Post-Training Confidence Profile (HPEP Map Construction):** This involves building and storing the HPEP map, which carries historical error information throughout the trading process to inform decision-making.

6. **Backtesting Trade Filtering (HPEP-Based Filtering):** This uses the HPEP map during backtesting to filter out potentially unprofitable trades based on past performance in similar market conditions. These refinements, inspired by principles of distributed systems monitoring and optimization, aim to enhance the model's robustness and performance.

### D. Model Enhancement and Refinement (Specialization)

This section details enhancements and refinements aimed at improving the stock prediction model's performance, robustness, and adaptability to diverse market conditions. These enhancements leverage the "Four Pillars System Architecture" (Specialists, Facilitators, Enforcers, and Continuity), with the changes described here falling primarily under the "Specialist" category.

**Dynamic Input Representation:** Currently, the model uses static candlestick images. We propose transitioning to a "Dynamic Rotating Plane" approach. This envisions the market as a 2D plane that dynamically re-centers and rotates based on Time, Price, and Volume. This dynamic representation offers a more responsive and nuanced view of market dynamics compared to static images.

**Dual-Engine Prediction System:** To enhance robustness and adaptability, we will implement a "Dual-Engine Perception" system. This system comprises two distinct models: a "Flow Engine" optimized for normal market conditions, and a "Shockwave Prediction Model (SPM)" specifically trained to identify and react to volatile market events. A weighted "seesaw" mechanism, governed by a calculated "Systemic Threat Level (STL)," will balance the outputs of these two engines to produce the final prediction.

**Client-Side Heavy Lifting:** Given the computational demands of training and refinement, we will adhere to the "Client-Side Heavy Lifting" principle. Tasks such as investigating prediction magnitude filtering, exploring trading based on maximum prediction accuracy, and implementing Historical Prediction Error Profiling (HPEP) will be performed on the client device. This offloads the server and accelerates experimentation and iteration cycles. Refined models developed client-side will be uploaded to a centralized "Universal Model Hub" on the lightweight backend, ensuring consistency and facilitating distribution of improved models across all client platforms.

**Additional Enhancements:**

- **Advanced Error Signal:** A composite error signal, incorporating Vector Deviation Error and Frame Shift Error, will provide a comprehensive measure of model health and deviations for improved monitoring and adjustment.

- **Performance-Based Healing:** Model recalibration will be triggered dynamically based on prediction accuracy, ensuring responsiveness to changing market conditions.

- **Multi-Scale Periodicity:** The model will incorporate and fuse data from multiple timeframes (intraday, daily, weekly) to better understand cyclical patterns and their influence on price movements.

- **"Rally Time" Prediction:** The model will be extended to predict the expected duration (in candlesticks) of the predicted price movement, adding a crucial temporal dimension to predictions.

- **Distributed Tracing:** System-wide distributed tracing using OpenTelemetry will be implemented to optimize performance, identify bottlenecks, and facilitate debugging.

**Explainability and Justification:**

To enhance transparency and regulatory compliance, we will implement the following:

- **Narrative Generation Service:** A dedicated service will generate human-readable explanations for each trade. This service will integrate with the Feature Store (for versioned input features and system state), utilize model-agnostic (LIME, SHAP) and model-specific (attention maps) attribution methods, and leverage a Large Language Model (LLM) to synthesize a coherent narrative. These narratives will be stored in the Karma Ledger for auditing and compliance.

- **Explanation AI Integration:** We will explore integrating an "explanation AI" to further enhance the transparency of the model's predictions and provide deeper insights into its decision-making process.

### D. Model Enhancement and Refinement

This section details enhancements to refine the model's predictive capabilities by incorporating order book dynamics, market depth analysis, and a novel anomaly detection mechanism. These enhancements address limitations in the available data and aim to improve the model's accuracy and robustness.

**Addressing Zerodha Data Limitations:**

Due to the fixed price increments (± ₹0.05 between -₹0.25 and +₹0.25) provided by Zerodha's market depth data, initial plans for spread-based calculations were deemed unsuitable. Instead, the model will focus on analyzing order quantities and the number of orders at each of the five fixed bid/ask levels provided by the Zerodha API. This approach maximizes the information extracted from the available data.

**Order Book Feature Integration:**

To leverage order book dynamics, two new specialist services will be implemented:

1. **`CalculateOrderBookImbalance`**: This service calculates the Order Book Imbalance (OBI) using the market depth dictionary as input. The output is a normalized float between -1.0 and +1.0, representing selling and buying pressure, respectively. The OBI will be incorporated as a fourth dimension into the data window for the `DynamicPlaneGenerator`, alongside Time, Price, and Volume, providing the model with insights into market sentiment.

2. **`GenerateDepthQuantityHeatmap`**: This service generates a heatmap visualization of order book quantities over time. The heatmap consists of 10 rows (5 bid and 5 ask levels) and _N_ columns representing the last _N_ time steps. Color intensity represents the quantity at a specific price level and time. This heatmap serves as input for the `MarketDepthAnomalyDetector` (described below).

**Market Depth Anomaly Detection:**

A new specialist service, `MarketDepthAnomalyDetector`, utilizes a Convolutional Neural Network (CNN) trained to detect anomalies within the Market Depth Heatmap. The output is a probability score, P(OrderBookShock), representing the likelihood of a sudden shift in market depth, serving as an indicator of potential market instability. This score will be integrated into the Comprehensive Threat Assessment Model (CTAM) alongside other threat indicators like P(PriceShock) and P(DerivativesShock), enhancing the CTAM's overall risk assessment capabilities.

**Enhanced Dynamic Plane Generation:**

The `DynamicPlaneGenerator` will be enhanced to incorporate real-time market depth information. The Workflow Broker will call the `DeriveOrderBookFeatures` service, which calculates OBI, Weighted Average Price (WAP), and Bid-Ask Spread. These features will be added as dimensions to the data window used by the Normalization and PCA services before being passed to the `DynamicPlaneGenerator`. This allows the Dynamic Plane to reflect current supply and demand dynamics, potentially improving prediction accuracy.

**Enhanced Paper Brokerage Simulation:**

The Paper Brokerage Simulator will be upgraded to utilize live market depth data for more realistic order filling. For market orders, it will simulate "walking the book" to model price slippage accurately. For limit orders, the simulation will place the order within the simulated order book and fill it based on the Last Traded Price (LTP) and available quantity in the live market depth data.

**Historical Prediction Error Profiling (HPEP) and Confidence Threshold:**

To improve model robustness, Historical Prediction Error Profiling (HPEP) will be implemented as an optimizable hyperparameter. HPEP involves profiling historical prediction errors to filter future trades, increasing the model's reliability. Further details on HPEP implementation and the use of a confidence threshold will be provided in a subsequent section.

### D. Model Enhancement and Refinement

This section details enhancements implemented to refine the model's predictive capabilities, improve trading performance, and increase robustness. These enhancements build upon the core model architecture, optimizing the existing framework rather than introducing fundamentally new architectures.

**Improving Trade Selection and Confidence:**

- **Prediction Magnitude Filtering:** To focus on higher-conviction trades, a prediction magnitude filter was implemented. Trades are executed only when the predicted return exceeds a tunable `accuracy_threshold` hyperparameter. This filters low-confidence predictions and prioritizes potentially more profitable opportunities.

- **Historical Prediction Error Profiling (HPEP):** An HPEP module was developed to profile the model's historical prediction errors. This module builds and stores an HPEP map post-training, which is then used during backtesting for more nuanced trade filtering and risk management. Analyzing the distribution and characteristics of past errors enables a more sophisticated approach to trade selection.

**Input Data and Architecture Refinements:**

- **Five-Day Window Justification and Specialized CNNs:** The choice of a 5-day candlestick input window was investigated and justified based on a review of existing literature. Furthermore, separate CNNs were trained for different holding periods to specialize predictions and potentially improve accuracy for various trading horizons.

- **Soft Labeling for CNN Predictions:** To enhance the model's representation of uncertainty, the CNN output was modified to generate a probability distribution over potential returns rather than a single deterministic prediction. The loss function was adjusted accordingly to accommodate this probabilistic output.

**Incorporating External Data and Resource Optimization:**

- **Incorporation of External Data:** Fundamental and macroeconomic data were incorporated into the model's input features to provide additional context and potentially enhance predictive power.

- **Memory Management Optimization:** Three memory management strategies (A, B, and C) were evaluated through experimentation to optimize resource utilization during training and inference.

**Dynamic Adjustments and Feedback Mechanisms:**

- **Error Signal Integration:** An error signal was integrated into the market movement algorithm to enhance its responsiveness to unexpected market fluctuations. This dynamic adjustment allows the model to learn from past errors and potentially reduce future prediction errors.

**Advanced Enhancements for Profitability and Robustness:**

- **Price Improvement Integration:** A "CalculatePriceImprovementRate" service was developed to track the rolling average of price improvement received on trades. This rate is used as a context token within the model, allowing it to learn correlations between price improvement and future market behavior.

- **Order Book Resilience:** The "ComputeOrderBookState" component was enhanced to generate a "Book Resilience Score," providing a measure of order book depth and stability. This score, combined with existing order flow analysis, enhances the model's conviction in specific trading signals.

- **Execution Quality Feedback Loop:** A feedback loop within the Self-Correction & Healing Controller monitors rolling average execution quality metrics, such as slippage and price improvement. Deteriorating execution quality triggers an increase in the system's "CorrectionFactor," making its predictions more conservative and mitigating the impact of potential errors. This dynamic adjustment enhances robustness in changing market conditions.

## D. Model Enhancement and Refinement (Specialization)

This section details enhancements and refinements to the core model, focusing on integrating an "Anxiety Model," leveraging asymmetric prediction strategies, and incorporating market regime and volatility information.

**Anxiety Model Integration:**

A novel "Anxiety Model" will be developed to assess real-time market conditions and dynamically adjust the trading system's behavior. This model processes high-frequency market depth data to generate an "Anxiety Level" (ranging from 0.0 to 1.0), reflecting market uncertainty and potential volatility.

The Anxiety Level influences two key system components:

- **Error Detection:** Higher Anxiety Levels increase the sensitivity of the Error Detector (Self-Correction & Healing Controller), enabling faster adaptation to potential market shifts.
- **Weight Shifting:** The Anxiety Level provides a faster input than the Systemic Threat Level (STL) to the "seesaw" weight shifting mechanism, dynamically prioritizing the Shockwave Prediction Model over the Flow Engine during periods of high market anxiety.

The Anxiety Model utilizes the following features derived from high-frequency market depth data:

- **Order-to-Quantity Ratio (Top 5 Levels):** The ratio of buy/sell orders to traded volume across the top five price levels.
- **Rate of Change of Order Book Imbalance (OBI):** The speed and direction of order book imbalance changes.
- **Level 1 Dominance:** The dominance of buy or sell orders at the best bid/ask price.
- **Book "Flicker" Rate:** The frequency of order book updates, indicating market activity and potential instability.

**Asymmetric Prediction and Feature Engineering:**

To address market asymmetries, separate prediction models (Bull_Flow_Engine and Bear_Flow_Engine) will be trained for bull and bear markets, respectively. A regime-detection model will dynamically select the appropriate engine based on the current market regime.

Asymmetric features, including Upside Volatility (standard deviation of positive returns) and Downside Volatility (standard deviation of negative returns), will be calculated by a new service, `CalculateAsymmetricFeatures`. The `Workflow_Broker` will provide these features as a context token to the Vision Transformer, enabling it to consider market asymmetry when processing visual information.

**Loss Function Modification:**

The loss function will be modified to penalize underestimation of losses more heavily than overestimation, reflecting the increased risk associated with larger-than-predicted losses. This asymmetric loss function enhances the model's ability to manage risk effectively.

**Further Development and Integration:**

Further architectural considerations and detailed functional specifications will be developed to solidify the integration of these enhancements within the overall system architecture, particularly within the SCoVA (Snapshot Computer Vision Algorithm) framework, which utilizes discrete visual snapshots of market data. These details will address the interaction between the Anxiety Model, asymmetric prediction strategies, and the core SCoVA architecture.

### D. Model Enhancement and Refinement

This section details enhancements to the model architecture, feature engineering, and training process to improve prediction accuracy and robustness. These enhancements focus on capturing the asymmetric nature of market dynamics, integrating contextual market information, and refining the Vision Transformer (ViT) architecture.

**Asymmetric Feature Engineering and Integration:**

A new service, `AsymmetricFeatureEngine`, will generate a feature vector capturing asymmetries in recent price action and volume data. This vector will serve as a context token for the ViT, providing crucial insights into market dynamics. The `Workflow_Broker` will retrieve this vector and pass it to the `Model_Inference_Service` along with the Dynamic Plane image tensor. This ensures the model receives both visual and asymmetry information. The ViT will be adapted to process this vector directly as input, leveraging its self-attention mechanism to learn relationships between visual patterns and the contextual information provided by the asymmetry features.

The following features will be implemented within the `AsymmetricFeatureEngine`:

- **Price and Volatility Asymmetry:**

  - **Upside vs. Downside Volatility:** Semi-deviation will capture the volatility differences between upward and downward price movements. This allows the model to differentiate between periods of high upside potential and periods of elevated downside risk.
  - **Volatility Skewness:** This will measure the asymmetry in the distribution of volatility.
  - **Volatility Kurtosis:** This will measure the "tailedness" of the volatility distribution, providing insights into extreme volatility events.

- **Volume and Participation Asymmetry:**

  - **Accumulation/Distribution Ratio:** This will measure the volume flow on up days versus down days, indicating buying and selling pressure.
  - **Order-to-Quantity Asymmetry:** This feature compares bid-side and ask-side order-to-quantity ratios, providing insights into the relative conviction of buyers and sellers.

- **Correlation Asymmetry:**
  - **Price-Volume Correlation State:** This feature calculates the correlation between log-returns and log-volume separately for positive and negative return candles. This helps differentiate between market fear (selling during high volume drops) and greed (buying during high volume rises).

**Market Regime Integration:**

The ViT architecture will incorporate information about the current market regime. An unsupervised clustering model will identify distinct market regimes. The resulting Regime ID, generated by this Asymmetric Regime Detection model, will be used as a context token within the ViT. This provides the model with a concise summary of market conditions, potentially improving its ability to adapt to different market dynamics. This approach replaces the previous use of raw asymmetric features as input, offering a more refined and interpretable representation.

**Risk-Averse Loss Function:**

A risk-averse loss function will be implemented to prioritize the avoidance of large losses. Specifically, prediction errors where the actual return is lower than predicted will be penalized more heavily than errors where the actual return is higher than predicted. Further, larger negative deviations from the predicted return will incur higher penalties. This tiered penalty system aims to make the model more sensitive to downside risk and prioritize capital preservation. Specific implementation details will be further explored.

**Future Considerations:**

While the current design emphasizes a balanced approach between complexity and performance, future investigation may explore additional enhancements to the model's understanding of market asymmetry and volatility. However, any such exploration will be carefully balanced against the risk of over-engineering and the importance of maintaining a simple, adaptable, and robust system.

## III. Model Enhancement and Refinement

This section details enhancements to the model architecture and input features to improve predictive accuracy and provide more nuanced market insights. A key enhancement is the implementation of "Dual-Token Context Injection" within the Vision Transformer (ViT) architecture. This approach aims to balance the explainability of discrete market regimes with the granular information provided by raw asymmetric features.

1. **Asymmetric Feature Engineering and Regime Identification:** The `AsymmetricFeatureEngine` will calculate a raw vector of asymmetric features, capturing characteristics of market dynamics such as upside/downside volatility and skewness. This vector provides granular detail about market conditions. The `IdentifyAsymmetricRegime` component will analyze this vector to output a discrete Regime ID, categorizing the market into distinct regimes for enhanced interpretability.

2. **Dual-Token Context Injection into ViT:** The ViT architecture will be adapted to accept two new context tokens: the Regime ID Token (categorical) and the Asymmetric Vector Token (continuous). The self-attention mechanism will leverage these tokens in conjunction with the dynamic plane image input for more informed predictions. This combined approach allows the model to learn relationships between visual patterns from candlestick images and the identified market regime, while also considering the detailed information embedded in the asymmetric feature vector.

3. **Dynamic Feature Vector Influence:** The influence of the Asymmetric Vector Token will be dynamically adjusted based on its explanatory power beyond the regime classification. If the feature vector does not add significant value, its computation will be minimized to avoid creating a bottleneck. This ensures efficient resource utilization while maintaining the potential for leveraging the feature vector when it contributes meaningfully.

4. **Terrain-Based Input Exploration:** An alternative approach using terrain categorization for the ViT input will be explored. This involves categorizing terrains into a predetermined number of bins and using the category as input. This leverages domain expertise and adds explanatory power but risks information loss due to reduced granularity. An investigation will determine the trade-off between model simplicity/explanatory power and information loss to assess the net benefit to predictive performance.

5. **Enhanced Narrative Generation:** The trade narrative generation service will be updated to incorporate the Regime ID and the raw asymmetric feature vector. This will provide more comprehensive explanations for each trade, increasing transparency and interpretability by including the identified market regime and the specific features contributing to the trade decision.

## III. Testing and Evaluation

This section details the process of evaluating the model's performance through backtesting and quantitative metrics. The focus is on validating the model's effectiveness in a simulated trading environment and analyzing its overall profitability and risk.

### A. Backtesting

A robust backtesting framework will be implemented to simulate trading strategies based on the model's predictions. This framework will incorporate rolling walk-forward validation to assess performance on unseen data and mitigate overfitting. Stress tests will be conducted to evaluate robustness under various market conditions. The backtesting logic will use the return label formula: `Return(t+h) = (Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last date of the chart image and `h` is the holding period (1 to 5 days). This ensures consistency between training labels and backtesting evaluation.

### B. Performance Evaluation

Model performance will be evaluated using key financial metrics, including Jensen's Alpha and the Sharpe Ratio, relative to an equally weighted index benchmark. The risk-free rate will be based on the 3-month Swedish Krona Short Term Rate (SWESTR). Transaction costs of 20 basis points per round trip will be included. Feature attribution tools like Grad-CAM or SHAP will be employed to understand the model's decision-making process and identify influential features within the candlestick charts, potentially informing further model refinement.

## III. Testing and Evaluation

This section details the procedures for backtesting the developed trading strategy and evaluating its performance. A critical aspect is ensuring consistency between data preparation, model prediction, and the backtesting environment, including the 5-day windowing and return calculation methodology.

### A. Backtesting

The backtesting framework simulates real-world trading using historical data. This involves:

- **Backtesting Logic:** This core logic simulates trades based on model predictions, adhering to the 5-day windowing approach. For each 5-day candlestick chart input, the model predicts the subsequent 5-day return, informing the trading decision (buy, sell, or hold).

- **Backtesting Framework:** A robust framework handles data loading from `stock_data/train/` and `stock_data/test/`, executes trading logic based on model predictions, and tracks performance metrics. It integrates the trade execution logic using actual prices from t+1 to t+5 (sourced from Yahoo Finance, consistent with the training data). Critically, it replicates the training return calculation within the `trade.py` script: `(Close(t+h) - Open(t+1)) / Open(t+1)`, ensuring consistent performance evaluation. Note that _h_ is the same length as the candlestick windowing strategy.

- **Walk-Forward Validation:** Rolling walk-forward validation mitigates overfitting and ensures generalizability. The model is trained on a past period and evaluated on a subsequent, unseen period, repeating this process across multiple rolling time windows.

- **Stress Testing:** The framework includes stress testing capabilities, simulating extreme market conditions (e.g., high volatility, significant downturns) to assess strategy robustness and resilience, identifying potential weaknesses and informing refinements.

### B. Performance Evaluation

After backtesting, the trading strategy's performance is evaluated using established financial metrics to provide quantitative measures of profitability, risk, and risk-adjusted return. These include:

- **Performance Metric Calculation:** Key performance indicators like Jensen's Alpha and the Sharpe Ratio are calculated to assess risk-adjusted returns, providing insights into excess returns relative to the risk taken.

- **Feature Attribution Analysis:** To understand prediction drivers, feature attribution tools (e.g., Grad-CAM, SHAP) identify influential features within the 5-day candlestick charts impacting buy/sell decisions. This enhances interpretability and informs model refinement.

### C. Model Input Verification and Output Interpretation

A critical testing component is verifying the model input data format. The CNN receives a 5-day candlestick chart (open, high, low, close prices) as input. Thorough verification ensures correct formatting and alignment with the expected 5-day window. Edge cases, particularly the last 5 data points, are carefully examined. The transition from `mplfinance` to `matplotlib` for chart generation necessitates rigorous data format verification.

Equally important is understanding the CNN's numerical outputs. The CNN transforms the visual candlestick chart input into numerical outputs, likely representing trading signals or predicted returns. Thorough documentation is required, clearly explaining these values and their derivation from the input image, enabling accurate interpretation of model predictions and performance evaluation. Using a 5-day candlestick chart as the sole input highlights the model's reliance on visual pattern recognition. Visualizing this input with `matplotlib` allows for inspection and confirmation.

## III. Testing and Evaluation

This section details the procedures for rigorously testing and evaluating the performance and robustness of the trained CNN models. The evaluation will leverage backtesting, established financial metrics, and feature attribution techniques, further enhanced by a confidence-based filtering mechanism using a Historical Prediction Error Profile (HPEP).

### A. Backtesting

A robust backtesting framework will be developed to simulate real-world trading conditions and assess the viability of the generated trading strategies. This framework will incorporate:

- **Trading Logic Implementation:** The core trading logic, based on buy/sell signals generated by the CNN model from candlestick chart images, will be integrated into the backtesting framework. This includes calculating returns based on simulated trades.
- **Walk-Forward Validation:** Rolling walk-forward validation will be employed to evaluate the model's performance on unseen data and assess its generalizability across different market periods.
- **Stress Testing:** The strategy will be subjected to a variety of market conditions, including periods of high volatility and market downturns, to assess its resilience to adverse scenarios.
- **HPEP-Based Trade Filtering:** A post-training confidence profile (HPEP map) will be generated by grouping validation set predictions into bins based on predicted return and calculating the accuracy within each bin. During backtesting, only trades where the predicted return falls into a bin with historical accuracy exceeding a predefined threshold will be executed. This filtering mechanism aims to mitigate risk by focusing on higher-probability trades.

### B. Performance Evaluation

Model performance will be evaluated using a combination of standard financial metrics and interpretability tools:

- **Risk-Adjusted Return Metrics:** Jensen's Alpha and the Sharpe Ratio will be calculated to quantify the risk-adjusted returns of the trading strategy.
- **Feature Attribution:** Feature attribution tools like Grad-CAM or SHAP will be employed to understand which features in the candlestick chart images are most influential in driving the model's predictions. This analysis helps ensure the model is learning meaningful patterns rather than spurious correlations. It's important to note that these tools will be applied recognizing the model is trained on scaled visual representations, not directly on price values.
- **HPEP Performance Analysis:** The effectiveness of the HPEP-based trade filtering will be analyzed by comparing the performance of the filtered trades against the unfiltered trades. This analysis will provide insights into the practical utility of the HPEP map in enhancing trading performance.

This combined approach of backtesting, performance metrics, feature attribution, and the HPEP-based filtering provides a comprehensive evaluation of the trading strategy, enabling informed decisions regarding deployment and further refinement.

## III. Testing and Evaluation

This section details the comprehensive testing and evaluation procedures used to assess the performance of the trained CNN models, encompassing backtesting, performance metrics, hyperparameter tuning, and a thorough investigation of soft labeling.

### A. Backtesting and Simulation

A robust backtesting framework is employed to simulate real-world trading scenarios and evaluate the model's performance. This framework incorporates:

- **Backtesting Logic:** The backtesting system simulates trade execution and portfolio management, providing a realistic assessment of trading performance. Specific details of the implementation are documented separately.
- **Walk-Forward Validation:** To mitigate look-ahead bias, a rolling walk-forward validation approach is used, ensuring the model is evaluated on unseen data.
- **Stress Testing:** Rigorous stress testing under various market conditions is conducted to evaluate the strategy's resilience and identify potential weaknesses.

### B. Performance Metrics and Feature Attribution

Model performance is quantified using key metrics, including:

- **Standard Metrics:** Metrics such as the Sharpe Ratio and Jensen's Alpha are used to evaluate risk-adjusted returns.
- **Feature Attribution:** Techniques like Grad-CAM or SHAP are employed to understand the model's decision-making process and identify the most influential features.

### C. Hyperparameter Optimization and Model Selection

The following strategies are employed for hyperparameter tuning and model selection:

- **Confidence Threshold:** A `confidence_threshold` hyperparameter, representing the minimum required prediction accuracy for trade execution, is tuned using optimization methods like grid search or Bayesian optimization.
- **Holding Period Optimization:** The `holding_days` parameter (ranging from 1 to 5) is optimized, potentially in conjunction with the `confidence_threshold`, using techniques like grid search, random search, or Bayesian optimization. This determines the optimal holding period for maximizing trading performance.
- **Separate CNNs for Holding Periods:** Separate CNNs are trained for each holding period (1 to 5 days). Performance is compared across models using metrics including validation loss, Sharpe ratio, alpha, and Mean Squared Error (MSE) to select the optimal model and holding period.
- **5-Candlestick Input Window Justification:** The rationale for using a 5-candlestick input window, based on existing literature (Jiang et al., 2023) and empirical effectiveness for pattern learning, is documented. While the 5-day output (prediction) window is chosen for design symmetry and practical trading frequency considerations, further investigation into optimal output window lengths may be beneficial in future work.

### D. Soft Labeling for Enhanced Performance

To address the limitations of hard labels and potentially improve model accuracy, a soft labeling approach is implemented:

1. **Return Discretization:** The continuous range of possible returns is discretized into bins (e.g., -5% to +5% in 0.5% increments). This transforms the regression problem into a classification problem suitable for soft labeling.
2. **Model Output Modification:** The CNN's output layer is modified to predict a probability distribution across these discretized return bins using a fully connected layer followed by a softmax activation function.
3. **Soft Label Generation:** Hard labels are transformed into soft labels using a Gaussian kernel centered around the actual return. The kernel's standard deviation, controlling label "softness," acts as a tunable hyperparameter.
4. **Soft Label Integration:** The modified CNN predicts a probability distribution over return bins, allowing for a more nuanced performance evaluation.

This multifaceted approach ensures a robust and comprehensive evaluation of the model's performance, informing deployment decisions and future research directions.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the performance of the developed trading strategies, focusing on backtesting methodologies, performance metrics, and risk management techniques.

### A. Backtesting

Rigorous backtesting was performed using a rolling walk-forward validation methodology to simulate real-world trading conditions and provide a reliable assessment of the strategies' effectiveness. This approach ensures the model's performance is evaluated on unseen data. Stress tests simulating periods of high volatility, market crashes, and other extreme scenarios were also conducted to evaluate the strategy's resilience. A key aspect of this process was incorporating the probabilistic predictions generated by the soft label CNN model (UUID: 20bea25d-4639-41ff-95ca-f2e4ec6fa5d0). This approach allows for a more nuanced evaluation by considering the uncertainty inherent in the predictions.

### B. Performance Evaluation

Standard performance metrics, including Jensen's Alpha and the Sharpe Ratio, were calculated to quantify the risk-adjusted returns of the trading strategies. Feature attribution tools like Grad-CAM or SHAP were employed to gain insights into the model's decision-making process and identify potential biases. The impact of trading costs, specifically the issue of high trading costs (UUID: b9765c84-158b-444d-bae3-10049f74aebd), was carefully analyzed. Factors such as frequent rebalancing (every 5 days), uniform weighting of decile components, the absence of initial transaction friction considerations, and short-selling constraints were examined. Quantifying the impact of these factors is crucial for understanding the true alpha-generating potential of the strategies. Initial estimates suggest these costs reduce annual returns by approximately 10%.

Backtesting results revealed a complex performance landscape. While a majority (6 out of 8) of the portfolios demonstrated positive alpha _before_ transaction costs, this performance deteriorated significantly _after_ costs were included. A notable exception was the First North All-Share (small-cap) index using the OMXS All-Share model, which achieved a positive alpha of +8.89% annually after transaction costs, representing a substantial outperformance of +37.57% annually compared to the benchmark return of -27.88%. This discrepancy highlights the tension between signal generation and trade execution efficiency. The positive alpha in the small-cap segment suggests a valid predictive signal, while the broader performance degradation underscores inefficiencies stemming from high turnover, uniform weighting, and the inclusion of low-confidence predictions.

Further analysis will investigate:

- Alpha generation breakdown by long vs. short trading legs.
- Per-index Sharpe ratios before and after transaction costs.
- The impact of short-selling constraints, particularly within the small-cap segment.
- The handling of unsuccessful trades, including the development of a stop-loss mechanism.

### C. Risk Management Enhancements

To address the identified inefficiencies and enhance the robustness of the trading strategy, the following risk management techniques were prototyped and will be integrated:

- **Stop-Loss Mechanism:** A stop-loss mechanism will automatically exit trades when they move against the predicted direction by a defined percentage (e.g., exiting a trade predicted to return +3.0% if the stock drops -2.5% within 2 days). This mitigates the risk of holding losing positions for the fixed 5-day period.

- **Prediction Confidence Thresholding:** A dynamic trade filtering layer will use prediction magnitude, historical prediction error profiling (HPEP), or predicted return volatility to execute trades only when the prediction confidence exceeds predefined thresholds. This focuses capital allocation on higher-conviction trades.

- **Risk-Based Weighting:** A sophisticated weighting scheme will allocate capital based on factors such as prediction confidence, the inverse of historical volatility, and the signal-to-noise ratio of predicted versus actual returns. This will optimize portfolio construction for risk-adjusted returns.

These enhancements aim to improve trading efficiency, reduce turnover, and increase overall profitability by leveraging the probabilistic nature of the soft label predictions and incorporating robust risk management principles. Further backtesting and evaluation will be conducted to optimize these enhancements and assess their impact on performance.

## III. Testing and Evaluation

This phase focuses on rigorously evaluating the model's performance and identifying potential areas for improvement. This involves backtesting with robust validation techniques, advanced performance evaluation using established financial metrics, and in-depth analysis of both successful and unsuccessful trades to inform model enhancements and refine exit strategies.

### A. Backtesting

A comprehensive backtesting framework will be implemented to simulate trading strategies using historical market data. This framework will be modular and flexible, accommodating various trading strategies and market conditions. To ensure the model's generalizability and avoid overfitting, rolling walk-forward validation will be employed. Stress testing capabilities will be integrated to assess the strategy's resilience under adverse market conditions, such as market crashes or periods of high volatility. This backtesting process will also incorporate the predicted rally time, adjusting position sizing based on this prediction and incorporating early exits if targets are not met within the predicted timeframe.

### B. Performance Evaluation and Model Refinement

Beyond standard financial metrics like the Sharpe Ratio and Jensen's Alpha, this phase will include a deeper analysis of trading performance:

- **Accuracy of Rally Time Prediction:** The accuracy of the rally time prediction will be evaluated by comparing predicted values to the observed time taken to reach the target return, using metrics like Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE).
- **Feature Attribution:** Feature attribution techniques (e.g., Grad-CAM, SHAP) will be used to understand the model's decision-making process for both successful and unsuccessful trades, identifying key features driving trade decisions and potential weaknesses in the model's predictions. This analysis will inform model refinement and the development of more sophisticated exit strategies.
- **Unsuccessful Trade Analysis:** A detailed analysis of unsuccessful trades will be conducted to identify recurring patterns or specific market conditions that contribute to losses. This will provide valuable insights for improving the model's predictive accuracy and refining exit strategies to mitigate losses. This analysis may also guide the development of dynamic exit strategies based on market conditions and evolving risk profiles.

## III. Testing and Evaluation

This section details the procedures for rigorously testing and evaluating the models developed for the SCoVA project. This includes standard practices like backtesting and performance metric analysis, as well as specialized techniques designed to assess the model's learning process and adaptability to evolving market conditions.

### A. Enhancing Model Learning and Adaptation

Several techniques will be employed to enhance the model's ability to learn from its mistakes and adapt to market dynamics:

- **Cost-Sensitive Retraining:** To mitigate the impact of costly misclassifications, a cost-sensitive learning approach will be implemented during retraining. Higher loss weights will be assigned to misclassified trades that resulted in significant losses, prioritizing the model's learning on avoiding these high-impact errors.

- **Hard Sample Bootcamp:** A "hard sample bootcamp" will focus training on difficult examples. After each training epoch, samples with high loss will be identified and added to a dedicated dataset. The model will then undergo a separate training cycle focused specifically on these hard cases, improving its ability to handle challenging market situations.

- **Meta-Model for Trade Review:** A meta-learner model will be developed to predict potential trade failures before execution. This model will be trained on the initial prediction from the primary model, features of the chart and trade setup, and the actual trade outcome. This second layer of analysis provides an additional check to prevent potentially unprofitable trades.

### B. Sequential Analysis: Reward Logic and Input Adjustments

Beyond addressing individual trade errors, the model's architecture and reward mechanisms will be adapted to handle the sequential nature of market data:

- **Enhanced Reward Logic:** The reward logic will be refined to incorporate temporal dependencies. Instead of evaluating rewards based solely on a single candlestick image, the model will consider sequences of images. This approach acknowledges that market movements are not isolated events but part of a continuous sequence.

- **Sequential Input Adjustment:** The model's input layer will be modified to accept sequences of candlestick images, enabling the incorporation of temporal dependencies. This adjustment is crucial for all proposed architectural changes involving sequential analysis.

### C. Architectural Comparison

A benchmark comparison experiment will evaluate the performance of three distinct architectural approaches for handling temporal context:

1. **Static Picture (Baseline):** Using a single candlestick image as input.

2. **Picture-Pair:** Using two consecutive candlestick images as input, leveraging a paired input data generator. This explores the impact of directly presenting temporal transitions to the model. The generator will create paired inputs by either concatenating or stacking the images.

3. **Sequence:** Employing a sequence of candlestick images as input to capture longer-term dependencies and trends. This will include a CNN + LSTM hybrid architecture, combining a CNN for feature extraction with an LSTM to model temporal relationships between candlestick patterns.

### D. Vision Transformer (ViT) Evaluation

A Vision Transformer (ViT) will be evaluated for processing sequences of three consecutive 5-day candlestick chart images. Two implementation options will be compared:

1. **EfficientNet Encoding:** Encoding each image with an EfficientNet model and feeding the resulting feature vectors into the transformer.

2. **Patch Embedding:** Utilizing ViT-style patch embedding, where each image is divided into patches, flattened, and augmented with positional encoding before being fed into the transformer.

### E. Feature Engineering Evaluation

The effectiveness of different feature engineering methods will be assessed by comparing image subtraction and feature subtraction for generating delta features. This comparison will determine which method provides a more informative representation of temporal changes in candlestick patterns for the ViT architecture.

## III. Testing and Evaluation

This section details the procedures for rigorously testing and evaluating the developed trading models. A robust backtesting framework, coupled with comprehensive performance evaluation and targeted validation of model architecture and training, are crucial for assessing the models' efficacy under realistic market conditions.

### A. Backtesting

A dedicated backtesting framework will be developed specifically for evaluating trading strategies based on the models. This framework will incorporate:

1. **Framework Development:** The framework will simulate trading activities based on model predictions, handling trade execution, portfolio management, and performance tracking.

2. **Walk-Forward Validation:** Rolling walk-forward validation will be employed to ensure model robustness and avoid overfitting. This involves training the model on a past period and validating it on a subsequent period, iteratively moving these windows forward in time.

3. **Stress Testing:** The trading strategy will undergo rigorous stress testing to assess its performance under various market conditions, including periods of high volatility and significant market downturns. This will help identify potential vulnerabilities and enhance its resilience.

### B. Performance Evaluation

Beyond trade simulation, rigorous performance evaluation is critical. This will involve calculating key metrics, including the Sharpe Ratio and maximum drawdown, to assess risk-adjusted returns. Furthermore, feature attribution tools (e.g., Grad-CAM, SHAP) will be employed to understand model decision-making, identify potential biases, and guide further development.

### C. Dataset and Tool Development

Two key tools will be developed to support the backtesting and evaluation process:

1. **Image Sequence Generator:** This tool will generate image sequences of candlestick charts using a sliding window approach, providing input data for the backtesting framework. It will utilize sequential candlestick windows and incorporate feature engineering using delta features, calculated either through image subtraction or feature subtraction from sequential windows.

2. **Dataset Design:** A dataset of image sequences will be constructed using a sliding window approach. Each sequence will contain charts from t-2, t-1, and t, with corresponding labels for return value, rally time (if applicable), and signal class.

### D. ViT Model Validation

Specific tests will be conducted to validate the Vision Transformer (ViT) model's architecture and training:

1. **Variable-Length Sequence Handling:** The ViT model will be tested with variable-length sequences of candlestick images to validate the implementation of masking or dynamic positional encodings and ensure correct processing of sequences of varying lengths (e.g., 3, 4, or 5 charts).

2. **Masking and Padding Verification:** The effectiveness of masking and padding for handling dynamic-length input will be rigorously tested. This will verify that padding with blank charts does not introduce bias and that the masking mechanism correctly directs the model to ignore padded elements.

3. **Positional Embeddings Validation:** The impact of positional embeddings on model performance will be evaluated to ensure the model correctly utilizes sequential information from the candlestick images.

4. **Input Image Count Limits:** The potential limitations on the number of input images (N) for the ViT training layer will be investigated. Experiments with different values of N (e.g., 3, 4, and 5) will determine the optimal number of input images for maximizing performance.

Through these comprehensive testing and evaluation procedures, the efficacy and robustness of the developed trading models will be thoroughly assessed.

## III. Testing and Evaluation

This section details the rigorous testing and evaluation procedures used to validate the performance and robustness of the models, comparing them against established baselines and developing methods for extracting meaningful trading information. This evaluation focuses on the conceptual soundness and potential advantages of predicting candlestick images instead of direct return values.

### A. Conceptual Validation

This phase assesses the alignment between human visual interpretation of trading patterns and the model's image-based prediction approach. By comparing how the model extracts information from candlestick images with how human traders identify and utilize patterns, this analysis aims to establish the conceptual validity of using images as the primary prediction target. This will be explored further in the Conceptual Soundness Evaluation (see Section [Number/Reference]).

### B. Architectural Advantages and Experimental Validation

The shift to image-based predictions offers several potential advantages, investigated through comparative analysis with traditional return prediction methods (see Sections [Number/Reference] for further details on the candlestick pattern prediction approach and predicting candlestick images instead of returns):

- **Representation Richness:** Candlestick images encapsulate richer information than numerical returns, potentially capturing subtle patterns and nuances.
- **Uncertainty Modeling:** Predicting an image inherently represents a distribution of possible future price movements, offering a more robust approach to uncertainty.
- **Causal Reasoning:** The visual nature of candlestick patterns may facilitate causal reasoning, linking visual features with market behavior.
- **Training Supervision:** Using images as targets may provide more direct and intuitive supervision, guiding the model to learn relevant visual features.
- **Interpretability:** Predicted images offer visually interpretable outputs, aiding understanding of the model's predictions and decision-making.
- **Generative Flexibility:** The image-based approach enables generative modeling, allowing for the generation of plausible future scenarios.

These advantages will be assessed through a designed experiment. This involves training a Vision Transformer (ViT) or U-Net based architecture to forecast candlestick patterns and then extracting predicted return values from the generated images. Model accuracy will be compared against baselines using metrics like Root Mean Squared Error (RMSE) and Structural Similarity Index Measure/Learned Perceptual Image Patch Similarity (SSIM/LPIPS). Practical performance will be assessed through backtesting, with a focus on profit performance.

### C. Return Extraction and Performance Evaluation

Extracting predicted open, high, low, and close prices from generated images is crucial for performance evaluation. Two methods will be explored:

1. **Pixel Location Analysis:** Analyzing pixel locations within the generated image to determine corresponding price levels.
2. **Rendering Predicted Image Data:** Directly accessing the underlying data used to render the image for price extraction, potentially offering greater precision.

Performance evaluation will utilize established financial metrics like Jensen's Alpha and the Sharpe Ratio to quantify risk-adjusted returns. Feature attribution tools such as Grad-CAM or SHAP will provide insights into the model's decision-making process by identifying influential features driving trade signals.

### D. Backtesting and Stress Testing

A robust backtesting framework will simulate the trading strategy on historical data using a rolling walk-forward validation scheme to assess performance on unseen data and prevent overfitting. This involves:

1. **Implementing backtesting logic:** Developing the core logic for simulating trades, managing positions, and calculating performance metrics.
2. **Building the backtesting framework:** Constructing a framework for efficient and reproducible backtesting experiments.
3. **Stress testing:** Conducting stress tests using various market scenarios (e.g., crashes, high volatility) to identify vulnerabilities and ensure resilience.

### E. Risk and Drawback Assessment

The evaluation includes assessing potential risks and drawbacks of image-based prediction:

- **Less Direct Evaluation:** Evaluating performance based on generated images adds complexity.
- **Compounding Errors:** Errors in image generation can propagate when translating images back to numerical predictions.
- **Loss of Direct Reward Supervision:** Training on images might hinder optimization by losing direct supervision from numerical rewards.

## III. Testing and Evaluation

This phase focuses on rigorously evaluating the model's performance, ensuring its predictions translate into actionable trading strategies, and comparing its efficacy against established alternatives.

### A. Visual Realism and Trading Relevance

The model should generate candlestick charts that not only resemble real-world market behavior but also offer identifiable and exploitable trading opportunities, including clear entry and exit points. Crucially, the predictions must be causally grounded, explaining _why_ specific patterns are predicted, not just _what_ they look like. This ensures the model generalizes to unseen market conditions and avoids overfitting to training data. Furthermore, the backtesting framework should assess the relationship between model accuracy in predicting visual patterns and actual financial performance, incorporating the necessity of a trade based on the predicted pattern.

### B. Addressing Technical Challenges

Training and evaluating image-based prediction models present unique challenges. The evaluation process must address ground truth alignment, appropriate loss functions for image fidelity, preventing mode collapse, and avoiding overfitting. Most importantly, the trading relevance of generated images needs rigorous assessment. This requires specialized training and evaluation methods tailored to image-based financial forecasting. Feature attribution tools like Grad-CAM or SHAP can provide valuable insights into the model's decision-making process and reliance on specific visual features.

### C. Comparative Performance Analysis

A dual-module framework, combining the image-based prediction model with a separate trade evaluation module for actionable signals, will enhance trading efficacy. This secondary module will analyze generated charts and identify trading opportunities. A rigorous comparison against traditional scalar-based return prediction models will assess relative strengths and weaknesses under various market conditions, including metrics like Jensen's Alpha and Sharpe Ratio. This comparative analysis will provide a comprehensive performance evaluation and contextualize the value proposition of the image-based approach. Finally, the backtesting process should consider transaction costs, slippage, and potential market impact, utilizing rolling walk-forward validation and stress tests to ensure robustness.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the dynamic 2D plane representation of market data. This involves traditional backtesting and performance evaluation, as well as a deeper analysis of the mathematical and information-theoretic implications of this novel representation. We compare the 2D framework to the existing 3D model to validate the claims made regarding its efficacy.

### A. Backtesting

We leverage existing backtesting infrastructure to evaluate the performance implications of integrating the 2D motion representation into the current trading strategy. This process includes trade execution, portfolio management, transaction cost calculations, and rolling walk-forward validation to mitigate overfitting. Stress testing will be conducted to assess the strategy's resilience under various market conditions. Specific details of the backtesting implementation will be provided in the dissertation, supported by relevant literature.

### B. Performance Evaluation

Performance is evaluated using standard metrics like the Jensen's Alpha and Sharpe Ratio. Critically, we also perform a comparative analysis between the 2D and 3D frameworks to determine the impact of the simplified representation on the overall trading strategy's performance, including its effect on unsuccessful trades.

### C. Mathematical and Information-Theoretic Analysis of the 2D Representation

Given the shift from a fixed 3D Cartesian frame to a dynamic 2D plane with rotational axes and a dynamic origin (similar to a Frenet frame or a tangent plane), we conduct a comprehensive analysis of the mathematical and information-theoretic implications:

- **Coordinate Transformations and Manifolds:** We investigate coordinate transformations and the concept of manifolds to understand how the dynamic 2D plane represents market data, including potential implications of representing the data in a non-Euclidean space.

- **Moving Frame Analysis:** We analyze the dynamic plane as a moving frame on a 1-dimensional differentiable manifold, exploring the implications of attaching and rotating an orthonormal frame at each point on the manifold.

- **Representing a Parabolic Trajectory:** We use a parabolic trajectory as a test case to demonstrate the system's functionality. This involves encoding the parabola (originally along the z-axis in the 3D frame) within the dynamic 2D plane through rotations of the axes, demonstrating its ability to capture different market behaviors and trends. This also allows for visualizing the impact of the dynamic origin point on motion representation compared to a fixed 3D origin.

- **Information Balance and Degrees of Freedom:** We analyze the information balance and degrees of freedom within the 2D representation (two coordinates (u,v) and three Euler angles/rotation matrix). This ensures no information loss or distortion during the transformation from 3D to 2D, enabling accurate interpretation of the represented data.

These analyses complement the traditional backtesting and performance evaluation, providing a holistic understanding of the dynamic 2D plane's effectiveness in representing market dynamics and informing potential refinements to the model.

## III. Testing and Evaluation

This section details the comprehensive evaluation process for the dynamic 2D plane within the financial trading model. This includes assessing its performance, robustness, and practical limitations across various potential applications, as well as its computational efficiency. Traditional backtesting and performance metrics (detailed in subsections A and B) are complemented by specific visualization requirements and an analysis of the dynamic plane's inherent characteristics.

### A. Backtesting (Continuity)

Standard backtesting procedures, including simulated trades on historical data, will assess the viability of the trading strategies developed. A rolling walk-forward validation approach mitigates overfitting and ensures model robustness over time. Stress tests will further evaluate resilience to adverse market events, providing a comprehensive performance evaluation.

### B. Performance Evaluation (Efficiency)

Beyond traditional metrics like Jensen's Alpha and the Sharpe Ratio, this project analyzes the computational efficiency of data representation. Memory and computational costs associated with storing full 3D positional data will be compared against storing 2D coordinates combined with orientation data (using quaternions or rotation matrices). This comparison will reveal potential performance gains from efficient data handling, particularly given the overhead of quaternion calculations.

### C. Dynamic Plane Characteristics and Limitations

Given the complexity of the dynamic 2D plane, understanding its limitations is crucial. The evaluation will focus on:

- **Path Dependence:** Analysis will determine how the plane's dynamic rotation affects the final representation based on the path taken, and whether this introduces unwanted variability or instability.

- **Singularities:** The evaluation will identify any potential singularities or instabilities arising from the plane's rotation and transformation, assessing their impact on system reliability.

- **Computational Overhead:** The computational cost of implementing and maintaining the dynamic 2D plane will be assessed to ensure it remains manageable within the trading model's constraints.

### D. Visualization and Validation

Effective visualization is critical for understanding the model's behavior. The following visualizations will be implemented using `matplotlib` and `numpy` (excluding `seaborn`):

- **Data Logging:** A robust mechanism will store planar coordinates (u, v) using an arc length counter, and orientation data using quaternions or rotation vectors. This data enables spiral path reconstruction and must respect memory constraints while remaining compatible with visualization requirements.

- **Visual Data Representation:** Clear and informative static and dynamic visualizations will represent the model's data and behavior.

- **3D Helix Visualization:** A standalone 3D helix plot (radius 1.0, pitch 0.5, 4 turns) will be generated, labeled "3D Helix in Laboratory Coordinates," serving as a reference for spatial transformations.

- **2D Planar Trace Visualization:** An enhanced, standalone 2D visualization of the planar trace within the moving chart will provide a clearer understanding of the planar projection of the model's movements. Improvements may include intuitive visual representation, tooltips, annotations, or supplementary documentation.

### E. Exploration of Potential Applications

The potential applications of the dynamic plane representation will be explored in several domains:

- **Robotics:** Its potential for robot navigation and control will be investigated, considering the rotating frame of reference's impact on path planning and obstacle avoidance.
- **Computer Graphics:** Applicability for representing and manipulating 2D objects will be assessed, including potential benefits or challenges for animation and rendering.
- **Navigation:** Its use in navigation systems will be investigated, analyzing how the changing frame of reference affects position tracking and route calculation.
- **Data Compression:** The possibility of using the dynamic plane for data compression will be evaluated, considering if it could lead to more efficient storage or transmission of information.

## III. Testing and Evaluation

This section details the rigorous testing and evaluation process for the trading agent, focusing on the dynamic projection system, backtesting, and performance evaluation. Before formal backtesting, the dynamic projection mechanism requires thorough verification.

**A. Dynamic Projection System Validation**

The dynamic projection system's core functionality must be validated to ensure its correct and efficient operation. This involves examining:

1. **Dynamic Updating:** How does the introduction of new candlestick data affect the re-centering and rotation of the feature space? Testing should focus on the mechanism's functionality and the impact of dynamic shifts on subsequent predictions.

2. **Rotation Mechanism:** The chosen rotation method (e.g., affine transformation, PCA rotation, learned rotation) requires validation for correctness and efficiency. This includes verifying its mathematical properties and impact on the feature space.

3. **Projection Goal:** The core objective of the dynamic projection (e.g., prediction invariance to prior trend direction, emphasizing relative local movement) must be explicitly stated and validated. Testing procedures should directly measure the success of achieving this goal. For example, if invariance to prior trend is the goal, model performance should be consistent across different prior trend scenarios.

4. **PCA Rotation Implementation (if applicable):** If PCA rotation is employed, its implementation must be verified. This includes ensuring the correct calculation of principal components and the intended application of rotation. The impact of PCA rotation on prioritizing relative local movement over absolute position requires empirical validation.

5. **Dynamic PCA Integration (if applicable):** The integration of dynamic PCA into the Vision Transformer/CNN pipeline requires careful testing to ensure proper data flow and transformation. This includes verifying the dimensional consistency and the correct application of PCA within the model's architecture.

**B. Backtesting**

Backtesting assesses the trading strategy's viability using historical market data. The process involves:

1. **Backtesting Logic Implementation:** Apply the core trading logic (including signal generation and trade execution rules) to historical data to generate a simulated trading history.

2. **Backtesting Framework Development:** Build a robust and flexible framework for efficient and reproducible testing. This framework should manage data loading, signal generation, trade execution, and performance calculation.

3. **Walk-Forward Validation:** Employ walk-forward analysis to evaluate performance over different periods and assess robustness to changing market conditions. This involves training on past data and testing on subsequent data, repeating this process over a rolling window.

4. **Stress Testing:** Conduct stress tests simulating extreme market events (e.g., crashes, volatility spikes, prolonged declines) to evaluate the agent's resilience.

**C. Performance Evaluation**

After backtesting, rigorously evaluate the trading agent's performance using established financial metrics:

1. **Risk-Adjusted Return Metrics:** Calculate Jensen's Alpha (risk-adjusted return compared to a benchmark) and the Sharpe Ratio (return per unit of risk) to assess profitability and risk profile.

2. **Feature Attribution Analysis:** Utilize tools like Grad-CAM or SHAP to understand the model's decision-making process. Analyze which candlestick chart features the model prioritizes for generating trading signals to gain insights into its behavior and potential improvements.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the performance and robustness of the trading agent, which operates on a dynamically adjusted plane representation of price data. This representation involves real-time rotation and origin shifting relative to local price movements, impacting both model architecture and data preprocessing. These aspects, while conceptually separate, are integral to evaluation and are thus addressed here.

Before backtesting, the data undergoes transformation to this dynamic plane representation:

1. **Local Movement Vector Calculation:** A local movement vector (v) is calculated at each timestep, representing the change in price and time between the current data point (P_current) and the previous data point (P_previous):

   ```
   v = P_current - P_previous
   ```

   where P_current and P_previous are the latest and previous price/time coordinates (X, Y), respectively. This vector determines the rotation angle for the dynamic plane.

2. **Rotation Matrix Calculation:** The rotation angle (θ) is calculated using:

   ```
   θ = arctan(v_y / v_x)
   ```

   This angle constructs the rotation matrix R(θ):

   ```
   R(θ) = [[cos(θ), sin(θ)], [-sin(θ), cos(θ)]]
   ```

   This matrix is applied to a surrounding candlestick window (e.g., the last 5-10 candlesticks), aligning the data with the local movement vector.

3. **Dynamic Origin Shift:** After rotation, the origin shifts to the current price point (P_current), centering recent price movement and placing the current price at (0,0). This normalization highlights oscillations around the current trend as deviations from zero on the Y-axis.

4. **Purpose of Dynamic Origin:** This dynamic refocusing maintains a consistent perspective on relative price fluctuations, enabling the model to focus on local patterns.

### A. Backtesting

Backtesting employs a rolling walk-forward validation methodology for a realistic performance assessment on unseen data. Stress tests evaluate system resilience under various market conditions. Crucially, performance is compared against several benchmarks:

- **No PCA:** Performance without dimensionality reduction.
- **Static PCA:** Performance with a fixed PCA transformation.
- **Simple Transforms:** Performance using simpler transformations.
- **Alternative Focus Methods:** Performance using alternative data-driven focusing methods.
- **Ablation Study:** Evaluating the individual contributions of PCA re-centering and rotation to quantify the benefits of dynamic PCA refocusing.

### B. Performance Evaluation

Performance is evaluated using standard financial metrics, including Jensen's Alpha and the Sharpe Ratio, providing insights into risk-adjusted returns. Feature attribution tools (e.g., Grad-CAM, SHAP) analyze the influence of input image features (candlestick or otherwise) on the agent's decisions, offering insights into model behavior and potential improvements. This aligns with the image-based input requirement, allowing visual interpretation of feature importance.

The computational cost of the dynamic PCA implementation is thoroughly investigated. Optimizations or alternative dimensionality reduction techniques will be explored if necessary to mitigate any performance impact. Furthermore, potential challenges related to dynamic PCA, such as overfitting, instability, and interpretability, will be addressed.

## III. Testing and Evaluation

This section outlines the testing and evaluation procedures crucial for verifying the robustness and performance of the dynamic snapshot generation process. It focuses on ensuring the visual integrity of the transformed candlestick data and validating the correct implementation of the dynamic rotation within the backtesting framework. While overall model performance evaluation using metrics like Jensen's Alpha and Sharpe Ratio is essential, this section specifically addresses the testing of the dynamic snapshot generation and its integration into the backtesting pipeline.

**A. Visual Integrity and Data Consistency Testing**

The following tests ensure the quality and consistency of the dynamically generated snapshots:

- **Rotation Artifact Testing:** This test evaluates the effectiveness of interpolation techniques (e.g., anti-aliasing) in minimizing distortions caused by rotation. Visual inspection of the snapshots and comparison with original candlestick charts will be performed, focusing on the clarity and smoothness of the rotated candlesticks.

- **Volatility Jump Handling Test:** This test evaluates the system's robustness against sudden, large price fluctuations. Simulated and historical data with sharp price movements will be used. The output will be examined for distortions or anomalies in the rotated snapshots, verifying the effectiveness of any smoothing or limiting techniques.

- **Axis Scaling Consistency Test:** This test verifies consistent axis scaling across all generated frames. The units per % move will be measured in a sample of snapshots, ensuring consistent input for the model.

**B. Dynamic Snapshot Generator Pipeline Verification**

These tests validate the implementation and integration of the dynamic snapshot generator:

- **Pseudocode Review:** A thorough review of the dynamic snapshot generator's pseudocode will confirm its logical correctness and completeness, ensuring all necessary steps are included and executed in the correct order.

- **Dynamic Snapshot Generation Test:** This test runs the generator with various input data, including normal and edge cases (e.g., periods of high and low volatility). It verifies the generator produces valid images, correctly reflecting the rotated and re-centered data, including the relative positioning of candlesticks to the new X' and Y' axes and the non-horizontal time axis.

**C. Dynamic Rotation Backtesting Integration and Verification**

These tests focus on the correct implementation and impact of the dynamic rotation within the backtesting environment:

- **Rotating Snapshot Generator Module Integration:** Testing will confirm the seamless operation of the `RotatingSnapshotGenerator` module within the backtesting pipeline. This includes verifying correct data flow and compatibility with the backtesting logic.

- **Dynamic Plane Redraw Verification:** Tests will validate the dynamic redrawing of the 2D plane at each time step, confirming the correct recalculation and update of the plane's orientation and axis alignment with every price and time change. Visualizations may be used to aid verification.

- **Dynamic Origin Refocus Confirmation:** Tests will verify that the origin of the 2D plane correctly refocuses at each new data point, ensuring rotation calculations are relative to the current data point and maintaining the dynamic reference frame.

- **Angle Theta Calculation Verification:** The method for calculating the rotation angle (theta) will be rigorously tested, ensuring it correctly incorporates the dynamic origin and rotating axes and accurately reflects local market dynamics captured by the principal components. This may necessitate adapting the existing calculation beyond simple price differences.

- **Dynamic Plane Generation Algorithm Validation:** Rigorous testing of the core dynamic plane generation algorithm will be conducted. This includes verifying its handling of time, price, and potentially volume data, its calculation of movement vectors, and the application of Principal Component Analysis (PCA) to define the local frame of reference. The resulting 2D plane should accurately capture the dominant movement trajectory and residual behavior. Detailed pseudocode documenting the algorithm's steps, from data input to 2D plane reconstruction, will be developed and reviewed.

These tests ensure the correct functionality of the dynamic plane mechanism and its integration within the trading strategy. They also provide a foundation for analyzing the impact of dynamic rotation on model performance during backtesting and subsequent evaluation.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to validate the functionality and performance of the dynamic plane generator and the subsequent CNN and ViT models trained on its output. Visualizations will play a key role in understanding the transformations applied to the market data.

### A. Dynamic Plane Validation

A critical first step is validating the output of the dynamic plane generator. This involves:

- **Visualization:** Creating a conceptual diagram illustrating the evolution and shifts of the dynamic 2D plane with each market movement. This visual representation will aid in understanding the behavior of the dynamic coordinate system and facilitate communication of the underlying concepts. It will also serve as a basis for visual inspection of the generated planes, ensuring they behave as expected and capture the intended market dynamics.

### B. Model Backtesting

Standard backtesting procedures will be employed to evaluate the overall trading strategy performance using the dynamic plane generator's output as input for both the CNN and ViT models. This includes:

- **Backtesting Implementation:** Implementing backtesting logic to simulate trading based on the model's predictions. This requires integrating the dynamic plane generator, the chosen prediction model (CNN or ViT), and the trade execution logic.
- **Framework Development:** Developing a robust backtesting framework to handle data loading, signal generation, trade execution, and performance tracking. This framework will accommodate the unique data format produced by the dynamic plane generator.
- **Walk-Forward Validation:** Employing walk-forward validation to assess the models' ability to generalize to unseen data, providing a more realistic estimate of real-world performance.
- **Stress Testing:** Conducting stress tests to evaluate the strategies' resilience under various market conditions, including periods of high volatility and market crashes.

### C. Performance Evaluation

Beyond standard performance metrics, the evaluation will focus on the effectiveness of the dynamic plane representation.

- **Standard Metrics:** Calculating standard performance metrics such as Jensen's Alpha and the Sharpe Ratio to quantify the strategies' risk-adjusted returns and provide a benchmark for comparison.
- **Feature Attribution:** Utilizing feature attribution tools like Grad-CAM or SHAP to understand the models' decision-making process, specifically how the dynamic plane representation contributes to the predictions. This includes investigating which features within the generated 2D plane are most influential on the model's output, providing insights for further refinement.

### D. Comparative Analysis: Static vs. Dynamic Data

A core experiment will compare the performance of models trained on static candlestick data versus the dynamic planes generated by the Rotating Dynamic Plane Generator. This comparison will assess the effectiveness of the dynamic representation in capturing market movements and improving predictive accuracy. The "Integration: Vision Transformer Pipeline" will incorporate the dynamic frames into the ViT training process, adhering to the format specified in "Design: Dynamic Candlestick Snapshots for ViT." The "Functional Requirement: Window Size Parameter" and "Functional Requirement: Volume Inclusion" will guide parameter adjustments, evaluating the impact of window size and optional volume data inclusion on model accuracy during backtesting.

## III. Testing and Evaluation

This section details the testing and evaluation procedures for both the trading models and the dynamic plane visualization. It highlights critical considerations regarding the stability and functionality of the dynamic plane rotation, particularly during the initial stages of animation with limited data. Addressing these constraints is essential for robust backtesting and performance evaluation.

### A. Dynamic Plane Visualization Testing

Thorough testing of the dynamic plane visualization is crucial for verifying its correct operation and ensuring the reliability of insights derived from it.

- **Visual Verification of Transformations:** Verify the correctness of the dynamic plane transformations (origin refocusing, rotation to principal axes) using visual inspection of example images. Show examples of the raw candlestick input (OHLCV over the 5-day window) _before_ and _after_ transformation. An animation showcasing the step-by-step evolution of the dynamic plane as new data points are added will further aid in verifying the dynamic behavior.
- **Image Rendering for Model Input:** Confirm that the dynamic plane generator successfully renders an image representation suitable for use as input to the CNN/ViT models. This is the primary function of the generator and requires thorough testing.
- **Minimum Data Point Handling:** Test the generator's behavior with fewer than two data points. The system should gracefully handle this edge case by generating a default representation or providing an appropriate error message.
- **Animation Simulator Testing:** Given the importance of the dynamic plane visualization, especially during backtesting, rigorous testing of its animation component is necessary.

### B. Backtesting Procedure and Considerations

The backtesting framework must account for potential issues arising from the dynamic plane rotation, particularly when dealing with a small number of data points in early frames.

- **Minimum Data Points for Rotation:** The animation requires at least two points for proper rotation. The backtesting framework should delay rotation and plotting until at least two valid data points are available, preventing errors in the early stages of the backtesting period.
- **Handling Single-Point Frames:** For frames with only a single data point, the visualization should display an empty canvas or placeholder to maintain consistency and avoid errors.
- **Data Offset Consistency:** Ensure consistent and correct formatting of data offsets, particularly when dealing with limited data points. This prevents dimension mismatches and ensures accurate calculations within the dynamic plane rotation.
- **PCA Instability with Limited Data:** Address the potential instability of PCA calculations when data points collapse onto a line, especially in early backtesting frames. Implement measures to detect and mitigate this instability, such as alternative dimensionality reduction techniques or error handling for degenerate cases.

### C. Dataset Generation for Model Training

Generating datasets based on the dynamic plane principle is crucial for training the CNN and ViT models. This process requires thorough testing to ensure data quality and reliability.

- **Transformation Verification:** Verify the correctness of data transformations (rotation, scaling, translation) applied to the raw market data during dataset generation.
- **Sequence Generation Consistency:** Validate the process of generating longer sequences of dynamic plane data to ensure consistency and prevent artifacts or biases.
- **Smoothing Technique Integration:** Explore and test the integration of smoothing techniques like Heikin-Ashi within the dynamic plane visualization and dataset generation. This can enhance visual clarity and potentially reveal flaws in data transformation or model predictions.

These procedures contribute to a comprehensive testing and evaluation process, ensuring the quality of training data, reliable visualizations, and robust backtesting results, ultimately leading to a more robust and reliable performance evaluation of the trading models.

## III. Testing and Evaluation

This section details the testing and evaluation procedures for the trading agent and its underlying data transformation pipeline. This includes performance evaluation using standard metrics, validation of the data preparation and transformation process, and specific tests designed to assess the robustness of the dynamic plane visualization under various market conditions.

### A. Data Pipeline Validation

Before comprehensive backtesting, the data pipeline and transformation logic are validated using a representative subset of the actual training data. This ensures data integrity and proper transformation before feeding it into the model. This validation process involves:

1. **Dataset Preparation:** A dedicated subset of real market data is prepared for these tests, mirroring the data used for model training.

2. **Batch Generator Validation:** The batch generator, responsible for efficiently feeding candlestick image data to the model, is tested to ensure correct batch creation and delivery.

3. **Transformation Logic Validation:** Small-scale tests, including visual inspection, are conducted to verify the core transformation pipeline, specifically the "refocusing of origin and static 3D axes to principal 2D rotating axes." This validation uses example images for comparison:

   - **Heiken Ashi Input (Before):** Example images of the raw Heiken Ashi candlestick data before transformation are generated and documented.

   - **Heiken Ashi Input (After):** Example images of the Heiken Ashi candlestick data after the transformation are generated and documented. This direct comparison allows for precise verification of the transformation's effect.

The use of static images simplifies the validation process and facilitates precise verification.

### B. Dynamic Plane Visualization Testing

The dynamic plane visualization is rigorously tested to ensure robustness and correct functionality, including:

1. **Minimum Points for Rotation:** The animation simulator is tested to ensure it adheres to the architectural requirement of a minimum of three stable points before initiating plane rotation to avoid PCA instability.

2. **Smooth Rotation Matrices:** Tests verify that the rotation matrices are smoothed or stabilized, preventing dimension blowups and ensuring smooth transitions in the animation.

3. **Standalone Functionality:** The standalone animation simulator is tested to verify correct functionality, including delayed rotation, smoothing of early plane formation, and the presence of a comprehensive step-by-step visualization.

4. **Dynamic Point Movement:** Visual inspection confirms that the animation accurately depicts the dynamic movement of points within the rotating frame.

5. **Live Frame Rotation and Recentering:** Visual inspection verifies that the animation correctly demonstrates the live rotation and re-centering of the frame.

### C. Simulated Market Condition Testing

The dynamic plane's performance is evaluated under simulated market conditions:

1. **Complex Price Pattern Simulation:** A simulation incorporating a rally, drop, and recovery phase tests the model's performance under nuanced price dynamics.

2. **Chaotic/Choppy Market Conditions Simulation:** A simulation of a chaotic, choppy sideways market assesses the dynamic plane's resilience to high volatility and rapid price fluctuations. This includes generating and visually comparing standard and rotated Heiken Ashi charts from the simulated data using the `generate_choppy_candlesticks(n=30)` function.

### D. Performance Evaluation

Standard performance metrics, including Jensen's Alpha and the Sharpe Ratio, are calculated to assess the trading agent's performance. Feature attribution tools such as Grad-CAM or SHAP are employed to understand model decisions.

## III. Testing and Evaluation

This section details the testing and evaluation procedures necessary to validate the model's performance and understand its behavior within the dynamic PCA-based input space. This involves standard backtesting and performance metrics, along with specialized techniques to address the dynamic nature of the input data.

### A. Market Regime Analysis and Visualization

Beyond traditional backtesting, the model's performance is analyzed across distinct market regimes to assess its robustness and adaptability. This involves:

1. **Simulating Market Regimes:** Three distinct market regimes are simulated: Trend-Reversal-Recovery, Choppy Sideways, and a strong linear uptrend, providing a diverse set of market conditions for evaluation.

2. **Visualizing Regimes on the Dynamic Plane:** Visualizations demonstrate how these regimes manifest within the transformed dynamic plane coordinate system, allowing for a qualitative assessment of its ability to capture regime-specific characteristics.

3. **Comparative Visualization:** Standard Heiken-Ashi charts are presented alongside their rotated dynamic plane counterparts for each simulated regime in a single, combined visualization. This direct comparison highlights the potential advantages of the dynamic plane representation.

4. **Analysis and Summary:** The behavior of the different market regimes within the dynamic plane is analyzed, and the implications for model learning are summarized for inclusion in the dissertation. This summary focuses on how the dynamic plane transforms the visual representation of each regime and whether these transformations improve the model's ability to learn and predict market behavior.

### B. Backtesting

Standard backtesting procedures are employed, incorporating considerations specific to the dynamic PCA input:

- **Backtesting Framework:** A robust framework is implemented using a rolling walk-forward validation methodology to ensure stable performance over time and mitigate overfitting. Stress testing is conducted to evaluate resilience under various market conditions.

- **PCA Parameter Sensitivity:** The impact of the PCA window size and any applied smoothing techniques is carefully monitored and mitigated within the backtesting framework to address potential overfitting due to small window sizes or noisy market data.

- **Relational Learning Evaluation:** The effectiveness of the model's relational learning—its ability to discern patterns in the dynamic PCA space—is a key evaluation criterion. This emphasizes the model's focus on dynamic relationships rather than fixed semantic meanings of price, time, and volume.

### C. Performance Evaluation

Traditional performance metrics like Jensen's Alpha and Sharpe Ratio are calculated. However, due to the unique nature of the model's inputs, the following analyses are essential for interpretability:

- **PCA Impact on Feature Representation:** Analysis focuses on how PCA dynamically affects the features represented within the candlestick images, confirming whether these features adapt based on input patterns, sometimes emphasizing price and volume, and at other times, time and volume, etc. Understanding this dynamic representation is crucial for accurate performance evaluation.

- **Geometric Pattern Recognition:** The model's success in recognizing geometric shapes and flows within the normalized PCA space is evaluated, as this is central to its core functionality, differentiating it from approaches based on traditional technical indicators.

- **Interpretability Projection:** Due to the dynamic axis rotation introduced by PCA, a mechanism is employed to project the model's focus back into the original Time-Price-Volume space. The effectiveness of this projection is evaluated, as it is essential for human understanding of the model's decision-making process.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the performance and robustness of the developed trading agent. The evaluation focuses on the dynamic rotating plane algorithm, its error correction mechanisms, and the impact of prediction errors on frame adjustments. This comprehensive approach aims to understand the agent's behavior across diverse market conditions.

### A. Backtesting

The evaluation begins with backtesting the trading strategy derived from the model's predictions using a robust framework. This framework incorporates:

- **Standard Backtesting Procedures:** Simulating trading performance using historical data.
- **Flexible Backtesting Framework:** A dedicated framework enabling configurable backtesting and detailed results analysis.
- **Rolling Walk-Forward Validation:** Evaluating performance across different time periods and market conditions to ensure robustness and mitigate overfitting.
- **Stress Testing:** Assessing resilience to extreme events by simulating various adverse market scenarios, including analysis of performance under both peripersonal and extrapersonal stress levels.

### B. Performance Evaluation

Traditional performance metrics and feature attribution techniques are employed to quantify the strategy's effectiveness and understand the model's decision-making process.

- **Performance Metrics:** Calculating key indicators like Jensen's Alpha and the Sharpe Ratio to assess risk-adjusted returns.
- **Feature Attribution:** Utilizing tools like Grad-CAM or SHAP to analyze the importance of different features in predicting successful trades and identifying potential areas for improvement.

### C. Dynamic Plane Algorithm Refinement

This subsection details the enhancements implemented to improve the Dynamic Plane algorithm's robustness and responsiveness to prediction errors. These enhancements are inspired by the concept of "rolling frame correction," analogous to biological wound healing, dynamically adjusting the Principal Component Analysis (PCA) frame based on a rolling analysis of prediction errors.

1. **Prediction Error Buffer:** A rolling buffer stores recent prediction errors, calculated as the difference between the predicted and realized movement within the rotated frame. The buffer size (e.g., 5-10 steps) is a crucial parameter influencing the system's responsiveness and stability and will be determined experimentally.

2. **Error Trend Detector:** This algorithm analyzes the prediction errors in the buffer, calculating their rolling mean and variance. A threshold, defined as a multiple (e.g., 1-2x) of the rolling standard deviation, triggers the frame correction process when exceeded, indicating a significant error trend.

3. **Rolling Frame Correction Algorithm:** Activated when the error trend detector identifies a significant trend, this algorithm adjusts the PCA frame. This adjustment involves incorporating the accumulated error information from the buffer to refine the frame's orientation and improve subsequent predictions. Details of the specific frame adjustment calculations will be provided in a subsequent section.

4. **Dual-Frame Estimation:** The system maintains two overlapping local frames: an "optimistic" frame with immediate rotations and a "stable" frame with slower smoothing. Predictions are dynamically weighted between these frames based on observed market consistency, blending rapid adaptation with cautious stability.

5. **Lagging Rotation Optimization:** A mechanism to dynamically adjust the lagging rotation is implemented. This involves a strategy to decrease the lag and revert to the normal rotation speed when appropriate, improving responsiveness to changing market conditions. The criteria for deactivating the lagging rotation will be detailed further.

6. **Removal of Static PCA Error Value:** The static, scalar float previously used in PCA calculations, representing prediction error memory, has been removed due to potential inaccuracies. The dynamic error buffer and rolling frame correction provide a more robust and adaptive approach to handling prediction errors.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the performance and robustness of the developed models and trading strategies. While standard backtesting provides a historical performance overview, this evaluation prioritizes analyzing prediction errors, specifically deviations between predicted and actual market movements within a dynamic 2D plane. Furthermore, we evaluate the efficacy of the rolling frame correction algorithm integrated into the dynamic plane generator.

### A. Rolling Frame Correction Evaluation

This subsection details the implementation and testing of the rolling frame correction algorithm within the dynamic plane generator.

- **Integration:** The rolling frame correction algorithm will be integrated into the existing `DynamicPlaneGenerator` class. This integration will be modular, allowing for easy activation and deactivation of the correction mechanism.

- **Frame Correction Implementation:** When the error trend detector signals a correction, the Principal Component Analysis (PCA) frame will be adjusted. This may involve re-weighting principal axes away from pure PCA towards a more stable configuration, potentially by discounting minor eigenvectors. The magnitude of this adjustment is a critical parameter requiring careful tuning.

- **Healing Phase Implementation:** Following a correction, a healing phase will gradually reduce the correction magnitude as the error returns to acceptable levels, following an exponential decay pattern. The decay rate is another parameter requiring careful tuning.

- **Error Visualization:** A visualization tool will be created to illustrate the "error spike → correction → healing decay" process over simulated sequences, providing clear insight into the algorithm's dynamics.

- **Frame Intervention Metric:** A metric will be developed to quantify "frame intervention" over a trading year, assessing how frequently the correction mechanism activates. This provides insights into the system's fluidity and adaptability to changing market conditions. A lower intervention rate suggests better inherent stability.

### B. Backtesting and Performance Evaluation

This subsection outlines the backtesting methodology and performance evaluation metrics.

- **Backtesting:** Standard backtesting procedures will be employed, including:

  - Implementing backtesting logic to simulate trades based on model predictions over historical data.
  - Developing a robust backtesting framework for repeatable and reliable performance assessments.
  - Implementing rolling walk-forward validation to evaluate performance on unseen data and mitigate overfitting.
  - Conducting stress tests to evaluate the strategy's resilience under various market conditions.

- **Performance Metrics:** Beyond traditional metrics, the evaluation will focus on understanding prediction errors:
  - Calculating standard performance metrics such as Jensen's Alpha and the Sharpe Ratio to benchmark risk-adjusted returns.
  - Utilizing feature attribution tools like Grad-CAM or SHAP to understand influential features in model predictions.
  - Monitoring deviation vectors in a dynamic 2D plane, representing the difference between predicted and realized movement vectors, to measure prediction accuracy in magnitude and direction.
  - Tracking angular error between predicted and realized movement vectors using the arccosine of the normalized dot product to measure directional accuracy.
  - Implementing an enhanced error trend detector analyzing deviations from expected relational movement within the dynamic 2D frame, considering both distance (magnitude of deviation vector) and angular errors. This detector will incorporate price, time, and volume considerations due to their dynamic interactions impacting error trends.
  - Refining any existing pseudocode related to error detection and analysis into robust, production-ready code for integration into the testing and evaluation framework.

This multifaceted testing and evaluation approach provides a comprehensive understanding of the system's performance, strengths, weaknesses, and areas for refinement. The emphasis on error analysis within a dynamic framework is crucial for improving prediction accuracy.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the performance of the developed models and trading strategies, focusing on the accuracy of the dynamic plane implementation and its predictive capabilities. This evaluation encompasses both traditional performance metrics and a detailed analysis of prediction accuracy within the dynamic 2D plane.

### A. Dynamic Plane Accuracy

Accurate error analysis requires comparing predicted and actual movements within a consistent coordinate system. The "freeze and compare" method achieves this by freezing the dynamic PCA frame calculated at time 't' and using it as the reference for predicting and evaluating movements over a short future horizon (e.g., 1-3 candlesticks). This approach assumes market structure remains relatively stable within this short timeframe.

1. **Freezing the Dynamic Frame:** The dynamic PCA frame calculated at time 't' is frozen. Both predicted and realized movement vectors are then interpreted within this frozen frame.

2. **Reprojecting Realized Movement:** The realized movement at 't+1' is reprojected back into the frozen PCA frame established at time 't' using the original PCA basis (rotation matrix). This enables direct comparison with the predicted movement, even if the PCA axes would differ if recalculated at 't+1'.

3. **Plane Consistency:** An investigation will clarify the consistency of the 2D plane derived from PCA between the predicted and realized movement matrices. Potential differences in price and volume values between these matrices could lead to different PCA axes. Any necessary transformations or alignment steps to ensure comparability will be implemented and documented.

Following these steps, the following error metrics are calculated:

- **Distance Error:** The magnitude difference between the predicted and realized displacement vectors within the frozen 2D plane.

- **Angular Error:** The orientation difference between the predicted and realized direction vectors within the frozen 2D plane.

A visual diagram will illustrate the two layers of rotation: the global frame transformation based on PCA and the local vector misalignment. This diagram will also depict how the calculated distance and angular errors relate to these rotations, clearly separating the initial frame creation rotation from the error correction calculations.

### B. Error Correction and Loss Function Refinement

To improve predictive accuracy, an error correction prototype and refined loss functions will be developed.

1. **Error Correction Prototype:** The existing error correction pseudocode will be expanded into a working prototype, enabling practical testing and validation of the logic. This prototype will incorporate lightweight frame adjustments (small rotational nudges) applied when accumulated angular errors exceed a predefined threshold.

2. **Loss Function Refinement:** New loss functions incorporating both distance and angular errors will be explored and potentially implemented. These will encourage the model to learn more accurate and directionally consistent predictions during training. A composite error score, combining distance and angular error using tunable alpha and beta parameters, will be used:

   ```
   Composite Error = (alpha * Distance Error) + (beta * Angular Error)
   ```

   The pseudocode implementation for an enhanced Error Trend Detector, integrating this composite error with rolling window analysis, will be developed and implemented. This will allow the model to dynamically adapt to changing market conditions by analyzing error trends within a rolling window. The potential for expanding this detector into a full dynamic rolling error correction module will be explored.

### C. Performance Evaluation

Beyond the detailed error analysis within the dynamic plane, traditional performance metrics such as Jensen's Alpha and Sharpe Ratio will be calculated to provide a comprehensive evaluation of the trading strategy's profitability.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the performance and robustness of the predictive model, specifically addressing the challenges posed by shifts in the PCA frame during evaluation. Two primary methods, "Freeze Frame" and "Reproject Realization," are compared against traditional distance and angular error calculations.

### A. Backtesting Methodology

Standard backtesting procedures, incorporating rolling walk-forward validation and stress tests, will be employed. Critically, these procedures will integrate the "Freeze Frame" and "Reproject Realization" methods to address the issue of evaluating predictions when the market-derived PCA plane shifts between prediction and realization.

### B. Performance Metrics and Error Analysis

Traditional performance metrics such as Jensen's Alpha and the Sharpe Ratio will be calculated. However, to accurately assess the impact of PCA frame shifts, dedicated evaluation procedures are necessary, clarifying the limitations of traditional error calculations that assume a static PCA plane. The proposed methods address this by accounting for dynamic market evolution.

- **Freeze Frame:** This method "freezes" the PCA rotation matrix (R) at the time of prediction. Both predicted and realized data points are projected onto this frozen plane, ensuring a consistent frame of reference regardless of subsequent market shifts.

- **Reproject Realization:** This method projects the realized movement vector onto the original PCA plane (defined at the time of prediction) using the original rotation matrix (R), providing a consistent evaluation framework within the original prediction context.

To illustrate these methods, numerical examples with predicted and realized movement values, along with calculated angle/distance errors within the frozen frame, will be provided. Visualizations will depict the prediction and realization paths in both the frozen and shifted frames for clear comparison. A dedicated "Freeze and Correct" module will encapsulate the logic for both methods. Robust pseudocode will detail the steps involved in freezing the PCA frame, projecting data points, and calculating errors, serving as the foundation for implementation within the backtesting framework.

### C. Robustness to Frame Drift and Total Error Calculation

The chosen PCA frame management approach (either Freeze Frame or Reproject Realization) must robustly handle shifts in market data distribution, preventing spurious results from minor fluctuations. This requires a revised error checking mechanism that accounts for the dynamic nature of the PCA planes (PCA1 and PCA2). Instead of calculating error on a static plane, deviations between predicted and real values relative to both PCA1 and PCA2 will be calculated individually.

A "Total Error" metric will comprehensively assess error, incorporating both the vector deviation error within the dynamic local frame and the error introduced by frame shifts:

```
Total Error = Vector Deviation Error + Frame Shift Error
```

The Frame Shift Error quantifies the difference between consecutive frames (basis vectors) using the principal angles between the two subspaces. In our two-dimensional case, this involves calculating the angle between PCA1 at time _t_ and PCA1 at time _t+1_, and similarly for PCA2. The Frame Shift Error is a weighted sum:

```
Frame Shift Error = α * Angle between PCA1 vectors + β * Angle between PCA2 vectors
```

where _α_ and _β_ are tunable weights. Pseudocode for the Total Error calculation, encompassing both the Vector Deviation Error and the Frame Shift Error, will be provided for clear and consistent implementation.

### D. Integrating Error Metrics into Model Decision-Making

This section will detail how the calculated error metrics (Total Error, Vector Deviation Error, and Frame Shift Error) can be integrated into the model's decision-making process. This might involve, for example, dynamically adjusting model parameters or prediction confidence based on the observed frame drift and prediction errors. Specific strategies for incorporating these error metrics will be outlined and evaluated.

## II. Error Analysis

A key aspect of testing involves simulating vector deviation and PCA frame drift to understand their impact on trading decisions. A controlled simulation will demonstrate the emergence and calculation of these errors step-by-step, providing insights into the dynamic frame's behavior under various market conditions and validating the error calculation algorithms. This simulation allows targeted investigation of specific error components and their interactions.

### B. Error as Confidence Indicator

Frame drift error's potential as a confidence indicator will be explored. The investigation will analyze how the magnitude and characteristics of frame drift can inform decisions about trading versus holding positions. High frame drift, potentially signaling increased market uncertainty, might suggest a more conservative approach. Conversely, low drift could indicate higher confidence in predictions. This research aims to develop a framework for incorporating frame drift into the decision-making logic.

### C. Error Weighting and Normalization

Calculating and combining different error components requires careful consideration. The following questions will be addressed:

- **Separate Weights for Vector and Frame Shift Errors:** The necessity of separate weights for vector deviation and frame shift errors will be investigated, comparing this approach to relying solely on existing alpha and beta parameters. The goal is to determine the optimal balance between these error types.
- **Normalization of Distance and Angular Errors:** Due to their differing units and scales, the current approach of directly adding distance and angular errors will be evaluated. This investigation will determine the necessity of normalization or other adjustments for accurate error aggregation, comparing different normalization techniques.
- **Weighted Error Calculation:** A comprehensive weighted error calculation will be implemented, combining vector deviation, angular error, PCA1 angle error, and PCA2 angle error:

  - `Vector Error = α₁⋅dᵥₑc + α₂⋅θᵥₑc`
  - `Frame Shift Error = β₁⋅θPCA₁ + β₂⋅θPCA₂`
  - `Total Error = γ₁⋅Vector Error + γ₂⋅Frame Shift Error`

  Where:

  - `α₁, α₂` control the trade-off between distance and angle within the frame.
  - `β₁, β₂` control the trade-off between PCA1 and PCA2 drift.
  - `γ₁, γ₂` control the overall trade-off between prediction error and frame instability.

This weighted approach allows fine-grained control and optimization of the error calculation, balancing different error components. Determining appropriate weights will be crucial for the evaluation process.

## III. Testing and Evaluation

This section details the procedures for evaluating the trading agent's performance and robustness. Rigorous testing is crucial given the dynamic market and the agent's complexity.

### A. Backtesting

Standard backtesting procedures with a rolling walk-forward validation approach will assess the model's performance on unseen data, simulating real-world trading. Stress tests will evaluate the strategy's resilience under various market conditions, including high volatility and unexpected events, to identify potential weaknesses and refine the trading strategy. This provides a realistic assessment of the agent's performance and adaptability.

### B. Performance Evaluation

Beyond backtesting, a rigorous performance evaluation will use established financial metrics. Jensen's Alpha and the Sharpe Ratio will assess risk-adjusted returns. Feature attribution tools (e.g., Grad-CAM, SHAP) will provide insights into the agent's decision-making, visualizing feature importance and potential biases. This analysis will identify areas for improvement and ensure long-term effectiveness. A structured error analysis will include:

- **Numerical Example:** A numerical example will demonstrate total error calculation from individual components, clarifying each factor's impact.
- **Pseudocode for Error Computation:** Formal pseudocode will detail the multi-weight error computation for transparency and reproducibility.
- **Default Weights:** Initial default values for α, β, and γ weights, based on trading intuition and physics principles, will provide a starting point for optimization.
- **Normalization of Angle Units:** Angle units will be normalized (e.g., mapping degrees to [0, 1]) for consistent addition with other error components.
- **Error Trend Detection and Healing Phase:** An Error Trend Detector will monitor a rolling window of errors, triggering a "Healing Phase" with a decaying correction factor applied to the PCA frame construction if errors accumulate too rapidly. Re-entry into a "Correction Mode" will occur if errors spike during the Healing Phase, creating a self-regulating system adaptable to changing market conditions.

### C. Rolling Error Tracking and Wound Healing

The system will implement a rolling error buffer and associated mechanisms for dynamic adjustment and correction based on recent performance.

1. **Rolling Error Buffer:** A rolling buffer will store the total error over the last N steps (e.g., 5-10).

2. **Rolling Statistics:** The system will continuously calculate the rolling mean and variance of the buffered errors.

3. **Wound Detection:** A "Wound Phase" is triggered when the mean error exceeds k times the rolling standard deviation (e.g., k=2).

4. **Healing Phase Detection:** A "Healing Phase" begins when the mean error falls below a threshold, typically a multiple of the rolling standard deviation (e.g., 1-1.5 times).

5. **Correction Factor Application:** During the "Wound Phase," a correction factor will be applied…

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the effectiveness of the error detection and healing system within the trading agent, including its impact on overall performance and adaptive capabilities. The evaluation goes beyond traditional backtesting and performance metrics by incorporating dynamic performance tracking and self-healing mechanisms.

### A. Backtesting and Simulation

- **Backtesting Implementation:** Standard backtesting procedures will be implemented to evaluate the model's performance over historical data, simulating trades based on the model's predictions and calculating relevant performance metrics. This will include incorporating the error detection and healing logic into the backtesting framework.

- **Backtesting Framework Development:** A robust backtesting framework will be developed to facilitate efficient and repeatable testing. This framework will incorporate data handling, trade execution simulation, performance metric calculation, and the simulation of various market conditions, including stress tests to assess resilience under extreme scenarios like market crashes and periods of high volatility.

- **Walk-Forward Validation:** Rolling walk-forward validation will be employed to assess the model's robustness and adaptability to changing market conditions. This method will progressively evaluate the model's performance on unseen data, simulating real-world deployment scenarios.

- **Healing Simulation:** A simulated "wound → correction → healing" scenario will be developed to demonstrate the complete healing process flow. This will allow for detailed observation and analysis of the system's behavior under controlled conditions.

### B. Performance Evaluation

- **Standard Metrics:** Traditional performance metrics, such as Jensen's Alpha and the Sharpe Ratio, will be calculated to quantify the model's risk-adjusted returns both with and without the healing system active.

- **Healing System Effectiveness:** Specific metrics related to the healing process will be tracked and analyzed, including the frequency of error detection, time spent in correction and healing phases, and the impact of corrections on overall performance. This analysis will help determine the effectiveness of the healing system in mitigating errors and improving long-term performance.

- **Feature Attribution Analysis:** Feature attribution tools like Grad-CAM or SHAP will be used to understand the model's decision-making process and the potential influence of the healing system on feature importance.

### C. Dynamic Performance Tracking and Healing Evaluation

- **Prediction Correctness Tracking:** The model's prediction correctness will be continuously tracked using a simple scoring system (+1 for correct, 0 otherwise). This data will be used to drive the dynamic decay rate adjustment and performance-based healing mechanisms.

- **Rolling Prediction Correctness Buffer:** A rolling buffer will store the prediction correctness scores for the last N timesteps to provide a dynamic measure of the model's recent performance. The size of the buffer (N) will be determined through experimentation.

- **True Prediction Value Tracking:** The actual predicted values generated by the algorithm will be tracked for analysis and for dynamically adjusting the decay rate.

- **Dynamic Decay Rate Adjustment Evaluation:** The effectiveness of the dynamic decay rate adjustment will be assessed by comparing performance with a fixed decay rate. This analysis will determine the benefits of adapting the decay rate based on real-time performance.

- **Performance-Based Healing Evaluation:** The performance-based healing mechanism will be thoroughly evaluated to ensure it responds appropriately to varying levels of prediction accuracy. The impact of healing on trade performance and overall stability will be analyzed.

- **Correction Factor Decay Mechanism Implementation and Tuning:** The chosen decay mechanism (e.g., exponential decay) and its parameters (e.g., λ=0.95) will be implemented and tuned based on the backtesting and simulation results. The effectiveness of the decay mechanism in smoothly transitioning back to normal operation will be evaluated.

- **Dynamic Correction Re-entry Logic Evaluation:** The logic for dynamic re-entry into Correction Mode will be tested to ensure appropriate and timely responses to recurring errors. Sensitivity to market volatility and the effectiveness of preventing prolonged periods of inaccurate predictions will be analyzed.

- **Healing Phase Visualization:** Visualizations of the healing phase during simulations will be generated to illustrate the system's response to error events and the effectiveness of the decay mechanism and dynamic adjustments.

This comprehensive testing and evaluation strategy combines traditional performance metrics with dynamic performance tracking and adaptive healing mechanisms to ensure robust and reliable model performance in real-world trading scenarios.

## III. Testing and Evaluation

This section details the necessary data transformations for both model evaluation through backtesting and for applying Principal Component Analysis (PCA) during testing. These transformations ensure consistent, meaningful inputs and reliable results.

### A. Data Preprocessing for Backtesting

Before backtesting, the following transformations are crucial for accurate and reliable evaluation:

- **Relative Return Transformation:** Transform raw prices into relative returns, calculating either percentage change or log returns relative to the first price in the input window. This mitigates the impact of extreme price spikes and provides a consistent basis for comparison across different stocks and time periods.

- **Fractional Elapsed Time Encoding:** Represent timestamps as fractional values between 0 and 1, calculated by dividing the elapsed time by the total time span of the window. This preserves chronological order and addresses potential issues with irregular time spacing, providing a normalized time input for the model.

- **Dynamic Scaling for Price and Volume:** Implement a dynamic scaling mechanism for price and volume to accommodate values not encountered during training. This prevents outlier values, such as record highs, from skewing visualization and model inputs during testing.

### B. Data Preprocessing for PCA

Preprocessing is essential for meaningful PCA results. Before applying PCA to the price (P), time (T), and volume (V) data, the following steps are performed within a rolling window of _N_ data points:

1. **Z-score Normalization:** Each feature (time, price, and volume) undergoes independent centering and scaling to achieve zero mean and unit variance. This ensures equal feature contribution to the PCA, regardless of original units or scales. The z-score is calculated as: `X_scaled[,i,.,.] = ((t<sub>i</sub> - μ<sub>t</sub>) / σ<sub>t</sub>, (p<sub>i</sub> - μ<sub>p</sub>) / σ<sub>p</sub>, (v<sub>i</sub> - μ<sub>v</sub>) / σ<sub>v</sub>)`, where μ and σ represent the mean and standard deviation for each feature within the rolling window.

2. **Specific Feature Handling:**

   - **Time Handling:** Timestamps can be handled using either:

     - **Relative Time Index:** A sequence of integers (1, 2, ..., _N_) representing the relative time within each window, subsequently z-score normalized. This is suitable for consistent window sizes containing the last _N_ trading candles.
     - **Absolute Clock Time:** Time deltas (Δt<sub>i</sub> = (timestamp<sub>i</sub> - μ<sub>t</sub>) / σ<sub>t</sub>) to incorporate diurnal patterns. Caution is advised with large timestamp gaps, which can inflate σ<sub>t</sub>.

   - **Volume Transformation:** Address the heavy-tailed distribution of volume using:
     - **Log Transformation:** `v<sub>i</sub>' = log(1 + v<sub>i</sub>)` compresses outliers.
     - **Robust Scaling:** Subtract the median volume from each data point and divide by the interquartile range (IQR), reducing sensitivity to extreme outliers.

3. **PCA Implementation:** After preprocessing, PCA is performed on the scaled data matrix `X_scaled` using Singular Value Decomposition (SVD):

   ```python
   u, s, vh = np.linalg.svd(X_scaled, full_matrices=False)
   axes = vh[:2]   # two principal directions in T-P-V space
   ```

   This example illustrates using the first two principal components.

### C. Considerations for Live Data Integration (Future Work)

The following points require investigation for future model deployment:

- **Live LTP Feed for Time Tracking:** Explore using a live Last Traded Price (LTP) feed for time tracking, considering the trade-offs between real-time updates and historical context.

- **Non-linear Time Representation:** Investigate alternative, non-linear time scales or other methods to better capture the relevance of time as a scalar input, especially for non-uniformly spaced data.

### D. Model Healing and Correction

The model incorporates a healing mechanism to recover from periods of poor performance. This "healing-by-correctness" system leverages a `dynamic_decay_rate` function to adjust a correction factor based on mean prediction correctness. The formula `Decay Rate = 1 - (mean_correctness - healing_threshold)` ensures accelerated decay when correctness surpasses the threshold. The healing logic modifies the correction factor based on the rolling prediction correctness over the last N steps. High correctness leads to a reduction in the correction factor, while deteriorating correctness maintains or re-escalates it. Formal, modular pseudocode will be developed for this system to ensure clear documentation and facilitate implementation.

## III. Data Preparation and Preprocessing

This section details the necessary data transformations and preparations required before model training and evaluation. These steps ensure consistent and reliable input data for the models.

### A. Feature Engineering and Transformation

Several crucial data transformations are applied to the price, volume, and time data to prepare them for model input. These transformations ensure data stability, comparability, and reduce the impact of outliers.

1. **Volume Transformation:** A logarithmic transformation (`np.log1p(volume)`) is applied to the volume data to mitigate the impact of extreme values. Subsequently, outliers are clipped at the 5th and 95th percentiles. This robust approach handles extreme values without excessively influencing the overall data distribution. Finally, the clipped volume data is min-max scaled to the [-1, +1] range.

2. **Price Transformation:** Log returns are calculated from the price data. Similar to volume, extreme outliers in the log returns are clipped at the 5th and 95th percentiles. This ensures the price data is robust to extreme price fluctuations.

3. **Time Representation:** Time is represented as fractional elapsed time (`time_frac`) within the trading window and linearly transformed to the [-1, 1] range using the formula: `(2 * time_frac) - 1`.

4. **Min-Max Scaling and PCA:** The `time_frac`, clipped log returns of price, and transformed volume data are min-max scaled to the [-1, 1] range. Critically, the minimum and maximum values used for scaling are determined _across the entire dataset_ for each feature, ensuring consistent scaling for subsequent Principal Component Analysis (PCA). These scaled features are then combined into a single matrix. PCA is performed on this matrix to reduce dimensionality and capture the primary directions of variance. Centering the matrix before PCA is generally unnecessary as the data is already zero-centered due to the symmetric [-1, 1] scaling.

### B. Data Validation and Visualization

To validate the data transformations and provide clear examples, a set of sample data is generated for visualization and analysis.

1. **Illustrative Examples:** Five distinct examples demonstrating varying price and volume behavior are created. Each example includes:
   - The original candlestick chart _before_ transformations.
   - The transformed data, visualized to illustrate the effects of the preprocessing steps. This visualization aids in understanding the impact of each transformation and verifying their effectiveness.

### C. Backtesting and Performance Evaluation

The transformed data is then used for backtesting and performance evaluation.

1. **Backtesting Implementation:** The backtesting procedure simulates trading strategy performance on historical data, following a defined backtesting framework.

2. **Walk-Forward Validation:** A rolling walk-forward validation approach is employed to assess the robustness of the strategy and mitigate overfitting to specific time periods.

3. **Stress Testing:** Stress tests are conducted to evaluate the strategy's resilience under various market conditions, including extreme volatility and adverse events.

4. **Performance Metrics:** Key performance metrics, including Jensen's Alpha and the Sharpe Ratio, are calculated to assess risk-adjusted returns.

5. **Feature Attribution Analysis:** Feature attribution techniques, such as Grad-CAM or SHAP, are used to understand the influence of different features on the model's predictions and facilitate model refinement.

## III. Testing and Evaluation

This section details the evaluation process, which involves generating representative candlestick patterns, transforming them using Principal Component Analysis (PCA), and visualizing the results to understand the impact of volume on trajectory shapes in the dynamic plane. This visual analysis serves as a crucial sanity check for the data transformation pipeline, ensuring the generated dynamic plane snapshots accurately represent market dynamics for model training and evaluation.

### A. Candlestick Pattern Generation and Visualization

Five distinct market patterns will be generated and visualized, both as original candlestick charts and their corresponding dynamic plane projections:

1.  **Uptrend with rising volume:** Sustained upward price movement with increasing trading volume.
2.  **Downtrend with volume spikes:** Consistent downward price trend punctuated by sudden increases in trading volume.
3.  **Reversal (down then up):** Shift in market direction from a downtrend to an uptrend.
4.  **Sideways chop:** Price consolidation within a defined range, typically with fluctuating volume.
5.  **Breakout spike then stabilize:** Sudden price surge followed by relative stability. Five variations of this pattern will be generated, each with a different volume profile.

Each candlestick chart will represent a single trading day with 10-minute intervals. The corresponding dynamic plane projections will be derived using time, log-return, and log-volume data. This data will be normalized to the [-1, 1] range and then rotated using PCA onto the PC1/PC2 plane. Each original chart and its transformed projection will be displayed as a pair for direct comparison.

### B. Volume Impact Analysis on Breakout Spike and Stabilize Pattern

To further analyze the influence of volume, five distinct "Breakout Spike then Stabilize" scenarios will be generated, each with varying volume profiles and visualized using PCA. The analysis will focus on the following aspects:

1. **Early Cluster (pre-spike):** Examining the proximity and distribution of data points before the spike occurs.
2. **Spike Jump:** Analyzing the magnitude and direction of the jump in the dynamic plane representation during the spike. This will help determine how volume influences the spike's trajectory.

## III. Testing and Evaluation

This section details the testing and evaluation procedures employed to assess the performance and robustness of the dynamic frame prediction model. While traditional backtesting and performance metrics are employed, the dynamic nature of the model necessitates specific evaluations related to frame construction and the model's response to market fluctuations.

### A. Backtesting

Standard backtesting procedures will be implemented, incorporating dynamic frame generation at each prediction step. This involves:

1. **Framework Development:** A backtesting framework will be developed to ingest historical market data (Time, Price, Volume), generate dynamic frames using PCA, and feed the resulting images to the predictive model. Simulated trades will be executed based on the predicted movement vector (Δx', Δy') and Rally Time.

2. **Walk-Forward Validation:** A rolling walk-forward validation approach will be employed to ensure robust performance across diverse market conditions. This involves training the model on a past period and validating it on a subsequent period, iteratively progressing through the historical data.

3. **Stress Testing:** Stress tests using historical periods of high volatility and market crashes will be conducted to assess the model's resilience under extreme market conditions. This is crucial for evaluating the effectiveness of the model's stabilization mechanisms.

### B. Dynamic Frame Evaluation

The dynamic nature of the frame construction introduces unique evaluation requirements:

1. **PCA Stability Analysis:** The stability of the PCA projection will be examined, particularly the influence of volume spikes on PC2, and how the projection returns to a stable state after such events. This involves analyzing the trajectory of the data points in the PCA projection space following a volume spike.

2. **Cluster/Trajectory Analysis:** The formation of new clusters or trajectories after a spike, representing periods of stabilization, will be analyzed. This will provide insights into the model's ability to adapt to changing market conditions and establish new stable patterns.

3. **Interpolation Method Evaluation:** The impact of linear interpolation on smoothing price-time-volume data will be assessed and compared to more complex methods. This will determine if linear interpolation offers sufficient smoothing without masking crucial signals. The rationale for excluding bivariate spline interpolation, due to its potential to smooth out critical spikes, will be documented. Exploration of 3D interpolation techniques will be conducted to ascertain whether the added complexity translates to tangible performance improvements over simpler methods like linear interpolation.

By combining traditional backtesting with specific evaluations tailored to the dynamic frame generation, this testing and evaluation process aims to provide a comprehensive assessment of the model's performance, robustness, and its ability to capture intended market dynamics under various conditions.

## III. Testing and Evaluation

This section details the testing and evaluation procedures necessary to validate the model and ensure its performance aligns with project objectives. Given the complexities of incorporating multi-timeframe data, this phase requires careful attention to data processing, model architecture, and the dynamic frame generation process.

### A. Dynamic Frame Generation and Self-Correction Evaluation

This subsection focuses on the specific evaluation of the dynamic frame generation and its self-correction mechanism.

1. **Frame Stability:** The stability of dynamically generated frames will be evaluated by measuring the variance of principal components (PC1 and PC2) over time. High variance may indicate instability and trigger the "Wound Detection" mechanism, warranting further investigation.

2. **Rotation and Refocusing:** Correct implementation of rotation and refocusing will be verified by analyzing generated images. The last data point should consistently be positioned at the origin (0,0) after projection and re-centering.

3. **Image Quality:** The quality of generated images will be evaluated based on clarity and information content. They must effectively capture relational market flow and be suitable for the Vision Transformer. Different rendering techniques (e.g., Candlesticks vs. Heiken-Ashi) may be compared.

4. **Self-Correction Mechanism ("Wound Detection" and "Healing Phase"):**

   - **Wound Detection Sensitivity:** The sensitivity of "Wound Detection" will be assessed by analyzing its effectiveness in identifying periods of high prediction error. This involves analyzing the relationship between the dynamic threshold (multiple of rolling standard deviation) and the triggering of the "Healing Phase."

   - **Healing Phase Effectiveness:** The "Healing Phase" will be evaluated by analyzing model predictive accuracy before, during, and after its activation. This includes evaluating the system's ability to restore full dynamism based on regained predictive accuracy. Specific metrics will track performance recovery following a period of high error.

### B. Backtesting and Performance Evaluation

Existing backtesting procedures will be extended to incorporate the multi-timeframe model and the dynamic frames. This includes evaluating model performance with varying market conditions and across different time horizons (intraday, daily, weekly, monthly). The backtesting framework will be adapted to handle the complexities introduced by the chosen architecture (ensemble or multi-input transformer).

1. **Standard Performance Metrics:** Standard metrics like Jensen's Alpha and Sharpe Ratio will assess profitability and risk-adjusted returns. These will be calculated based on simulated trades within the backtesting framework.

2. **Multi-Timeframe Performance Analysis:** Performance will be evaluated considering the impact of different timeframes on overall predictive power. This includes analyzing the contribution of individual timeframes and their combined effect.

3. **Weight Assignment Strategy Analysis (if applicable):** If a weighting strategy (static or dynamic) is employed for combining multi-timeframe information, its impact on predictive power will be thoroughly analyzed. The goal is to identify the optimal weighting scheme.

4. **Feature Attribution Analysis (Optional):** Feature attribution tools like Grad-CAM or SHAP may be used to understand the model's focus on specific features within the dynamically generated images and across different timeframes. This can provide insights into the model's decision-making process.

### C. Addressing Architectural and Data Challenges

This subsection addresses specific challenges associated with implementing and evaluating the multi-timeframe model.

1. **Data Pipeline Complexity:** The data pipeline must handle the integration of multi-timeframe data (e.g., 10-minute, daily, weekly, monthly) using the Dynamic Rotating Plane method. Rigorous testing will ensure data integrity and efficient processing, minimizing overhead.

2. **Model Architecture Evaluation:**

   - **Ensemble Model:** Individual specialist models (trained on specific timeframes) will be evaluated both individually and in combination. This evaluation will focus on identifying potential conflicts or inconsistencies in predictions across different timeframes.

   - **Multi-Input Transformer Model:** The effectiveness of the attention mechanism in dynamically weighting input from different timeframes will be tested. This includes analyzing attention weights to understand timeframe importance and the rationale behind these assignments.

## III. Testing and Evaluation

This section details the testing and evaluation procedures for the SCoVA project, emphasizing the importance of rigorous backtesting and performance analysis in assessing the model's effectiveness and robustness. The evaluation process will leverage both quantitative metrics and visual analysis to provide a comprehensive understanding of the model's behavior.

### A. Backtesting (Continuity)

A robust backtesting framework is crucial for evaluating the model's performance under various market conditions and across different time horizons. This framework will incorporate:

- **Flexible Data and Timeframe Configuration:** Users will be able to specify the asset universe, time period, and specific timeframes for analysis, enabling tailored backtesting scenarios.
- **Dynamic Plane Integration:** The backtesting procedure will integrate the dynamic plane method, allowing for a thorough evaluation of the model's ability to leverage contextual information from different timescales.
- **Walk-Forward Validation and Stress Testing:** Walk-forward validation will be employed to assess the model's out-of-sample performance and adaptability to changing market dynamics. Stress tests will be conducted to evaluate robustness under extreme market conditions, including periods of high volatility and significant price movements.
- **Self-Correcting Mechanism Evaluation:** The backtesting process will specifically evaluate the effectiveness of the self-correcting mechanism in mitigating errors related to vector deviation and frame shifting.
- **Normalization Consistency:** The consistent application of the [-1, +1] normalization for time, price, and volume data will be maintained throughout the backtesting process.

### B. Performance Evaluation (Enforcer)

Performance evaluation will extend beyond standard metrics to provide deeper insights into the model's decision-making process:

- **Quantitative Metrics:** Standard performance metrics such as Jensen's Alpha and the Sharpe Ratio will be calculated to provide a quantitative assessment of the model's performance.
- **Comparative Analysis:** A comparative analysis against a baseline intraday model will be conducted to quantify the performance improvement achieved by incorporating multi-scale data.
- **Feature Attribution Analysis:** Techniques like Grad-CAM or SHAP will be employed to understand the model's reliance on different timeframes and identify potential biases or over-reliance on specific timescales.
- **Visualization:** The backtesting results will be visualized using clear and informative charts, including:
  - **Equity Curve:** A solid blue line depicting the cumulative profit and loss over time.
  - **Benchmark Comparison:** A dashed grey line representing the performance of a benchmark for direct comparison.
  - **Training Loss Curves:** Distinct colors or line styles to differentiate and analyze training loss curves for potential issues like overfitting.

## III. Testing and Evaluation

This section details the rigorous testing and evaluation procedures used to assess the performance and robustness of the developed trading agent. These procedures go beyond standard backtesting and performance evaluation, incorporating advanced metrics, visualizations, and healing mechanisms.

### A. Backtesting

Backtesting simulates real-world trading scenarios using historical market data. The framework allows flexible configuration of the data universe (e.g., NIFTY 50, NIFTY 500, or a custom watchlist) and the desired date range. It automatically splits the date range into appropriate training and validation sets for walk-forward analysis, iteratively training on past data and testing on subsequent periods. This approach evaluates the model's adaptability to changing market conditions. The framework also incorporates stress tests to assess resilience under extreme market events like high volatility and crashes.

The backtesting process integrates the configurable dynamic plane used by the model. Users can specify the candlestick type (e.g., Heiken-Ashi or standard), the local window size, and the features (price, time, volume) included in dynamic plane generation. This allows for evaluating performance under various dynamic plane configurations. A lookahead period is also implemented, allowing evaluation of predictive performance over different time horizons (e.g., 1, 3, 5, 10 candles).

### B. Performance Evaluation and Monitoring

Comprehensive performance evaluation uses a combination of standard metrics, advanced visualizations, and specialized tools to provide a deep understanding of the agent's behavior.

Standard financial metrics like the Sharpe and Sortino Ratios, Max Drawdown, Win Rate, Profit Factor, and Average Trade Duration are calculated. Jensen's Alpha is used to assess risk-adjusted returns compared to a benchmark. A distribution plot of trade returns visualizes the profit/loss distribution.

To gain further insights, a comprehensive monitoring display provides tabbed diagnostic plots for the Learning Curve, Error Signal Diagnostics, and Healing Mechanism Status. In backtesting mode, real-time metrics are displayed. Furthermore, feature attribution tools (e.g., Grad-CAM, SHAP) are employed to analyze the model's decision-making process by identifying the influence of different candlestick patterns on trade signals. This aids in model interpretability and potential refinement.

Finally, a side-by-side visualization compares a standard Heiken-Ashi chart with the model's dynamically rotated and re-centered 2D plane image, updating with each new candlestick. This provides insights into the model's real-time interpretation of market data and how dynamic plane parameters affect this interpretation.

### C. Advanced Evaluation and Healing Mechanisms

Beyond standard metrics, a "Total Error" metric provides a holistic performance view. This metric combines a "Vector Error" (difference between predicted and actual price movement vectors) and a "Frame Shift Error" (temporal misalignment between predicted and actual movements). Adjustable weighting parameters allow prioritizing specific aspects of prediction accuracy.

To maintain long-term performance, two healing triggers activate model retraining and recalibration: a performance-based trigger, activated when prediction accuracy falls below a user-defined threshold, and a time-based trigger, activated when the Total Error remains above a threshold for a specified duration.

## III. Testing and Evaluation

This section details the testing and evaluation procedures crucial for validating the performance and robustness of the developed trading agent. This process encompasses traditional backtesting and performance analysis, advanced testing methodologies to assess specific model functionalities, and the development of a user interface for streamlined experiment management and analysis.

### A. Backtesting and Performance Evaluation

- **Backtesting:** This simulates the agent's trading strategy on historical data to assess its performance under various market conditions, incorporating a rolling walk-forward validation approach and stress tests.
- **Performance Metrics:** Standard performance metrics such as Jensen's Alpha and Sharpe Ratio will be calculated to evaluate risk-adjusted returns. The evaluation will also assess the effectiveness of the dynamic capital allocation strategy, including the impact of starting capital and the chosen probability distribution for trades. Finally, the implemented error detection and healing strategies will be rigorously tested by simulating various error scenarios and measuring the system's ability to detect, diagnose, and recover dynamically.

### B. Advanced Testing Procedures

Beyond basic backtesting, these advanced procedures provide a comprehensive evaluation:

- **Multi-Scale Context Fusion:** The effectiveness of different context fusion methods (Attention-Based, Concatenation, and Weighted Average) will be evaluated to determine the optimal approach for integrating long-term and short-term market trends.

- **Context Awareness:** Testing will determine the optimal number of candlesticks per frame (allowing for variability) and the total number of frames, establishing the best input representation for different market scenarios.

- **Transfer Learning:** The generalizability of the trained models will be assessed by evaluating the effectiveness of transfer learning across different markets (US-US, US-India, and India-India).

- **Context-Aware Periodicity:** The optimal configuration of frames, candles per frame, and stock category (market capitalization, sectors, share price bins) will be determined for weighted predictions based on daily, weekly, monthly, quarterly, and annual periodicity. This context-aware approach aims to exploit periodic patterns in market behavior.

- **PCA Analysis:** Model performance will be evaluated using a reduced-dimensionality representation of the input data derived from Principal Component Analysis (PCA).

- **Hyperparameter Permutation Testing:** A comprehensive hyperparameter search involving testing all possible permutations for one epoch will establish baseline performance values and guide further optimization efforts.

### C. User Interface Development

A dedicated user interface will be developed to facilitate experiment management, data handling, and result visualization. This includes:

- **Backtesting Interface:** This interface will allow users to configure and execute backtests, visualize results, and analyze performance under various market conditions.
- **Real-time Monitoring Dashboards:** These dashboards will display key performance indicators dynamically, enabling real-time monitoring and rapid strategy adjustments.
- **Data Management Interface:** This component will support downloading, preparing, and managing diverse market data for both backtesting and performance evaluation.

## III. Testing and Evaluation

This phase focuses on rigorously assessing the performance and robustness of the developed trading agent.

### A. Backtesting

Backtesting is crucial for evaluating the agent's performance on historical data and ensuring its adherence to ethical guidelines. This involves simulating trading based on the agent's generated signals, incorporating transaction costs and platform limitations.

1. **Backtesting Framework:** A robust and flexible backtesting framework will be developed. This framework will simulate trade execution, portfolio management, and performance metric calculation, incorporating factors like trading fees, slippage, and initial capital. It will be designed to accommodate the specific requirements of the Zerodha KiteConnect platform, reflecting realistic operational constraints.

2. **Rolling Walk-Forward Validation:** To avoid overfitting and ensure the agent's generalizability, a rolling walk-forward validation approach will be employed. This involves training the model on a past period and validating it on a subsequent period, iteratively rolling the training and validation windows forward.

3. **Stress Testing:** The strategy will undergo rigorous stress testing under various market conditions, including extreme volatility and market crashes. This will evaluate its resilience and identify potential vulnerabilities, ensuring its robustness in adverse scenarios.

4. **Ethical Considerations (Dharmic Mandate Integration):** The backtesting framework will incorporate the "Dharmic Mandate" by implementing hard-coded ethical rules the agent cannot violate. This includes adherence to principles like Satya (truthfulness, ensuring immutable logs), Shaucha (purity, using rigorously cleaned data), and Santosha (contentment, balancing profit maximization with stability). The framework will detect and flag any violations of these rules during backtesting.

### B. Performance Evaluation

This subsection details the performance evaluation process, considering both traditional metrics and ethical constraints.

1. **Standard Performance Metrics:** Standard performance metrics such as Jensen's Alpha and the Sharpe Ratio will be calculated to quantify the agent's risk-adjusted returns.

2. **Ethical Considerations (Dharmic Mandate Impact):** Performance evaluation will consider the impact of the "Dharmic Mandate" on the agent's returns. High performance achieved through ethically questionable practices will be deemed unacceptable. The limitations imposed by the Zerodha KiteConnect platform, such as transaction costs, will be transparently incorporated into the performance analysis, aligning with the principle of Satya.

3. **Feature Attribution Analysis:** Techniques like Grad-CAM or SHAP will be used to understand the model's decision-making process and identify the most influential features driving its trading decisions. This analysis will improve model interpretability and potentially refine the trading strategy by identifying areas for improvement or bias.

## III. Testing and Evaluation

This section details the testing and evaluation procedures for the SCoVA agent. A robust evaluation framework is crucial for assessing the agent's performance and ensuring its alignment with the project's objectives. This framework encompasses both quantitative backtesting and qualitative analysis of the agent's decision-making process.

### A. Backtesting

Backtesting is essential for evaluating the viability and robustness of the trading strategies employed by the SCoVA agent. This involves simulating the agent's performance on historical market data.

1. **Implementation:** The backtesting logic meticulously executes the trading algorithm on historical market data, evaluating each trade signal against actual market prices to calculate profit/loss. Detailed logs capture all transactions, including entry and exit points, trade durations, and associated profit/loss figures. This granular data is crucial for subsequent analysis and refinement. The implementation details, including specific programming languages, libraries (e.g., `backtrader`, `zipline`), and data structures, are documented for reproducibility.

2. **Framework:** The backtesting framework provides the infrastructure for conducting the simulations. It encompasses data handling, trade execution logic, performance metric calculations, and reporting functionalities. Designed with modularity and extensibility, the framework facilitates the integration of new strategies, data sources, and performance metrics. Documentation includes architectural details, class diagrams, function signatures, and data flow diagrams.

3. **Walk-Forward Validation:** To simulate real-world trading conditions, walk-forward analysis is employed. Historical data is divided into in-sample and out-of-sample periods. The model is trained on the in-sample data and tested on the subsequent out-of-sample period. This process is repeated, rolling the time window forward, providing a realistic assessment of the strategy's performance and stability over time. Specific parameters for the walk-forward periods (e.g., training and testing period lengths) are documented.

### B. Performance Measurement and Analysis

Beyond basic profit/loss calculations, a comprehensive suite of performance metrics is used to assess the SCoVA agent. These metrics include, but are not limited to:

- **Sharpe Ratio:** Evaluates risk-adjusted return.
- **Jensen's Alpha:** Measures excess return compared to the market benchmark.
- **Maximum Drawdown:** Assesses the largest peak-to-trough decline during the backtesting period.

These metrics are analyzed in conjunction with the detailed transaction logs from the backtesting process to provide a holistic understanding of the agent's behavior and performance characteristics.

### C. Feature Importance Analysis

Understanding the agent's decision-making process is crucial for both performance evaluation and model refinement. Feature attribution tools, such as Grad-CAM or SHAP, are employed to identify the most influential features driving the agent's trading decisions. This analysis enhances model interpretability and can guide further development and optimization.

## III. Testing and Evaluation

This section details the comprehensive testing and evaluation process for the SCoVA project, encompassing both the core trading model and the supporting system architecture. The evaluation of the trading model focuses on backtesting, performance metrics such as Jensen's Alpha and the Sharpe Ratio, and feature attribution techniques like Grad-CAM or SHAP. In addition, the following system components will undergo rigorous testing:

### A. Backtesting and Model Performance

The core of model evaluation revolves around backtesting, simulating the trading strategy using historical data. The `ExperimentRunner` backend service, running on Cloud Run, executes individual backtests, fetching historical data, initializing the trained model, and simulating trades. Results, including key performance metrics, are stored in Firestore and visualized via the Campaign Runner UI. The backtesting framework incorporates:

1. **Backtesting Logic:** The `ExperimentRunner` service handles trade simulation, portfolio management, and performance metric calculation. The framework is designed for flexibility to accommodate various trading strategies and evaluation metrics.

2. **Rolling Walk-Forward Validation:** To mitigate overfitting and ensure generalizability, rolling walk-forward validation is employed. The model is trained on a past period and tested on a subsequent period, repeating this process with rolling time windows to evaluate performance under diverse market conditions. The specific rolling window parameters (e.g., training window length, testing window length, rolling window increment) will be explicitly stated and justified.

3. **Stress Testing:** Stress tests, simulating extreme market scenarios (e.g., market crashes, high volatility periods, unexpected news events), are integrated within the `ExperimentRunner` functionality. Results are logged in Firestore for analysis via the Campaign Runner UI. Specific historical periods known for extreme market behavior will be used.

4. **Performance Metrics:** Standard performance metrics, including Jensen's Alpha and the Sharpe Ratio, are calculated by `ExperimentRunner` and stored in Firestore. The Campaign Runner UI facilitates comparison across different experiment runs and campaigns. The formulas used, including adjustments for transaction costs and slippage, will be documented.

5. **Feature Attribution:** Grad-CAM or SHAP will be employed to identify influential input features, providing insights into model behavior and market dynamics. The `ExperimentRunner` service facilitates this analysis, and results are visualized through the UI. The specific implementation, including libraries and any modifications, will be detailed.

### B. System Component Testing

Beyond model-specific testing, the broader system architecture requires rigorous evaluation:

- **Backend (Python 3.11+, Cloud Run, Cloud Tasks):** Testing focuses on performance, scalability, and reliability, including load and stress tests to ensure resilience and handling of anticipated request volumes. The effectiveness of containerization and task orchestration will be verified.

- **Frontend (PWA with Flutter):** Comprehensive testing includes functional, usability, performance, and cross-browser compatibility tests.

- **Data Storage (Firestore, Cloud Storage):** Testing validates data integrity, consistency, and efficient data access, particularly for Parquet-formatted historical market data.

- **Development & Deployment (IDX.google, Firebase Hosting, Cloud Run/Functions):** Testing ensures smooth integration of IDX.google with deployment targets and seamless deployments without service disruption.

- **Analytics & Live Trading Module:** Thorough testing ensures the accuracy and clarity of the interactive results dashboard and the robustness of the live trading module.

## III. Testing and Evaluation

This section details the testing and evaluation procedures necessary to validate the performance, stability, and efficiency of the distributed training architecture, focusing on the interplay between the server and client (iPad) components, particularly for resource-intensive tasks like Vision Transformer training within a Progressive Web App (PWA).

### A. PWA Suitability and Stability Assessment

Given the computational demands of Vision Transformer training, a thorough assessment of PWA suitability for this task is crucial. This assessment should include:

- **Feasibility Study:** Investigate the capabilities and limitations of PWAs for handling GPU-intensive tasks, considering potential benefits and drawbacks, including the risk of browser crashes, especially during prolonged operation on the target iPad.
- **Crash Probability Assessment:** Quantify the likelihood of browser crashes under heavy GPU loads within the PWA environment on the iPad. Factors to consider include device capabilities, browser limitations, PWA resource management, and the interplay between these elements.
- **Resource Limitation Analysis:** Identify potential resource bottlenecks within the PWA environment, focusing on memory constraints, data access limitations, and GPU utilization on the iPad.

### B. Performance Evaluation

Performance evaluation will encompass both traditional financial metrics and metrics specific to the distributed training architecture:

- **Financial Metrics:** Calculate standard performance indicators like Jensen's Alpha and the Sharpe Ratio to assess the trading strategy's effectiveness.
- **Model Convergence:** Evaluate the convergence rate and stability of the global model on the server, considering the contributions from individual client models trained on iPads.
- **Communication Efficiency:** Measure the time and resources consumed by communication between the server and client devices, including data transfer and model updates. This should include analyzing the impact of network conditions on performance.
- **Client-Side Performance (iPad):** Track performance metrics on the iPad, including training speed, CPU usage, memory consumption, GPU utilization, and battery drain during model training.

### C. UI/UX Integration for Testing and Evaluation

The user interface will be adapted to provide transparency and control over the distributed training process:

- **Real-time Status Indicators:** Display real-time progress within the Campaign Runner and Experiment Designer for data downloading, image generation, local training on the iPad, model uploading, and server-side aggregation.
- **iPad Resource Monitoring:** Integrate resource monitoring directly within the iPad UI, displaying CPU usage, memory consumption, GPU utilization, and other relevant metrics during training to allow users to assess the impact on device resources.
- **Federated Averaging Visualization:** Visualize the federated averaging process, potentially illustrating the contributions of individual client models to the global model to enhance understanding of the collaborative learning process.
- **Backtesting Integration:** Ensure seamless integration of the distributed training paradigm within the existing backtesting framework, including rolling walk-forward validation and stress tests, accommodating both server-side aggregation and client-side training while maintaining data consistency and accurate performance measurement.

These comprehensive testing and evaluation procedures will ensure the robust, efficient, and stable operation of the distributed training architecture within the PWA environment on the target iPad device.

## III. Testing and Evaluation

This section details the testing and evaluation procedures necessary to assess the performance and stability of the application, particularly for computationally intensive tasks on an iPad. Given the potential resource limitations of PWAs, alternative solutions, including native iOS applications and a hybrid approach leveraging server-side resources, will be evaluated.

### A. PWA Performance and Stability Assessment

If PWA development proceeds, the following performance and stability aspects require thorough validation within the iPad environment:

- **PWA Performance Testing:** Performance testing will address user concerns regarding GPU-intensive tasks, specifically Vision Transformer training within the PWA. Data on TensorFlow.js and WebGPU performance on iPadOS will be collected. This data will inform optimization strategies and identify practical limitations.
- **GPU Resource Access:** An investigation into accessing iPad's GPU resources via the browser within a PWA will be undertaken. This includes assessing the impact of WebGPU and other relevant web standards on training speed and stability.
- **Stability and Resource Constraints:** User concerns about browser crashes, resource constraints (including the 50MB cache limit), and long-running processes will be addressed. This investigation will explore mitigation strategies, such as web workers and memory management techniques.

### B. Alternative Solution Evaluation: Native iOS and Hybrid Approach

Recognizing potential PWA limitations, alternative solutions will be concurrently evaluated:

- **Native iOS Application:** Developing a native iOS application in Swift will be explored. This approach may offer improved performance and access to system resources. Compatibility with the envisioned hybrid architecture (described below) will be a key consideration.
- **Hybrid Architecture:** A hybrid approach combining local and server-side processing will be investigated. This involves offloading computationally intensive tasks, such as initial model training, to a server with dedicated GPU resources (e.g., Google Cloud Run with NVIDIA T4 GPUs). The iPad would handle fine-tuning with smaller batches using Web Workers, enabling personalization and faster iteration.

### C. Hybrid Architecture Testing and Implementation (If Applicable)

If the hybrid architecture is pursued, the following testing procedures will be implemented:

- **Server-Side Processing:** The server will handle initial model training, large-scale computations, and store the primary model. It will receive weight updates (deltas) from the client.
- **Client-Side Fine-tuning and Backtesting:** The client (iPad, whether PWA or native app) will focus on model fine-tuning using recent data, interactive backtesting for shorter periods, and live inference. This utilizes WebSockets and the DynamicPlaneGenerator.
- **Campaign Runner UI Integration:** The "Campaign Runner" UI will feature an "Execution Target" dropdown, allowing users to select "Cloud GPU (Full Training)", "On-Device (Fine-Tuning Only)", or "On-Device (Interactive Backtest)" to control processing location.

### D. Backtesting in the Chosen Environment

Regardless of the chosen architecture (PWA, native, or hybrid), the existing backtesting framework will be adapted and utilized. This includes executing backtests within the chosen environment, monitoring resource utilization, and employing rolling walk-forward validation. This ensures consistent evaluation across different implementation strategies.

## III. Testing and Evaluation

This section details the testing and evaluation procedures for the SCoVA agent, focusing on the iOS application. These procedures are crucial before deploying the model to other platforms (web and Android).

### A. Backtesting

Backtesting is performed on the iOS app using historical market data to assess the viability and robustness of the generated trading strategies. The following procedures are implemented:

1. **Backtesting Logic:** Simulates trading activity based on the agent's signals, incorporating trading costs and realistic market conditions.
2. **Backtesting Framework:** A dedicated, modular framework manages data flow, strategy execution, and performance metric calculations, allowing for extensibility to different market scenarios and trading strategies.
3. **Rolling Walk-Forward Validation:** Evaluates performance across different time periods to assess adaptability and prevent overfitting to specific market conditions.
4. **Stress Testing:** Subjects the strategy to extreme market scenarios (e.g., high volatility, significant downturns) to understand its limitations and potential risks.

### B. Performance Evaluation

Performance evaluation, also conducted on the iOS app, uses quantitative metrics and qualitative analysis to assess the SCoVA agent's effectiveness.

1. **Performance Metrics:** Jensen's Alpha and the Sharpe Ratio are calculated to evaluate risk-adjusted returns compared to a benchmark.
2. **Feature Attribution Analysis:** Feature attribution tools (e.g., Grad-CAM, SHAP), compatible with the iOS Swift environment, analyze the model's decision-making process by identifying influential input features. This aids in understanding potential biases, refining the model, and improving interpretability. Leveraging client-side model training with Core ML and efficient local data storage (Core Data or Realm) facilitates this analysis.

While the Python backend provides the raw numerical data for backtesting and performance evaluation, ensuring data consistency and efficient access, the core computations are performed client-side on the iOS app. Before deployment to other platforms, the experimental setup used during model fine-tuning will be removed, and image processing will be limited to daily predictions and occasional re-tuning triggered by a higher error rate to optimize performance.

## III. Testing and Evaluation

This section details the testing and evaluation procedures necessary to ensure the robustness and reliability of the SCoVA agent across its various deployment targets. This includes traditional backtesting and performance evaluation of the core trading strategy, as well as rigorous testing of the agent's integration and performance within each specific platform (PWA, Native Android).

**A. Backtesting and Performance Evaluation of the Core Trading Strategy**

The backtesting process is crucial for evaluating the effectiveness of the trading strategy driven by the CNN and ViT models. This involves simulating the strategy's performance on historical data to understand its potential profitability and risk characteristics. The following methodologies will be employed:

1. **Backtesting Framework:** A robust and flexible backtesting framework will be developed to simulate trading decisions based on the model's predictions. This includes calculating portfolio values, tracking trades, and accounting for transaction costs. The framework will support various configurations and parameters, allowing for comprehensive evaluation of different models and strategies.

2. **Rolling Walk-Forward Validation:** A rolling walk-forward validation scheme will be implemented to simulate real-world trading conditions and assess the model's performance over time. This approach helps avoid overfitting and provides a more realistic assessment of the strategy's robustness.

3. **Stress Testing:** Stress tests will be conducted by simulating various market scenarios, including periods of high volatility and market crashes. This analysis will help identify potential weaknesses and improve the strategy's resilience.

4. **Performance Metrics:** Key performance indicators, such as Jensen's Alpha and the Sharpe Ratio, will be calculated to provide a quantitative assessment of the strategy's risk-adjusted returns and its ability to outperform the market.

5. **Feature Attribution:** Feature attribution tools like Grad-CAM or SHAP will be utilized to understand the model's decision-making process and identify the most important features driving predictions. This analysis will provide insights into the model's behavior and inform further refinement.

**B. Platform-Specific Testing**

Given the multi-platform deployment strategy, platform-specific testing is essential to ensure consistent performance and stability.

1. **Progressive Web App (PWA):** Testing of the PWA implementation will focus on:

   - **Functional Equivalence:** Verifying the functional equivalence of the TypeScript/JavaScript DynamicPlaneGenerator compared to the original Python version, with particular attention to numerical precision and performance using libraries like ndarray.
   - **TensorFlow.js Integration:** Thorough testing of model loading, prediction generation, and retraining processes within the browser environment using TensorFlow.js.

2. **Native Android App:** Testing of the Native Android application will focus on:

   - **Functional Equivalence:** Verifying the functional equivalence and performance of the Kotlin DynamicPlaneGenerator implementation.
   - **TensorFlow Lite Integration:** Ensuring correct model conversion and efficient on-device inference using TensorFlow Lite.
   - **Performance and Resource Usage:** Evaluating performance and memory usage characteristics on target Android devices.

3. **Cross-Platform Consistency:** Testing will also cover the robustness of the decoupled architecture, specifically the data exchange and model update process between the backend and both the PWA and Native Android application.

**C. Error Rate Monitoring and Retraining**

The functionality of the error rate monitoring system will be thoroughly tested. This includes validating its ability to accurately detect increasing error rates, trigger the re-tuning process, and seamlessly integrate updated models within both the PWA and Native Android environments. This also necessitates testing the retraining pipeline in both deployment contexts.

## III. Testing and Evaluation

This section details the testing and evaluation process, focusing on the integration, performance, and portability of the machine learning models within the cross-platform mobile application (iOS and Android).

### A. Backtesting

Given the mobile deployment target using Core ML (iOS) and TensorFlow Lite (Android), the backtesting procedure must account for platform-specific performance characteristics, including latency and resource utilization. The existing backtesting framework will be extended to incorporate these mobile-specific evaluations. This involves:

1. **Adapting the Backtesting Framework:** Modifying the framework to accommodate the requirements of Core ML and TensorFlow Lite. This may include creating separate routines reflecting the distinct characteristics of each platform.
2. **Platform-Specific Performance Testing:** Integrating the Swift `CoreMLHandler` and its TensorFlow Lite equivalent into the backtesting process. Performance will be measured not only in terms of trading performance (e.g., returns), but also execution speed, memory usage, and other relevant indicators on representative devices.
3. **Refined Walk-Forward Validation:** The existing walk-forward validation methodology will be retained, but the evaluation criteria will be expanded to include the platform-specific performance metrics described above. This ensures robust model performance across different time periods and under the constraints of the target mobile environments.
4. **Stress Testing on Mobile:** Conducting stress tests specifically designed for mobile environments. These tests will simulate conditions such as low-power states, limited network connectivity, and background process interference to assess the resilience of the model integration under realistic usage scenarios.

### B. Performance Evaluation

While traditional performance metrics like Jensen's Alpha and Sharpe Ratio remain important, the evaluation must also consider the nuances of mobile deployment. Feature attribution techniques will be used to understand model behavior on these platforms.

1. **Standard Performance Metrics:** Jensen's Alpha and Sharpe Ratio will be calculated using the backtesting results obtained from the platform-specific implementations. This provides a baseline comparison with the model's performance in a non-mobile environment.
2. **Mobile-Specific Performance Metrics:** Metrics relevant to mobile performance, such as inference speed, memory footprint, and power consumption, will be collected and analyzed. This provides a comprehensive assessment of the model's suitability for mobile deployment.
3. **Feature Attribution:** Feature attribution tools (e.g., Grad-CAM, SHAP) will be adapted for compatibility with Core ML and TensorFlow Lite. This will provide insights into the model's decision-making process on each platform and potentially reveal platform-specific discrepancies or biases.

### C. Model Management and Portability

A robust strategy for managing and ensuring portability of the different model formats (`.mlmodel` for iOS and `.tflite` for Android) is crucial. This strategy will encompass:

1. **Version Control:** Implementing version control for both model formats to track changes and ensure reproducibility.
2. **Storage and Updates:** Defining a clear process for storing and updating models on both platforms, considering factors such as application size and update frequency.
3. **Testing and Validation:** Establishing a rigorous testing and validation procedure for each new model version on both platforms before deployment.

This comprehensive approach ensures a thorough evaluation of model performance, considering both financial metrics and the practical realities of cross-platform mobile deployment.

## III. Testing and Evaluation

This section details the procedures for rigorously testing and evaluating the developed trading models. Given the focus on financial market prediction, backtesting and performance evaluation are paramount. Further testing considerations, such as platform-specific performance assessments, model conversion validation, and synchronization testing for on-device training updates, may arise as the project expands to different deployment environments.

### A. Backtesting

- **Implementation:** A robust backtesting system will be implemented to simulate the trading strategy's performance using historical market data. This involves feeding historical data to the trained model and executing simulated trades based on its predictions, incorporating details like transaction costs, slippage, and market impact.
- **Framework Development:** A dedicated backtesting framework will be built to support the backtesting process. This framework will handle data ingestion, signal generation, trade execution simulation, portfolio tracking, and performance metric calculation.
- **Walk-Forward Validation:** Rolling walk-forward validation will be employed to assess the model's robustness and adaptability across varying market conditions and time periods. This method involves training the model on past data, testing on a subsequent period, and iteratively rolling these windows forward to evaluate performance and mitigate overfitting.
- **Stress Testing:** The trading strategy will undergo rigorous stress testing using simulated extreme market scenarios, including market crashes and periods of high volatility, to evaluate its resilience and identify potential vulnerabilities.

### B. Performance Evaluation

- **Performance Metrics:** Key performance indicators, including Jensen's Alpha and the Sharpe Ratio, will be calculated to quantify the risk-adjusted returns of the trading strategy. These metrics will be benchmarked against relevant market indices or alternative strategies to assess the model's effectiveness.
- **Feature Attribution Analysis:** Feature attribution techniques, such as Grad-CAM or SHAP, will be used to understand the model's decision-making process. Analyzing which features most significantly influence predictions enhances model interpretability, potentially revealing biases and informing further refinements to the trading strategy.

## III. Testing and Evaluation

This section details the process for rigorously testing and evaluating the performance of the developed trading agent. It involves backtesting the agent's trading strategy on historical data, conducting stress tests, and evaluating its performance using relevant metrics.

### A. Backtesting

Backtesting is crucial to assess the viability and robustness of the trading agent. This subsection outlines the steps involved in the backtesting process:

1. **Backtesting Logic Implementation:** This involves implementing the core logic for simulating trades based on the agent's generated signals on historical market data. This includes simulating order execution, portfolio management, and transaction costs.

2. **Backtesting Framework Development:** A robust and flexible backtesting framework will be developed to facilitate the backtesting process. This framework will handle data loading, signal generation, trade execution simulation, and performance reporting.

3. **Walk-Forward Validation:** To avoid overfitting to the historical data and ensure the agent generalizes well to unseen market conditions, a rolling walk-forward validation approach will be employed. This involves training the model on a past period and validating it on a subsequent period, rolling forward through the dataset.

4. **Stress Testing:** Stress tests will be conducted to evaluate the resilience of the trading agent to adverse market conditions. This involves simulating extreme market scenarios, such as market crashes or periods of high volatility, to assess the potential impact on the agent's performance.

### B. Performance Evaluation

Evaluating the performance of the trading agent is essential to gauge its effectiveness. This subsection outlines the key performance metrics and analysis techniques used:

1. **Performance Metrics:** The performance of the trading agent will be assessed using key financial metrics, including Jensen's Alpha and the Sharpe Ratio. These metrics provide insights into the risk-adjusted returns generated by the agent.

2. **Feature Attribution Analysis:** To gain a deeper understanding of the agent's decision-making process, feature attribution techniques like Grad-CAM or SHAP will be employed. These techniques highlight the input features that contribute most significantly to the agent's trading decisions, allowing for better interpretability and potential model refinement.

## III. Testing and Evaluation

This section details the comprehensive testing and evaluation process designed to assess the robustness, reliability, and security of the developed trading models and infrastructure. This includes backtesting the trading strategies, analyzing password security for system access, and evaluating model performance using established financial metrics.

### A. Backtesting

Backtesting is crucial for evaluating the viability of the trading strategies generated by the CNN and ViT models. This involves simulating trades using historical market data under realistic conditions.

- **Backtesting Implementation:** The core backtesting logic will simulate trades based on model-generated signals, incorporating transaction costs (e.g., commissions, slippage) and realistic order execution limitations. This simulation will track portfolio value, returns, and other relevant performance indicators.
- **Framework Development:** A robust and efficient backtesting framework will be developed to manage the entire process, from data ingestion and trade execution to performance calculation and visualization. This framework will facilitate systematic evaluation under various market scenarios.
- **Walk-Forward Validation:** To mitigate overfitting and ensure robust performance across different market regimes, a rolling walk-forward validation approach will be employed. This method trains the model on a past period and tests it on a subsequent, out-of-sample period, repeatedly moving the training and testing windows forward in time.
- **Stress Testing:** Rigorous stress testing will be conducted to assess the resilience of the trading strategies under extreme market conditions, such as market crashes, periods of high volatility, and unexpected events. This analysis will help identify potential vulnerabilities and ensure the robustness of the strategies.

### B. Security Analysis

Given the sensitive nature of financial trading, robust security measures are paramount. This section outlines the analysis to determine appropriate password security for system access.

- **Defining "Uncrackable":** A precise definition of "uncrackable" will be established, considering factors such as cracking time, available resources, current best practices, and the potential impact of future advancements in computational power. This definition will serve as the benchmark for password security.
- **Minimum Password Length:** An analysis will be conducted to determine the minimum alphanumeric password length required to achieve the defined level of "uncrackability" against modern brute-force attacks. This will consider current and projected cracking speeds using state-of-the-art hardware (e.g., high-end GPUs).
- **Cracking Speed Research:** Research will be conducted to determine current brute-force cracking speeds for various character sets and password lengths, using readily available high-performance hardware and software.
- **NIST Guideline Integration:** Relevant NIST guidelines and recommendations on password length, complexity, and management will be reviewed and incorporated into the security analysis.
- **Password Length vs. Cracking Time Analysis:** A comprehensive analysis will explore the relationship between password length, character set complexity, and the time required for a successful brute-force attack. This analysis will also evaluate the effectiveness of robust hashing algorithms like bcrypt and Argon2 in enhancing password security.

### C. Performance Evaluation

Following backtesting, the performance of the trading strategies will be rigorously evaluated using established financial metrics.

- **Performance Metric Calculation:** Key performance indicators (KPIs), such as Jensen's Alpha and the Sharpe Ratio, will be calculated to assess risk-adjusted returns. Jensen's Alpha measures the excess return compared to a benchmark, while the Sharpe Ratio quantifies the return per unit of risk. These metrics will help evaluate the profitability and consistency of the strategies.
- **Feature Attribution Analysis:** Feature attribution techniques, such as Grad-CAM and SHAP, will be employed to understand the model's decision-making process and identify the key features driving its predictions. This analysis will provide insights into model behavior, enhance interpretability, and potentially inform further model refinement.

## III. Testing and Evaluation

This phase focuses on rigorously assessing the performance of the developed stock prediction models and trading strategies. It involves backtesting the strategy on historical data and evaluating its performance using relevant financial metrics.

### A. Backtesting

Backtesting is a crucial step to evaluate the viability of the trading strategy generated by the models. This subsection outlines the implementation of the backtesting framework and its key components:

1. **Implement Backtesting Logic:** This involves implementing the core logic for simulating trades based on the model's predictions on historical market data. This includes simulating order execution, calculating portfolio value over time, and handling transaction costs.

2. **Develop Backtesting Framework:** This involves creating a robust and flexible framework that allows for easy configuration and execution of backtests. This framework should support different trading strategies, time periods, and data sources.

3. **Implement Walk-Forward Validation:** To avoid overfitting and ensure the model generalizes well to unseen data, rolling walk-forward validation will be employed. This involves training the model on a past period, testing it on a subsequent period, and then rolling the training and testing windows forward in time.

4. **Stress Testing:** The backtesting framework will also incorporate stress testing capabilities to assess the strategy's resilience under various extreme market scenarios, such as market crashes and periods of high volatility.

### B. Performance Evaluation

After backtesting, the model's performance needs to be quantified using appropriate metrics. This allows for objective comparison with benchmarks and aids in further refinement.

1. **Performance Metrics:** Key metrics such as Jensen's Alpha, Sharpe Ratio, and maximum drawdown will be calculated. Jensen's Alpha measures the risk-adjusted return of the strategy compared to a benchmark, while the Sharpe Ratio measures the return per unit of risk. Maximum drawdown measures the largest peak-to-trough decline in portfolio value, providing insight into the strategy's potential downside risk.

2. **Feature Attribution Analysis:** Techniques like Grad-CAM and SHAP (SHapley Additive exPlanations) will be employed to understand the model's decision-making process. These tools identify which features of the input data are most influential in driving the model's predictions, providing valuable insights into its behavior and potential areas for improvement.

## III. Testing and Evaluation

This section details the testing procedures used to validate the performance and robustness of the trading strategy and the supporting systems. A multi-stage approach, incorporating both simulated and live market data, ensures comprehensive evaluation while minimizing computational costs.

### A. Backtesting and Simulation

The initial testing phase relies on historical data and simulated trading environments:

- **Backtesting:** Backtesting is conducted to evaluate the historical performance of the trading strategy. This involves simulating trades based on past market data and analyzing the resulting returns. A robust backtesting framework facilitates this process, providing performance metrics and visualizations.
- **Rolling Walk-Forward Analysis:** Rolling walk-forward validation assesses the model's performance over time and its ability to generalize to unseen data, providing insights into the strategy's consistency and stability.
- **Stress Testing:** Stress tests evaluate the resilience of the strategy under various market conditions, including extreme scenarios like market crashes and periods of high volatility, identifying potential vulnerabilities.

### B. Model Evaluation and Feature Importance

After backtesting, the model's performance is rigorously evaluated:

- **Performance Metrics:** Key performance indicators, including Jensen's Alpha and the Sharpe Ratio, are calculated to assess the risk-adjusted returns of the strategy and benchmark its performance.
- **Feature Attribution:** Feature attribution techniques, such as Grad-CAM and SHAP, are employed to understand the model's decision-making process by identifying the most influential features (aspects of the candlestick charts). These insights can inform feature engineering and model architecture improvements.

### C. System Validation and Deployment Testing

Beyond backtesting and model evaluation, further tests ensure the reliability and stability of the entire trading system:

- **Unit and Integration Tests:** Unit and integration tests, using mock data (CSV/JSON) encompassing edge cases (e.g., price spikes, flat periods, data gaps), thoroughly test individual modules and their interactions, focusing on components like the Dynamic Plane Generator, data normalization functions, and the data loader.
- **End-to-End Dry Run:** An end-to-end dry run mode within the Experiment Designer UI simulates the complete workflow, from data retrieval to model input preparation, validating data flow and system integration without the computational cost of model training. A placeholder model update simulates the complete process.
- **On-Device Smoke Testing:** On-device smoke tests using a small, computationally inexpensive "dummy" neural network (mirroring the actual ViT model's input/output structure) with limited real data validate the Core ML training setup (loss calculation, backpropagation, weight updates) before full model deployment.
- **Short Training Runs:** Short, 1-epoch training sessions with limited data ranges are used for efficient and cost-effective testing and debugging during development.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to validate the system's functionality and robustness. A multi-stage approach is employed, focusing on both individual components and the integrated system.

### A. Unit and Integration Testing

Individual components of the system are rigorously tested in isolation using unit tests. This ensures each module functions correctly and adheres to its specifications. Subsequently, integration tests verify the seamless interaction between these modules. Specific tests include:

- **Data Loading Verification:** Confirming the correct loading and transformation of data within the pipeline, specifically by the `DynamicPlaneGenerator`. This ensures data integrity and proper formatting for the Core ML training process.
- **Training Loop Execution:** Validating the execution of a complete training step within the Core ML framework, including the forward pass, loss calculation, and backpropagation. This confirms the framework's ability to execute training without errors.
- **Weight Update Extraction:** Verifying the successful extraction of updated model weights after training. This validates the model update mechanism within Core ML.
- **Weight Access and Transmission:** Ensuring the application can access the updated weights and correctly package and transmit these updates to the backend system.

### B. End-to-End System Simulation

Following component-level testing, the entire system is tested in a simulated environment. This “dry run” uses historical market data to validate end-to-end functionality, including data acquisition, preprocessing, model inference, and signal generation, without executing real-world actions like trading. This comprehensive test helps uncover potential integration issues and verifies the system's overall performance.

### C. On-Device Smoke Testing

A simplified “smoke test” is performed on the target device (iPad) using a readily-available dummy neural network. This test focuses on identifying any major integration or deployment issues within the target environment. The current necessity of a single-epoch training run during this smoke test will be investigated. If feasible, this step will be bypassed to optimize resource utilization. This smoke test serves as a final verification step before deploying the fully trained model.

### D. Cognitive Threat Analysis Module (CTAM) Evaluation

This phase introduces a Cognitive Threat Analysis Module (CTAM) designed to enhance the system's resilience to unexpected market events, referred to as "shocker events." Testing and evaluation of the CTAM will focus on its effectiveness in identifying and mitigating the impact of these events. This will include assessing the CTAM's ability to analyze various data sources, including equity charts, and potentially futures, options, and derivatives data, to predict and respond to potential market disruptions. Specific metrics for evaluating the CTAM's performance will be defined and tracked.

A comprehensive technical document detailing the testing methodology and results will be produced. This documentation will serve as a valuable resource for future reference, maintenance, and potential reproduction of the system.

## III. Testing and Evaluation

This section details the testing and evaluation procedures necessary to assess the effectiveness and robustness of the SCoVA agent, particularly its systemic threat awareness and response capabilities. This evaluation focuses on the integrated Continuous Threat Assessment Module (CTAM) and its impact on the agent's performance under various market conditions, including "shocker" events characterized by volatility spikes, volume anomalies, and rapid price changes.

The testing strategy incorporates several key components:

**A. CTAM Functionality and Integration:**

This phase focuses on verifying the correct implementation and integration of the CTAM within the existing framework.

- **Shocker Event Definition and Simulation:** Formalize the definition of "shocker events" using quantifiable metrics derived from time-series data (volatility spikes, volume anomalies, rapid price changes). Develop a mechanism to generate simulated "shocker events" for testing purposes and evaluate the CTAM's ability to identify them in both simulated and historical data.
- **CTAM Performance and Integration:** Evaluate the CTAM's visual detection and analysis capabilities using lightweight CNN models trained to identify "shocker event" patterns in financial charts (equities, futures, options). Assess the balance between detection accuracy and computational efficiency for real-time anomaly detection. Thoroughly test the integration of the CTAM with the DynamicPlaneGenerator, focusing on the DynamicPlaneGenerator's response to the CTAM’s “Threat Level” score and the effects of modifying the smoothing function or learning rate based on the CTAM’s output.

**B. Backtesting and Performance Evaluation:**

This phase assesses the impact of the CTAM on the SCoVA agent's trading decisions and overall performance.

- **Enhanced Backtesting Framework:** Expand the existing backtesting framework to incorporate the CTAM's Systemic Threat Level (STL) into the simulation logic. The framework should process multi-source visual inputs (equity charts, futures charts, options chain data) and simulate diverse market conditions, including periods of low and high volatility, and simulated "shocker events" (flash crashes, black swan events).
- **Performance Metrics:** Utilize standard performance metrics (Jensen's Alpha, Sharpe Ratio) to evaluate the overall performance of the SCoVA agent with and without the integrated CTAM. Measure the changes in these metrics attributable to the CTAM's influence on the agent's trading strategy.
- **CTAM Effectiveness:** Evaluate the CTAM's effectiveness across multiple dimensions:
  - **Threat Detection Accuracy:** Analyze the true positive and false positive rates of the specialized CNN "threat detectors."
  - **STL Calibration:** Assess the calibration and effectiveness of the STL score in reflecting actual systemic risk, potentially comparing it with established market volatility indices or expert assessments. Analyze how different STL levels correlate with agent performance, particularly regarding proactive risk management (Pratyahara/withdrawal).
  - **Feature Attribution:** Employ feature attribution techniques (Grad-CAM, SHAP) to understand which features in the input data are most influential in driving the CTAM's threat assessment, validating its logic and identifying potential biases.
- **Walk-Forward Validation:** Conduct rolling walk-forward validation to assess the CTAM's predictive power and stability over time, considering its sensitivity to both "flow" and "shock" market regimes.

This comprehensive testing and evaluation approach ensures a robust assessment of both the SCoVA agent's performance and the effectiveness of the integrated CTAM. The evaluation prioritizes not only profitability but also the agent's resilience and adaptability in the face of systemic threats and volatile market conditions.

## III. Testing and Evaluation

This section details the rigorous testing and evaluation procedures crucial for validating the SCoVA project's models. While the provided checklist excerpt focuses on architectural considerations, these considerations directly impact the testability, interpretability, and ultimately, the reliability of our results. The principles of Continuity, Enforcement, Facilitation, and Specialization are paramount to structuring a robust evaluation framework. Specifically, addressing the current workload imbalance and clarifying role responsibilities within the testing and evaluation process are key.

The current disproportionate workload on Specialist components hinders thorough testing and comprehensive performance analysis. Redistributing tasks across all roles—Continuity, Enforcer, and Facilitator—will alleviate this bottleneck and streamline the process. This redistribution necessitates a re-architecting of roles within the testing and evaluation phase:

- **Continuity:** Components in this role will oversee the entire testing lifecycle, ensuring a smooth and consistent process. This includes managing dependencies, preparing data and models, executing tests, and documenting results. They ensure the consistent and reliable execution of the evaluation process from start to finish.

- **Enforcer:** Components in this role will be responsible for quality control and validation. This includes verifying the correctness of backtesting logic, managing computational resources, monitoring test execution, and identifying and mitigating errors in the evaluation process. They enforce the standards and procedures for robust and reliable testing.

- **Facilitator:** Components in this role will manage the data flow and communication related to testing. This encompasses providing the necessary data for backtesting, collecting and aggregating results, and disseminating these results to the other roles. They facilitate the seamless exchange of information within the testing framework.

- **Specialist:** With a more balanced workload, Specialist components can focus on core model development and refinement based on the feedback received from the testing process. The simplification of Specialist roles to specific tasks enhances testability and allows for more targeted analysis of model behavior using feature attribution tools like Grad-CAM and SHAP.

The current Backtesting and Performance Evaluation components require further decomposition to align with these roles. Specific functions within Backtesting, such as data preparation, simulation execution, and result aggregation, must be categorized according to the four principles. Similarly, within Performance Evaluation, individual metric calculations (e.g., Jensen's Alpha, Sharpe Ratio) and the application of feature attribution tools represent distinct functionalities that need to be categorized and potentially restructured.

This restructuring will lead to a more robust and efficient testing and evaluation process. By clearly defining roles and responsibilities and distributing the workload effectively, we ensure the reliability of our results, optimize resource utilization, and ultimately deliver a more robust and reliable final product.

## III. Testing and Evaluation

This phase focuses on rigorously evaluating the performance and robustness of the developed models and trading strategies. Automated services ensure smooth execution and thorough analysis of both training and backtesting procedures. This includes pre- and post-flight analysis, real-time performance monitoring, and state control of backtesting jobs.

### A. Backtesting (Continuity)

Backtesting incorporates pre- and post-flight analysis services to enhance reliability and insights:

- **Pre-flight Validation Service:** Before each backtesting run, this service verifies data integrity, confirms the backtesting environment's health, and validates the backtesting template schema. This proactive approach ensures runs start with valid inputs, preventing wasted resources and inaccurate results.

- **Post-flight Analytics Service:** After a backtesting run, this service analyzes the results, generates high-level insights, and updates the system's knowledge base. This includes summarizing key performance metrics, updating the model registry with results, and identifying recurring performance patterns. This automated analysis facilitates continuous learning and strategy improvement.

### B. Performance Evaluation (Enforcement)

Performance evaluation assesses trading strategy effectiveness. In addition to standard metrics like Jensen's Alpha and Sharpe Ratio, and utilizing feature attribution tools (Grad-CAM, SHAP), real-time monitoring and state control ensure reliability and stability:

- **Real-time Process Monitor:** This service continuously monitors the health and resource consumption of backtesting jobs. By tracking resource utilization (CPU, memory, disk I/O) and detecting potential issues like training stagnation, the monitor ensures smooth and efficient operation. Anomalies trigger alerts or automated interventions to prevent disruptions.

- **Execution State Controller:** This service manages the state of running backtesting processes, providing functionalities for pausing, resuming, and gracefully terminating jobs. This allows for greater control and flexibility during evaluation, ensuring efficient resource management and enabling intervention if unexpected issues arise.

### C. Supporting Services (Specialization)

Several specialized services provide essential functionalities for a comprehensive evaluation pipeline:

- **Model Inference Service:** This service performs inference on image tensors generated from candlestick data, crucial for backtesting and evaluating trained CNN models.

- **Coordinate Rotation Service:** This service provides a standardized way to rotate numerical arrays, enabling systematic testing of the rotation logic and its impact on model predictions, essential for evaluating the dynamic plane implementation.

- **PCA Service:** This service provides Principal Component Analysis capabilities for in-depth analysis of data and model outputs, potentially revealing underlying patterns and relationships that impact performance.

- **Normalization Service:** This service provides numerical array normalization, ensuring consistent input to the model during testing and evaluation, critical for reliable and comparable results.

While these specialized services are not directly invoked within the primary backtesting and performance evaluation procedures, they are crucial building blocks for a robust and flexible evaluation framework. Further integration details within the larger testing framework should be documented. The "ExperimentRunner," communication restrictions between specialist services, data storage access controls, function naming conventions, and functional pillar refactoring mentioned in the checklist contribute to the overall architecture and code organization, but are outside the scope of this specific testing and evaluation overview.

## III. Testing and Evaluation

This section details the process for rigorously testing and evaluating the performance of the developed trading agent. Given the architectural requirement of inheriting from four base classes (`ContinuityService`, `EnforcementService`, `FacilitationService`, and `SpecialistService`), specific services will handle the tasks outlined below. Technical specifications and pseudocode for these services, including the backtesting framework and performance evaluation metrics, will be drafted according to the inheritance-based design approach. This section also describes the data preprocessing pipeline necessary to prepare data for both training and evaluation.

### A. Data Preprocessing

Before training and evaluation, a dedicated preprocessing pipeline transforms raw data into a suitable format. This pipeline consists of several specialized services:

1. **`NormalizeWindow`:** This service normalizes raw numerical array data based on a provided configuration dictionary. To maintain focus and minimize dependencies, this service is restricted from importing `google-cloud-storage`, `google-cloud-firestore`, and `requests`.

2. **`ComputePrincipalComponents`:** This service calculates the top two principal component vectors from the normalized array output by `NormalizeWindow`. To enforce reliance on core numerical computation libraries, it is restricted from importing external libraries except `numpy`.

3. **`ProjectToPlane`:** This service projects the original data onto a 2D plane defined by the principal components calculated by `ComputePrincipalComponents`. This dimensionality reduction simplifies data representation while preserving key information.

4. **`TrainOneEpoch`:** This service focuses solely on training the model for a single epoch. It receives model artifact bytes, training data tensors, and configuration parameters as input, returning updated model artifact bytes. Its design isolates the training process, promoting code reusability and simplifying testing. This service is agnostic to the data's origin or destination.

### B. Backtesting (Continuity)

The backtesting process, managed by a dedicated `ContinuityService`, ensures continuous and consistent evaluation of the trading agent's performance against historical data. Key steps include:

1. **Backtesting Logic Implementation:** The `ContinuityService` encapsulates the core backtesting logic, handling data retrieval, simulated trade execution, and result aggregation.

2. **Backtesting Framework Development:** A comprehensive and reusable backtesting framework, adhering to inheritance principles, will be developed for evaluating various trading strategies.

3. **Rolling Walk-Forward Validation:** The `ContinuityService` will implement a rolling walk-forward validation approach to assess model performance across diverse market conditions and time periods, mitigating overfitting risks.

4. **Stress Testing:** The `ContinuityService` will conduct rigorous stress tests to evaluate the agent's resilience under various market scenarios, including extreme volatility and unexpected events.

### C. Performance Evaluation (Enforcer)

An `EnforcementService` manages performance evaluation, ensuring objective and consistent measurement of key performance indicators:

1. **Performance Metrics Calculation:** The `EnforcementService` calculates and reports key performance metrics, such as Jensen's Alpha and the Sharpe Ratio, providing quantifiable measures of risk-adjusted returns.

2. **Feature Attribution Analysis:** The `EnforcementService` will leverage feature attribution tools like Grad-CAM or SHAP to analyze the influence of individual features on trading decisions, providing insights into the agent's behavior.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the performance and robustness of the trading agent, particularly given its dynamic architecture incorporating the "Dynamic Rotating Plane" and "Dual-Engine Perception" (Flow Engine and Shockwave Prediction Model - SPM). These procedures encompass both backtesting methodologies and performance evaluation metrics.

### A. Backtesting

A robust backtesting framework will be developed to simulate trading decisions based on historical market data and the model's predictions. This framework will encompass data handling, trade execution simulation, and performance metric calculation. The following methodologies will be employed:

- **Rolling Walk-Forward Validation:** This method assesses the model's performance across different time periods and market conditions, providing a realistic evaluation of its adaptability and generalizability.
- **Stress Testing:** Stress tests will evaluate the agent's resilience to extreme market events and identify potential weaknesses, particularly relevant to the SPM designed for chaotic market behavior.

### B. Performance Evaluation

Performance evaluation will leverage both standard quantitative metrics and qualitative feature attribution analysis to understand the agent's behavior and decision-making process:

- **Performance Metrics:** Key performance indicators, including Jensen's Alpha and the Sharpe Ratio, will quantify the risk-adjusted returns of the trading strategy, enabling comparisons against benchmarks and alternative strategies.
- **Feature Attribution Analysis:** Tools such as Grad-CAM or SHAP will be employed to identify the key factors driving the model's predictions. This analysis provides insights into the relative importance of different features, enhancing our understanding of the model's decision-making process and potentially revealing areas for refinement. The dynamic nature of the "Dynamic Rotating Plane" and the interplay between the Flow Engine and SPM make this analysis particularly crucial for understanding how the agent interprets and reacts to market conditions.

## III. Testing and Evaluation

This section details the testing and evaluation strategy employed to assess the performance, stability, and reliability of the developed trading agent. A multi-stage approach is used, incorporating both simulated and real-world market data, and progressing from local tests to on-device validation.

### A. Backtesting

Backtesting forms the core of the evaluation process, simulating real-world trading conditions and assessing the agent's performance over time. A rolling walk-forward validation approach is implemented, training the model on historical data, validating it on a subsequent period, and then iteratively shifting these windows forward. This method mitigates overfitting to specific time periods and provides a more robust performance assessment. Stress tests, simulating periods of high volatility and market downturns, are also conducted to evaluate the agent's resilience under adverse conditions.

### B. Performance Evaluation

Standard performance metrics, including Jensen's Alpha and the Sharpe Ratio, are calculated to quantify the risk-adjusted returns of the trading strategy. Feature attribution analysis, using tools like Grad-CAM or SHAP, is employed to understand the model's decision-making process and identify the key features driving its predictions. This analysis is performed client-side, leveraging the available computational resources and providing insights into model interpretability.

### C. Multi-Stage Testing Strategy

A cost-effective, three-stage testing strategy is implemented to ensure comprehensive validation:

1. **Local Tests (Mock Data):** Rigorous local testing using mock or simulated market data facilitates efficient debugging and verification of the core logic without the cost of live data. This stage focuses on unit and integration testing of individual components and the combined system.

2. **End-to-End "Dry Run" Simulations (Real Data, No Training):** End-to-end simulations with real market data, but without model training, validate the entire data pipeline, trade execution logic (if applicable), and performance evaluation calculations. This identifies potential issues in data handling and processing before computationally expensive training processes begin.

3. **On-Device "Smoke Tests" (Dummy Model):** On-device "smoke tests" with a simplified or dummy model verify the complete system functionality within the target deployment environment. This validates the deployment pipeline, integration with brokerage APIs (if applicable), and real-time data feeds, ensuring a smooth transition to live trading with the fully trained model.

### D. Advanced Diagnostics and Monitoring (for Complex Models)

For more complex models, advanced diagnostic and monitoring mechanisms are implemented to ensure ongoing health and performance:

- **Advanced Error Signal:** A comprehensive error signal, incorporating both Vector Deviation Error and Frame Shift Error, provides a nuanced view of the model's internal state and potential deviations.

- **Performance-Based Healing:** Model recalibration is dynamically linked to prediction accuracy, ensuring timely adjustments when performance degrades.

- **Multi-Scale Periodicity Analysis:** Data from multiple timeframes (e.g., intraday, daily, weekly) is incorporated to enhance the model's understanding of cyclical market patterns.

- **"Rally Time" Prediction:** The model is enhanced to predict the expected duration (in candlesticks) of predicted price movements for optimized trade timing.

- **Distributed Tracing:** OpenTelemetry is used to implement distributed tracing, providing detailed insights into system performance and facilitating troubleshooting.

## III. Testing and Evaluation

This section details the rigorous testing and evaluation process designed to assess the trading agent's performance before live deployment. This process encompasses comprehensive backtesting, stress testing, and a crucial paper trading phase to bridge the gap between simulated and real-world trading.

### A. Backtesting

Backtesting serves as the initial stage of performance evaluation, simulating the trading agent's behavior against historical market data. A robust backtesting framework, incorporating the following elements, ensures the accuracy and reliability of the results:

- **Realistic Market Simulation:** The backtesting engine incorporates critical factors like market impact, latency, slippage, order queues, commissions, and fees to provide a high-fidelity simulation of real-world market dynamics. Leveraging professional backtesting libraries like Backtrader or Zipline is under consideration.
- **Rolling Walk-Forward Validation:** This methodology assesses the model's performance across different time periods and market conditions, providing a more robust evaluation than static backtesting.
- **Stress Testing:** Stress tests subject the trading strategies to various extreme market scenarios, such as significant price fluctuations and periods of high volatility, to evaluate their resilience.

### B. Paper Trading

Before deploying the agent to live trading, a paper trading environment provides a crucial intermediate step. This simulates real-world trading without risking capital, allowing for further refinement and identification of potential issues:

- **Paper Trading Implementation:** The paper trading module integrates with the backtesting framework and leverages the rolling walk-forward validation methodology for realistic simulations.
- **Paper Brokerage Simulator:** A dedicated `Paper_Brokerage_Simulator` service mimics the Zerodha Kite Connect API, handling order execution and portfolio management within the paper trading environment. It maintains its internal state in Firestore and exposes the same API endpoints as the `Live_Execution_Enforcer` for seamless transition to live trading. The simulator generates realistic fills based on live market data.
- **Live Execution Enforcer Integration:** The `Live_Execution_Enforcer` is designed to support both LIVE and PAPER modes, selectable via a toggle in the Live Trading Dashboard UI. This facilitates easy switching between the `Paper_Brokerage_Simulator` and the Zerodha Kite Connect API for live trading.

### C. Explainability and Compliance

To ensure transparency and regulatory compliance, a narrative generation service provides human-readable explanations for each trading decision:

- **Narrative Generation Service:** The `Narrative_Generation_Service`, triggered after each trade, leverages the Feature Store, model-agnostic attribution methods (LIME and SHAP), model-specific attribution methods (e.g., attention maps if applicable), and potentially a Large Language Model (LLM) to synthesize information into a clear narrative.
- **Feature Store Integration:** Integration with the Feature Store allows access to versioned input features and system state information, ensuring accurate and contextualized explanations.
- **Attribution Methods:** Implementing both model-agnostic and model-specific attribution methods pinpoints the most influential features driving each trade, providing insights into the model’s decision-making process.
- **Reporting and Auditing:** Generated narratives are stored in the Karma Ledger, creating an audit trail for compliance and facilitating detailed analysis of trading strategies. This addresses potential "black box" concerns by providing clear justifications for each trade.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the trading agent's performance in simulated and live market environments. The testing framework incorporates live market data, including tick data and market depth, for a realistic evaluation.

### A. Backtesting

The backtesting framework has been enhanced to incorporate live market data and simulate real-world trading conditions. This includes simulating order fills using live tick data from Zerodha, considering bid/ask prices, volume, and partial fills. Specifically:

- **Market Order Simulation:** Market orders simulate "walking the book," accounting for potential slippage due to liquidity consumption at different price levels.
- **Limit Order Simulation:** Limit orders are simulated within the live market depth order book, with execution based on the last traded price (LTP) and available quantity at the limit price.

This enhanced methodology, including live market depth data, provides a more robust assessment of the agent's performance and resilience to market fluctuations.

### B. Live Trading Simulation (Paper Trading)

A "Paper Trading" mode, integrated into the Live Trading Dashboard, allows seamless switching between live and simulated trading. This mode utilizes the `Paper_Brokerage_Simulator` to manage the paper portfolio state (cash balance, positions, order statuses), persisting data in a dedicated Firestore collection (`paper_portfolio`). Key components include:

- **Realistic Fill Simulation:** The `Paper_Brokerage_Simulator` uses live tick data from Zerodha to simulate fills, mimicking real-world execution based on live bid/ask prices and volume, including partial fills.
- **Live Data Integration:** Live WebSocket tick data from Zerodha provides up-to-the-second market information, enhancing simulation accuracy by reflecting current market dynamics and network latency.

### C. Market Depth Analysis and Feature Engineering

Market depth data is crucial for understanding and responding to real-time market dynamics. The following enhancements leverage this data:

- **Market Depth Integration:** The system analyzes and documents all components interacting with market depth data to understand its influence on the agent's decision-making.
- **Order Book Feature Derivation:** A new `DeriveOrderBookFeatures` service processes raw market depth data to generate features like Order Book Imbalance (OBI), Weighted Average Price (WAP), and Bid-Ask Spread. These features are then input to the `DynamicPlaneGenerator`.

Due to Zerodha's market depth precision limitations (increments of ₹0.05 between -₹0.25 and +₹0.25), spread calculations were replaced with order quantity and count at each of the five fixed bid/ask levels. This necessitated adjustments to the backtesting framework and the introduction of two new services:

- **`CalculateOrderBookImbalance` Service:** Calculates the OBI, a normalized value (-1.0 to +1.0) representing selling and buying pressure. This service requires testing to verify its correct calculation and effectiveness as a `DynamicPlaneGenerator` input feature. Its impact on the overall trading strategy will be assessed through backtesting, walk-forward validation, and stress tests.

- **`GenerateDepthQuantityHeatmap` Service:** Generates a heatmap visualizing order book quantities over time, used as input for the `MarketDepthAnomalyDetector` CNN. Testing will verify the heatmap generation and usability as CNN input. Performance evaluation will focus on the anomaly detector's ability to identify meaningful patterns and contribute to the trading strategy's overall performance. This will also be evaluated through backtesting.

### D. Performance Evaluation

Following backtesting and paper trading, performance will be rigorously evaluated using the following metrics:

- **Jensen's Alpha and Sharpe Ratio:** Quantify the risk-adjusted performance of the strategy.
- **Feature Attribution (Grad-CAM/SHAP):** Analyze the model's decision-making process and the influence of different features to inform further model refinement.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the performance and robustness of the trading agent. The evaluation strategy adapts to the evolving development of the agent, incorporating standard metrics like backtesting and performance analysis, while also addressing specific features like price improvement, execution quality, and the novel "Anxiety Model."

### A. Backtesting

A robust backtesting framework, employing rolling walk-forward validation and stress tests, forms the core of the evaluation process. This approach ensures the model's performance is assessed across diverse market conditions and time periods. The backtesting procedure will be continuously refined to reflect real-world trading dynamics. Specifically:

- **Price Improvement Simulation:** The backtesting framework will simulate price improvement for limit orders, filling buy orders at the ask price if it's lower than the limit and sell orders at the bid price if it's higher. This provides a more realistic profit and loss (P&L) assessment.
- **Stress Testing:** Stress tests will be conducted to evaluate resilience under adverse market conditions, including scenarios with high trading costs and short-selling constraints. Unsuccessful trades will be analyzed to understand their causes and refine the agent's logic.
- **Anxiety Model Integration:** As the project progresses, the backtesting framework will be updated to incorporate the influence of the Anxiety Model on the primary trading algorithm (DynamicPlane), comparing performance with and without its influence.

### B. Performance Evaluation

Standard performance metrics, including Jensen's Alpha and Sharpe Ratio, will be used to quantify the agent's risk-adjusted returns. However, the evaluation will also incorporate more specialized analyses:

- **Execution Quality:** Metrics like "Slippage" (negative price difference) and "Price Improvement" (positive price difference) will be tracked and incorporated into a refined "Dharma Adherence Score" to provide a nuanced view of execution quality. The "Price Improvement Rate" will also be analyzed to assess real-world profitability.
- **Feature Attribution:** Tools like Grad-CAM or SHAP will be employed to understand the influence of features (e.g., Price Improvement Rate, Book Resilience Score, Anxiety Level) on the model's predictions, informing model refinement and understanding the relationship between execution quality, internal state, and profitability.
- **Anxiety Model Analysis:** A post-hoc analysis will correlate the Anxiety Model's output ("Anxiety Level") with the DynamicPlane algorithm's performance and historical order book data. This analysis aims to validate the effectiveness of the Anxiety Model in predicting potential errors or the need for model adjustments.

## III. Testing and Evaluation

This section details the testing and evaluation procedures for the SCoVA (Snapshot Computer Vision Algorithm). SCoVA leverages computer vision by analyzing discrete, dynamically generated visual snapshots of market data, rather than continuous time series data. This distinction informs the testing and evaluation approach, focusing on SCoVA's effectiveness in interpreting these discrete market snapshots.

### A. Backtesting

Backtesting will be performed using a framework specifically designed for SCoVA's snapshot-based approach. This framework will incorporate:

1. **Backtesting Logic:** The core logic will simulate trades based on SCoVA’s output when presented with historical market snapshots.
2. **Framework Development:** A dedicated framework will be developed to manage the backtesting process, ensuring correct handling of the discrete input data.
3. **Rolling Walk-Forward Validation:** A rolling walk-forward validation methodology will be employed to robustly assess performance and generalization capabilities across different historical periods, mitigating the risk of overfitting.
4. **Stress Testing:** The framework will incorporate stress tests to evaluate SCoVA’s resilience under adverse market conditions, such as extreme volatility or market crashes.

### B. Performance Evaluation

Performance evaluation will utilize established financial metrics while accounting for the unique snapshot-based approach.

1. **Performance Metrics:** Standard metrics such as Jensen's Alpha and the Sharpe Ratio will be calculated to assess risk-adjusted returns.
2. **Feature Attribution Analysis:** To understand SCoVA's decision-making process, feature attribution tools like Grad-CAM or SHAP will be employed to identify the most influential features within the market snapshots. This insight will inform refinements to the snapshot generation process and potentially enhance the algorithm's performance.

## III. Testing and Evaluation

This section details the testing and evaluation procedures designed to assess the performance and robustness of the developed models, with a particular focus on the integration of risk-averse measures and asymmetric features derived from market dynamics. These enhancements aim to improve model performance by explicitly accounting for the differing characteristics of market rises and falls, and by prioritizing the mitigation of large losses.

### A. Backtesting

The existing backtesting framework will be extended to incorporate the new asymmetric features and the impact of the risk-averse loss function. This involves modifying the input data pipeline to include the output of the `AsymmetricFeatureEngine` service. The backtesting process will utilize a rolling walk-forward validation methodology and stress tests to evaluate model performance under various market conditions. The effectiveness of the risk-averse loss function in mitigating downside risk will be a key performance indicator during this phase.

### B. Performance Evaluation

Standard performance metrics such as Jensen's Alpha and the Sharpe Ratio will be calculated for both individual trading engines (Bull_Flow_Engine and Bear_Flow_Engine) and the overall system performance utilizing the regime-detection model. This comprehensive assessment will quantify the efficacy of the asymmetric approach and the risk-averse loss function. Furthermore, feature attribution tools like Grad-CAM or SHAP will be employed to gain insight into the model's decision-making process, specifically focusing on the contribution of the following:

- **Asymmetric Features:** How features like Upside and Downside Volatility, Accumulation/Distribution Ratio, Order-to-Quantity Asymmetry, and Price-Volume Correlation State influence predictions.
- **Contextual Information:** The impact of integrating Regime ID as a context token within the Vision Transformer.
- **Loss Function Impact:** The influence of the risk-averse loss function on prediction behavior, particularly during periods of market stress.

This analysis will validate the impact of the `CalculateAsymmetricFeatures` service, its integration with the Vision Transformer, and the effectiveness of the risk-averse loss function in mitigating losses.

### C. Asymmetry and Risk-Aversion Impact Analysis

A dedicated analysis will be conducted to evaluate the effectiveness of the integrated asymmetric features and the risk-averse loss function. This analysis will focus on:

- **Asymmetric Feature Impact:**

  - **Volatility Asymmetry:** Evaluating the influence of semi-deviation, volatility skewness, and volatility kurtosis on model performance during periods of market rises and falls.
  - **Volume and Participation Asymmetry:** Analyzing the contribution of the Accumulation/Distribution Ratio and Order-to-Quantity Asymmetry features to the model's ability to distinguish buying and selling conviction.
  - **Correlation Asymmetry:** Investigating how the Price-Volume Correlation State feature improves the model's ability to differentiate between fear and greed in the market.

- **Risk-Averse Loss Function Impact:**
  - **Downside Protection:** Assessing the effectiveness of the loss function in reducing the magnitude and frequency of large losses.
  - **Prediction Error Distribution:** Examining the distribution of prediction errors, comparing performance between the asymmetric and general risk-averse loss functions, and contrasting these results with a model trained using a standard loss function. Particular attention will be paid to the scenarios where `predicted_return > actual_return`, especially when actual losses significantly exceed predicted losses.
- **State-Dependent Attention Mechanism:** Evaluating the efficacy of the state-dependent attention mechanism within the Vision Transformer by comparing performance across different market regimes (e.g., high vs. low volatility).

This comprehensive analysis will provide a detailed understanding of the individual and combined contributions of the asymmetric features and the risk-averse loss function to overall model performance and inform future model refinements.

## III. Testing and Evaluation

This section details the testing and evaluation procedures used to assess the performance of the developed models and trading strategies. While standard backtesting methodologies are employed, the primary focus is evaluating the impact of architectural enhancements, specifically the integration of the Asymmetric Feature Engine, including regime detection and terrain categorization, and the dual-token context injection into the Vision Transformer (ViT). This evaluation aims to balance potential performance improvements with the complexity and maintainability of the implemented solution.

### A. Backtesting

A robust backtesting framework, incorporating trade execution, portfolio management, and transaction costs, is used to simulate trading activity with historical data. A rolling walk-forward validation approach mitigates overfitting and ensures robust evaluation. Stress tests are conducted to assess the strategy's resilience under various market conditions, including periods of high volatility and market downturns. Specific attention will be given to evaluating how the asymmetric feature vector contributes to explanatory power beyond the regime classifier during backtesting, analyzing scenarios where its contribution is significant versus minimal.

### B. Performance Evaluation

Standard performance metrics such as Jensen's Alpha and the Sharpe Ratio are calculated to assess risk-adjusted returns. Feature attribution tools (e.g., Grad-CAM, SHAP) are employed to analyze the impact of the dual-token context injection (Regime ID and Asymmetric Vector tokens) on model predictions, quantifying their contribution to overall performance and explainability. Further analysis will isolate the impact of each token type on trading decisions to determine their independent contributions to overall performance.

### C. Regime Detection and Terrain Categorization Evaluation

This subsection outlines the evaluation process for enhancements related to unsupervised clustering for regime detection and the impact of terrain categorization.

**C.1. Regime Detection Evaluation:**

- **Unsupervised Clustering Model Training:** An unsupervised clustering model (e.g., Gaussian Mixture Model (GMM), Self-Organizing Map) is trained offline using historical asymmetric feature vectors to identify 4-8 distinct market regimes. Evaluation focuses on cluster stability and interpretability using metrics like silhouette score or Davies-Bouldin index.
- **IdentifyAsymmetricRegime Service Testing:** The "IdentifyAsymmetricRegime" service is tested with unseen data to evaluate its real-time market regime classification accuracy. The effectiveness of the Regime ID as a ViT context token is evaluated by comparing model performance with and without this contextual information.

**C.2. Terrain Categorization Evaluation:**

- **Impact of Granularity Loss:** The impact of categorizing terrain-based input features into bins is investigated to determine the potential loss of granularity and its effect on the ViT's predictive capabilities. The trade-off between model simplicity and information retention is analyzed by comparing performance using categorized terrain input versus original granular environmental metrics (humidity, temperature, AQI) using metrics like precision, recall, and F1-score.
- **Explanatory Power Analysis:** The added explanatory power of the terrain-based approach is qualitatively assessed by analyzing its correlation with market behavior and insights into model predictions.

This comprehensive evaluation process ensures the effectiveness and robustness of the proposed model enhancements before integration into the final system, while considering the balance between performance gains and complexity.

## A. Backtesting

This section details the backtesting procedures implemented to evaluate the performance and robustness of the developed trading strategies under realistic market conditions. A robust backtesting framework will be developed, incorporating walk-forward validation and stress testing. This rigorous evaluation is crucial for identifying potential weaknesses and ensuring the strategy's long-term viability before live deployment.

The backtesting framework integrates seamlessly with the established data pipeline, ensuring consistency throughout the project lifecycle. Specifically, it utilizes the same return calculation methodology as the model training process: `Return(t+h) = (Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` represents the last date of the input candlestick chart and `h` is the holding period (a hyperparameter ranging from 1 to 5 days). Furthermore, the framework leverages the filename convention (e.g., `YYYY-MM-DD__YYYY-MM-DD__3.25__h=3__ABB.ST.png`) or the associated metadata file to ensure the correct holding period (`h`) is applied for each trade simulation.

To avoid look-ahead bias and ensure robust performance evaluation, the framework implements rolling walk-forward validation. This involves training the model on historical data up to a specific point and then validating it on a subsequent, out-of-sample period. This process is repeated, rolling the training and validation windows forward in time, mirroring the training process's utilization of the last 30% of the data (July 1, 2020, to December 31, 2021) as the validation set. This out-of-sample validation provides a realistic assessment of the strategy's performance on unseen data.

Finally, rigorous stress testing is performed to assess the strategies' resilience under various challenging market conditions, including periods of high volatility and significant market downturns. The stress tests consider the 5-day windowing used for both input features (candlestick chart images) and output labels (return calculations) to maintain consistency with the model training process. This comprehensive evaluation, combining walk-forward validation and stress testing, aims to provide a thorough understanding of the strategies' long-term viability and stability in real-world market conditions. This approach simulates the top and bottom 10% of predicted returns triggering buy (long) and sell (short) signals respectively, accurately reflecting the intended trading logic.

## A. Backtesting Methodology

This section details the backtesting methodology employed to rigorously evaluate the performance and robustness of the developed trading strategies. A robust backtesting framework, incorporating walk-forward validation, stress testing, and a Historical Prediction Error Profiling (HPEP) mechanism, is crucial for assessing viability and identifying potential weaknesses before live deployment.

**1. Backtesting Framework Development:** A comprehensive and flexible backtesting framework will be developed. This framework will simulate trading strategies based on model predictions, incorporating realistic market conditions such as transaction costs and slippage. It will be designed to support various trading strategies and allow for easy configuration and modification of backtesting parameters. The framework will integrate the trained CNN model, handle data ingestion and preprocessing (including generating candlestick chart images from OHLCV data), execute virtual trades based on model signals, manage portfolio positions, and calculate relevant performance metrics (e.g., portfolio value, trading costs, Sharpe ratio).

**2. Rolling Walk-Forward Validation:** To avoid overfitting and ensure robustness, rolling walk-forward validation will be implemented within the backtesting framework. This involves training the model on a past period of data, validating its performance on a subsequent, unseen period, and then iteratively rolling the training and validation windows forward through the historical data. This approach simulates the dynamic nature of market conditions and the model's ability to adapt to new information, providing a more realistic performance evaluation than a single train-test split.

**3. Stress Testing:** Rigorous stress tests will be conducted to assess the resilience of the trading strategy under various adverse market conditions. These scenarios will include periods of high volatility, market crashes, and other extreme events. The tests will cover different market sectors and varying liquidity conditions to identify potential weaknesses and vulnerabilities in the strategy. The outcomes will inform risk management decisions and potential refinements to the strategy.

**4. Historical Prediction Error Profiling (HPEP) for Trade Filtering:** A key element of the backtesting process is the integration of a Historical Prediction Error Profiling (HPEP) mechanism to filter trades based on confidence levels. This involves:

- **Post-Training Confidence Profile Generation:** After model training, a confidence profile, or HPEP map, will be created. This map will group validation set predictions into bins based on predicted return ranges (e.g., -5% to -3%, -3% to -1%, ..., +3% to +5%). For each bin, the accuracy (percentage of directionally correct predictions) will be calculated. Optionally, other metrics like average error magnitude or Sharpe ratio can be included in the profile.

- **Backtesting Trade Filtering using HPEP:** During backtesting, before executing a trade based on a model prediction, the corresponding prediction bin in the HPEP map will be consulted. Trades will only be executed if the confidence level (accuracy or other chosen metric) within the corresponding bin meets a predefined threshold. This approach aims to improve the risk-adjusted performance of the trading strategy by filtering out trades with lower confidence levels.

## Backtesting and Model Evaluation

Backtesting is crucial for evaluating the trading strategies developed in this project. This section details the procedures and considerations for robust backtesting, particularly focusing on the impact of design choices like the input window, hard vs. soft labels, and the implications of soft labeling techniques on backtesting results.

A robust backtesting framework is essential for assessing the viability of the models and identifying potential weaknesses. This framework will incorporate:

- **Backtesting Logic:** The core backtesting logic will simulate trading activities based on the model's predictions. This involves integrating the trained models, trade execution logic, and performance metrics calculations. For models trained with soft labels, this includes incorporating probabilistic predictions into the trading strategies.

- **Framework Development:** A comprehensive and flexible backtesting framework will be developed to handle various scenarios and incorporate different models, including those utilizing probabilistic outputs. The framework will facilitate detailed performance analysis and enable stress testing.

- **Rolling Walk-Forward Validation:** A rolling walk-forward validation scheme will be implemented. This involves training the model on a past period of data, testing it on a subsequent period, and rolling the training and testing windows forward to evaluate performance over the entire dataset. This approach provides a more realistic assessment of the model's ability to adapt to changing market conditions.

- **Stress Testing:** Stress tests will be conducted using various market scenarios, including extreme volatility, prolonged downturns, and sudden price jumps. This ensures the strategy's robustness under adverse conditions and will incorporate considerations for the impact of high trading costs.

**Key Design Choice Considerations:**

Two key design choices significantly impact the interpretation and reliability of backtested performance:

- **Input/Output Window:** The 5-day input window, justified by existing literature (Jiang et al., 2023), consists of 5 candlestick images. However, the 5-day output (holding period) lacks similar empirical backing. Backtesting will analyze the sensitivity of results to this holding period assumption by considering alternative durations.

- **Hard vs. Soft Labels:** The impact of the CNN's hard labels will be investigated. A soft label approach, with the CNN outputting a probability distribution, might improve robustness. This requires analyzing the implications of hard labels on validation performance and exploring methods for generating and using soft labels in both training and backtesting. Integrating a probabilistic approach into the backtesting framework could involve varying position sizes based on the predicted probability distribution.

**Impact of Soft Labeling on Backtesting:**

The use of soft labels and probabilistic predictions significantly influences backtesting results. Key aspects for evaluation include:

- **Discretized Return Space:** The impact of the chosen bin size for discretizing the return space will be assessed. Finer granularity might lead to more nuanced trading decisions but could also increase sensitivity to noise.

- **Soft Label Effectiveness:** Backtesting will reveal whether soft labels improve performance compared to hard labels. Trade frequency and profitability across different return bins will provide insights into the efficacy of this technique.

- **Softmax Output Layer:** The impact of using a softmax output layer to generate a probability distribution will be analyzed. The distribution's shape and confidence levels will be examined in relation to trading decisions and outcomes.

Careful analysis of backtesting results will focus on both overall performance metrics and the specific impact of these soft-labeling techniques to guide further model refinement and optimization.

## Backtesting Methodology

Backtesting results reveal a complex picture of the trading strategy's performance. Initial tests using the OMXS All-Share model applied to the First North All-Share (small-cap) index yielded a promising annualized alpha of +8.89% after transaction costs, outperforming the benchmark return of -27.88% by +37.57%. However, this result appears to be an outlier. A broader analysis across eight portfolios showed that while six achieved positive alpha before transaction costs, most experienced negative or only marginally positive alpha after costs. This discrepancy highlights inefficiencies stemming from high turnover (trades executed every 5 days), uniform position weighting, and the absence of filtering for low-confidence predictions.

Further investigation is needed to understand the drivers of alpha and identify potential improvements. This includes analyzing the contributions of long and short positions, comparing per-index Sharpe ratios before and after costs, and evaluating modifications to portfolio construction logic. Addressing the high turnover rate, uniform weighting, and inclusion of low-confidence trades is crucial for enhancing performance.

Real-world trading constraints also warrant consideration. The current analysis suggests potential limitations due to short-selling constraints, particularly impacting the small-cap segment, ironically the most profitable in this study. The impact of these constraints requires further quantification. Additionally, a deeper understanding of how unsuccessful trades (where outcomes contradict model predictions) were handled in the benchmark analysis is necessary, especially given the absence of a stop-loss mechanism.

The backtesting framework itself requires several key enhancements. It must be extended to incorporate "rally time" prediction, requiring the system to consider not only predicted returns but also the predicted time to reach the target return. This involves supporting the multi-head neural network architecture that predicts both return and rally time, and incorporating these predictions into trade execution logic. Specifically, the predicted rally time will determine the holding period for each trade. If the target price isn't reached within this window, the backtesting framework should simulate an exit according to a predefined logic (e.g., at the end of the predicted rally time window).

The framework should also be adaptable to different models, including survival analysis models that predict the probability distribution over time until the target price is reached. This necessitates handling probabilistic outputs and integrating them into trade execution and evaluation.

Finally, the backtesting process should employ rolling walk-forward validation and stress testing. These methodologies must account for both return and rally time predictions, evaluating the strategy's robustness under various market conditions and the impact of incorrect rally time predictions. Stress testing should include simulations of extreme market events like crashes and periods of high volatility to assess potential risks and limitations.

## Backtesting

This section details the process of rigorously backtesting the developed trading strategy using historical data. Backtesting simulates trades based on the model's predictions in a realistic market environment, allowing for performance evaluation and identification of potential weaknesses before live deployment. This section focuses specifically on the backtesting methodology; discussions of model architectures, training regimens, benchmark comparisons, and specific implementation details are addressed in other sections of this document.

The key components of the backtesting process are:

1. **Backtesting Logic Implementation:** This involves developing the core functionality to simulate trades based on historical data and the model's predictions. It includes fetching historical data, executing simulated trades according to the model's output (including entry and exit conditions, position sizing, and order management), and calculating key performance metrics such as returns, Sharpe ratio, and drawdown.

2. **Backtesting Framework Development:** A robust and flexible framework is crucial for organizing and executing the backtesting process. This framework should facilitate easy configuration of different trading strategies, parameters, and historical data periods. It should also provide clear and concise reporting of performance metrics and visualizations.

3. **Rolling Walk-Forward Validation:** This method mitigates the risk of overfitting to a specific historical period and provides a more realistic assessment of the model's performance over time. The process involves training the model on a past period and validating its performance on a subsequent, unseen period. The training and validation windows are then iteratively moved forward through time.

4. **Stress Testing:** Stress testing evaluates the strategy's resilience under various adverse market scenarios, such as crashes, periods of high volatility, and unexpected events. These tests typically involve simulating scenarios beyond the historical data used for training and validation, pushing the strategy to its limits and exposing potential vulnerabilities. Specific stress tests will include:

   - **Volatility-Aware Exit Thresholds:** Dynamic exit thresholds based on the Average True Range (ATR) will be implemented. The ATR will be calculated dynamically, and the exit tolerance scaled as a multiple of the ATR (e.g., 1.5x ATR), allowing the exit strategy to adapt to changing market volatility.
   - **Time-Based Confidence Decay Exit Logic:** Trades will be exited if the predicted reward does not materialize within a predefined timeframe, incorporating temporal awareness into the model's decision-making.
   - **Prediction Divergence Exit Logic:** When the model is retrained, significant divergence between new and previous predictions (e.g., exceeding a 3-4% threshold) will trigger an exit, ensuring the system reacts to substantial changes in the model's outlook.
   - **Portfolio Contextual Exit Logic:** Underperforming trades relative to their prediction group average over a specified period will be exited, introducing a form of internal attribution and relative performance evaluation.

5. **Error Map Generation:** During backtesting and walk-forward validation, a detailed error map will be created. This map will track predicted and actual returns for each trade, highlighting unsuccessful trades (e.g., predicted long but the return was negative). Analyzing patterns in these unsuccessful trades allows the model to learn from its mistakes. This information can then be used to filter or re-weight similar samples in future training iterations, improving overall model performance.

### A. Backtesting

This section details the implementation of a robust backtesting framework to evaluate the performance of the developed trading strategies, specifically those using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) with sequential candlestick data and delta features. The framework will incorporate walk-forward validation and stress testing to ensure the robustness and generalizability of the results.

1. **Backtesting Framework Development:** A dedicated backtesting framework will be built to simulate trading based on the models' predictions. This framework will manage data ingestion, signal generation, trade execution, and performance tracking. It will seamlessly integrate with the model's prediction mechanism, taking as input the predicted candlestick patterns generated by the CNN and ViT models.

2. **Data Integration and Feature Handling:** The backtesting framework will utilize the prepared dataset of sequential candlestick windows and their corresponding labels (return value, rally time, signal class). The framework will also incorporate the delta features, derived either through image subtraction or direct calculation from the candlestick data, to assess their impact on trading performance.

3. **Walk-Forward Validation:** To avoid overfitting and assess the models' ability to generalize to unseen data, a rolling walk-forward validation approach will be employed. This involves training the models on a past period and validating their performance on a subsequent period, progressively moving the training and validation windows forward in time. This simulates real-world trading scenarios more accurately by evaluating the models' adaptability to changing market conditions.

4. **Stress Testing:** The trading strategies will be subjected to rigorous stress tests to evaluate their resilience under various adverse market conditions. This will involve simulating extreme market events, such as crashes and periods of high volatility, to understand the potential risks and limitations of the strategies and inform risk mitigation measures. These stress tests will simulate scenarios like market crashes and periods of high volatility to assess potential impact and inform necessary adjustments.

5. **Trade Logic and Signal Interpretation:** A crucial component of the backtesting framework is the logic that translates the predicted candlestick patterns into actionable trading signals (buy, sell, or hold). This logic will mirror how a human trader might interpret these patterns, requiring careful design and potentially incorporating thresholds and other decision-making parameters. Further details on this signal interpretation logic will be documented as the framework is developed.

## A. Backtesting

Backtesting is crucial for evaluating the performance of the candlestick image prediction model in a simulated trading environment and identifying potential weaknesses before live deployment. Given the unique nature of using image-based predictions, the backtesting framework requires careful design to address specific challenges, including distinguishing visual realism from actual trading value and ensuring the model doesn't simply memorize historical patterns.

The following steps outline a robust backtesting procedure:

1. **Framework Development:** Construct a comprehensive backtesting framework incorporating a dual-module design: a chart generator and a trade evaluator. The chart generator produces candlestick images, while the trade evaluator analyzes these images, identifies potential trading patterns, and generates buy/sell signals. This framework should address potential training and evaluation fragility by carefully considering issues like ground truth alignment and pattern overfitting.

2. **Backtesting Logic:** Implement the core backtesting logic, simulating trades based on the signals generated by the trade evaluator. Critically, the process should not only simulate chart generation but also evaluate the actionability and profitability of identified patterns within those charts. This distinguishes between realistic chart generation and profitable trade identification.

3. **Rolling Walk-Forward Validation:** Implement rolling walk-forward validation to assess performance and generalizability across different market periods. This involves repeatedly training the model on a past period and testing it on a subsequent, unseen period. This mitigates overfitting to specific market conditions and ensures the model adapts to evolving market dynamics. It also contributes to ensuring the causal grounding of predictions, verifying the model's ability to generalize beyond memorized historical patterns.

4. **Stress Testing:** Subject the strategy to rigorous stress tests simulating diverse market scenarios, including extreme volatility, market crashes, and prolonged downturns. This assesses resilience to adverse events and uncovers potential weaknesses. These stress tests should be integrated into the experimental protocol to compare the image-based prediction approach against traditional scalar-based models. This comparative analysis will highlight the relative strengths and weaknesses of each method, informing future model refinement and risk management strategies. Specifically, stress testing should investigate the model's robustness against potentially compounding errors and the ambiguity of financial implications derived from generated images.

This comprehensive backtesting procedure will provide a robust evaluation of the trading agent's performance, informing necessary adjustments and refinements before deployment. The results will also serve as crucial evidence for validating the proposed approach against baseline models, including assessing performance based on backtested profit and addressing the requirement for extracting open/high/low/close values and derived returns from the generated candlestick images.
Backtesting is crucial for evaluating the performance and robustness of the developed trading strategies. This section details the implementation of a comprehensive backtesting framework and the procedures used to assess the strategies under various market conditions.

The backtesting process involves the following key steps:

1. **Backtesting Logic Implementation:** The core logic simulates trades based on the model's predictions. This includes defining trading rules, order execution, portfolio management, and the incorporation of factors like transaction costs, slippage, and market impact. Detailed documentation of this logic is essential for transparency and reproducibility.

2. **Backtesting Framework Development:** A robust and flexible backtesting framework will be developed to facilitate the simulation and analysis of trading strategies over historical data. This framework will handle data management, signal generation, trade execution, portfolio tracking, performance metric calculation, and reporting. Its modular design will accommodate various trading strategies and data inputs.

3. **Rolling Walk-Forward Validation:** To ensure the model's robustness and avoid overfitting, a rolling walk-forward validation approach will be employed. The model is trained on a past period of data and tested on a subsequent period, with the training and testing windows rolled forward through the historical data. This method helps evaluate the model's adaptability to changing market dynamics and provides a more realistic assessment of its out-of-sample performance.

4. **Stress Testing:** Rigorous stress tests will be conducted to assess the resilience of the strategies under adverse market conditions. These tests will simulate extreme market events, such as market crashes, periods of high volatility, and other relevant scenarios. The goal is to identify potential vulnerabilities and refine the strategies to mitigate risks and improve their performance under challenging circumstances.

### A. Backtesting (Continuity)

This section details the backtesting procedure for the trading strategy. Backtesting simulates the strategy's performance over historical market data, providing crucial insights into its potential effectiveness and identifying any weaknesses before live deployment. The backtesting framework should be robust, flexible, and capable of handling various market conditions and parameter configurations.

1. **Core Backtesting Logic:** Implement the core logic to simulate trades based on the model's signals, accurately calculating portfolio performance (including transaction costs and slippage) over the historical dataset.

2. **Backtesting Framework:** Develop a modular and extensible backtesting framework. This framework should accommodate different performance metrics, trading strategies, and data sources. It should also facilitate visualizing results and analyzing the impact of various parameters.

3. **Rolling Walk-Forward Validation:** Employ rolling walk-forward validation to rigorously assess out-of-sample performance and mitigate overfitting. This involves training the model on a past period and validating it on a subsequent period, iteratively rolling these windows forward through the data.

4. **Stress Testing:** Conduct thorough stress tests to evaluate the strategy's resilience under adverse market conditions, such as market crashes, periods of high volatility, and extreme price movements. This helps identify potential vulnerabilities and enhance the strategy's robustness. While visualization of the stress test results using planar coordinates and orientation data is desirable, it's important to use an efficient storage mechanism to avoid excessive memory consumption. Ensure this data can be easily retrieved for further analysis and reconstruction of the strategy's behavior under stress. Note that while advanced visualizations like animations are desirable for understanding complex market dynamics, current performance limitations necessitate a focus on static charts generated using `matplotlib`. Each chart should be a standalone figure for clarity, avoiding the use of subplots.

## Backtesting Methodology

This section details the implementation of a backtesting framework designed to rigorously evaluate the performance and robustness of the trading strategy, especially considering the dynamic nature of the input data generated by the dynamic projection system. This dynamic projection, with its re-centering and rotation of the feature space, necessitates specific considerations within the backtesting procedure to ensure accurate and reliable results.

Before implementing the full backtesting logic, the following aspects of the dynamic projection's interaction with the trading model must be addressed:

1. **Trading Model Integration:** The backtesting framework must accurately reflect how the dynamic 2D projection influences trading signals. Specifically, the logic needs to incorporate how each new candlestick, or group of candlesticks, affects the re-centering and rotation of the feature space _before_ generating the subsequent trading signal. This ensures the backtested results accurately represent real-world model behavior.

2. **Rotation Mechanism Definition:** The specific mathematical transformation used for rotation within the dynamic projection (e.g., affine transformation, PCA rotation, learned rotation) must be clearly defined and consistently applied during backtesting. This clarity is crucial for reproducibility and understanding the rotation's impact on performance.

3. **Alignment with Projection Goals:** The backtesting process should align with the dynamic projection's primary goal. For example, if the objective is prediction invariance to previous trend direction or focusing on relative local movement, the backtesting framework must reflect this (e.g., by using performance metrics sensitive to relative price changes).

4. **Dynamic PCA Integration (If Applicable):** If PCA rotation is used, it must be correctly integrated into the backtesting pipeline. This involves dynamically rotating and re-centering the candlestick image sequences using PCA at each backtest step, seamlessly integrating this dynamic adjustment into the trading simulation. The backtesting logic should simulate the model's operation in a locally optimized frame of reference, prioritizing relative (local) price behavior over absolute positions on candlestick charts. This local optimization should be reflected in the backtesting calculations and performance evaluation.

The backtesting framework itself comprises the following core components:

1. **Backtesting Logic:** This core component simulates the trading strategy over historical market data, executing trades based on the model's predictions (derived from the dynamically projected data) and tracking portfolio performance over time. This includes defining trading rules, calculating portfolio values, and handling transaction costs.

2. **Framework Development:** A robust and flexible backtesting framework is required. This framework should encompass data handling (including seamless integration with the dynamic snapshot generator), trade execution simulation, performance metric calculation (e.g., Sharpe Ratio, Jensen's Alpha), and visualization tools. Critically, it must account for the dynamic rotation of candlestick data within each snapshot.

3. **Walk-Forward Validation:** Rolling walk-forward validation is essential to evaluate out-of-sample performance and mitigate overfitting. This involves training the model on a past period and validating it on a subsequent period, repeatedly shifting these windows forward. This is particularly crucial given the dynamic input data and potential market regime shifts.

4. **Stress Testing:** Thorough stress testing under various market conditions (including crashes and high volatility) is necessary to assess the strategy's resilience and identify potential weaknesses. This helps ensure the strategy's robustness before live deployment.

### Backtesting

This section details the backtesting procedure used to evaluate the performance and robustness of trading strategies developed using the dynamic plane concept. Accurate backtesting is crucial for assessing the viability of these strategies before real-world deployment. Given the complexity of the dynamic plane generation, it's essential that the backtesting framework faithfully replicates this process.

**Data Transformation and Backtesting:**

The dynamic plane implementation significantly impacts how price movements are represented and fed into the model. The backtesting framework must precisely mirror this transformation:

- **Dynamic Origin Refocus:** The framework must account for the dynamically shifting origin of the 2D plane. With each price and/or time change, the origin repositions. This dynamic reference point must be accurately reflected in the backtesting calculations.
- **Dynamic Plane Redraw:** The continuous redrawing of the 2D plane with every system change requires careful consideration. The backtesting process should recalculate and update the plane and axes orientations for each price or time change, ensuring consistency between model training and backtesting environments.
- **Dimensionality Reduction:** The reduction from three axes (time, price, volume) to two rotational axes on the dynamic 2D plane impacts how price movement is represented. The backtesting framework must incorporate this dimensionality reduction, recognizing that the two rotational axes capture the parabolic curve of price movement originally represented in 3D space.
- **Rotation Artifact Handling:** Potential distortions introduced during image rotation (mitigated by anti-aliasing techniques in the dynamic snapshot generation) must be evaluated to ensure they don't negatively impact backtesting results.
- **Volatility Jump Handling:** The backtesting process must evaluate performance during periods of high volatility. This ensures the smoothing or limiting mechanisms within the dynamic snapshot generator effectively prevent unrealistic trading behavior.
- **Consistent Axis Scaling:** Consistent axis scaling in the dynamic snapshots must be maintained throughout the backtesting period. Inconsistencies could lead to misleading performance metrics.

**Backtesting Framework and Procedure:**

A robust backtesting framework will be implemented, simulating trading activities based on model-generated signals. This framework incorporates the dynamic plane generator logic, processing candlestick data (time, price, and optionally volume) to create the dynamic 2D plane representations. These representations, formatted as images or numerical features, are then fed into the trained CNN or ViT model. The model's output generates trading signals. This process mirrors the dynamic plane generator's implementation, encompassing window selection, movement calculation, dynamic frame construction, rotation, refocusing, and snapshot rendering. The pseudocode for the dynamic plane generator will be integrated into the backtesting framework documentation for clarity and traceability. A conceptual diagram illustrating the evolution of the dynamic 2D plane will further aid in understanding system behavior during backtesting.

**Validation and Stress Testing:**

- **Rolling Walk-Forward Validation:** To avoid overfitting and ensure generalizability, a rolling walk-forward validation approach will be used. This involves training the model on a past period and testing it on a subsequent period, repeating this process by "rolling" the training and testing windows forward in time.
- **Stress Testing:** The trading strategy will undergo various stress tests simulating extreme market volatility, sustained uptrends and downtrends, and various liquidity scenarios. This rigorous testing identifies potential weaknesses and enhances robustness. The use of PCA-based local frame definition at each backtesting timestep further enhances the realism of the simulation by reflecting the dynamic recalculation inherent in the model.
  Backtesting

This section details the process of rigorously evaluating the trading strategy's performance using historical data. This involves simulating the trading strategy within a dedicated backtesting framework, validating its performance using a rolling walk-forward approach, and assessing its robustness through stress testing.

Backtesting Implementation

The core backtesting logic simulates trading decisions based on the model's predictions applied to historical market data. This involves:

1. **Data Acquisition:** Fetching the necessary historical market data required for the backtest.
2. **Signal Generation:** Applying the trained model to the historical data to generate trading signals.
3. **Trade Execution:** Simulating the execution of trades based on the generated signals, taking into account factors such as trading fees and slippage.
4. **Performance Calculation:** Calculating relevant performance metrics, such as Sharpe Ratio, Jensen's Alpha, and maximum drawdown, to evaluate the strategy's historical performance.

Backtesting Framework

A robust and flexible backtesting framework is crucial for this process. The framework should:

1. **Handle Data:** Efficiently manage and process large historical datasets.
2. **Execute Trades:** Simulate trade execution with realistic parameters, including transaction costs and slippage.
3. **Calculate Metrics:** Calculate a comprehensive range of performance metrics.
4. **Visualize Results:** Provide tools for visualizing and analyzing backtest results, including equity curves, drawdown profiles, and trade statistics.
5. **Adapt to Different Scenarios:** Accommodate various market scenarios and trading strategies.

Rolling Walk-Forward Validation

To mitigate overfitting and ensure the strategy's adaptability to changing market conditions, a rolling walk-forward validation methodology is employed. This process involves:

1. **Training Period:** Training the model on a past period of historical data.
2. **Testing Period:** Evaluating the trained model's performance on a subsequent out-of-sample period.
3. **Rolling Forward:** Incrementally shifting the training and testing windows forward through the historical data, repeating the training and testing process.
4. **Performance Aggregation:** Aggregating the performance metrics across all rolling periods to assess the model's consistency and stability.

Stress Testing

Stress testing is crucial for identifying potential vulnerabilities and assessing the robustness of the trading strategy under adverse market conditions. This involves:

1. **Scenario Design:** Carefully crafting realistic stress test scenarios, including market crashes, periods of high volatility, and sustained downtrends.
2. **Simulation:** Running the backtesting framework under each stress test scenario.
3. **Vulnerability Analysis:** Evaluating the strategy's performance under duress and identifying potential weaknesses.
4. **Refinement:** Using the insights gained from stress testing to refine and improve the trading strategy.

## Backtesting

This section details the procedures for backtesting the model under simulated market conditions to assess its robustness and performance. This comprehensive evaluation includes simulating diverse market regimes, visualizing their behavior on the dynamic plane, and employing robust backtesting methodologies like walk-forward validation and stress testing.

### Market Regime Simulation and Visualization

To understand the model's behavior across different market conditions, three distinct regimes will be simulated and visualized:

1. **Trend-Reversal-Recovery:** This regime simulates a period of trending price movement followed by a reversal and subsequent recovery.

2. **Choppy Sideways Market:** This regime simulates a period of high volatility and frequent, rapid price fluctuations characteristic of a sideways market. A function, `generate_choppy_candlesticks(n=30)`, will generate the simulated data.

3. **Strong Linear Uptrend:** This regime simulates a sustained, directional upward price movement.

For each regime, both standard Heiken-Ashi charts and their rotated dynamic plane counterparts will be generated. These visualizations will be presented in a consolidated panel for direct comparison, highlighting the differences in representation between traditional charting and the dynamic plane transformation. The implementation will leverage existing functions like `generate_heiken_ashi`, `plot_heiken_ashi_candlestick`, `dynamic_rotate_recenter_heiken`, and `plot_rotated_heiken`, and address previous visualization challenges like matplotlib's figure stacking. The output for the choppy market regime will be saved to `/mnt/data/standard_heiken_ashi_choppy.png` and `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`.

The behavior of each market regime within the dynamic plane will be analyzed and summarized, exploring the implications for model learning and performance. These findings will be documented for inclusion in dissertation drafts.

### Backtesting Framework and Methodology

A robust backtesting framework will be developed to evaluate the model's performance within the context of the dynamic plane transformations. This framework will incorporate the following:

1. **Dynamic PCA-based Input Transformation Integration:** The framework will handle the nuances of the dynamic PCA-based input transformations, enabling accurate trade simulation based on the model's geometric pattern recognition capabilities.

2. **Rolling Walk-Forward Validation:** To avoid look-ahead bias and ensure real-world applicability, a rolling walk-forward validation approach will be employed. This involves training the model on historical data, validating its performance on a subsequent period, and iteratively shifting these windows forward in time.

3. **Stress Testing:** The framework will incorporate stress testing to assess the model's resilience under various adverse market conditions, including crashes, high volatility periods, and unexpected news events. This will help identify potential vulnerabilities and refine the trading strategy.

## A. Backtesting (Continuity)

This section details the implementation of a robust backtesting framework to rigorously evaluate the performance and adaptability of the developed trading agent, particularly in light of the dynamic plane implementation and its correction mechanisms. The framework incorporates continuous feedback and improvement, aligning with the "Continuity" principle.

The following key components and methodologies ensure a comprehensive assessment of the agent's capabilities under diverse market conditions:

**1. Backtesting Framework Development:** A comprehensive backtesting framework will be constructed to simulate trading activity based on the agent's generated signals. This framework integrates the agent's predictions with historical market data, handles trade execution logic, position management, incorporates transaction costs, and calculates key performance indicators such as returns, Sharpe ratio, and maximum drawdown. Critically, it also integrates the error signal mechanisms described below to enhance the dynamic plane algorithm.

**2. Rolling Walk-Forward Validation:** To avoid overfitting and ensure the model's robustness across varying market periods, rolling walk-forward validation will be employed. This involves training the model on a past period of data and testing it on a subsequent period, iteratively rolling the training and testing windows forward through the dataset. This approach provides a realistic evaluation of the agent's ability to generalize to unseen data and adapt to evolving market dynamics.

**3. Stress Testing:** Rigorous stress tests will be conducted to evaluate the strategy's resilience under adverse market conditions. These tests will simulate various scenarios, including extreme events like market crashes, periods of high volatility, and sudden price shocks. This analysis will reveal potential weaknesses and inform further model refinement.

**4. Integrating Dynamic Plane Correction Mechanisms:** The backtesting framework explicitly incorporates the following error analysis mechanisms derived from the dynamic rotating plane algorithm:

    * **Frame Confidence Correction:** Discrepancies between predicted and actual market movements will trigger adjustments to the rotational frame assumptions, improving the model's learning and adaptation capabilities.

    * **Prediction Error Memory:**  A rolling memory of recent prediction errors will be maintained.  Consistent misalignments between predicted and actual market structure will adjust the weighting applied to the dynamic plane rotation, enhancing its stability.

    * **Feedback-Driven Frame Smoothing:** The rotation rate of the dynamic plane will be dynamically adjusted based on the magnitude of prediction errors. Higher error rates will trigger slower rotation, increasing smoothing and promoting more conservative predictions during volatile periods.

    * **Dual-Frame Estimation:** The framework employs two parallel frames – an "optimistic" fast-reacting frame and a "stable" slow-smoothed frame. Predictions from these frames are dynamically weighted based on observed market consistency, balancing rapid adaptation with stability.

**5. Simulating the Peripersonal vs. Extrapersonal Gap:** A method will be developed to simulate the "peripersonal vs. extrapersonal" gap within the backtesting environment. This biologically inspired concept explores the agent's response to predictions within and beyond its immediate "reach" (prediction horizon) to understand how prediction horizons influence trading decisions.

By incorporating these elements, the backtesting process provides a thorough evaluation of the trading agent's performance, adaptability, and robustness, ultimately contributing to the development of a reliable and resilient trading strategy.

### A. Backtesting (Continuity)

This section details the backtesting procedure, a critical step in evaluating the performance and robustness of the trading strategy. This framework integrates the dynamic plane generator and its rolling frame correction, simulating real-world trading activity and analyzing its behavior under various market conditions. Specifically, the backtesting process allows for visualization of the "error spikes → correction → healing decay" cycle and quantifies "frame intervention" frequency, offering crucial insights for parameter optimization and system stability. While the underlying algorithms for frame correction and healing are detailed elsewhere, their practical effectiveness is rigorously assessed here.

The backtesting process encompasses the following key components:

1. **Framework Development:** A comprehensive backtesting framework will be constructed. This framework will simulate trade execution, portfolio management, and performance tracking, providing a realistic simulation environment based on historical market data and model predictions.

2. **Rolling Walk-Forward Validation:** To mitigate overfitting and ensure robust performance across varying market conditions, a rolling walk-forward validation approach will be employed. This involves training the model on a historical period and validating it on a subsequent out-of-sample period, iteratively shifting the training and testing windows forward through time.

3. **Stress Testing:** Rigorous stress testing will be conducted to evaluate the strategy's resilience under adverse market conditions, including extreme volatility, market crashes, and prolonged periods of stagnation. This process will include simulations incorporating sophisticated calculations, such as deviation vector monitoring and angular error tracking within the dynamic 2D plane. This detailed analysis of prediction errors, considering both magnitude and direction, enhances the reliability of the error trend detector. Furthermore, the error trend detector will be refined to account for the interplay of price, time, and volume, enabling a more nuanced understanding of error dynamics.
   Backtesting and Performance Evaluation within the Dynamic Plane

This section details the backtesting procedure, emphasizing its integration with the dynamic plane and error correction mechanisms. It outlines the framework development, incorporating a rolling walk-forward validation approach and stress testing to assess resilience under various market conditions. Crucially, this section focuses on the specific considerations arising from the dynamic plane implementation.

The backtesting framework must handle the dynamic plane transformations and error calculations:

- **Dynamic Snapshot Generation:** Each backtesting step generates a dynamic snapshot based on current market conditions, applying the rotation and translation logic defined in the Dynamic Plane Implementation section. This ensures the model is evaluated on data consistent with its training.

- **Distance and Angle Error Calculation:** The framework calculates the Distance Error (magnitude difference) and Angle Error (orientation difference) between the predicted and realized displacement vectors in the dynamic 2D plane at each trading step. This provides ongoing assessment of the model's magnitude and directional prediction accuracy.

- **Exclusion of Initial Frame Rotation:** The initial frame creation rotation from the initial PCA transformation is _not_ included in these error calculations. The focus is on the _local_ vector misalignment within the already transformed dynamic plane.

- **Diagram of Rotations and Prediction Error:** A diagram illustrating the relationship between the global frame transformation (PCA), local vector misalignment, and the resulting Distance and Angle Errors will be included. This diagram will clearly depict the two layers of rotation and how error calculations relate to the local vector.

Addressing the dynamic nature of the PCA plane requires specific considerations within the backtesting framework:

- **Consistent Short-Term Frame (Freeze & Compare):** Backtesting maintains a consistent frame of reference for short-term predictions (1-5 candlesticks). Market drift within this timeframe is assumed negligible. The dynamic frame is "frozen" at the time of prediction and the realized movement vector is projected back onto this frozen frame for comparison. This simplifies the process and avoids recalculating PCA for every price tick. Pseudocode for this "Freeze & Compare" methodology will be provided:

  - _[Insert Pseudocode for Freeze & Compare Here]_

- **Visual Simulation of Frame Alignment:** A visual simulation will illustrate the "Freeze & Compare" mechanism and ensure correct alignment between predicted and realized movements. This will clarify how the frozen frame is used for comparison.

- **PCA Basis Data Structure:** A lightweight data structure will store the PCA basis (rotation matrix) for each window, enabling efficient reprojection of realized movement vectors.

- **Visualizations of Freeze Frame and Projection:** Clear visual examples will be provided to clarify the distance/angular error calculations within the dynamic PCA plane, distinguishing them from calculations based on a static plane.

This "Freeze & Compare" approach within the dynamic PCA frame, coupled with robust walk-forward validation and stress testing, provides a comprehensive evaluation of the trading strategy's performance and the efficacy of the error correction mechanisms. The visualizations and pseudocode will enhance understanding and facilitate verification.

## Backtesting Methodology and Error Analysis

This section details the implementation of a robust backtesting framework designed to evaluate trading strategies under dynamic market conditions, specifically addressing the challenges posed by shifting PCA frames of reference. The framework incorporates two primary methods for maintaining consistency between predicted and realized market movements: "Freeze Frame" and "Reproject Realization".

**Freeze Frame:** This method mitigates the impact of PCA frame shifts by "freezing" the rotation matrix 'R' at the time of prediction. Both predicted and realized data points are then projected using this same matrix, ensuring a consistent frame of reference for comparison.

**Reproject Realization:** This method applies the original rotation matrix 'R' from the prediction time to the realized movement vector, projecting the realized movement back into the original prediction frame.

The effectiveness of these methods will be demonstrated through numerical examples, visualizations, and comparisons of prediction and reality paths in both frozen and shifted frames. Robust pseudocode for a "Freeze and Correct" module will encapsulate the core logic of this process.

**Dynamic PCA Frame Management within the Backtesting Framework:**

The backtesting framework must accommodate the dynamic nature of PCA frames. This includes implementing logic to manage updates to the PCA frames during backtesting and selecting the most effective approach ("Freeze Frame" or "Reproject Realization") to maintain relational consistency between predicted and realized data points. This prevents inaccuracies caused by minor market fluctuations.

**Error Calculation Methodology:**

Addressing the dynamic nature of PCA frames requires adjustments to error calculations. Instead of calculating error between vectors on a static plane, the framework calculates deviation errors between PCA1 and PCA2 for both real and predicted values. The total error incorporates both vector deviation error within the dynamic local frame and the frame shift error (change between PCA axes): `Total Error = Vector Deviation Error + Frame Shift Error`. Pseudocode will detail this calculation.

Frame shift, or frame drift, is measured by calculating the difference between two sets of basis vectors (frames) using principal angles between two subspaces. In a 2D context, this involves computing the angle between PCA1*t and PCA1*{t+1}, and between PCA2*t and PCA2*{t+1}. The frame error is calculated as: `Frame Error = α * Angle between PCA1 vectors + β * Angle between PCA2 vectors`, where α and β are tunable weights, allowing for adjustment of the importance of each PCA vector's drift.

**Further Backtesting Considerations and Refinements:**

The following algorithmic considerations, related to error analysis and interpretation within the dynamic framework, will inform the backtesting process and subsequent model refinement:

- **Simulation of Vector Deviation and PCA Frame Drift:** A small-scale simulation will visualize and quantify vector deviation and PCA frame drift, aiding in understanding the error components used in backtesting.

- **Frame Drift as a Confidence Indicator:** High frame drift error may indicate lower confidence in predictions, potentially informing hold decisions. This will be explored during backtesting.

- **Error Component Weighting:** The relationship and weighting of vector deviation error and frame shift error (including the sufficiency of the existing α and β parameters) will be investigated.

- **Normalization of Distance and Angular Errors:** The impact of directly adding distance and angular errors, despite differing units and scales, will be evaluated. Normalization or alternative adjustments will be considered.

- **Weighted Error Calculation:** A weighted error calculation combining vector deviation, angular error, PCA1 angle error, and PCA2 angle error will be implemented and assessed. Separate, tunable weights (α1, α2, β1, β2, γ1, and γ2) for each component will be used. The specific formula requires further definition and will be included in subsequent documentation.

### Backtesting

This section details the backtesting procedures used to evaluate the trading strategy's performance in a simulated trading environment. A robust backtesting framework, incorporating walk-forward validation, stress testing, and a dynamic error handling mechanism, will be developed.

**Backtesting Framework:** This framework will simulate trades based on the model's predictions, manage the portfolio, calculate transaction costs, and implement the error detection and "healing" process described below. Critically, the framework will incorporate a mechanism for handling order execution, portfolio management, and transaction costs.

**Error Handling (Wound and Healing):** The framework will incorporate a "wound and healing" process to manage periods of increased prediction error. This involves:

1. **Error Calculation and Weights:** Total prediction error will be calculated using a weighted sum of different error components, including vector error and frame shift error:

   - Vector Error = α1⋅θ<sub>PCA1</sub> + α2⋅θ<sub>PCA2</sub> + ... + α<sub>n</sub>⋅θ<sub>PCAn</sub>
   - Frame Shift Error = β1⋅θ<sub>PCA1</sub> + β2⋅θ<sub>PCA2</sub>
   - Total Error = γ1⋅Vector Error + γ2⋅Frame Shift Error

   Numerical examples and pseudocode will be created to document this process, address the normalization of angle units, and propose initial default values for the weights (α, β, γ). Tuning these weights will be critical during backtesting.

2. **Rolling Error Buffer and Statistics:** A rolling buffer will store the total prediction error over a recent period (e.g., 5-10 trading windows). The rolling mean and standard deviation of these errors will be continuously calculated.

3. **Wound Detection and Healing Phase:** A "Wound Phase" will be triggered when the mean error exceeds a threshold (e.g., k=2 times the rolling standard deviation). During this phase, a correction factor will be applied to adjust predictions. As the mean error drops below a lower threshold (e.g., 1-1.5 times the rolling standard deviation), a "Healing Phase" begins. During the Healing Phase, the correction factor decays (e.g., exponentially with λ=0.95), allowing the model to gradually return to normal operation. This decay rate will be subject to further tuning.

4. **Dynamic Correction Re-entry:** If errors spike again during the Healing Phase, exceeding the predefined threshold, the system will dynamically re-enter the "Wound Phase," restarting the correction process.

5. **Visualization:** The behavior of this error handling system (frequency and duration of phases, impact of decay, effectiveness of re-entry) will be visualized to inform parameter tuning.

**Rolling Walk-Forward Validation:** A rolling walk-forward approach will be used to evaluate performance over time and mitigate overfitting. The model will be trained on a past period and tested on a subsequent period, repeating this process over multiple rolling time windows.

**Stress Testing:** Stress tests simulating high volatility, market crashes, and other extreme events will assess the strategy's robustness and the effectiveness of the error correction mechanism under duress.

A dedicated module will be developed to encapsulate the logic for error identification, correction factor application, decay mechanisms, and dynamic re-entry. Initial tuning values for thresholds and decay rates will be determined through analysis of common market regimes. This modular design will facilitate implementation and maintainability.

## Backtesting and Data Preprocessing

This section details the backtesting procedures and data preprocessing steps necessary for evaluating the performance and robustness of the trading agent. The backtesting process focuses on a "Healing-by-Correctness" system, which dynamically adjusts the agent's behavior based on recent predictive accuracy. Additionally, this section outlines the necessary data normalization steps prior to applying Principal Component Analysis (PCA).

### Backtesting Procedure

The backtesting process incorporates standard procedures like trade simulation, performance metric calculation, and transaction cost handling. Critically, it integrates the Healing-by-Correctness system to adapt the agent's behavior based on its predictive accuracy. The following steps outline the implementation and evaluation of this mechanism:

1. **Backtesting Framework Development:** A robust and flexible backtesting framework will be developed. This framework will facilitate the integration of different trading strategies, performance evaluation metrics, and—crucially—the dynamic adjustment of the agent's behavior as dictated by the Healing-by-Correctness system.

2. **Backtesting Logic Implementation:** The core backtesting logic will simulate trades based on the agent’s signals, calculate portfolio performance metrics, and handle transaction costs. This logic forms the foundation for the Healing-by-Correctness system.

3. **Rolling Walk-Forward Validation:** A rolling walk-forward validation approach will be employed to evaluate performance across diverse market conditions and mitigate overfitting risks. The Healing-by-Correctness system will remain active during validation to assess its adaptability to changing market dynamics.

4. **Stress Testing:** Rigorous stress tests will be conducted to evaluate the resilience of both the trading strategy and the Healing-by-Correctness system under extreme market conditions, including scenarios of significantly deteriorated predictive accuracy. This will help identify potential vulnerabilities and inform system refinements.

**Healing-by-Correctness System Implementation:**

The Healing-by-Correctness system will be implemented and evaluated within the backtesting framework according to these specifications:

- **Healing Logic Update:** The healing logic will dynamically adjust a "correction factor" applied to the agent's actions. This adjustment is proportional to the agent's rolling mean prediction correctness over a recent period (N steps). Higher correctness leads to a reduced correction factor (faster healing), while lower correctness maintains or increases it.

- **Dynamic Decay Rate Function:** A function, `dynamic_decay_rate(mean_correctness)`, will calculate the correction factor's decay rate based on the mean prediction correctness. This governs the agent's healing rate as accuracy improves. A suggested implementation is: `Decay Rate = 1 - (mean_correctness - healing_threshold)`, ensuring faster decay with high correctness and slower decay when just above the threshold.

- **Formal Pseudocode:** Formal, modular pseudocode will fully document the Healing-by-Correctness system, including prediction correctness tracking and the dynamic decay rate function. This will ensure clarity and facilitate understanding of the system's logic and integration.

- **Toy Example Simulation:** A simplified toy example will demonstrate the system's complete flow: a simulated performance "wound," correction factor application, and the eventual "healing" process as predictive accuracy improves.

- **Initial Healing Thresholds:** Initial healing thresholds, defining the accuracy levels triggering the healing process, will be proposed based on reasonable trading expectations (e.g., 75-80% directional correctness) and subject to empirical tuning during backtesting.

### Data Preprocessing for PCA

Prior to applying PCA to Price (P), Time (T), and Volume (V) data during backtesting, careful data preprocessing is essential. Each feature within a rolling window of _N_ data points will be independently centered and scaled using z-score normalization. This ensures features with varying units and scales do not disproportionately influence PCA results.

1. **Calculate Mean and Standard Deviation:** Calculate the mean (μ) and standard deviation (σ) for each feature (P, T, V) within the rolling window:

   ```
   μ_feature = (1/N) * Σ feature_i
   σ_feature = sqrt((1/N) * Σ (feature_i - μ_feature)^2)
   ```

2. **Scale the Data:** Apply z-score normalization to scale the data:

   ```
   X_scaled[,i,.,.] = [(t_i - μ_t) / σ_t, (p_i - μ_p) / σ_p, (v_i - μ_v) / σ_v]
   ```

**Specific Considerations for Time and Volume:** _(This section was incomplete in the original and requires further information to be completed appropriately)_

## Time and Volume Preprocessing for PCA

Two key data aspects, time and volume, require specific transformations before applying Principal Component Analysis (PCA):

**Time Handling:** Two approaches are available for representing time:

1. **Relative Time Index:** For analyses using fixed-length windows of _N_ trading candles, a simple relative index (1, 2, ..., _N_) can be used. This sequence should then be z-score normalized. This approach is suitable when the focus is on the relative position within the window rather than absolute time.

2. **Absolute Clock Time:** To incorporate diurnal patterns or irregular time intervals, calculate normalized time deltas:

   ```
   Δt_i = (timestamp_i - μ_t) / σ_t
   ```

   where μ_t and σ_t are the mean and standard deviation of timestamps within the relevant period. Note that large timestamp gaps can inflate σ_t, potentially diminishing the influence of time within the PCA.

**Volume Transformation:** Given the typically heavy-tailed distribution of trading volume, apply one of the following transformations _before_ z-score normalization:

1. **Log Transformation:** `v_i' = log(1 + v_i)` compresses outliers and stabilizes variance.

2. **Robust Scaling:** Subtract the median volume from each data point and divide by the interquartile range (IQR). This method is less sensitive to extreme outliers than z-score normalization alone.

**PCA Implementation:**

After preprocessing time and volume, and applying any other necessary feature transformations, PCA is performed on the scaled data matrix (X_scaled) using Singular Value Decomposition (SVD):

```python
u, s, vh = np.linalg.svd(X_scaled, full_matrices=False)
axes = vh[:2]   # Extract the first two principal components
```

Typically, the first two principal components are used for subsequent analysis, but this can be adjusted as needed.

## Backtesting Data Preparation

Robust backtesting requires careful data preparation to ensure reliable results. The following transformations are crucial for effective backtesting using PCA:

1. **Fractional Elapsed Time:** Represent time as a fraction of the total elapsed time within each backtesting window. Calculate this by subtracting the minimum timestamp from each timestamp and dividing by the difference between the maximum and minimum timestamps. This handles irregular time intervals and potential data gaps effectively.

2. **Window-Relative Returns:** Transform price data into percentage or log returns relative to the first price in each window. This anchors the price series to zero at the beginning of each window, enabling comparisons across different assets and time periods while reducing the impact of absolute price magnitudes on the PCA.

3. **Robust Volume Scaling:** Apply a log transformation to volume data to mitigate the impact of outliers. Then, normalize the log-transformed volume using median and interquartile range (IQR) scaling. This combined approach handles extreme volume spikes more effectively than standard deviation-based scaling.

4. **Data Normalization for Strategy Evaluation:** Normalize price, volume, and time data to the range of -1 to +1 before applying the trading strategy. This ensures features contribute equally to the model and prevents any single feature from dominating due to scale differences.

   - **Time:** Normalize fractional elapsed time (`time_frac`) to [-1, 1] using: `(2 * time_frac) - 1`.

   - **Price:** Use log returns from the opening price, then either divide by the maximum absolute value within the window or apply min-max scaling to map the values to the [-1, 1] range.

   - **Volume:** Apply the log transformation and robust scaling (using median and IQR) as described in the previous step.

5. **PCA Input Matrix Construction:** Construct a 3-dimensional matrix with columns representing fractional elapsed time, window-relative returns (or log returns), and robustly scaled volume. This matrix serves as the input for PCA.

6. **PCA Application:** Perform PCA on the constructed matrix. The resulting principal components capture the underlying relationships between these three key variables, providing a robust representation for backtesting.

While a live feed of Last Traded Prices (LTPs) might be considered, its limitations for backtesting must be acknowledged. LTP feeds lack historical context, making accurate reconstruction of past market conditions difficult and potentially leading to unreliable backtesting results.

### A. Backtesting

This section details the backtesting procedure, a critical step in evaluating the performance and robustness of the trading strategy. The process leverages a rolling walk-forward validation approach, incorporates rigorous stress testing, and directly compares performance using both pre-transformation and post-transformation data to assess the impact of the dynamic plane transformation.

1. **Data Preparation:** Before commencing backtesting, the following data transformations are essential:

   - **Log Transform and Percentile Clipping for Volume:** Apply a natural logarithm transformation (`np.log1p(volume)`) to the volume data to normalize its distribution and reduce the impact of extreme outliers. Clip any remaining extreme values at the 5th and 95th percentiles. Finally, min-max scale the clipped volume data to the [-1, +1] range.

   - **Percentile Clipping for Price:** Clip extreme outliers in the log return of price at the 5th and 95th percentiles to remove extreme price fluctuations. After clipping, apply min-max scaling to the [-1, +1] range.

   - **Min-Max Scaling for All Features:** Apply min-max scaling to all features (transformed volume, log return of price, and time fraction) to the [-1, +1] range using the minimum and maximum values _across the window_ to maintain context within each time period. This ensures consistent input to the model.

   - **Principal Component Analysis (PCA):** Combine the normalized time fraction, log return of price, and transformed volume data into a single matrix. Perform PCA on this matrix to reduce dimensionality and potentially improve model performance. Optionally center the matrix before applying PCA.

   - **Visual Verification Examples:** Create five distinct examples of price and volume action, each including the original and transformed candlestick images (scaled to [-1, 1]) from a single-day, 10-minute interval chart. These examples facilitate visual inspection and validation of the data preprocessing steps.

2. **Backtesting Framework Development:** A robust backtesting framework will be built to simulate trading activity based on the model's predictions. This framework will incorporate two distinct backtesting procedures:

   - **Pre-Transformation Backtesting:** Using pre-transformation candlestick images (analogous to previous CNN input), generate single-day, 10-minute interval charts for each backtesting period. These charts will be fed into the older CNN model for prediction and trading simulation, enabling direct comparison with earlier results.

   - **Post-Transformation Backtesting:** Using the transformed and normalized images (dynamic plane snapshots), generate sequential five-candlestick windows for each period in the backtesting process. These transformed images will be used as input to the current CNN model.

3. **Rolling Walk-Forward Validation:** A rolling walk-forward validation methodology will be employed to evaluate the model's performance on unseen data and mitigate overfitting. This involves training the model on a past period and validating its performance on a subsequent, out-of-sample period, repeating this process by "rolling" the training and validation windows forward through time.

4. **Stress Testing:** Stress testing will evaluate the strategy's resilience under various market conditions, including high volatility, significant downturns, and sudden price movements ("volatility jumps"). This analysis will identify potential weaknesses and assess the impact of these conditions on the dynamic plane transformations and trading decisions. Stress testing scenarios will also consider the model's performance across various market patterns such as uptrends, downtrends, reversals, sideways movement, and breakouts.

This comprehensive backtesting process ensures a thorough evaluation of the model's performance and provides insights into its practical applicability in real-world trading scenarios. The visualization examples and dual backtesting approach (pre- and post-transformation) aid in understanding the impact of the dynamic plane transformation on model inputs and predictions.

### A. Backtesting Methodology

This section details the implementation of a robust backtesting framework to evaluate the performance of the developed trading strategies. The framework incorporates the dynamic nature of the model's input images and integrates the self-correction mechanism.

The backtesting process follows these key steps:

1. **Dynamic Input Image Generation:** For each step in the backtesting period, a dynamic input image is generated based on current market conditions using the following procedure:

   a. **Dynamic Plane Construction:** Principal Component Analysis (PCA) is applied to normalized Time, Price, and Volume data within a rolling window. The two principal components (PC1 and PC2) define the dynamic 2D plane.

   b. **Rotation and Refocusing:** Historical market data within the rolling window is projected onto the 2D plane. The coordinate system is re-centered, placing the most recent data point at the origin (0,0).

   c. **Image Rendering:** The transformed 2D data representation is rendered as an image, potentially using Candlestick or Heiken-Ashi charts, for input to the predictive model.

2. **Prediction and Evaluation:**

   a. **Model Prediction:** The model predicts a 2D movement vector (Δx', Δy') within the dynamic plane and a 'Rally Time' representing the predicted movement duration.

   b. **Performance Evaluation:** The backtesting framework evaluates these predictions against actual market movements over the predicted 'Rally Time', calculating relevant performance metrics.

3. **Self-Correction and "Healing" Mechanism:**

   a. **Error Calculation:** The Total Error signal is calculated based on Vector Deviation Error (distance and angular error) and Frame Shift Error.

   b. **Wound Detection and Healing Phase:** The framework implements the "Wound Detection" and "Healing Phase" logic. Model predictive accuracy is monitored, and the dynamism of the plane generation is adjusted using a dynamic threshold (e.g., a multiple of the rolling standard deviation). When performance falls below the threshold ("Wound Detection"), the system enters the "Healing Phase," gradually restoring full dynamism as accuracy improves.

4. **Walk-Forward Validation and Stress Testing:**

   a. **Rolling Walk-Forward Validation:** The entire process is repeated for each step in the backtesting period using a rolling walk-forward approach to ensure robust performance evaluation and avoid overfitting.

   b. **Stress Testing:** The strategy is subjected to various stress tests using historical and simulated market scenarios, including periods of high volatility and market crashes, to assess its robustness and resilience.

## Backtesting Methodology

This section details the implementation of a robust backtesting framework designed to rigorously evaluate the trading agent's performance under realistic market conditions. The framework incorporates multi-scale data (intraday, daily, weekly, and monthly) processed using the Dynamic Rotating Plane method, and employs a rolling walk-forward validation scheme to mitigate overfitting.

The backtesting process encompasses the following key components:

1. **Framework Development:** A comprehensive backtesting framework will be developed to manage data loading, alignment, signal generation based on model predictions, simulated trade execution, and performance metric calculation. The framework will specifically accommodate the complexities of multi-scale data integration and the dynamic nature of the Dynamic Rotating Plane method. This ensures the backtesting environment accurately reflects how the model utilizes this approach in real-time trading scenarios.

2. **Data Integration and Processing:** The framework will handle the integration and processing of data from various sources and granularities (intraday, daily, weekly, and monthly). This includes loading, aligning, and preparing the data for use within the backtesting environment, considering potential processing overhead associated with handling diverse datasets.

3. **Rolling Walk-Forward Validation:** A rolling walk-forward validation approach will be employed to evaluate model performance over time and mitigate overfitting to specific market periods. Each validation period will utilize multi-scale data available up to that point in time, ensuring a realistic assessment of the agent's performance in dynamic market conditions and preventing look-ahead bias.

4. **Stress Testing:** Rigorous stress testing will be conducted to assess the trading strategy's resilience under various adverse market conditions, including periods of high volatility, low liquidity, and varying trends across different timeframes. These tests will evaluate the impact of the multi-scale data integration and the Dynamic Rotating Plane method on the agent's performance under duress.

5. **Performance Evaluation and Attribution:** Model performance will be evaluated using appropriate metrics and compared against a baseline intraday model to quantify the value added by incorporating multi-scale data and the dynamic plane. Attribution analysis will provide insights into the contribution of each timeframe to the agent's overall performance. Furthermore, analyzing performance during major market events will offer a deeper understanding of how different market conditions influence the agent's behavior.
   Backtesting is crucial for evaluating the trading strategy's performance and robustness before live deployment. This process involves simulating trades based on historical market data and the model's predictions. The following steps will be undertaken:

6. **Implement Backtesting Logic:** This involves coding the buy/sell signals generated by the model and applying them to historical data. Calculations will include portfolio values, trade executions, and associated costs like slippage and commissions, where applicable.

7. **Develop a Robust Backtesting Framework:** This framework will manage the entire backtesting process, including data loading, signal generation using the trained model, trade execution, and performance evaluation. Key features include configurable parameters (e.g., initial capital, trading fees), strategy flexibility, and comprehensive performance reporting. The framework will also allow specifying the asset universe (e.g., NIFTY 50, NIFTY 500, custom watchlist) and the backtesting date range. This date range will be further subdivided into training and validation sets for model training.

8. **Employ Rolling Walk-Forward Validation:** This technique mitigates overfitting and enhances robustness by dividing the historical data into multiple periods. The model is trained on one period and validated on the subsequent period, with this process repeated by rolling the window forward through the data. This simulates real-world trading where the model must adapt to new market conditions.

9. **Conduct Stress Testing:** This involves subjecting the backtested strategy to various extreme market scenarios (e.g., market crashes, periods of high volatility) to evaluate its resilience and identify potential weaknesses. This analysis informs risk assessment and potential strategy improvements.

## Backtesting

This section details the backtesting process for evaluating the trading strategy developed within the SCoVA project. A robust backtesting framework, incorporating rolling walk-forward validation and stress testing, will assess the strategy's performance under various market conditions and parameter settings.

**Backtesting Framework:**

A dedicated backtesting framework will be developed to simulate trading activity based on the model's predictions. This framework will incorporate realistic market conditions, including transaction costs and slippage. The framework will be designed to support:

- **Configurable Lookahead Period:** The framework will accommodate the configurable lookahead period used during model training (e.g., 1, 3, 5, 10 candlesticks). This ensures consistency between training and evaluation and allows for assessing performance across different prediction horizons.

- **Rolling Walk-Forward Validation:** To mitigate overfitting and evaluate performance on unseen data, rolling walk-forward validation will be employed. The model will be trained on a past period and tested on a subsequent period, with the training and testing windows rolled forward through the dataset.

- **Stress Testing:** Rigorous stress tests will be conducted, simulating diverse market scenarios, including market crashes, periods of high volatility, and other extreme events. This will assess the strategy's resilience and identify potential vulnerabilities. Stress testing will also incorporate variations in hyperparameters (see _Hyperparameter Permutation Testing_) and consider context-aware periodicity (see _Context-Aware Periodicity_).

**Enhanced Monitoring and Metrics:**

During backtesting, a comprehensive monitoring display will provide real-time performance metrics, presented in a clear and accessible grid format. These metrics will include:

- Sharpe Ratio
- Sortino Ratio
- Maximum Drawdown
- Win Rate
- Profit Factor
- Average Trade Duration

A distribution plot of trade returns will visualize the spread and frequency of profits and losses, offering a complete picture of the strategy's behavior in the simulated market environment.

**Additional Backtesting Analyses:**

Beyond the core backtesting procedures, two additional analyses will be performed:

- **Context-Aware Periodicity:** Backtesting will incorporate weighted predictions based on daily, weekly, monthly, quarterly, and yearly periodicities. This involves optimizing the configuration for various stock categories (market caps, sectors, and share price bins) and determining the most effective combination of frames and candles per frame within each context. This aligns with the broader concept of _Context Awareness_, which aims to optimize the number of candles and frames used in predictions based on specific market conditions.

- **PCA Analysis on Dynamic Plane:** The entire backtesting process, including walk-forward validation and stress testing, will be repeated using a 2-PCA-axes dynamic plane representation of the market data. This will provide insights into the model's performance when using a dimensionality-reduced representation of market state.

- **Hyperparameter Permutation Testing:** To establish baseline performance and explore the impact of hyperparameter variations, comprehensive permutation testing will be conducted during stress testing. This will inform the selection of optimal hyperparameter settings for the trading strategy.

## A. Backtesting

This section details the implementation and execution of a robust backtesting framework for the SCoVA project. Backtesting is crucial for evaluating the trading agent's performance in a simulated historical market environment before live deployment. This process leverages realistic data, incorporates ethical considerations (the "Dharmic Mandate"), and employs rigorous validation techniques.

**1. Data Acquisition and Integration (Zerodha KiteConnect API):**

The backtesting framework integrates seamlessly with the Zerodha KiteConnect API. This API is crucial for obtaining realistic historical market data, including order execution details and portfolio tracking, which are essential for accurate performance evaluation. While live data streaming via WebSockets is not directly used in backtesting, the integration with KiteConnect ensures consistency in data acquisition and management between backtesting and live trading.

**2. Backtesting Framework Development:**

The framework is designed for flexibility and adaptability to various models and trading strategies. Key functionalities include:

- **Data Ingestion and Preprocessing:** Historical data retrieved via KiteConnect is rigorously cleaned and validated (Shaucha - Purity) to ensure accuracy and avoid biases in the backtesting process.
- **Simulation Engine:** This core component accurately simulates market conditions, order execution, and portfolio updates, providing a realistic testing environment.
- **Trade Logic Implementation:** The specific trading strategy derived from the model is implemented here, translating predicted signals into simulated trades.
- **Performance Evaluation & Reporting:** Comprehensive reports are generated, detailing performance metrics that reflect both profitability and adherence to the Dharmic Mandate. These reports facilitate in-depth analysis and auditing of the agent's behavior (Satya - Truthfulness).

**3. Dharmic Mandate Implementation:**

Beyond profitability, the backtesting process adheres to the following ethical principles:

- **Satya (Truthfulness):** Immutable logs and transparent reporting mechanisms ensure the accuracy and verifiability of backtesting results.
- **Shaucha (Purity):** Rigorous data validation and cleaning processes ensure the integrity of the backtesting environment.
- **Santosha (Contentment):** Performance evaluation considers long-term sustainability and stability, not just short-term gains, promoting a balanced approach to trading.

**4. Walk-Forward Validation and Stress Testing:**

To ensure model robustness and avoid overfitting, a rolling walk-forward validation approach is employed. The model is trained on a past period, validated on a subsequent period, and the process is repeated, rolling the windows forward in time. This provides a realistic assessment of performance across varying market conditions.

Furthermore, the strategy undergoes rigorous stress testing, simulating adverse market scenarios like crashes, high volatility, and extreme price movements. This identifies potential weaknesses and informs improvements to the trading strategy, contributing to its resilience in real-world conditions.

### A. Backtesting (Continuity)

This section details the implementation of a robust backtesting framework crucial for evaluating the performance of the trading agent under realistic market conditions. This framework, inspired by the "Kurukshetra (The Field of Action)" concept, emphasizes rigorous testing and simulated execution of strategies.

The backtesting process incorporates the following key components:

1. **Backtesting Logic:** The core backtesting logic simulates trades based on the model's predictions using historical market data. This includes precise tracking of entry and exit points, holding periods, and profit/loss calculations for each trade. Detailed logging captures all relevant trade parameters, such as predicted and actual returns, transaction costs, and any other relevant metrics.

2. **Backtesting Framework Development:** A flexible and robust framework is implemented to manage the backtesting process. This framework supports various models, parameters, and trading strategies, ensuring adaptability and maintainability. It facilitates data ingestion, trade simulation, performance calculation, and reporting. The framework is designed for reproducibility and consistency in the backtesting procedures.

3. **Rolling Walk-Forward Validation:** To assess the model's robustness and mitigate overfitting, a rolling walk-forward validation scheme is employed. This involves training the model on a past period and validating its performance on a subsequent period, iteratively rolling these windows forward across the historical dataset. This method provides a more realistic evaluation of the strategy's performance over time.

4. **Stress Testing:** Rigorous stress testing is conducted to evaluate the agent's resilience under diverse market conditions. This includes simulating extreme scenarios such as market crashes, periods of high volatility, and black swan events, using both historical and synthetic data. These tests help identify potential vulnerabilities and inform the development of robust risk management strategies. The "Campaign Runner," enabling large-scale automated backtesting based on predefined experiment templates, further enhances the comprehensiveness and efficiency of the stress testing process. The use of saved experiment templates also ensures consistency and reproducibility.

### A. Backtesting (Continuity)

This section details the implementation of backtesting procedures to rigorously evaluate the performance of the developed trading strategies. The framework will incorporate rolling walk-forward validation and stress testing to simulate real-world market conditions and assess long-term robustness.

1. **Implement Backtesting Logic:** This involves developing the core logic to execute simulated trades based on the model's predictions, incorporating transaction costs, slippage, and other market frictions. A detailed log of all simulated trades and associated costs will be maintained for subsequent analysis.

2. **Develop a Comprehensive Backtesting Framework:** This framework should not only execute trades but also calculate performance metrics (e.g., Sharpe ratio, maximum drawdown), provide trade statistics (e.g., win rate, average profit/loss), and offer visualization tools. A modular design, separating data handling, trade execution, and performance evaluation, will promote maintainability and reusability.

3. **Implement Rolling Walk-Forward Validation:** This technique involves training the model on a past period, validating it on a subsequent period, and rolling these windows forward. This simulates real-world trading conditions and helps assess the model's adaptability. The size of the rolling windows should be carefully chosen and justified. Detailed performance records for each window will be maintained.

4. **Conduct Thorough Stress Testing:** This involves simulating extreme market events (e.g., crashes, volatility spikes), varying trading costs, and incorporating slippage to identify potential weaknesses and assess robustness. The specific stress test scenarios, parameters varied, and corresponding outcomes will be documented.

### A. Backtesting (Continuity)

This section details the implementation and validation of the trading strategy using a hybrid approach leveraging both client-side (iPad) and server-side resources.

Backtesting will be conducted using a combination of interactive sessions on the iPad and larger-scale analyses on the server. Interactive backtesting on the iPad, facilitated by the DynamicPlaneGenerator, will focus on shorter periods and recent data, allowing for rapid evaluation and model fine-tuning. Resulting weight updates (deltas) from these fine-tuning sessions will be synchronized with the server. More computationally intensive operations, such as full training runs and multi-permutation campaigns, will be executed on the server via Google Cloud Run jobs leveraging GPU acceleration (e.g., NVIDIA T4). The iPad app will act as a control interface for these server-side jobs, allowing users to initiate, monitor, and retrieve results.

The revised app interface (whether PWA or native iOS) will provide an "Execution Target" selection offering the following options:

- **Cloud GPU (Full Training):** Initiates a full training and backtesting run on the server.
- **On-Device (Fine-Tuning Only):** Performs fine-tuning and backtesting on recent data locally on the iPad.
- **On-Device (Interactive Backtest):** Runs an interactive backtest session on the iPad for a shorter period.

This hybrid approach allows for efficient and flexible backtesting, capitalizing on the strengths of both client-side and server-side processing. The following core backtesting procedures will be implemented:

1. **Backtesting Logic:** The core backtesting logic, encompassing trade execution, portfolio management, and performance tracking, will be implemented on both the client and server to support the different execution targets.

2. **Rolling Walk-Forward Validation:** A rolling walk-forward validation approach will be employed on both the client and server to rigorously assess the model's performance across various time periods and market conditions. This iterative process trains the model on past data and validates it on subsequent periods, rolling the training and validation windows forward to simulate real-world trading scenarios.

3. **Stress Testing:** Stress tests will be conducted on the server, leveraging its computational power to simulate extreme market conditions, such as market crashes and periods of high volatility. These tests will evaluate the strategy's resilience and identify potential vulnerabilities, informing risk management strategies.
   The backtesting process is crucial for evaluating the performance and robustness of the developed trading strategy before deployment. It involves simulating trades based on historical market data and the model's predictions. This process helps identify potential weaknesses and refine the strategy under various market conditions.

The following steps outline the comprehensive backtesting procedure:

1. **Implement Backtesting Logic:** This involves developing the core logic to simulate trades using historical market data and the model's generated signals. This includes defining precise trading rules, calculating portfolio values over time, and tracking key performance metrics such as returns, Sharpe Ratio, maximum drawdown, and other relevant indicators.

2. **Develop a Robust Backtesting Framework:** A flexible and comprehensive backtesting framework is essential. This framework should facilitate:

   - **Data Handling:** Efficient loading and management of historical market data (OHLCV - Open, High, Low, Close, Volume).
   - **Signal Generation:** Seamless integration with the trained model to generate trading signals based on the historical data.
   - **Trade Execution Simulation:** Accurate simulation of trade execution, incorporating factors like slippage (the difference between expected price and execution price) and commission costs.
   - **Portfolio Management:** Tracking portfolio holdings, cash balance, and the evolution of performance metrics over time.
   - **Reporting and Visualization:** Generating detailed performance reports, including visualizations of key metrics like cumulative returns, drawdown, and profit/loss.

3. **Employ Rolling Walk-Forward Validation:** To avoid overfitting and ensure the model generalizes well to unseen data, rolling walk-forward validation is employed. This method trains the model on a past period of data and tests it on a subsequent period, rolling the training and validation windows forward through the historical dataset. This provides a more realistic assessment of the model's performance in a dynamic market environment.

4. **Conduct Stress Testing:** Stress tests are crucial for evaluating the resilience of the trading strategy under adverse market conditions. This involves simulating extreme scenarios, such as market crashes, periods of high volatility, and unexpected news events. These tests help identify potential vulnerabilities and allow for refinements to mitigate risks and improve the strategy's robustness.

After thorough backtesting and model refinement, any experimental setup should be removed before deploying the model to a live trading environment. During deployment and subsequent operation, resource-intensive processes, such as image processing (if applicable), should be optimized for performance, potentially limiting their execution to daily predictions or triggered by specific events like a significant increase in the model's error rate.
This section details the implementation of a robust backtesting framework to evaluate the performance of the developed trading strategy. The following key steps will be undertaken:

1. **Implement Core Backtesting Logic:** This involves developing the core logic to simulate trades based on the model's signals and historical market data. This includes defining the order execution logic (e.g., market orders, limit orders), calculating potential slippage and transaction costs, and managing portfolio positions over time.

2. **Develop a Flexible Backtesting Framework:** A modular and extensible backtesting framework will be constructed to facilitate experimentation and analysis. This framework will handle data ingestion, signal generation from the trained model, trade execution simulation, and performance metric calculation. Its design will prioritize flexibility to accommodate different models, trading strategies, and asset universes.

3. **Employ Rolling Walk-Forward Validation:** To robustly evaluate out-of-sample performance and mitigate overfitting, a rolling walk-forward validation methodology will be employed. This involves training the model on a past period, validating its performance on a subsequent period, and iteratively rolling these periods forward through the historical dataset. This approach provides a more realistic assessment of the model's performance in live trading scenarios.

4. **Conduct Comprehensive Stress Testing:** The developed trading strategy will undergo rigorous stress testing to assess its resilience under various adverse market conditions. Simulations will include historical market crashes, periods of high volatility, and extreme price movements. This analysis will identify potential vulnerabilities and inform the development of robust risk management measures.

### A. Backtesting (Continuity)

This section details the implementation and execution of a backtesting strategy to evaluate the performance of the developed trading agent. A robust and reliable backtesting process is crucial for assessing the agent's effectiveness before live deployment.

1. **Implement Backtesting Logic:** This involves coding the backtesting mechanism, including trade execution logic based on the agent's signals. The implementation should accurately reflect real-world conditions, incorporating factors like transaction costs, slippage, and market liquidity. It should process the agent's buy/sell signals and calculate the resulting portfolio performance over time.

2. **Build a Backtesting Framework:** A dedicated framework is necessary for efficient and structured backtesting. This framework should manage data loading, signal generation, trade execution, and performance evaluation. Modularity and flexibility are key considerations to accommodate various trading strategies and scenarios, enabling easy integration of new features and data sources.

3. **Employ Rolling Walk-Forward Validation:** This method involves splitting the historical data into multiple periods. The model is trained on an initial period and then tested on a subsequent, out-of-sample period. This process is repeated, rolling the training and testing periods forward. Walk-forward validation provides a more realistic performance evaluation and helps avoid overfitting to specific market conditions.

4. **Conduct Stress Testing:** Stress testing assesses the strategy's robustness by exposing it to various extreme market scenarios, such as market crashes, periods of high volatility, and sudden price shocks. This analysis reveals potential vulnerabilities and weaknesses in the agent's logic and helps evaluate the overall risk profile of the trading strategy. Identifying these weaknesses is critical for risk management and optimizing performance under adverse conditions.

## A. Backtesting (Continuity)

This section details the backtesting process crucial for evaluating the trading strategy's performance and identifying potential weaknesses. A multi-stage approach, incorporating elements from the cost-effective testing principles outlined elsewhere, ensures efficient resource utilization while thoroughly validating the strategy.

1. **Mock Data Backtesting:** Initial backtesting utilizes mock data, including edge cases like price spikes, flat periods, and gaps, to rapidly iterate and identify logic errors within the backtesting framework itself, independent of model performance. This mirrors the "Mock Data Creation" principle emphasizing robust testing with diverse scenarios.

2. **Dry Run Backtesting:** Before full backtesting, a "dry run" with real market data, but without actual trade execution, verifies data handling, calculations, and signal generation in a real-world context without the computational costs of detailed trade simulation. This adapts the "Dry Run Toggle" and "End-to-end dry run" concepts to backtesting.

3. **Simplified Model Backtesting (Smoke Test):** Similar to using a dummy model for smoke tests, initial backtesting can employ a simplified trading model or limited historical data. This quickly validates the complete backtesting pipeline (data acquisition, preprocessing, signal generation, trade execution, and performance evaluation) before a full backtest. This aligns with the "Dummy Model for Smoke Tests" principle, applying reduced complexity for initial validation. Consider whether a pre-trained simplified model can be used directly, clarifying the purpose of this stage.

4. **Full Backtesting & Walk-Forward Validation:** Once the framework and basic logic are validated, full backtesting with the complete model and extensive historical data is performed. This process incorporates rolling walk-forward validation, training the model on a past period and testing on a subsequent period, rolling the windows forward to assess out-of-sample performance and robustness to changing market conditions.

5. **Stress Testing:** Rigorous stress testing simulates various market conditions, including extreme events like market crashes and periods of high volatility, to assess strategy resilience. This aligns with cost-effectiveness goals, allowing simulated adverse events without the expense of real-world market exposure during testing. Leveraging mock data generation for unit/integration testing further enables cost-effective and repeatable stress tests.
   Backtesting is crucial for evaluating the trading strategy's performance and robustness, especially concerning its response to "shocker events"—unexpected market occurrences with significant impact. While other sections detail the technical implementation of the backtesting framework, rolling walk-forward validation, and stress testing, this section focuses on integrating "shocker events" and the Cognitive Threat Analysis Module (CTAM) into the backtesting process.

Effective backtesting provides insights into potential weaknesses and areas for improvement before live deployment. It establishes a performance baseline against which real-time event detection can be compared and evaluated. This is particularly relevant given the emphasis on predicting "shocker events" through computer vision analysis of equity charts, futures, options, and derivatives data, alongside the continued importance of historical data analysis. Backtesting enables us to assess the model's ability to identify these events in historical data, quantify its performance under various market conditions, and refine its sensitivity to potential disruptions. These insights will directly inform the development of the real-time event detection system.

To rigorously evaluate the system's handling of "shocker events," the backtesting framework must incorporate the following:

1. **Shocker Event Simulation:** The backtesting framework will simulate "shocker events" based on the characteristics defined in the "Define \"Shocker\" Events" section. This ensures the trading strategy is tested against diverse market scenarios, including periods of high volatility, rapid price changes, and volume anomalies. The implementation will be flexible, allowing configuration of various shock types and frequencies.

2. **CTAM Integration:** The CTAM plays a crucial role in backtesting. As historical data is processed, the CTAM will analyze the data for "shocker event" patterns and generate a "Threat Level" score. The backtesting framework will incorporate this score into the trading logic, recording the CTAM's output for each period and demonstrating how the trading strategy responds to different "Threat Level" scores. This analysis will evaluate the CTAM's effectiveness in mitigating the impact of "shocker events," specifically focusing on how the DynamicPlaneGenerator adjusts its behavior (potentially through smoothing function modifications).

## Backtesting

This section details the backtesting procedures necessary to evaluate the performance of the trading strategies. Given the evolution of the strategy, the backtesting approach has also evolved. The following subsections outline the different backtesting procedures corresponding to different stages of strategy development, culminating in the final approach used for evaluating the complete trading agent.

### Initial Backtesting Approach (CTAM Focus)

This initial backtesting focused on the Convolutional Threat Assessment Module (CTAM) and its ability to identify "shocker events."

1. **Backtesting Framework:** A basic backtesting framework was implemented to simulate trading based on CTAM-generated alerts. This included incorporating historical market data, such as candlestick data, futures data, and options heatmaps.

2. **CTAM Performance Validation:** The backtesting process focused on validating the effectiveness of the lightweight CNN models within the CTAM. Historical data was analyzed to determine whether the CTAM correctly identified and classified "shocker events." This analysis included assessing the accuracy and computational efficiency of the CNNs and informing adjustments to the CTAM's sensitivity.

3. **Strategy Response to CTAM Alerts:** Backtesting also evaluated the trading strategy's response (e.g., position adjustments or learning rate changes) to CTAM alerts. This aimed to assess the effectiveness of incorporating CTAM insights into the overall trading logic.

### Second Backtesting Approach (Error Correction Model)

This phase of backtesting evaluated a trading strategy based on an error-correction model, highlighting its limitations.

1. **Backtesting Framework Implementation:** A backtesting framework was implemented to simulate trading strategies using historical data, including candlestick data, futures data, and options heatmaps. This framework simulated trades based on signals generated by the error-correction model and calculated performance metrics.

2. **Walk-Forward Validation:** A rolling walk-forward validation approach was used to evaluate the model's performance over time and mitigate overfitting.

3. **Stress Testing and Model Retraining:** Stress tests were conducted to evaluate the strategy's robustness under various market conditions, including high volatility and unusual events. The model retraining strategy was also evaluated during stress testing.

**Limitations of the Error-Correction Model:** This backtesting revealed limitations inherent in the error-correction model, including a mean reversion bias and oversimplification of market dynamics. This led to the development of a dual-system architecture.

### Third Backtesting Approach (Dual-System: Flow and Threat Engines)

This backtesting focused on the interaction between the Flow Engine and the Shockwave Prediction Model (SPM) within the dual-system architecture.

1. **Integrated Backtesting Framework:** A backtesting framework was developed to simulate trading activity and evaluate the combined performance of the Flow Engine and the SPM. The framework incorporated the dynamic "seesaw" weighting mechanism between the two systems, allowing for testing under various market conditions and Systemic Threat Levels (STLs).

2. **Walk-Forward Validation and STL Adaptation:** Rolling walk-forward validation assessed the models' adaptability to changing market dynamics and their out-of-sample performance, especially concerning the STL-based adaptive strategy weighting.

3. **Stress Testing and Opportunistic Threat Response:** Stress tests evaluated the system's robustness under extreme market conditions similar to "shocker events." This analysis focused on the dynamic weighting mechanism's behavior and its impact on portfolio performance, verifying the effectiveness of the SPM's "opportunistic threat response."

4. **Profit Maximization Focus:** Backtesting prioritized profit maximization assessment, analyzing the return generated by the combined predictions and the influence of the dynamic weighting mechanism under various STL scenarios.

### Final Backtesting Approach (Complete Trading Agent)

This section details the final backtesting procedures used to evaluate the complete trading agent.

1. **Backtesting Logic Implementation:** The core functionality to simulate trades based on the agent's generated signals and historical market data was implemented.

2. **Comprehensive Backtesting Framework:** A robust backtesting framework was developed to manage data ingestion, signal generation, trade execution, and portfolio tracking. This framework incorporated all components of the final trading agent, including the CTAM, Flow Engine, SPM, and dynamic weighting mechanism. This ensured a realistic simulation of the agent's behavior in a historical market environment.

### A. Backtesting (Continuity)

This section details the process of rigorously backtesting the trading strategy using historical market data to evaluate its performance, robustness, and ensure its readiness for live trading. This process adheres to the architectural principles outlined in the system documentation and emphasizes a continuous and reliable evaluation approach.

Before executing backtests, a **pre-flight validation service** verifies data integrity, environment stability, and template schema validity. This ensures the accuracy and reliability of the backtesting process from the outset. After each backtest run, a **post-flight analytics service** processes the results, generates high-level insights, and updates the system's knowledge base for continuous learning and strategy improvement.

The backtesting process itself is structured around the following key steps:

1. **Implement Backtesting Logic:** This involves developing the core logic to simulate trades based on the model's signals, calculate portfolio value over time, and track key performance metrics such as Jensen's Alpha and the Sharpe Ratio. This implementation carefully considers real-world factors like trading costs, slippage, and commission fees to ensure realistic performance evaluation.

2. **Develop a Robust Backtesting Framework:** A flexible and modular backtesting framework is essential for supporting various backtesting methodologies, including walk-forward analysis and stress testing. This framework facilitates detailed reporting, analysis of results, and easy adaptation to changes in the trading strategy or data sources. It adheres to the principles of atomic function decomposition and aligns with the four pillars of the system architecture (Continuity, Enforcement, Facilitation, and Specialization).

3. **Implement Walk-Forward Validation:** To avoid overfitting and ensure the strategy generalizes well to unseen market conditions, rolling walk-forward validation is employed. The model is trained on a past period of data and tested on a subsequent period. This process is repeated, rolling the training and testing windows forward through the dataset, providing a more realistic assessment of the strategy's performance in dynamic market conditions.

4. **Conduct Stress Tests:** Stress testing evaluates the strategy's resilience under various market scenarios, including extreme events like market crashes and periods of high volatility. By simulating the strategy's performance using historical data that incorporates such events, potential vulnerabilities are identified, allowing for refinements and risk mitigation strategies to be implemented.

## Backtesting (Continuity)

This section details the implementation and execution of the backtesting procedures to evaluate the performance of the developed trading agent. A robust and reliable backtesting framework is crucial for assessing the viability of the trading strategy before deployment. This framework adheres to architectural principles that promote modularity, maintainability, and efficiency.

**Architectural Guidelines:**

- **Isolation of Specialists:** Components like signal generation, trade execution, and performance evaluation are isolated through clearly defined interfaces. This modular design prevents unintended side effects from modifications and simplifies debugging.
- **Asynchronous Communication:** A publish/subscribe (pub/sub) model enables asynchronous communication between components. This addresses potential latency issues and allows independent operation, improving overall efficiency.
- **Enforced Resource Access via Enforcers:** Dedicated Enforcer components control access to resources like market data and trading accounts, enhancing security, data consistency, and auditability.
- **Orchestrated Workflows via Facilitators:** Facilitator components manage the overall backtesting workflow, coordinating the different specialist components and ensuring smooth and efficient execution.
- **Inheritance for Code Structure:** Class inheritance structures services within the framework, promoting a clear separation of concerns, strong typing, and improved long-term maintainability.

**Backtesting Procedure:**

The backtesting process is implemented through the following steps:

1. **Backtesting Logic Implementation:** This involves implementing the core backtesting logic, simulating trades based on the agent's signals. This includes calculating portfolio value over time, handling transaction costs (including slippage modeling, if applicable, and commission structures), and specifying the trade execution method (e.g., market orders, limit orders). The specific implementation will be thoroughly documented.

2. **Backtesting Framework Development:** This entails building the framework within which the backtesting logic operates. The chosen framework (whether a dedicated library or a custom solution) should be justified, considering its ability to handle different data frequencies, asset classes, and performance metrics. Any custom implementations will be fully documented.

3. **Rolling Walk-Forward Validation:** To ensure robustness and avoid overfitting, a rolling walk-forward validation approach is employed. The historical data is divided into periods, training the model on one and validating it on the subsequent one. This process is repeated, rolling the windows forward. The specific parameters of the walk-forward periods (e.g., training and validation window lengths) will be defined and justified.

4. **Stress Testing:** To evaluate resilience under adverse market conditions, the strategy undergoes stress testing. This involves simulating performance during historical market turmoil (e.g., crashes, high volatility periods) or using simulated market events. The specific stress test scenarios and results will be documented, enabling identification of vulnerabilities and subsequent strategy refinement.

### A. Backtesting (Continuity)

This section details the backtesting procedure used to evaluate the trading agent's performance under realistic market conditions. The process, implemented as a `ContinuityService`, simulates the agent's trading strategy against historical data to assess its effectiveness and identify potential weaknesses before live deployment. This adheres to the architectural principle of using four base service classes (ContinuityService, EnforcementService, FacilitationService, and SpecialistService).

The following steps outline the backtesting procedure:

1. **Implement Backtesting Logic:** This involves developing the core logic within the `ContinuityService` to execute simulated trades based on the agent's signals within a historical market environment. This includes defining trading rules, order execution logic, and portfolio management strategies. The implementation should accurately reflect real-world trading, considering factors such as transaction costs, slippage, and market impact.

2. **Develop Backtesting Framework:** This encompasses designing and implementing a robust and flexible backtesting framework. This framework should handle historical market data, execute simulated trades, generate performance metrics, and facilitate various trading strategies and evaluation metrics. The framework should also address potential performance bottlenecks, particularly considering network latency and overhead within the microservice architecture. This might involve optimizing data transfer, using asynchronous communication patterns (e.g., Google Cloud Pub/Sub for non-critical updates), and integrating performance testing throughout the development process.

3. **Implement Rolling Walk-Forward Validation:** This technique divides the historical data into rolling training and testing periods, simulating real-world deployment and evaluating the agent's adaptability to changing market dynamics. This approach mitigates overfitting and provides a more robust performance evaluation. The implementation should be optimized to minimize network overhead between involved microservices.

4. **Conduct Stress Tests:** Exposing the backtesting framework to various extreme market scenarios, including market crashes, periods of high volatility, and unexpected events, is crucial. This assesses the trading agent's robustness and resilience under adverse conditions, identifying vulnerabilities and the potential for significant losses. These tests should cover both historical events and hypothetical worst-case scenarios, focusing on potential network bottlenecks identified during performance testing. Mitigation strategies like service discovery and load balancing should be explored and implemented.

Detailed technical specifications and pseudocode for the `ContinuityService` will be provided to ensure clear implementation and integration within the larger system. This documentation will also address specific considerations for performance and scalability within the microservice architecture.

## A. Backtesting (Continuity)

This section details the implementation of a robust backtesting framework to evaluate the performance and reliability of trading strategies before deployment. This framework simulates real-world market conditions and trading scenarios using historical data. While this section defines core backtesting elements, advanced diagnostics and performance monitoring are addressed elsewhere.

The backtesting process involves the following key steps:

1. **Backtesting Engine Development:** A high-fidelity backtesting engine will be developed to simulate trading strategies and evaluate performance. This engine will incorporate real-world market complexities to provide realistic performance estimations. Leveraging professional backtesting libraries like Backtrader or Zipline will be explored to expedite development and ensure a robust and feature-rich environment. Key considerations for the engine include:

   - **Market Impact:** Simulating the effect of large trades on asset prices.
   - **Latency:** Accounting for delays in order execution.
   - **Slippage:** Modeling the difference between expected and actual trade prices.
   - **Order Queues:** Replicating the dynamics of order matching in exchanges.
   - **Commissions and Fees:** Incorporating transaction costs into the simulation.

2. **Backtesting Framework Construction:** The overarching backtesting framework will integrate the backtesting engine with data loading, strategy implementation, performance reporting, and visualization tools. This framework will be modular and extensible to support various trading strategies, data sources, and performance metrics. It will also be well-documented to ensure ease of use and maintainability.

3. **Rolling Walk-Forward Validation:** Rolling walk-forward validation will be employed to assess the model's ability to generalize to unseen data. This method involves training the model on a past period, testing it on a subsequent period, and then rolling the training and testing windows forward. This simulates real-world trading scenarios and provides a more realistic estimate of future performance.

4. **Stress Testing:** Thorough stress tests will be conducted to evaluate the resilience of the trading strategies under various adverse market conditions. These tests will use both historical and synthetic data representing various scenarios, including:

   - **Extreme Volatility:** Periods of significant price fluctuations.
   - **Market Crashes:** Simulating rapid and substantial market declines.
   - **Prolonged Bull/Bear Markets:** Evaluating performance during extended upward or downward trends.

This rigorous backtesting process aims to identify potential vulnerabilities and improve the robustness of the trading strategies before live deployment.

### A. Backtesting

This section details the implementation of a robust backtesting and paper trading framework crucial for evaluating and refining trading strategies before live deployment. This framework simulates real-time trading using live market data, allowing for realistic order placement and fill simulation.

**Backtesting Framework and Live Data Integration:**

A comprehensive backtesting framework will be developed to execute simulated trades based on the model's generated signals. To ensure realism, this framework will integrate live tick data from a provider like Zerodha via a WebSocket feed. This real-time data captures market dynamics, including network latency and realistic bid-ask spreads, bridging the gap between historical backtesting and live trading. The framework will also leverage rolling walk-forward validation to mitigate overfitting and provide a robust performance evaluation.

**Realistic Fill Simulation and Market Depth Feature Engineering:**

The `Paper_Brokerage_Simulator` will use live tick data to simulate realistic order fills, considering live bid/ask prices, volume, and partial fills. This detailed simulation accurately reflects the practical challenges of order execution.

Furthermore, to enhance backtesting accuracy, market depth data will be utilized. A dedicated `DeriveOrderBookFeatures` service will process raw market depth data to generate derived features such as Order Book Imbalance (OBI), Weighted Average Price (WAP), and Bid-Ask Spread. These features will be input to the `DynamicPlaneGenerator`, improving the system's ability to capture nuanced market dynamics.

**Paper Trading and Live Trading Integration:**

To minimize risk before live deployment, a paper trading phase is essential. The `Paper_Brokerage_Simulator` will emulate the Zerodha Kite Connect API, providing a realistic simulation of order execution and portfolio management within a dedicated Firestore collection named `paper_portfolio`. The existing `Live_Execution_Enforcer` will be modified to support both LIVE and PAPER trading modes via a toggle in the Live Trading Dashboard UI. This unified interface facilitates direct performance and behavior comparisons between simulated and live trading. In PAPER mode, the `Live_Execution_Enforcer` routes trade requests to the `Paper_Brokerage_Simulator` instead of the Zerodha Kite Connect API.

**Explainability and Auditability:**

Beyond performance metrics, the backtesting process will provide insights into the rationale behind each simulated trade. The `Narrative_Generation_Service` will be triggered after each simulated trade. This service will leverage the Feature Store to retrieve relevant historical data and the system state. It will then employ attribution methods (LIME, SHAP, and model-specific methods like attention maps) to identify influential features. Finally, it will use these insights and an LLM to generate human-readable explanations for each trade, articulating the rationale, contributing factors, and expected outcome. These narratives are invaluable for compliance and auditing, strategy refinement, and building trust and transparency.

## A. Backtesting

This section details the implementation of a robust backtesting framework to rigorously evaluate the performance and robustness of the developed trading agent under realistic market conditions. This framework addresses the specific requirements of the project, including handling limitations in market data precision and incorporating newly developed specialist services.

The backtesting process will encompass the following key components:

1. **Backtesting Logic Implementation:** The core backtesting logic will simulate trading decisions based on the model's predictions, incorporating the revised approach of using order quantity and count information from the five available bid/ask levels provided by the Zerodha API. This simulation will include order execution, portfolio management, and the calculation of transaction costs. Crucially, the logic will integrate the Order Book Imbalance (OBI) value calculated by the `CalculateOrderBookImbalance` service as an additional input feature.

2. **Backtesting Framework Development:** A comprehensive and modular backtesting framework will be developed to encapsulate the backtesting logic and manage the complexities introduced by the project's specific data and services. This framework will handle data loading, simulation parameters, integration with the `CalculateOrderBookImbalance` and `GenerateDepthQuantityHeatmap` services, and performance reporting.

3. **Rolling Walk-Forward Validation:** A rolling walk-forward validation approach will be employed to mitigate the risk of overfitting and assess the model's generalization ability on unseen data. This process involves training the model on a past period of data, validating it on a subsequent period, and iteratively rolling these windows forward through the historical dataset.

4. **Stress Testing:** Rigorous stress testing will be conducted to evaluate the resilience of the trading strategy under various adverse market conditions, including periods of high volatility, market crashes, and unexpected events. These tests will specifically examine the impact of integrating signals from the `MarketDepthAnomalyDetector` CNN, generated by the `GenerateDepthQuantityHeatmap` service, on overall trading performance during these stressful scenarios. This analysis will identify potential weaknesses and vulnerabilities in the strategy and inform further refinements.

### A. Backtesting

This section details the backtesting procedures used to evaluate the performance and robustness of the trading strategies, including the SCoVA (Snapshot Computer Vision Algorithm) and the DynamicPlane algorithm, under various market conditions. The backtesting framework is also instrumental in generating data for training and refining the Anxiety Model, which guides the DynamicPlane algorithm. Given SCoVA's reliance on discrete, dynamically generated visual snapshots of market data, the backtesting framework is designed to accommodate this specific data format.

The backtesting process involves the following key steps:

1. **Backtesting Framework Development:** A robust and flexible backtesting framework is developed to simulate trading strategies under historical market conditions. This framework handles data ingestion (including SCoVA's visual snapshots), signal generation from both SCoVA and DynamicPlane, trade execution, portfolio management, and performance reporting. Critically, it also records the algorithms' actions, storing them for later analysis and training of the Anxiety Model. This framework incorporates considerations for transaction costs, slippage, short-selling limitations, and potential price improvements for a realistic performance evaluation.

2. **Rolling Walk-Forward Validation:** A rolling walk-forward validation approach is implemented within the framework. This mitigates overfitting by progressively training the models on past data and testing them on subsequent, unseen data, mimicking real-world trading scenarios. This ensures the strategies and the Anxiety Model remain robust and adaptive to evolving market dynamics.

3. **Stress Testing:** The strategies undergo rigorous stress testing using historical and hypothetical market scenarios, including periods of high volatility, market crashes, and varying liquidity levels. This assesses their resilience under adverse conditions. These scenarios inform the "flow" and "shock" trading modes guided by the Anxiety Model, and evaluate the impact of order book dynamics on signal reliability and trade execution. A "Book Resilience Score," generated by the `ComputeOrderBookState` specialist, is incorporated into the stress testing analysis, providing further insights into strategy robustness. The framework also evaluates the feedback loop related to execution quality under these stressful conditions.

## A. Backtesting

This section details the process of backtesting the trading strategy. The backtesting process will adhere to the following core steps:

1. **Implement Backtesting Logic:** This involves developing the core logic to simulate trading decisions based on the model's output and historical market data. This includes accurately handling order execution, transaction costs, and slippage, while considering the discrete snapshot nature of the input data. The integration of the `AsymmetricFeatureEngine` and its generated feature vector as a context token for the Vision Transformer (ViT) will be explicitly incorporated into the backtesting logic to accurately reflect its influence on trading decisions.

2. **Build the Backtesting Framework:** A robust and flexible backtesting framework is essential for reliable results. This framework will manage data loading, signal generation (including the provision of the asymmetric feature vector alongside the Dynamic Plane image tensor at each timestep), trade execution, portfolio management, and performance reporting. It should accommodate various parameters, including initial capital, trading fees, and different backtesting periods, and facilitate the integration of different models and strategies. The framework's design will ensure efficient handling of the context token within the adapted ViT architecture.

3. **Use Rolling Walk-Forward Validation:** A rolling walk-forward validation approach will be employed to assess the model's performance over time and its ability to adapt to changing market conditions. This involves training the model on a past period and validating it on a subsequent period, rolling the window forward through the historical data. This provides a more robust assessment of real-world applicability and mitigates overfitting to specific market regimes.

4. **Stress-Test the Strategy:** Stress testing involves subjecting the trading strategy to various extreme market conditions (e.g., market crashes, periods of high volatility, sudden liquidity changes) to assess its resilience and identify potential weaknesses. This analysis will focus on the interaction between the asymmetric features, the model's predictions, and portfolio performance under duress. The results will inform refinements to both the model parameters and the broader risk management strategy.

### A. Backtesting

This section details the implementation of a robust backtesting framework to rigorously evaluate the trading agent's performance using historical data. The backtesting process simulates real-world trading conditions, including order execution, slippage, and commission costs. It incorporates a rolling walk-forward validation scheme to assess the model's adaptability to changing market conditions. The framework is designed to handle the nuances of features like dual-token context injection (regime ID and raw asymmetric vector), accurately simulating their influence on trading decisions across various market regimes. Stress tests, simulating extreme market events like crashes and periods of high volatility, will be conducted to ensure the strategy's robustness under adverse conditions. This comprehensive evaluation will provide insights into the impact of different market conditions and feature vector influences on profitability, risk, and other key performance indicators.

### B. Performance Evaluation

This section outlines the quantitative and qualitative methods used to assess the trading strategy's performance.

**Quantitative Performance Measurement:** The primary performance metrics are the annualized Jensen's Alpha and Sharpe Ratio. Jensen's Alpha measures the risk-adjusted return of the portfolio compared to an equally weighted market benchmark, reflecting the strategy's ability to generate alpha. The Sharpe Ratio quantifies the risk-return trade-off, indicating the excess return per unit of risk. To ensure realistic performance assessment, transaction costs of 20 basis points per round-trip trade are deducted from portfolio returns before calculating these metrics. The risk-free rate used in these calculations is based on the 3-month Swedish Krona Short Term Rate (SWESTR).

**Feature Importance Analysis:** Understanding the drivers behind the model's decisions is crucial for refinement and validation. Feature attribution tools like Grad-CAM or SHAP will be employed to provide visual and quantitative insights into the importance of various input features (e.g., candlestick patterns, volume changes). This analysis will help identify potential biases, validate learned patterns, and highlight areas for improvement in the model's decision-making process.

## B. Performance Evaluation

This section details the performance evaluation of the trading strategy derived from the CNN and ViT models. The evaluation focuses on both quantitative performance metrics and qualitative insights into the models' decision-making process.

**1. Performance Metrics:**

The following standard financial metrics will be calculated based on backtested trading results using a rolling walk-forward validation methodology:

- **Jensen's Alpha:** This metric measures the risk-adjusted excess return of the strategy compared to the market benchmark, providing insights into the models' ability to generate alpha.
- **Sharpe Ratio:** This metric quantifies the risk-return trade-off by calculating the return achieved per unit of risk taken.

These metrics will be benchmarked against traditional market indices and other relevant trading strategies. Additionally, the Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) will be used to evaluate the accuracy of the models' 5-day future return predictions, calculated as `((Close(t+5) - Open(t+1)) / Open(t+1)) * 100`.

**2. Feature Attribution and Interpretability:**

Understanding the models' decision-making process is crucial. To achieve this, feature attribution tools like Grad-CAM and SHAP will be employed. These tools will highlight the regions of the 5-day candlestick chart images that are most influential in driving the models' predictions.

Since the models are trained on visual patterns rather than raw price data, dates, or ticker symbols, feature attribution will focus on identifying visually significant features within the candlestick charts themselves. Examples of such features include:

- Candlestick shapes and patterns (e.g., dojis, hammers, engulfing patterns)
- Volume changes relative to price movements
- The relationship between price action and technical indicators (e.g., moving averages)

This analysis will provide insights beyond overall performance metrics and help:

- Validate the models' learning by confirming they focus on relevant visual patterns.
- Identify potential biases or limitations in the models' understanding of market dynamics.
- Refine the models and trading strategy by highlighting areas for improvement.

**3. CNN and ViT Output Clarification:**

A detailed analysis will be conducted to clarify how the CNN and ViT models transform the 5-day candlestick chart images into concrete trade signals (entry, exit, and predicted return). This analysis will address:

- How the raw pixel data is processed and interpreted by the model layers.
- How the models generate numerical outputs representing trade signals.
- The specific logic used to translate these outputs into actionable trading decisions.

Furthermore, the input data format and handling of edge cases will be rigorously verified. This includes confirming the correct representation of Open, High, Low, Close (OHLC) prices and volume within the 5-day window, and carefully examining the handling of the last _n_ data points to ensure they do not negatively impact the results.

## B. Performance Evaluation

This section details the performance evaluation of the developed trading strategies, encompassing standard financial metrics, feature attribution analysis, and an investigation into the impact of trading costs. The evaluation also considers the influence of key architectural decisions, such as the input window length and the use of hard labels, on the overall performance.

**1. Performance Metrics:** The primary performance indicators used are Jensen's Alpha and the Sharpe Ratio, providing insights into the risk-adjusted returns of the strategies.

**2. Feature Attribution:** Feature attribution tools, such as Grad-CAM or SHAP, will be employed to understand the CNN's decision-making process. By visualizing the importance of different features within the 5-day candlestick input images, these tools can reveal the patterns the model learns and identify potential biases or areas for improvement. This analysis is crucial for refining the model and enhancing its interpretability.

**3. Historical Prediction Error Profiling (HPEP):** A novel Historical Prediction Error Profiling (HPEP) method will be implemented to refine trade selection. After training, a confidence profile will be constructed by grouping validation set predictions into bins based on predicted return ranges (e.g., -5% to -3%, -3% to -1%, ..., +3% to +5%). For each bin, the accuracy (percentage of directionally correct predictions) will be calculated. Optionally, the average error magnitude or Sharpe ratio within each bin can also be computed. This creates an HPEP map linking predicted return magnitudes to historical accuracy. During backtesting, this map will be used to filter trades. A trade, based on the model's prediction, will only be executed if the corresponding HPEP bin's historical accuracy exceeds a predefined threshold. This risk-aware approach focuses on predictions within historically successful ranges.

**4. Architectural Considerations:** The CNN model utilizes a 5-day input window, justified by prior research (Jiang et al., 2023). While this input length has empirical backing, the corresponding 5-day prediction window is an independent design choice that requires further investigation regarding its impact on backtesting outcomes. Additionally, the current use of hard labels for training might limit the model's accuracy. Exploring a soft label approach, where the CNN outputs a probability distribution across discretized return bins, is planned. This involves modifying the output layer with a softmax activation function, transforming hard labels into soft labels (potentially using a Gaussian kernel), and utilizing appropriate loss functions like KL Divergence or Cross-Entropy.

**5. Trading Cost Analysis:** A critical aspect of performance evaluation is analyzing the impact of trading costs. The effects of frequent rebalancing, transaction friction, uniform weighting of decile components, and short-selling constraints on annual returns and alpha generation will be quantified. This analysis will inform strategies for mitigating these costs and improving overall performance.

## B. Performance Evaluation

This section details the performance evaluation metrics and methodologies used to rigorously assess the trading strategy's effectiveness. The evaluation considers standard financial metrics, risk management, model interpretability, and addresses specific challenges related to the trading strategy's design, such as transaction costs and the limitations of short-selling.

**1. Performance Metrics and Benchmarking:**

Standard performance metrics, including Jensen's Alpha and the Sharpe Ratio, will be calculated to benchmark the strategy against the First North All-Share index and quantify its risk-adjusted returns. A detailed breakdown of alpha generated by long versus short positions will be performed, along with a comparison of per-index Sharpe ratios before and after transaction costs. This analysis will help isolate the impact of transaction costs and identify areas for improvement.

**2. Feature Attribution and Model Interpretability:**

Feature attribution tools like Grad-CAM or SHAP will be utilized to gain insights into the model's decision-making process and understand which features of the candlestick charts are most influential in driving predictions. This analysis enhances model interpretability and provides a basis for refinement.

**3. Addressing Trading Strategy Limitations:**

The evaluation will explicitly address the limitations observed in preliminary results, specifically the impact of high turnover, uniform position weighting, and the absence of smart trade filtering. It will also consider the constraints imposed by the limited availability of short-selling opportunities, particularly within the small-cap segment.

**4. Analyzing Unsuccessful Trades and Stop-Loss Mechanisms:**

A critical area of analysis is the handling of unsuccessful trades—instances where actual outcomes contradicted model predictions. The evaluation will clarify how such trades were handled in the absence of a stop-loss mechanism and explore the potential benefits of incorporating a stop-loss strategy. This will include backtesting with varying stop-loss levels to optimize parameters and mitigate downside risk.

**5. Enhancing Portfolio Management:**

The evaluation will investigate enhancements to portfolio construction logic, including:

- **Dynamic Trade Filtering:** Implementing a dynamic trade filtering layer based on predicted return volatility or Historical Prediction Error Profiling (HPEP) data. This layer will use prediction confidence thresholding to execute trades selectively, based on predicted risk and potential return. Backtesting will be used to optimize filter thresholds.

- **Risk-Based Weighting:** Implementing a sophisticated weighting scheme that considers prediction confidence, the inverse of historical volatility, and the signal-to-noise ratio of predicted vs. actual returns during validation. This will optimize capital allocation by prioritizing trades based on their risk-return profiles.

## B. Performance Evaluation

This section details the process of evaluating the performance of the trading agent and identifying areas for potential improvement. This involves analyzing both standard financial metrics and the model's decision-making process using explainable AI (XAI) techniques. Furthermore, a dedicated process for learning from trading errors will be implemented.

**1. Quantitative Performance Measurement:**

Standard performance metrics, such as Jensen's Alpha and the Sharpe Ratio, will be calculated to provide a quantitative assessment of the trading agent's risk-adjusted returns and its ability to outperform a benchmark. Jensen's Alpha measures the excess return of the portfolio compared to its expected return, given its beta. The Sharpe Ratio quantifies the excess return per unit of volatility, providing insight into the consistency of returns.

**2. Explainable AI (XAI) for Model Interpretability:**

Feature attribution tools like Grad-CAM or SHAP (SHapley Additive exPlanations) will be employed to understand the model's decision-making process. By visualizing the importance of different features in the input candlestick charts, we can gain insights into which patterns or trends the model is focusing on. This analysis helps identify potential biases, strengths, and weaknesses, leading to further model refinement and improved interpretability. This deeper understanding of feature importance can inform model enhancements by highlighting the most influential aspects of the candlestick data and ensuring the model learns meaningful relationships, not spurious correlations.

**3. Learning from Trading Errors:**

A crucial aspect of performance evaluation is systematically analyzing and learning from the agent's mistakes. An "error map" will be created during validation, recording details of individual trades, particularly focusing on "bad trades" where the predicted direction mismatches the actual market movement.

Analysis of the error map will aim to identify patterns associated with bad trades, considering candlestick patterns, market conditions, and other contextual factors. These insights will be used to improve the agent in several ways:

- **Refined Training Data:** Problematic trading scenarios identified in the error map can be filtered out, down-weighted, or even re-weighted with higher importance during future training cycles to improve model generalization and handling of challenging scenarios.

- **Enhanced Exit Strategies:** The error map analysis can inform the development of more sophisticated exit strategies, including:
  - **Volatility-Aware Exits:** Dynamic thresholds based on recent volatility using the Average True Range (ATR).
  - **Time-Based Confidence Decay:** Exiting trades if predicted rewards don't materialize within the expected timeframe.
  - **Prediction Divergence:** Exiting trades if retraining leads to significant prediction divergence.
  - **Portfolio Contextual Exits:** Exiting trades underperforming their prediction group average.

**4. Benchmarking Temporal Models:**

Different approaches for handling temporal data will be evaluated, including static picture, picture-pair, and sequence models. A comparative analysis will identify the most effective method for incorporating temporal context from candlestick data. This will involve developing a data generator for paired candlestick images and exploring hybrid architectures like CNNs combined with LSTMs.

This iterative process of evaluation, interpretation, error analysis, and refinement is crucial for developing a robust and profitable trading agent, ultimately aiming to maximize performance as measured by metrics like Jensen's Alpha and the Sharpe Ratio.

## Model Architecture and Training Considerations for Robust Performance Evaluation

This section outlines key architectural and training considerations for the Vision Transformer (ViT) model that are crucial for a robust performance evaluation. Addressing these aspects ensures the model is appropriately configured and trained for optimal results before formal performance evaluation using metrics like Jensen's Alpha, the Sharpe Ratio, and feature attribution tools (e.g., Grad-CAM, SHAP).

**ViT Input Sequence Length:**

- **Input Image Count Constraint:** A key consideration is whether the ViT model has a fixed input image count constraint. If the model only accepts a fixed number of input images (e.g., three 5-day candlestick charts), this limitation must be acknowledged and its impact on performance results understood. If the input image count is a flexible parameter, the optimal sequence length must be determined.

- **Experimentation with Input Image Count (N):** Experimenting with different input sequence lengths (e.g., _N_ = 3, 4, or 5 images) is crucial for empirically evaluating the impact on model performance. This experimentation will inform the selection of the optimal _N_ that maximizes performance during formal evaluation.

**ViT Architectural and Training Details:**

- **Positional Embeddings:** Correct implementation of positional embeddings is fundamental for the ViT to capture the sequential nature of candlestick data. Without positional embeddings, the model treats input images as an unordered set, potentially degrading its predictive accuracy.

- **Handling Variable-Length Input:** To handle variable-length input sequences, masking and padding are essential. This ensures the model can process sequences of different lengths during both training and evaluation. Proper implementation is crucial for reliable performance evaluation and generalization to real-world market data.

- **Training with Variable-Length Sequences:** Training the ViT with variable-length sequences, facilitated by masking and padding, allows the model to be evaluated on more realistic scenarios where input sequence lengths may vary. This enhances the comprehensiveness and real-world applicability of the performance evaluation.

By addressing these ViT-specific architectural and training aspects, we ensure a more robust and meaningful performance evaluation. This lays the groundwork for a thorough assessment using established financial metrics and feature attribution tools, which will be detailed in a subsequent section.

## B. Performance Evaluation

This section details the performance evaluation process, encompassing both quantitative metrics and qualitative analyses tailored to the image-based nature of this trading strategy. While standard financial metrics provide a baseline assessment, we also address the specific challenges and opportunities presented by using candlestick images for prediction.

**1. Quantitative Performance Metrics:**

We will calculate standard financial metrics such as Jensen's Alpha and the Sharpe Ratio. Jensen's Alpha measures the risk-adjusted return of the strategy compared to a market benchmark, indicating the strategy's ability to generate excess returns. The Sharpe Ratio quantifies the risk-return trade-off, representing the return per unit of risk. These metrics provide a quantitative assessment of the overall trading strategy performance.

**2. Qualitative Analysis with Feature Attribution:**

To understand the model's decision-making process, we will employ feature attribution tools like Grad-CAM or SHAP. These techniques highlight the regions within candlestick images that most influence the model's predictions, offering insights into the visual patterns driving trading decisions. This analysis helps validate the image-based approach by comparing the model's focus with human interpretations of trading patterns, contributing to an assessment of the approach's interpretability.

**3. Image-Based Prediction Considerations:**

Due to the unique nature of image-based financial modeling, the following considerations are crucial for a comprehensive performance evaluation:

- **Return Extraction Methodology:** A clearly defined method for extracting open, close, high, and low prices from the generated candlestick images is essential for accurate return calculation. This might involve analyzing pixel locations or converting image data into a numerical format. The chosen methodology will be explicitly documented and its impact on performance analysis will be considered.

- **Risk and Drawback Assessment:** Image-based prediction introduces specific risks:

  - **Indirect Evaluation:** Deriving returns from images is less direct than using numerical predictions, potentially introducing inaccuracies.
  - **Compounding Errors:** Errors in image generation can compound during return extraction, affecting overall performance.
  - **Indirect Reward Supervision:** Training on images rather than direct returns might weaken the link between predictions and financial outcomes.
  - **Ambiguity of Financial Implication:** Mapping image features to financial meaning can be ambiguous, hindering interpretation and performance attribution.

- **Comparison with Existing Models:** We will compare the image-based approach with traditional models (e.g., scalar regression, probabilistic return models) across various aspects: output type, supervisory signal, link to trading, learned structure richness, interpretability, ambiguity risk, data requirements, and modeling complexity. This comparison provides a holistic view of the approach's strengths and weaknesses.

- **Actionable Trading Decisions:** The evaluation will assess the model's ability to translate predicted candlestick formations into clear buy/sell/hold recommendations, ensuring that visually accurate predictions lead to actionable trading strategies. The focus will be on whether a trade is suggested, not just pattern identification.

By addressing these quantitative and qualitative aspects, including the specific considerations related to image-based predictions, we aim to provide a robust and nuanced evaluation of the model's performance and its potential for practical trading applications.

## B. Performance Evaluation

This section details the rigorous evaluation of the trading strategy's performance, moving beyond visual fidelity to assess its profitability and understand its decision-making process. This includes quantitative assessment using standard performance metrics and qualitative analysis through feature attribution techniques.

**1. Performance Metrics:**

The following metrics will be calculated to quantify the risk-adjusted returns of the trading strategy:

- **Jensen's Alpha:** This measures the risk-adjusted return compared to a market benchmark, indicating whether the strategy outperforms a passive investment approach.
- **Sharpe Ratio:** This quantifies the return per unit of risk, providing insights into the strategy's risk-return profile.

These metrics will be presented in comparative charts and tables within the dissertation and compared against relevant benchmarks and alternative trading strategies. A protocol will be designed to compare performance against traditional scalar return prediction models to assess relative strengths and weaknesses.

**2. Feature Attribution:**

Feature attribution techniques, such as Grad-CAM and SHAP, will be employed to understand which features of the input data (e.g., candlestick patterns, transformed 2D representations) are most influential in driving the model's predictions. This analysis will:

- Provide insights into the model's decision-making process, helping identify potential biases or areas for improvement.
- Reveal the visual or abstract patterns the model leverages, potentially informing the development of simpler, interpretable benchmark models.
- In the case of dynamic 2D representations, illuminate how transformations and rotations influence the model's perception of market movements and the effectiveness of encoding curves within this representation.
- Explore the relationship between visual pattern recognition accuracy and actual financial performance (profitability of trades).

Experimental logging templates will ensure consistent tracking and reporting of the feature attribution analysis. This analysis will contribute significantly to understanding the model's behavior and support the dissertation's findings.

### B. Performance Evaluation

This section details the performance evaluation of the CNN and ViT trading models. We will use quantitative metrics to assess the overall profitability and risk-adjusted returns, and feature attribution techniques to understand the models' decision-making process.

1. **Performance Metrics:** The models' performance will be evaluated using the following metrics:

   - **Jensen's Alpha:** This metric measures the risk-adjusted return of the strategy compared to the market benchmark (e.g., S&P 500), accounting for the risk-free rate of return. A positive alpha indicates the strategy outperformed its expected return given its risk level.

   - **Sharpe Ratio:** This metric quantifies the excess return of the portfolio per unit of risk (standard deviation). A higher Sharpe ratio indicates better risk-adjusted performance. The calculation will incorporate the portfolio's returns over the backtesting period and the risk-free rate of return.

2. **Feature Attribution:** To gain insights into the models' decision-making process and understand which features drive predictions, we will employ the following techniques:

   - **Grad-CAM (Gradient-weighted Class Activation Mapping):** Grad-CAM will visualize the regions of the input candlestick chart images that are most influential on the models' predictions. This allows us to understand which patterns or features the models are learning to recognize.

   - **SHAP (SHapley Additive exPlanations):** SHAP will provide a more granular understanding of feature importance by quantifying the contribution of each feature to specific predictions. This will help determine the impact of individual candlestick patterns, volume, and moving averages on the models' buy/sell decisions.

By combining these performance metrics and feature attribution techniques, we will rigorously evaluate the effectiveness of the developed trading strategies and gain a deeper understanding of their underlying mechanisms.

## B. Performance Evaluation

This section details the performance evaluation of the trading agent, encompassing quantitative metrics, visual analysis, and feature attribution. The evaluation focuses on both predictive accuracy and financial performance within the context of the dynamic projection system.

**Key Performance Indicators (KPIs):**

The following metrics will be used to assess the model's effectiveness:

- **Mean Squared Error (MSE):** Measures the average squared difference between predicted and actual returns. Lower MSE values indicate better predictive accuracy.
- **Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and actual returns. Similar to MSE, lower MAE values suggest improved prediction accuracy.
- **Directional Accuracy:** Represents the percentage of correctly predicted price movements (up or down). This metric is crucial for evaluating the model's ability to capture market direction.
- **Sharpe Ratio:** Evaluates the risk-adjusted return of the trading strategy, calculated as the excess return (return above the risk-free rate) divided by the standard deviation of returns. A higher Sharpe Ratio indicates better risk-adjusted performance.
- **Hit Rate:** The percentage of trades that result in a profit, providing a direct measure of trading effectiveness.
- **Profit Metrics:** These include total profit, average profit per trade, and maximum drawdown, essential for assessing the financial viability of the strategy.
- **Compute Efficiency:** Assesses the computational resources required for model training and prediction, crucial for practical deployment and scalability.
- **Model Convergence:** Evaluates the stability and convergence of the training process, ensuring the model reaches a stable and optimal state.

**Jensen's Alpha:** While traditionally used, Jensen's Alpha will be interpreted within the context of the dynamic projection system and its impact on risk-adjusted returns.

**Impact of Dynamic Projection:** The influence of the dynamic PCA integration on performance metrics requires careful analysis. Specifically, the impact on the Sharpe Ratio and Jensen's Alpha will be investigated. The goal of the dynamic projection—achieving prediction invariance to previous trend direction or focusing on relative local movement—needs clarification to accurately interpret performance. The specific rotation mechanism (affine transform, PCA rotation, or learned rotation) and its influence on stability and interpretability will be documented. How the dynamic projection, particularly the re-centering and rotation of the feature space influenced by new candlestick data, affects performance will also be analyzed. This includes understanding how prioritizing relative (local) price behavior over absolute geometric positions within feature vectors contributes to the overall system effectiveness.

**Feature Attribution:**

Feature attribution techniques, such as Grad-CAM and SHAP, will be employed to understand the model's decision-making process and identify influential features driving predictions. These methods will provide insights into the relationship between input features (candlestick patterns and other incorporated data) and model outputs (trade signals), enhancing our understanding of the model's behavior and potentially revealing biases.

**Visualization:**

Clear and informative visualizations using `matplotlib` will support performance analysis. Visualizations may include accompanying explanations or tooltips to enhance understanding. While animation is a possibility for visualizing the dynamic projection system, static charts will be prioritized to avoid potential performance issues. The visualizations will adhere to the single chart per function call requirement.

## B. Performance Evaluation

This section details the performance evaluation of the trading strategies developed using the dynamic plane representation of market data. This evaluation incorporates both standard financial metrics and explainable AI (XAI) techniques to provide a comprehensive assessment of the strategy's effectiveness and the model's behavior.

1. **Standard Financial Metrics:** The model's performance will be rigorously evaluated using established financial metrics:

   - **Jensen's Alpha:** This metric will determine the risk-adjusted excess return of the strategy compared to the market benchmark.
   - **Sharpe Ratio:** This will assess the risk-return trade-off, measuring the excess return per unit of risk.

   These metrics provide a quantitative assessment of the strategy's profitability and consistency. The evaluation will explicitly consider the impact of the dynamic plane generator on these metrics by comparing performance against strategies using static or no plane representations.

2. **Feature Attribution with XAI Techniques:** Understanding the model's decision-making process within the context of the dynamic plane is crucial. We will employ feature attribution techniques to gain insights into which features of the dynamically generated plane influence the model's output. Specifically:

   - **Grad-CAM/SHAP Analysis:** Techniques like Grad-CAM (Gradient-weighted Class Activation Mapping) and SHAP (SHapley Additive exPlanations) will highlight the influential features within the dynamic plane. This analysis will reveal how the dynamic redrawing of the coordinate system affects the model's focus on different aspects of market data.
   - **Connecting to Dynamic Plane Generator:** This analysis will be directly linked to the pseudocode of the dynamic plane generator to ensure alignment between the model's interpretation and the plane's construction.
   - **Visual Interpretation with Conceptual Diagram:** The feature attribution results will be interpreted in conjunction with the conceptual diagram of the dynamic 2D plane evolution to provide a clear visual understanding of the model's behavior.

By combining traditional performance metrics with XAI techniques and considering the specific influence of the dynamic plane, this evaluation framework provides a thorough assessment of the developed trading strategies. This approach facilitates identifying potential areas for improvement in both the model and the dynamic plane generator itself.
Performance Evaluation

This section details the process of rigorously evaluating the performance of the trading models, including both quantitative assessment using established metrics and qualitative analysis leveraging feature attribution techniques. Furthermore, considerations specific to the dynamic plane visualization are addressed to ensure the reliability and interpretability of the evaluation process.

**Quantitative Performance Metrics:**

The core performance metrics for this project are Jensen's Alpha and the Sharpe Ratio. Jensen's Alpha measures the risk-adjusted return of the strategy compared to the market benchmark, providing insight into the strategy's alpha generation capabilities. The Sharpe Ratio quantifies the risk-return trade-off, representing the excess return per unit of volatility. These metrics will be calculated and compared against benchmarks and alternative strategies to provide a quantitative assessment of the strategy's effectiveness and suitability for practical application.

**Feature Attribution and Model Interpretability:**

Understanding the rationale behind a model's predictions is crucial for both refinement and trust. Feature attribution techniques, such as Grad-CAM or SHAP, will be employed to visualize and quantify the influence of input features (aspects of the candlestick charts or derived features from the dynamic plane) on the model's trading decisions. This analysis aids in:

- **Identifying Key Drivers:** Determining which features are most influential in the model's predictions.
- **Uncovering Potential Biases:** Recognizing any undue reliance on spurious correlations or irrelevant features.
- **Validating Causal Grounding:** Connecting the model's focus to understandable market patterns, enhancing confidence in the predictions.

**Considerations for Dynamic Plane Visualization:**

The dynamic plane visualization, while not a performance metric itself, plays a crucial role in understanding model behavior. Therefore, ensuring its robustness and stability is essential for a comprehensive performance evaluation. Key considerations include:

- **Minimum Data Points for Rotation:** The dynamic plane rotation should only initiate after a sufficient number of data points (e.g., three) are available to ensure stability and prevent erratic behavior in the early stages of the visualization.
- **Handling Limited Data:** Strategies for gracefully handling scenarios with limited data points, such as in the initial frames of the animation, must be implemented. This might involve displaying a placeholder, delaying rotation until sufficient data is available, or employing alternative visualization techniques.
- **Addressing PCA Instability:** The potential instability of Principal Component Analysis (PCA) when data points are collinear, particularly with limited data, must be acknowledged and addressed. Strategies for mitigating this issue could include robust PCA variants or alternative dimensionality reduction techniques.

**Data Pipeline and Simulation:**

A robust and scalable data pipeline is crucial for generating training data and conducting simulations. This includes generating batched datasets based on the dynamic plane principle for both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Furthermore, simulating longer sequences of data is essential for stress-testing the models and evaluating their performance in diverse market regimes. Experimenting with smoothing techniques, such as applying Heikin-Ashi to the dynamic plane, can further refine the input data and potentially improve model performance. These elements, while not direct performance metrics, are critical for ensuring the reliability and generalizability of the performance evaluation results.

### B. Performance Evaluation

This section details the methods used to evaluate the performance of the trading strategies developed using the models. We utilize established financial metrics and explainability techniques to rigorously assess the efficacy of our approach.

1. **Performance Metrics:** We calculate the following key performance indicators:

   - **Jensen's Alpha:** This metric measures the risk-adjusted return of the strategy compared to a market benchmark. A positive alpha suggests the strategy outperformed its expected return, given its level of risk. We will use a suitable market index as the benchmark.

   - **Sharpe Ratio:** This ratio quantifies the risk-adjusted return of the strategy. It represents the excess return per unit of volatility, where volatility is measured by the standard deviation of the strategy's returns. A higher Sharpe ratio indicates better risk-adjusted performance.

2. **Feature Attribution:** To understand the driving factors behind the model's predictions, we employ the following feature attribution tools:

   - **Grad-CAM (Gradient-weighted Class Activation Mapping):** Grad-CAM visually highlights the regions of the candlestick chart images most influential in the model's decision-making process. This allows us to understand which visual patterns the model learns and uses for its trading signals.

   - **SHAP (SHapley Additive exPlanations):** SHAP values provide a game-theoretic approach to understanding feature importance. They quantify each feature's contribution to the model's prediction for a specific instance, allowing for a more granular analysis of the model's behavior.

By combining these performance metrics and feature attribution techniques, we gain a comprehensive understanding of the model's effectiveness and the rationale behind its trading decisions. This rigorous evaluation process ensures the developed strategies are robust and reliable.

### C. Robustness Testing under Simulated Market Conditions

This section focuses on evaluating the model's performance under various simulated market conditions, including complex and chaotic scenarios. This is crucial for understanding the robustness and limitations of the dynamic plane implementation.

We simulate different market behaviors and generate corresponding candlestick data:

- **Complex Price Patterns:** We simulate realistic market behavior, incorporating rally, drop, and recovery phases to assess model performance under complex price dynamics. This goes beyond simple price patterns and provides a more thorough evaluation of the model's effectiveness in real-world scenarios.

- **Chaotic/Choppy Market Conditions:** We simulate chaotic, choppy sideways markets to test the dynamic plane's robustness when faced with extreme volatility and rapid price fluctuations. Analyzing behavior under these conditions helps identify potential weaknesses and areas for improvement.

To generate and visualize data for these simulations, we perform the following tasks:

- **Choppy Candlestick Data Generation:** The `generate_choppy_candlesticks(n=30)` function generates simulated choppy market data, characterized by small, random price fluctuations around a base price. We use this data to create both standard and dynamic Heiken-Ashi charts for comparison.

- **Standard Heiken-Ashi Chart Creation:** Using the generated choppy candlestick data, we create a standard Heiken-Ashi chart using the `generate_heiken_ashi` and `plot_heiken_ashi_candlestick` functions. This chart serves as a baseline comparison for the rotated dynamic Heiken-Ashi chart. The resulting chart is saved to `/mnt/data/standard_heiken_ashi_choppy.png`.

- **Rotated Dynamic Heiken-Ashi Chart Creation:** We apply the dynamic plane transformation to the choppy Heiken-Ashi data using the `dynamic_rotate_recenter_heiken` function. The resulting rotated dynamic Heiken-Ashi chart is plotted using the `plot_rotated_heiken` function and saved to `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`. Comparing this chart with the standard Heiken-Ashi chart allows us to visually assess the impact of the dynamic plane transformation under choppy market conditions.

## Performance Evaluation

This section details the performance evaluation of the trading strategies developed within this project. Beyond standard metrics, the evaluation incorporates interpretability tools and an analysis of the dynamic plane transformation's behavior (where applicable).

**Quantitative Performance Measurement:** Standard financial metrics, including Jensen's Alpha and the Sharpe Ratio, will be calculated to assess risk-adjusted returns and benchmark the strategy's performance.

**Interpretability and Feature Attribution:** Feature attribution tools like Grad-CAM or SHAP will be employed to understand the model's decision-making process. This analysis will identify influential features driving predictions, offering insights into model behavior and potential areas for improvement. For models utilizing a dynamic PCA transformation, this analysis will also investigate how PCA impacts feature representation by highlighting the features the model focuses on after the transformation. This addresses the need for interpretability within the dynamic context and helps validate that the model learns meaningful patterns.

**Dynamic Plane Analysis (where applicable):** The efficacy and stability of dynamic plane transformations will be evaluated by analyzing the behavior of the following mechanisms:

- **Frame Confidence Correction:** Post-prediction, actual market movement will be compared against expected movement. Discrepancies will correct rotational frame assumptions, enabling continuous assessment and adjustment of the dynamic plane projections.

- **Prediction Error Memory:** A rolling memory of recent prediction errors across dynamic frames will be maintained. Frequent misalignments between the PCA frame and realized market structure will trigger rotation weight adjustments, adapting dynamic plane stability based on historical performance.

- **Feedback-Driven Frame Smoothing:** During periods of high prediction error, the rotation speed of the dynamic plane will be adjusted to enhance stability and prevent overfitting to noise. This addresses concerns about the impact of "Window Size and Smoothing" on observed patterns.

This multi-faceted approach, combining quantitative performance metrics, feature attribution analysis, and dynamic plane behavior analysis (where relevant), ensures a robust and comprehensive evaluation of the developed trading strategies. It provides a thorough understanding of both overall performance and the underlying decision-making processes.

## Performance Evaluation

This section details the performance evaluation of the trading agent, moving beyond standard metrics to incorporate a nuanced understanding of prediction error, including its magnitude and direction, and the stability of the dynamic plane adjustments.

**Standard Metrics:** Standard financial metrics such as Jensen's Alpha and the Sharpe Ratio will be calculated to provide a baseline assessment of risk-adjusted returns and benchmark the agent against market performance.

**Error Analysis:** Recognizing that the direction of error can be as crucial as its magnitude, especially in dynamic systems like financial markets, this project incorporates a novel error analysis approach. Even small misalignments in predicted trajectory can compound over time. This analysis will include:

- **Removal of Static Error in PCA:** A previously used static error value within Principal Component Analysis (PCA) calculations will be removed, as this static representation of prediction error memory is deemed potentially inaccurate and detrimental to performance.

- **Lagging Rotation Deactivation Strategy:** A strategy will be developed to manage the lag introduced by lagging rotation, including determining when to decrease or deactivate this lag and revert to normal operation. This aims to optimize the balance between utilizing error signals and maintaining responsiveness to current market conditions.

- **Simulating Peripersonal vs. Extrapersonal Gap:** A method will be proposed to simulate the biological concept of "peripersonal vs. extrapersonal" gap within backtesting. This will help understand how predictive abilities change over different forecasting horizons, analogous to how biological systems react to stimuli within and beyond immediate reach.

- **Error-Signal Augmented Dynamic Plane Algorithm:** A visual representation (sketch) of the dynamic plane algorithm augmented with the proposed error signal mechanism will be created to facilitate comprehension and analysis of its behavior.

- **Lightweight Prediction-Error Feedback Implementation:** Prediction-error feedback will be integrated into model training through mechanisms like auxiliary loss functions or frame stability monitoring, allowing the model to learn from its mistakes and improve predictive accuracy.

**Feature Attribution:** Feature attribution techniques like Grad-CAM and SHAP will be employed to understand the model's decision-making process by identifying influential features. This provides insights into model behavior and potential areas for improvement, informing the development of more targeted error correction strategies.

**Frame Intervention Metric:** A "frame intervention" metric will be developed to monitor the frequency of rolling frame correction algorithm adjustments to the PCA frame due to error spikes over a trading year. A high intervention rate could indicate instability or over-sensitivity to noise, while a low rate suggests a more fluid, adaptive system. This metric will be valuable for understanding the long-term behavior and robustness of the dynamic plane implementation.

This enhanced evaluation framework, with its focus on composite error, directional drift, and frame stability, provides a more comprehensive assessment of the agent's performance than traditional financial metrics alone. This is crucial for understanding the effectiveness of the dynamic plane implementation and guiding further model refinement.

## B. Performance Evaluation

This section details the performance evaluation of the trading strategy, focusing on both standard financial metrics and the accuracy of predictions within the dynamic 2D PCA framework. A key challenge is comparing predicted and realized market movements within this dynamic coordinate system.

**Standard Performance Metrics:** Standard metrics such as Jensen's Alpha and the Sharpe Ratio will be calculated to assess the overall profitability and risk-adjusted returns of the strategy. These provide a quantitative measure of the strategy's effectiveness compared to a benchmark.

**Feature Attribution:** Tools like Grad-CAM or SHAP will be utilized to understand which features of the candlestick images are most influential in the model's predictions, providing valuable insights into its decision-making process.

**Evaluating Predictions within the Dynamic PCA Framework:** The following procedures will ensure accurate comparison between predicted and realized movements within the dynamic coordinate system:

- **Freezing the Dynamic Frame:** For short-term predictions (1-5 candlesticks), the dynamic PCA frame calculated at time 't' will be frozen and used for predicting and evaluating movements. This assumes minimal significant market structure shifts within this short interval. Both predicted and realized movement vectors are then interpreted within this same frozen frame, improving computational efficiency and ensuring consistency.

- **Reprojecting Realized Movement:** The realized movement at 't+1' will be reprojected back into the PCA frame established at 't' using the original PCA basis (rotation matrix). This enables direct comparison within the initial prediction frame, even if recomputing PCA at 't+1' yields a different result.

- **Analyzing PCA Plane Consistency:** An investigation will be conducted to clarify the consistency of the 2D plane formed by PCA between the predicted and realized data. Given potential differences in price and volume values, and consequently different PCA axes, it is crucial to determine if a transformation or alignment step is required to ensure comparability.

**Enhancing Predictive Capabilities:** While not strictly performance _evaluation_, the following tasks are crucial for enhancing the model's predictive capabilities and, consequently, its overall performance:

- **Error Correction Prototype:** The existing error correction pseudocode will be expanded into a working prototype, forming the foundation for a robust error correction mechanism within the model.

- **Lightweight Frame Adjustment:** Lightweight correction nudges (small frame adjustments) will be applied when accumulated angular errors exceed a defined threshold. This proactive error mitigation aims to continuously refine the dynamic frame and improve prediction accuracy.

- **Visual Simulation of Vectorial Misalignments:** A visual simulation will be created to demonstrate the cumulative effect of small vectorial misalignments, illustrating how directional drift can lead to significant deviations even with small individual errors. This visualization will contrast this behavior with simple price error accumulation.

- **New Loss Functions:** New loss functions will be drafted that penalize both scalar and angular drift during model training, encouraging the model to learn more accurate and stable predictions and directly addressing directional error.

## B. Performance Evaluation

This section details the performance evaluation procedures, focusing on ensuring consistent comparisons of predicted and realized market movements within a potentially shifting PCA plane. Two primary methods, "Freeze Frame" and "Reproject Realization," address this challenge. Visualizations and pseudocode will further clarify implementation and illustrate these methods.

**1. Addressing PCA Frame Shifts**

Two methods mitigate the impact of PCA frame shifts:

- **Freeze Frame:** This method "freezes" the PCA rotation matrix (R) at the time of prediction. Both predicted and realized data points are then projected using this initial rotation matrix, ensuring a consistent frame of reference for performance comparison, even if the market's underlying principal components drift.

- **Reproject Realization:** This method projects the realized movement vector using the original rotation matrix (R) from the time of prediction. Like Freeze Frame, this maintains consistency by evaluating performance within the original prediction's frame of reference.

These methods differ from traditional distance and angular error calculations that assume a static PCA plane. Freeze Frame and Reproject Realization explicitly account for potential PCA shifts, providing more robust performance evaluation in dynamic market conditions. This distinction will be further highlighted with visualizations and examples.

**2. Error Metrics and Calculations**

Performance evaluation considers both prediction accuracy within a given PCA frame and the error introduced by the frame's dynamic nature. A weighted Total Error combines these components:

- **Vector Deviation Error:** This measures the difference between predicted and actual movement vectors within the local dynamic frame (defined by PCA1 and PCA2). It quantifies prediction accuracy within a specific frame instance. This combines distance error (`dvec`) and angular error (`θvec`) using weights α₁ and α₂: `Vector Error = α₁⋅dvec + α₂⋅θvec`.

- **Frame Shift Error:** This quantifies the change in the PCA frame between time steps, calculated using the principal angles between consecutive PCA basis vectors. In our 2D case, this involves calculating the angle between PCA1 at time _t_ and _t+1_, and between PCA2 at time _t_ and _t+1_. The Frame Shift Error combines these angular errors (`θPCA1` and `θPCA2`) with weights β₁ and β₂: `Frame Shift Error = β₁⋅θPCA1 + β₂⋅θPCA2`.

- **Total Error:** This integrates Vector Error and Frame Shift Error using weights γ₁ and γ₂: `Total Error = γ₁⋅Vector Error + γ₂⋅Frame Shift Error`. This balances the importance of prediction accuracy within the frame and the frame's stability.

The weights (α₁, α₂, β₁, β₂, γ₁, and γ₂) are tunable hyperparameters controlling the trade-off between different error components. This weighted approach provides a more comprehensive understanding of model performance than traditional metrics alone. We also employ standard metrics like Jensen's Alpha and the Sharpe Ratio.

**3. Implementation Details**

A "Freeze and Correct" module will encapsulate the logic for both Freeze Frame and Reproject Realization, handling the freezing of the PCA frame and the projection of data points. Pseudocode for this module and the Total Error calculation will be provided as a blueprint for implementation. Numerical examples and visualizations will demonstrate these techniques, showcasing predicted vs. realized movement and illustrating the prediction and reality paths in both frozen and shifted frames. A lightweight data structure will store the PCA basis (rotation matrix) for each window to enable efficient reprojection.

## B. Performance Evaluation

This section details the performance evaluation methodology, encompassing both traditional metrics and a novel dynamic error analysis and correction system designed to adapt to changing market conditions. This system allows the trading agent to adjust its behavior based on recent performance, improving robustness and resilience.

**Traditional Performance Metrics:**

Standard performance metrics, including Jensen's Alpha and the Sharpe Ratio, will be calculated to provide a baseline assessment of risk-adjusted returns. These metrics will benchmark the agent's performance against market benchmarks and alternative strategies.

**Feature Attribution:**

Feature attribution techniques, such as Gradient-weighted Class Activation Mapping (Grad-CAM) or SHapley Additive exPlanations (SHAP), will be employed to understand the influence of different input features on the model's predictions. This analysis will identify key drivers of successful trades, reveal potential biases, and inform future model refinement and feature engineering efforts.

**Dynamic Error Analysis and Correction:**

A dynamic error analysis and correction system will be implemented, incorporating the following components:

1. **Error Calculation:** A multi-weight error metric will be computed, incorporating normalized distance and angular errors. Normalization will ensure accurate aggregation despite differing units and scales. Formal pseudocode will define the error computation process, and a numerical example will illustrate the calculation. Initial default weights (α, β, γ) for the error components will be derived from trading intuition and principles of physics, subject to subsequent optimization.

2. **Rolling Error Monitoring and Statistics:** A rolling buffer will store the total error over a defined window (e.g., 5-10 steps). Rolling statistics (mean and standard deviation) calculated from this buffer will provide a quantifiable measure of recent performance fluctuations.

3. **Wound Detection and Correction Phase:** When the mean error exceeds a threshold (e.g., _k_ times the rolling standard deviation, where _k_=2), a "Wound Phase" will be triggered. During this phase, a correction factor will be applied to the PCA frame construction (rotation or smoothing operations) to mitigate the impact of increased error. If errors spike again during the Healing Phase, the system will re-enter the Wound Phase to prevent further divergence.

4. **Healing Phase and Correction Factor Decay:** A "Healing Phase" begins when the mean error falls below a lower threshold (e.g., between 1 and 1.5 times the rolling standard deviation). During this phase, the correction factor decays (e.g., exponentially with λ=0.95 or linearly), allowing the system to smoothly return to normal operation. The specific decay mechanism and parameters will be tuned through backtesting and simulation.

5. **Visualization and Tuning:** The activation of the healing system will be visualized over a sequence of trades to diagnose issues and refine parameters. Thresholds for phase transitions and decay rates will be tuned based on simulations and backtesting, considering common market regimes.

This comprehensive evaluation process aims to not only measure performance but also to deeply understand the factors driving the agent's behavior, enabling continuous improvement and adaptation to dynamic market conditions. Further research will explore the use of Frame Drift Error as a confidence indicator, potentially informing trading decisions based on the stability of the dynamic plane. A small-scale simulation will also demonstrate vector deviation and PCA frame drift for enhanced understanding.

## B. Performance Evaluation

This section details the performance evaluation procedures, focusing on prediction correctness tracking and a dynamic healing process based on performance, as well as traditional performance metrics and feature attribution analysis.

**Prediction Correctness and Dynamic Healing**

The agent's performance is evaluated by tracking prediction correctness over a rolling window of _N_ steps. This metric informs a dynamic decay rate for the correction factor applied during the healing process.

- **Prediction Correctness Tracking:** Each prediction is assigned a binary score: 1 for a correct prediction (direction and/or magnitude) and 0 otherwise.

- **Rolling Prediction Correctness Buffer:** A rolling buffer stores the prediction correctness scores for the last _N_ timesteps, providing a dynamic view of recent accuracy. The mean prediction correctness, calculated from this buffer, drives the dynamic adjustments.

- **Dynamic Decay Rate Adjustment:** The model's decay rate, mitigating the impact of past disruptions, is dynamically adjusted based on the mean prediction correctness. Higher correctness leads to a reduced decay rate, lessening the influence of past disruptions.

- **Performance-Based Healing:** The correction factor is dynamically modified in proportion to the observed predictive recovery within the rolling buffer. This approach ties the healing process directly to demonstrated predictive accuracy, ensuring optimal responsiveness to market dynamics.

- **Dynamic Decay Rate Implementation:** A function, `dynamic_decay_rate(mean_correctness)`, calculates the decay rate. The proposed implementation is `Decay Rate = 1 - (mean_correctness - healing_threshold)`. This formula ensures faster decay when mean correctness significantly exceeds the `healing_threshold`.

- **Healing Thresholds:** Initial healing thresholds, representing the required predictive accuracy to initiate accelerated healing, will be set within a reasonable trading context, with a suggested starting point of 75-80% directional correctness.

To facilitate implementation and understanding:

- **Formal Pseudocode:** Formal, modular pseudocode will be developed for the entire Healing-by-Correctness system, incorporating prediction correctness tracking and the dynamic decay rate functionality.

- **Toy Example Simulation:** A toy example will simulate the healing process, from initial "wound" (poor performance) through correction and recovery, to validate the system's behavior.

**Traditional Performance Metrics and Feature Attribution**

Beyond the dynamic healing process, traditional performance metrics will be used:

- **Performance Metrics:** Jensen's Alpha and Sharpe Ratio will be calculated to assess risk-adjusted returns and benchmark the trading strategy's performance.

- **Feature Attribution:** Tools like Grad-CAM or SHAP will be employed to understand the model's decision-making process and identify influential features. However, several data representation challenges must be addressed for accurate feature attribution:

  - **Non-linear Time Representation:** Time will be encoded using a non-linear scale to preserve chronological order and provide meaningful scalar values for interpretation.

  - **Encoding Time as Fractional Elapsed Time:** Timestamps will be represented as fractional values between 0 and 1, representing the normalized elapsed time within the candlestick window.

  - **Transforming Price into Relative Returns:** Price data will be transformed into relative returns (percentage change or log returns) with respect to the first price in the window, mitigating the impact of extreme price movements and differing price ranges.

This rigorous evaluation approach, encompassing both dynamic healing and traditional metrics, is crucial for building robust and reliable trading strategies.

### B. Performance Evaluation

This section details the process for evaluating the performance of the trading strategy. This involves calculating standard performance metrics like Jensen's Alpha and the Sharpe Ratio, as well as employing feature attribution techniques such as Grad-CAM and SHAP to understand the model's decision-making process. Proper data normalization is crucial for both performance evaluation and the effective application of these attribution tools.

1. **Data Normalization for Feature Attribution and Performance Metrics:** Normalize the price (P), volume (V), and time (T) data to ensure consistent input to the model and avoid features with larger scales dominating the analysis. The following transformations ensure data comparability across different stocks and time periods:

   - **Time (T):** Represent time as fractional elapsed time within the trading period, normalized to the range [-1, 1] using the formula: `(2 * time_frac) - 1`.

   - **Price (P):** Calculate the log return of the price, anchored to the opening price. To prevent extreme values from skewing the results, either divide by the maximum absolute log return within the window or apply min-max scaling to the range [-1, 1].

   - **Volume (V):** Apply a log transformation followed by robust scaling using the median and interquartile range (IQR). This mitigates the impact of extreme volume spikes and ensures the majority of volume data points fall within a defined range.

2. **Performance Metrics Calculation:** Calculate Jensen's Alpha and the Sharpe Ratio to quantify the risk-adjusted return of the trading strategy. These metrics provide a robust assessment of the model's ability to generate excess returns compared to a benchmark, considering the level of risk taken.

3. **Feature Attribution Analysis:** Utilize Grad-CAM or SHAP to understand the influence of each feature (price, volume, time) on the model's predictions. This facilitates:

   - **Validation:** Confirming the model focuses on relevant patterns rather than spurious correlations.
   - **Debugging and Refinement:** Identifying potential weaknesses and areas for improvement in the model.
   - **Transparency and Interpretability:** Providing insights into the model's decision-making process.

By combining quantitative performance metrics with qualitative insights from feature attribution, we gain a comprehensive understanding of the model's effectiveness and its decision-making rationale. This combined approach allows for informed decisions regarding model refinement and deployment.

### B. Performance Evaluation

This section details the performance evaluation methodology used to assess the effectiveness of the trading strategies developed within the SCoVA project. This includes quantitative metrics for assessing profitability and risk-adjusted returns, as well as feature attribution techniques for understanding the model's decision-making process.

**Performance Metrics:**

- **Jensen's Alpha:** This metric measures the risk-adjusted return of the trading strategy compared to a market benchmark. A positive alpha indicates that the strategy outperforms the market after accounting for risk.

- **Sharpe Ratio:** This metric quantifies the risk-return trade-off of the trading strategy. It calculates the excess return per unit of risk, where risk is measured by the standard deviation of portfolio returns. A higher Sharpe ratio indicates better risk-adjusted performance.

**Feature Attribution:**

Understanding the model's decision-making process is crucial for validating its effectiveness and identifying potential biases. Feature attribution techniques, such as Grad-CAM and SHAP, will be employed to determine which features of the transformed input data (derived from candlestick and volume data) are most influential in driving trading decisions. Specifically, analyzing the impact of features within the dynamic plane snapshots (generated after PCA rotation and normalization of time, log return, and log volume) will be key to understanding how the model leverages these transformed inputs. This analysis can be correlated back to the original candlestick and volume data, revealing whether the model focuses on relevant market patterns. Furthermore, understanding the influence of data transformations (log returns, PCA rotation, normalization) on feature importance will provide insights into the effectiveness of these preprocessing steps.

### B. Performance Evaluation (Enforcer)

This section details the methods used to rigorously evaluate the performance of the trading models developed in this project. The goal is to provide a comprehensive assessment of model profitability, risk profile, and decision-making process. Several analyses will be conducted, leveraging both traditional financial metrics and interpretability tools.

1. **Standard Performance Metrics:** The models will be evaluated using standard financial metrics to quantify their risk-adjusted returns.

   - **Jensen's Alpha:** This metric measures the risk-adjusted return of the trading strategy compared to a chosen market benchmark. A positive alpha indicates outperformance relative to the market after accounting for risk.

   - **Sharpe Ratio:** This ratio quantifies the risk-return trade-off of the strategy, providing insights into its consistency. A higher Sharpe Ratio suggests better risk-adjusted performance.

2. **Multi-Scale Temporal Model Evaluation:** The performance of the multi-scale temporal model, which incorporates predictions from different time horizons (e.g., intraday, daily, weekly, monthly), will be compared against a baseline intraday-only model. This comparison will quantify the benefits of integrating longer-term market context. Performance will be assessed during both normal market conditions and significant market events to understand the model's robustness and adaptability to volatility.

3. **Periodicity Weighting Scheme Optimization:** The impact of different weighting schemes for various periodicities (daily, weekly, monthly, quarterly, and yearly) will be evaluated. The goal is to determine the optimal balance for leveraging these different timeframes to improve predictive power and capture cyclical patterns, including after-market forces not directly reflected in intraday data.

4. **Interpretability and Feature Attribution:** To understand the model's decision-making process, feature attribution techniques like Grad-CAM or SHAP will be employed. These tools will provide insights into which features (e.g., candlestick patterns, technical indicators, or specific periodicity data) are most influential in driving buy/sell decisions. This analysis will help identify potential biases, improve model interpretability, and enhance robustness. This analysis will also be used to evaluate the impact of different timeframes on model predictions, informing weighting strategies and potential architectural refinements for multi-scale models.

### B. Performance Evaluation (Enforcer)

This section details the performance evaluation of the developed trading models. This evaluation combines quantitative metrics with qualitative insights derived from feature attribution techniques to provide a comprehensive assessment of model efficacy.

**Performance Metrics:**

Two key performance indicators will be calculated:

- **Jensen's Alpha:** This metric measures the risk-adjusted performance of the strategy by comparing the portfolio's return to the expected return given its beta. A positive alpha suggests the strategy outperformed its expected return, while a negative alpha indicates underperformance.

- **Sharpe Ratio:** This metric quantifies the risk-return trade-off by calculating the excess return per unit of volatility (standard deviation). A higher Sharpe Ratio indicates better risk-adjusted performance. The calculation will incorporate the portfolio returns and the risk-free rate of return.

**Feature Attribution:**

Understanding the rationale behind model predictions is crucial for trust and refinement. The following feature attribution tools will be employed:

- **Grad-CAM (Gradient-weighted Class Activation Mapping):** For CNN-based models, Grad-CAM will visualize the influential regions within the input candlestick images. This helps identify the visual patterns driving the model's buy/sell decisions.

- **SHAP (SHapley Additive exPlanations):** For models like Vision Transformers (ViT) or those incorporating external features, SHAP values will quantify each feature's contribution to the final prediction. This provides insights into feature importance and helps ensure the model's decisions are based on meaningful factors.

By combining these performance metrics and feature attribution techniques, a thorough evaluation of the models is ensured. This provides confidence in their efficacy and identifies potential areas for improvement.

### B. Performance Evaluation

This section details the performance evaluation of the developed trading strategies. A robust evaluation involves both quantitative metrics and qualitative insights into the model's decision-making process.

1. **Quantitative Performance Metrics:** Standard financial metrics will be used to assess the risk-adjusted returns of the strategy. These include:

   - **Jensen's Alpha:** Measures the risk-adjusted return compared to a market benchmark.
   - **Sharpe Ratio:** Quantifies the return achieved per unit of risk taken.

2. **Qualitative Analysis with Feature Attribution:** Understanding the model's decision-making process is crucial. Feature attribution techniques, such as Grad-CAM and SHAP (SHapley Additive exPlanations), will be employed to visualize and interpret the influence of different features (e.g., aspects of candlestick charts) on the model's predictions. This analysis helps identify potential biases and provides valuable insights for model refinement.

3. **Backtesting Enhancements:** The backtesting visualization will be enhanced to provide a comprehensive performance overview, including:

   - **Real-time Metric Grid:** A grid displaying key performance indicators (Sharpe Ratio, Sortino Ratio, Max Drawdown, Win Rate, Profit Factor, and Average Trade Duration) in real-time during backtesting.
   - **Trade Return Distribution:** A visual representation of the distribution of trade returns to understand the consistency and characteristics of profits and losses.

4. **Advanced Performance Analysis:** Beyond standard metrics, the following analyses will be conducted:

   - **Context Awareness:** Evaluate performance based on the optimal number of candlesticks per frame and the total number of frames used as input.
   - **Transfer Learning Effectiveness:** Assess the generalizability of learned patterns by evaluating model performance across different markets (e.g., US and India). Comparisons will include US-US, US-India, and India-India market transfers.
   - **Context-Aware Periodicity:** Assess performance using weighted predictions based on different time periodicities (daily, weekly, monthly, quarterly, and yearly), optimizing the configuration of frames, candles per frame, and stock categorization for each.
   - **Principal Component Analysis (PCA):** Repeat the performance evaluation using a dynamic plane constructed from the first two principal components to assess performance in a reduced dimensionality representation.
   - **Hyperparameter Permutation Testing:** Establish baseline performance levels by training the model for a single epoch using all possible hyperparameter combinations within the specified date ranges. This will identify potentially promising regions for further hyperparameter tuning.

### B. Performance Evaluation (Enforcer)

This section details the key performance indicators (KPIs) and analytical tools used to rigorously evaluate the trading agent's performance, ensuring alignment with both financial objectives and the project's ethical framework grounded in Dharmic principles.

1. **Quantitative Performance Metrics:** Standard financial metrics will provide a baseline assessment of the trading agent's profitability and risk-adjusted returns.

   - **Jensen's Alpha:** This metric measures the risk-adjusted return of the portfolio compared to the market benchmark. A positive alpha indicates the strategy outperformed its expected return given its risk profile.

   - **Sharpe Ratio:** This ratio quantifies the risk-return tradeoff by calculating the excess return (portfolio return minus the risk-free rate) per unit of volatility. A higher Sharpe Ratio signifies better risk-adjusted performance.

2. **Feature Attribution Analysis:** Understanding the rationale behind the model's trading decisions is crucial for enhancing trust, refining the model, and identifying potential biases. We will employ the following feature attribution techniques:

   - **Grad-CAM (Gradient-weighted Class Activation Mapping):** For CNN-based models, Grad-CAM will visualize the areas of the input candlestick charts that most influence the model's predictions, providing insights into learned patterns.

   - **SHAP (SHapley Additive exPlanations):** SHAP values offer a game-theoretic approach to explain individual predictions by quantifying each feature's contribution. This allows us to understand the driving factors behind specific trade decisions.

3. **Ethical Performance Evaluation:** Beyond traditional financial metrics, this project integrates ethical considerations into the evaluation process, aligning with the Dharmic principles outlined in the project architecture. This involves assessing the agent's performance in relation to:

   - **Integration with Zerodha KiteConnect API:** Evaluating performance within a live trading environment using KiteConnect for data acquisition, order execution, and portfolio management provides a realistic measure of the agent's capabilities.

   - **Alignment with the Bhagavad Gita's Four Paths to Liberation (Gyaan, Bhakt, Karam, Raaj):** We will develop specific metrics to quantify how the agent embodies these principles, potentially focusing on efficient knowledge application (Gyaan), disciplined execution (Karam), and controlled risk management (Raaj).

   - **Adherence to Dharmic Principles (Satya, Shaucha, Santosha):** Metrics will be defined to assess the agent's ethical conduct, including verifying data integrity (Satya), and evaluating the balance between profit and stability (Santosha). Consistent, ethical gains will be prioritized over volatile returns obtained through questionable tactics.

Further development of this section requires defining specific, quantifiable metrics for evaluating performance based on the philosophical and ethical considerations outlined above. This will involve exploring how these abstract principles translate into concrete, measurable outcomes in a trading context.

### B. Performance Evaluation (Enforcer)

This section details the performance evaluation procedures for the Swaha project's trading strategy. It outlines the metrics used to assess trading performance, the methods employed to understand model decision-making, and considerations for computational efficiency.

1. **Performance Metrics:** Two key performance indicators will be calculated:

   - **Jensen's Alpha:** This metric quantifies the risk-adjusted return of the trading strategy relative to its expected return, given its beta. A higher alpha suggests superior performance. The specific formula used, along with a clear explanation of its variables and implementation details within the codebase (including libraries and functions), will be documented. Example calculations using representative data points will be provided, along with an interpretation of the results and a discussion of the metric's limitations.

   - **Sharpe Ratio:** This metric assesses the risk-return trade-off of the strategy by calculating the ratio of excess return to the standard deviation of returns. A higher Sharpe Ratio indicates better risk-adjusted performance. Similar to Jensen's Alpha, the calculation details, implementation, interpretation, and limitations will be thoroughly documented.

2. **Feature Attribution:** Understanding the model's decision-making process is crucial. Either Grad-CAM or SHAP will be utilized to gain insights into the influence of different features (aspects of the candlestick charts) on model predictions. The rationale for selecting the chosen method, its technical implementation details (including code snippets and relevant references), and the interpretation of its output will be documented. Visualizations of the attribution results, along with the libraries and functions used to generate them, will be included. Potential challenges and limitations of the chosen method, particularly in the context of candlestick image data, will be discussed.

3. **Computational Efficiency:** While not a direct performance metric, computational cost is an important consideration. This section will address the computational resources required for image generation and processing, and discuss strategies for optimization to balance performance and efficiency.

### B. Performance Evaluation

This section focuses on evaluating the performance and stability of the Progressive Web App (PWA), particularly concerning GPU-intensive tasks like Vision Transformer training on an iPad. This evaluation addresses key user concerns regarding potential performance bottlenecks, instability due to resource constraints, and informs architectural decisions.

The evaluation will encompass the following:

- **PWA Performance and Stability Data Collection:** Data will be gathered on TensorFlow.js and WebGPU performance and stability within iPadOS. This includes researching current browser memory limits for PWAs and typical crash rates for GPU-intensive workloads. The goal is to validate user concerns regarding potential performance limitations and instability stemming from resource constraints, including the 50MB cache limit. Cross-platform inconsistencies will also be considered.

- **Feasibility Analysis of Vision Transformer Training:** A feasibility analysis will be conducted to determine the viability of training a Vision Transformer model on an iPad within a PWA. This analysis will focus on resource constraints, particularly GPU access and memory limitations.

- **iPad GPU Constraint Investigation:** This investigation will explore the specific challenges of accessing iPad GPU resources from a browser-based PWA and assess their impact on training speed and stability. Different web standards and potential workarounds will be explored to mitigate these limitations and ensure a stable and optimized application.

- **Addressing User Concerns about Stability:** User concerns regarding the stability of running computationally intensive tasks within a PWA on an iPad will be directly addressed. The analysis will focus on mitigating the risks of browser crashes due to resource limitations and extended processing times. Strategies such as using web workers and careful memory management will be considered.

This comprehensive performance evaluation will inform decisions about the PWA's architecture and implementation, especially the balance between client-side and server-side processing. The results will determine the overall feasibility of the project and guide optimization efforts.

### B. Performance Evaluation

This section details the metrics and methods used to evaluate the performance of the trained trading models. This involves calculating key performance indicators (KPIs) and leveraging feature attribution techniques to understand the model's decision-making process. The evaluation occurs primarily on the client-side (iOS app) after the trained model is deployed. The Python backend's role is limited to providing the necessary raw data and potentially storing the resulting performance metrics.

**1. Performance Metrics:**

- **Jensen's Alpha:** This metric measures the risk-adjusted return of the portfolio compared to its expected return given its beta. A positive alpha indicates the portfolio outperformed its benchmark, while a negative alpha suggests underperformance.

- **Sharpe Ratio:** This metric quantifies the risk-return trade-off by calculating the excess return per unit of standard deviation. A higher Sharpe ratio signifies better risk-adjusted performance.

**2. Feature Attribution:**

Feature attribution techniques provide insights into the model's decision-making process by identifying the influence of different input features (aspects of the candlestick charts) on the predictions. This analysis aids in understanding the model's behavior, identifying potential biases, and potentially improving its performance. Two prominent methods will be employed:

- **Grad-CAM (Gradient-weighted Class Activation Mapping):** Grad-CAM visually highlights the regions of the input candlestick chart that are most influential in the model's prediction, allowing for a better understanding of which patterns or features drive the buy/sell signals.

- **SHAP (SHapley Additive exPlanations):** SHAP values provide a more rigorous and theoretically sound approach to feature attribution by quantifying the contribution of each feature to a specific prediction. This allows for a deeper understanding of how each day within the candlestick window contributes to the overall trade signal.

Before final evaluation and deployment to other platforms (web and Android), the model undergoes a final fine-tuning process to optimize performance on the target hardware. Furthermore, image processing is streamlined for deployment to ensure efficiency on resource-constrained devices.

### B. Performance Evaluation (Enforcer)

This section details the performance evaluation of the trading strategy. We will use two key performance indicators (KPIs): Jensen's Alpha and the Sharpe Ratio. Furthermore, to understand the model's decision-making process, we will employ feature attribution techniques.

**1. Jensen's Alpha Calculation:**

[Detailed explanation of how Jensen's Alpha will be calculated, including the chosen benchmark (e.g., S&P 500), the time period for calculation, and any specific considerations related to the trading strategy. Include the formula used and data sources for the risk-free rate of return and market return.]

**2. Sharpe Ratio Calculation:**

[Detailed explanation of how the Sharpe Ratio will be calculated, including the risk-free rate used, the time period for calculation, and any adjustments made for trading frequency. Include the formula used, the data source for the risk-free rate of return, and how negative returns are handled.]

**3. Feature Attribution:**

We will use Grad-CAM and SHAP to gain insights into the model's predictions.

- **Grad-CAM:** [Explanation of how Grad-CAM will be applied to the CNN model to visualize the importance of different regions within the candlestick chart images.]

- **SHAP:** [Explanation of how SHAP values will be calculated and interpreted to understand the contribution of individual features to the model's output within the context of the trading strategy.]

This comprehensive performance evaluation will assess the trading strategy's effectiveness and identify potential areas for improvement.

### B. Performance Evaluation

This section details the key performance indicators (KPIs) and interpretability methods used to rigorously assess the effectiveness of the developed trading models. The primary goal is to quantitatively measure the model's ability to generate excess returns compared to a benchmark, and to understand the underlying decision-making process.

1. **Performance Metrics:** The model's performance will be evaluated using Jensen's Alpha and the Sharpe Ratio.

   - **Jensen's Alpha:** This metric quantifies the risk-adjusted return of the model compared to the expected return given its beta. A positive alpha indicates that the model outperforms its benchmark, while a negative alpha suggests underperformance.

   - **Sharpe Ratio:** This metric calculates the risk-adjusted return relative to the portfolio's volatility (standard deviation of returns). A higher Sharpe Ratio signifies superior risk-adjusted performance.

   These metrics provide a comprehensive view of the model's ability to generate consistent and risk-adjusted returns. The specific benchmark used and the time period over which these metrics will be calculated (e.g., daily, weekly, monthly, annualized) will be defined in the experimental setup.

2. **Feature Attribution:** To gain insights into the model's decision-making process, feature attribution techniques such as Grad-CAM (Gradient-weighted Class Activation Mapping) and SHAP (SHapley Additive exPlanations) values will be employed.

   - **Grad-CAM:** This technique visually highlights the regions of the input candlestick chart images that are most influential in driving the model's predictions.

   - **SHAP Values:** These values quantify the contribution of individual features to each prediction, providing a more granular understanding of feature importance.

   Applying these methods will reveal the patterns and features the model leverages for its predictions, aiding in model refinement, validation, and the identification of potential biases. The specific application of these tools to the chosen models (e.g., CNNs, ViTs) and the interpretation of the results will be detailed further in the implementation section.

### B. Performance Evaluation

This section details the methods used to evaluate the performance of the trading strategies developed within the project. Robust evaluation is crucial for understanding the strengths and weaknesses of the models and for making informed decisions about deployment.

1. **Performance Metrics:** The core performance metrics used are Jensen's Alpha and the Sharpe Ratio. Jensen's Alpha measures the risk-adjusted return of the strategy compared to the market benchmark. A positive alpha indicates that the strategy outperformed its benchmark, while a negative alpha suggests underperformance. The Sharpe Ratio measures the risk-adjusted return of the strategy itself, considering its volatility. A higher Sharpe Ratio signifies better risk-adjusted performance. These metrics will be calculated over the backtesting period and analyzed to assess the overall effectiveness of the developed strategies.

2. **Feature Attribution:** To understand the model's decision-making process and gain insights into the importance of different features, we will employ feature attribution techniques such as Grad-CAM (Gradient-weighted Class Activation Mapping) and SHAP (SHapley Additive exPlanations). Grad-CAM visually highlights the regions of the input data (e.g., candlestick charts) that are most influential for the model's predictions. SHAP values quantify the contribution of each feature to individual predictions, providing a more granular understanding of the model's behavior. This analysis helps identify which aspects of the input data are driving the trading signals, allowing for further refinement of the model architecture and feature engineering process.

### B. Performance Evaluation

This section details the methods used to rigorously evaluate the performance of the trained trading models. The goal is to assess their profitability and understand their risk-adjusted returns, as well as the underlying decision-making process.

1. **Performance Metrics (Jensen's Alpha and Sharpe Ratio):** The models' performance will be evaluated using Jensen's Alpha and the Sharpe Ratio. Jensen's Alpha measures the risk-adjusted excess return of the strategy compared to a market benchmark. A positive alpha indicates outperformance after accounting for risk. The Sharpe Ratio quantifies the return per unit of risk, calculated as the excess return (portfolio return minus the risk-free rate) divided by the portfolio's standard deviation. A higher Sharpe Ratio signifies better risk-adjusted performance. These calculations will be based on simulated trading results over the backtesting period.

2. **Feature Attribution (Grad-CAM or SHAP):** To understand the models' decision-making process, we will employ feature attribution techniques such as Grad-CAM (Gradient-weighted Class Activation Mapping) or SHAP (SHapley Additive exPlanations). These tools help identify which features of the input data (e.g., candlestick chart patterns) are most influential in driving the models' predictions. This analysis can provide valuable insights for model refinement, validation, and a deeper understanding of the relationship between input features and trading signals.

### B. Performance Evaluation (Enforcer)

This section details the rigorous evaluation of the developed trading model's performance using quantitative metrics and qualitative insights derived from feature attribution techniques.

1. **Performance Metrics:** The model's performance will be assessed using Jensen's Alpha and the Sharpe Ratio. Jensen's Alpha measures the risk-adjusted excess return compared to a market benchmark. A positive alpha indicates outperformance, while a negative alpha suggests underperformance. The Sharpe Ratio quantifies the risk-adjusted return per unit of volatility. A higher Sharpe Ratio signifies a better risk-return trade-off. These metrics will be calculated over the backtesting period to compare different model configurations and assess overall effectiveness.

2. **Feature Attribution:** Understanding the model's decision-making process is crucial. Therefore, feature attribution tools like Grad-CAM and SHAP will be employed. Grad-CAM visually highlights the regions of the candlestick chart images that the model focuses on when making predictions, providing insights into influential patterns. SHAP values quantify the contribution of each feature to specific predictions, allowing for a granular understanding of feature importance. This analysis aids in identifying potential biases, validating the model's learning, and deepening our understanding of the relationship between market patterns and predicted returns.

## B. Performance Evaluation

This section details the performance evaluation of the trading models, focusing on both traditional metrics and specialized techniques designed to assess robustness, particularly in the context of unexpected market events ("shocker events"). The evaluation will leverage standard financial metrics alongside a Cognitive Threat Analysis Module (CTAM) and feature attribution tools.

First, standard performance metrics such as Jensen's Alpha and the Sharpe Ratio will be calculated to quantify risk-adjusted returns and benchmark the model against market performance. This provides a baseline assessment of profitability and efficiency.

Second, recognizing the limitations of traditional metrics in capturing model behavior during market anomalies, we will employ the CTAM. This module is designed to detect and analyze "shocker events" within the time series data by identifying patterns indicative of these events, such as volatility spikes, volume anomalies, and rapid price changes. Lightweight CNNs within the CTAM will analyze financial charts for these patterns, balancing detection accuracy with real-time computational efficiency. The CTAM will generate a "Threat Level" score which will inform the DynamicPlaneGenerator's behavior. This interaction, specifically how the DynamicPlaneGenerator adjusts its smoothing function or learning rate in response to the CTAM's score, requires careful definition and testing.

Third, to understand the model's decision-making process and enhance interpretability, we will use feature attribution tools like Grad-CAM or SHAP. These tools will identify the most influential features (e.g., specific candlestick patterns, volume changes) driving the buy/sell signals generated by the CNN models. This analysis is crucial for model refinement, identifying potential biases, and building trust in the system, especially in the context of the CTAM, where understanding the source of "threat" signals is paramount for effective mitigation.

Finally, the evaluation must acknowledge the current model architecture's limitations, specifically its inherent mean reversion bias due to the error-correction mechanism. This bias may lead to underperformance during periods of high volatility or "shock" events. Furthermore, the simplified "wound" and "healing" analogy of market dynamics limits the model's ability to capture the complex interplay of market forces. Future work will explore a dual-system approach ("exploitation" for stable markets and "exploration" for volatile markets) to address these limitations and allow for a more nuanced and comprehensive performance evaluation in diverse market conditions.
Performance Evaluation (Enforcer)

This section details the performance evaluation process for the algorithmic trading strategies. This process involves calculating key performance indicators, employing feature attribution techniques, and monitoring the health and resource consumption of training and backtesting jobs. The Enforcer role is responsible for ensuring the integrity and stability of these processes.

**Key Performance Indicators (KPIs):**

- **Jensen's Alpha:** This metric measures the risk-adjusted return of the strategy compared to a market benchmark. A positive alpha indicates that the strategy outperformed its benchmark, while a negative alpha suggests underperformance.

- **Sharpe Ratio:** This ratio assesses the return per unit of risk. A higher Sharpe Ratio indicates a better risk-adjusted return.

**Feature Attribution:**

Feature attribution techniques, such as Grad-CAM and SHAP, will be employed to understand the model's decision-making process. These methods identify the input features that contribute most significantly to the model's predictions, providing insights into the patterns the model learns and informing further model refinement.

**Process Monitoring and Control:**

- **Real-time Process Monitor:** A dedicated service will monitor the health and resource consumption (CPU usage, memory usage, disk I/O, etc.) of all running training and backtesting jobs. This monitor will also detect potential issues such as training stagnation.

- **Execution State Controller:** This service manages the state of ongoing training and backtesting processes, providing functionalities for pausing, resuming, and terminating jobs. This ensures efficient resource management and prevents runaway processes.

### B. Performance Evaluation (Enforcer)

This section details the performance evaluation of the trading strategies developed within the system. Given the Enforcer's role in managing data access and ensuring the integrity of the evaluation process, this section focuses on the rigorous assessment and validation of results using established financial metrics and in-depth analysis leveraging feature attribution techniques.

The Enforcer plays a critical role in guaranteeing that the performance evaluation process accesses data and resources in a controlled and secure manner. This includes historical data, trade records, and market information, ensuring calculations are consistent and adhere to predefined rules. This is especially critical when dealing with sensitive financial data. Furthermore, by strictly channeling all resource access through the Enforcer, unauthorized access is prevented, and data integrity is maintained.

1. **Performance Metrics Calculation:** The core performance metrics will be Jensen's Alpha and the Sharpe Ratio. Jensen's Alpha measures the risk-adjusted return of the strategy compared to the market benchmark, providing insights into the value added by the model. The Sharpe Ratio quantifies the risk-return trade-off, representing the excess return per unit of risk. The Enforcer ensures consistent and standardized calculation methodologies for these metrics across different model configurations and experiments.

2. **Feature Attribution Analysis:** Understanding the model's decision-making process is crucial for both performance analysis and model refinement. The Enforcer manages the data flow required for feature attribution tools like Grad-CAM (Gradient-weighted Class Activation Mapping) and SHAP (SHapley Additive exPlanations). These tools will be employed to determine the influence of different input features on the model's predictions. This analysis provides insights into which aspects of the input data are driving the trading signals, allowing for a better understanding of the model's behavior and potential areas for improvement. Furthermore, this analysis contributes to ensuring the causal grounding of predictions, connecting model outputs to understandable market patterns.

The Enforcer's implementation utilizes class inheritance to maintain a clear and organized structure for managing access to different resources required for performance evaluation. This inheritance structure can define specific access control rules for various data types, ensuring that performance calculations adhere to strict data governance principles. Additionally, the Enforcer isolates specialist components like feature attribution tools, managing their interaction with the core evaluation framework and ensuring their outputs are correctly integrated into the overall performance analysis. Where long-running performance evaluations are necessary, the Enforcer can manage asynchronous communication using a pub/sub model, ensuring results are delivered without blocking the main workflow.
Performance Evaluation

This section details the key performance indicators (KPIs) and analytical tools used to assess the effectiveness of the developed trading strategies. This evaluation encompasses both established financial metrics and advanced techniques for understanding model behavior.

- **Core Performance Metrics:** The primary performance metrics will be Jensen's Alpha and the Sharpe Ratio. Jensen's Alpha measures the risk-adjusted return of the strategy compared to a market benchmark, while the Sharpe Ratio quantifies the risk-return trade-off by calculating the excess return per unit of volatility. These metrics provide a robust assessment of the strategy's profitability and consistency relative to market risk.

- **Feature Attribution Analysis:** To gain deeper insights into the model's decision-making process, feature attribution tools like Grad-CAM or SHAP will be employed. These techniques visualize the influence of different input features (e.g., open, close, high, low prices, volume) on the model's predictions. This analysis helps identify potential biases, improves model interpretability, and can inform further model refinement and feature engineering efforts.

- **Predictive Time Horizon (Rally Time Prediction):** The system will also predict the expected duration (measured in candlestick intervals) for a predicted price movement to occur. This "Rally Time" prediction provides additional context for evaluating trading decisions, allowing for a more nuanced assessment of potential profits and risks. This prediction can also be incorporated into the performance evaluation by assessing its accuracy and its contribution to overall trading strategy effectiveness.

### B. Performance Evaluation

This section details the process of evaluating the performance of the trading agent within the SCoVA project. This involves calculating standard performance metrics, utilizing explainable AI (XAI) techniques for insight into model behavior, and assessing the effectiveness of supporting components like portfolio construction and risk management.

1. **Quantitative Performance Metrics:** Standard financial metrics will be used to quantify the agent's performance. These include:

   - **Jensen's Alpha:** Measuring risk-adjusted return compared to the market benchmark.
   - **Sharpe Ratio:** Assessing the risk-return trade-off by considering return consistency relative to volatility.

   These metrics will be calculated over the backtesting period and used to compare different model configurations and hyperparameters. A robust backtesting engine, incorporating real-world market conditions like slippage, commissions, and market impact (using libraries like Backtrader or Zipline), is crucial for accurate calculation and avoids overly optimistic performance estimates.

2. **Explainable AI (XAI) and Narrative Generation:** Understanding the rationale behind model predictions is crucial for both model improvement and trust. Therefore, the following techniques will be employed:

   - **Feature Attribution:** Tools like Grad-CAM or SHAP will highlight the input features (e.g., regions in candlestick images) most influential in driving buy/sell decisions, providing insights into learned patterns and potential biases.
   - **Narrative Generation Service:** A dedicated service will generate human-readable explanations for each trade. This service will integrate with the Feature Store to retrieve the relevant versioned input features and system state, and utilize XAI methods (LIME, SHAP, attention maps) to pinpoint influential factors. This provides a qualitative assessment of the strategy's decision-making process.

3. **Component Performance Evaluation:** Beyond core trading agent metrics, the performance of supporting components is crucial:

   - **Portfolio Construction:** The chosen optimization algorithm (e.g., Mean-Variance Optimization, Risk Parity) will be evaluated using metrics like portfolio turnover, diversification ratios, and consistency.
   - **Risk Management:** The impact of risk rule interventions (e.g., maximum drawdown breaches) on the portfolio's return and risk profile will be assessed.
   - **Adaptive Blending (if applicable):** If an adaptive blending mechanism is employed, the weighting decisions and their correlation with market conditions will be tracked to evaluate its effectiveness across different market regimes.

## B. Performance Evaluation (Enforcer)

This section details the comprehensive performance evaluation process, encompassing both quantitative metrics and a realistic simulated trading environment. This approach ensures robust assessment of the trading strategy before deployment to live markets.

**Quantitative Performance Metrics:**

Key performance indicators, such as Jensen's Alpha and the Sharpe Ratio, will be calculated to assess the risk-adjusted returns of the trading strategy. These metrics provide a quantitative measure of the strategy's performance compared to a benchmark and its consistency over time. Furthermore, feature attribution tools like Grad-CAM or SHAP will be employed to understand the influence of different features on the model's predictions, providing insights into the model's behavior and potential areas for improvement. This feature importance analysis can also inform the narrative explanations generated for individual trades, enhancing transparency and understanding.

**Simulated Trading Environment:**

A robust paper trading environment is crucial for thorough testing before live market deployment. This environment will leverage a new Enforcer service, the `Paper_Brokerage_Simulator`, designed to mimic the Zerodha Kite Connect API. This simulator will facilitate realistic order execution and portfolio management, mirroring live trading functionality.

- **Realistic Fills and Live Data:** The `Paper_Brokerage_Simulator` will utilize live WebSocket tick data and Zerodha market depth data to simulate realistic fills, including partial fills, based on actual bid/ask prices and volume. This real-time data integration ensures accurate representation of market impact on order execution and overall strategy performance. A comprehensive review of all current usages of market depth data will be conducted to ensure consistency and appropriate utilization within the system.

- **Order Book Feature Engineering:** To further enhance the realism and informativeness of the simulation, a new specialist service, `DeriveOrderBookFeatures`, will process raw market depth data and generate derived features such as Order Book Imbalance (OBI), Weighted Average Price (WAP), and Bid-Ask Spread. These derived features will be valuable inputs to the `DynamicPlaneGenerator`, improving the model's understanding of market dynamics.

- **Paper Portfolio Management and UI Integration:** The `Paper_Brokerage_Simulator` will manage the paper portfolio state, including cash balance, positions, and order statuses, persisting this information in a dedicated Firestore collection for analysis and historical tracking. The `Live_Execution_Enforcer` will be modified to support both LIVE and PAPER trading modes, controlled by a toggle within the Live Trading Dashboard UI. This UI update will also display paper portfolio performance, providing valuable insights without risking real capital.

This combined approach of quantitative analysis and realistic simulation, coupled with explainable AI (XAI) techniques such as narrative generation powered by LLM integration and recording within the Karma Ledger, aims to provide a thorough understanding of the agent's performance and decision-making process, building trust and ensuring regulatory compliance.

### B. Performance Evaluation

This section details the performance evaluation methodologies used to assess the effectiveness of the developed trading strategies. This involves both traditional financial metrics and a novel "Anxiety Model."

**Traditional Performance Metrics:** Standard performance metrics, such as Jensen's Alpha and the Sharpe Ratio, will be calculated to quantify the risk-adjusted returns. These metrics provide a benchmark for comparison and evaluate the overall effectiveness of the models.

**Anxiety Model:** A key component of this evaluation is the implementation and analysis of an "Anxiety Model." This meta-learning model aims to predict the behavior of the primary trading algorithm (DynamicPlane) and provide an "Anxiety Level" based on real-time market depth data. This functions as a real-time performance monitoring and diagnostic tool.

- **Training:** The Anxiety Model will be trained using historical backtest data. The target variable for training is the Total Error (Vector Deviation + Frame Shift) observed from DynamicPlane during backtesting.

- **Input Features:** The model will use several features derived from high-frequency market depth data, including:

  - Order-to-Quantity Ratio
  - Rate of Change of Order Book Imbalance (OBI)
  - Level 1 Dominance
  - Book "Flicker" Rate

- **Post-Hoc Analysis:** A crucial step involves correlating DynamicPlane's actions (stored in a feature store) with historical order book data. This analysis trains the Anxiety Model to recognize patterns indicative of potential errors or the need for model adjustments.

- **Real-Time Monitoring:** The Anxiety Model will continuously monitor the live stream of market depth data to provide real-time "Anxiety Levels," offering insights into the current state and potential risks associated with DynamicPlane's operation. This allows for dynamic adjustments and risk management.

## B. Performance Evaluation

This section details the performance evaluation of the trading strategies and the underlying SCoVA (Snapshot Computer Vision Algorithm) model. The evaluation uses a combination of traditional performance metrics, feature attribution techniques, and a novel approach emphasizing downside risk and market regime awareness. SCoVA's core functionality—using discrete, dynamically generated visual snapshots of market data rather than continuous time series—is central to interpreting these results. This "snapshot" approach influences both the model's architecture and the features engineered for performance analysis.

**1. Traditional Performance Metrics:**

The primary performance indicators are Jensen's Alpha and the Sharpe Ratio. Jensen's Alpha measures the risk-adjusted return of the strategy compared to a market benchmark, indicating the strategy's ability to generate excess returns. The Sharpe Ratio assesses the risk-return trade-off, quantifying the return earned for each unit of risk taken.

**2. Feature Attribution:**

To understand SCoVA's decision-making process, feature attribution tools like Grad-CAM (Gradient-weighted Class Activation Mapping) and SHAP (SHapley Additive exPlanations) will be employed. These techniques provide visual and quantitative insights into which features of the candlestick images (and other inputs) are most influential in the model's predictions, facilitating model refinement and identifying potential biases.

**3. Asymmetric Risk Management and Feature Engineering:**

Beyond traditional metrics, the evaluation emphasizes downside risk and market regime awareness through specialized loss functions and engineered features:

- **Risk-Averse and Asymmetric Loss Functions:** Model training incorporates a custom loss function designed to prioritize the avoidance of large losses. This asymmetric loss function applies varying penalty factors depending on the magnitude and direction of the prediction error, penalizing the underestimation of losses more severely than overestimation or accurate predictions of gains.

- **Asymmetric Feature Engineering:** The `AsymmetricFeatureEngine` service calculates features that describe asymmetries in recent price action and volume, providing context for the model and enabling a deeper analysis of market dynamics. These features are used as context tokens for the Vision Transformer. Key features include:
  - **Price & Volatility Asymmetry:** Upside vs. Downside Volatility (using semi-deviation), Volatility Skewness, and Volatility Kurtosis.
  - **Volume & Participation Asymmetry:** Accumulation/Distribution Ratio and Order-to-Quantity Asymmetry.
  - **Correlation Asymmetry:** Price-Volume Correlation State for positive and negative return candles.

**4. Trading Modes:**

Based on an Anxiety Model's assessment of market conditions and error potential, the system dynamically switches between "flow" and "shock" trading modes. The performance of these modes will be evaluated as a critical aspect of the overall strategy. Further details on the Anxiety Model will be provided in the relevant section.

This multifaceted evaluation framework, incorporating both standard metrics and specialized analyses, provides a comprehensive assessment of the trading strategy's performance and its alignment with a risk-managed approach. The integration of asymmetric features, alongside the dynamic trading modes, allows for a nuanced understanding of market behavior and the strategy's response to varying conditions.

## IV. Deployment and Monitoring

This section details the procedures for deploying the trained models and monitoring their performance in a live trading environment. While the primary focus of this project is research and model development, deployment and monitoring considerations are essential for assessing real-world applicability and identifying potential issues.

### A. Deployment

The deployment process focuses on establishing a robust link between experimental results and the underlying codebase to ensure reproducibility and facilitate further analysis.

- **Code-Experiment Linkage:** Maintain clear documentation and organization within the codebase to establish a transparent connection between experiments and their corresponding code implementations. This includes documenting the experimental setup, parameters, and resulting models.

- **Model Reproduction (Optional):** While not strictly required, providing a clear guide for reproducing the trained models is beneficial for future research and validation. This guide should detail environment setup, data acquisition and preprocessing, and model training procedures with specific parameters.

- **Targeted Documentation (Optional):** Upon request, the deployment documentation can focus on specific aspects of the code repository or individual models for targeted analysis and a more in-depth understanding.

### B. Monitoring

Monitoring deployed models involves analyzing their performance and identifying potential issues that may impact trading outcomes.

- **Performance Metrics and Cost Analysis:** Continuously monitor key performance indicators, including Jensen's Alpha and the Sharpe Ratio. Critically, track transaction costs (commissions and slippage) and assess their impact on realized alpha.

- **Constraint and Risk Management:** Evaluate the impact of short-selling constraints, particularly on small-cap stock performance. Implement a stop-loss mechanism to mitigate potential losses and a risk-based weighting scheme to optimize portfolio diversification and manage overall risk exposure.

- **Trade Analysis and Model Refinement:** Establish a clear methodology for analyzing both successful and unsuccessful trades. Understanding the reasons behind unsuccessful trades is crucial for identifying model weaknesses and informing future development. This analysis should consider the interplay between the regime classification and the raw asymmetric feature vector within the Dual-Token Context Injection approach.

## IV. Deployment and Monitoring

This section outlines the process of deploying the developed models and establishing a monitoring framework to track performance and identify potential issues. While this project focuses on research and dissertation writing, deployment considerations are crucial for reproducibility and connecting the codebase to the experimental results. These considerations also lay the groundwork for future practical applications of the SCoVA system.

### A. Deployment

The primary deployment task for this project involves establishing a clear link between the codebase and the experiments conducted. This ensures reproducibility and allows for further investigation and validation of the results.

1. **Connect Code to Experiments:** The codebase should be organized and documented to clearly illustrate how different parts of the code relate to specific experiments mentioned in the dissertation. This includes referencing specific commits, branches, or configuration files used for each experiment.

2. **Reproducibility Guide (Optional):** Providing a concise guide on how to reproduce the trained models is valuable for future research and allows others to validate the findings. This guide should detail the training process, dependencies, and specific hyperparameter settings.

3. **Targeted Documentation (Optional):** If required by stakeholders, provide focused analysis and documentation for specific parts of the repository or model architecture.

### B. Monitoring

While not implemented in this research project, a robust monitoring framework is crucial for assessing performance and managing risk in a live trading environment. The following key aspects should be considered for future deployments:

1. **Trading Cost Analysis:** Analyze the impact of trading costs, including commissions and slippage, on the agent's overall alpha. High trading costs can significantly erode profitability.

2. **Short-Selling Constraint Analysis:** Investigate the effects of short-selling constraints on performance, particularly within the small-cap market segment. These restrictions can limit profitability from declining prices.

3. **Unsuccessful Trade Analysis:** Establish a clear process for evaluating unsuccessful trades to understand the underlying causes and refine the trading strategy.

4. **Stop-Loss Mechanism:** Implement a stop-loss mechanism to mitigate potential losses by automatically exiting trades when a predetermined loss threshold is reached.

5. **Risk-Based Portfolio Weighting:** Develop and implement a risk-based weighting scheme for portfolio allocation. Adjusting position sizes based on the perceived risk of each trade can enhance overall portfolio performance.

6. **Performance Evaluation:** Calculate Jensen's Alpha and the Sharpe Ratio for each portfolio to assess performance against an equally weighted benchmark index. Deduct transaction costs of 20 basis points per round trip trade from portfolio returns before annualizing results. Use the 3-month Swedish Krona Short Term Rate (SWESTR) as the risk-free rate.

## IV. Deployment and Monitoring

This section outlines considerations for deploying the developed models and monitoring their performance in a live trading environment. While this document doesn't detail the technical deployment process (e.g., server setup, API integration), it emphasizes critical considerations for real-world application and ongoing performance analysis. Although the current focus is on research and backtesting, addressing these aspects is crucial for future development.

### A. Deployment (Facilitator)

The current codebase is structured to facilitate reproducibility of the experimental results. This tight coupling between the code and experiments ensures that reported performance metrics can be verified and traced back to specific code versions. This foundation will streamline future deployment efforts, allowing for a smooth transition from research to a live trading environment. Furthermore, the code includes documentation on setting up the environment, preprocessing data, and training the model. Specific aspects requiring deeper analysis, such as handling the transition from `mplfinance` to `matplotlib` for candlestick chart visualization and the CNN's transformation of candlestick images into numerical outputs, are also addressed within the repository's documentation. Details about input data format, size (_n_), and handling of edge cases are included to ensure complete reproducibility.

### B. Monitoring (Enforcer)

Effective monitoring is crucial for understanding and improving real-world model performance. The following key areas require careful monitoring:

- **Impact of Trading Costs:** Trading costs, including commissions and slippage, can significantly erode realized alpha. Careful monitoring and analysis of these costs are essential for accurate profitability assessments.

- **Constraints on Short Selling:** Restrictions on short selling, especially for small-cap stocks, can limit the model's ability to capitalize on bearish market predictions. Monitoring the impact of these constraints is critical for evaluating overall performance.

- **Analysis of Unsuccessful Trades:** A rigorous analysis of unsuccessful trades is vital for identifying systematic errors and refining the model. This involves understanding the reasons behind losses and determining whether specific market conditions or model limitations contributed to the outcome.

- **Stop-Loss Mechanism:** Implementing a stop-loss mechanism is essential for risk management. This mechanism automatically exits positions when losses reach a predefined threshold, mitigating potential downside. Monitoring its effectiveness is crucial for risk control.

- **Risk-Based Weighting:** A risk-based weighting scheme further enhances risk management by allocating capital based on the perceived risk of each trade. This optimizes the portfolio for risk-adjusted returns. Monitoring the implementation and performance of this scheme is critical for long-term stability and consistent returns.

## IV. Deployment and Monitoring

This section details the process of connecting the developed models to the experimental results, establishing a foundation for reproducibility, and outlining key monitoring considerations for potential real-world trading scenarios. While full deployment into a live trading environment is beyond the scope of this research, these steps enhance the rigor of the study and provide valuable insights into practical application.

### A. Deployment (Facilitator)

Reproducibility is paramount. This section outlines the steps to link the codebase to the experiments conducted, facilitating future validation and extension of the research.

1. **Connect Code to Experiments:** Each experiment's associated code version, parameters, and data are documented, creating a clear mapping between the dissertation's presented results and the corresponding code commits. This allows others to easily locate the necessary resources for reproduction.

2. **Reproduce Models (Optional):** A simplified guide for reproducing key models and experiments, potentially including scripts or Dockerfiles to recreate the development environment, can be provided if resources permit.

3. **Focus on Specific Aspects (Optional):** Upon request, deeper analysis and documentation can be provided for specific aspects of the repository or chosen models, such as a detailed exploration of the architecture or specific code modules.

### B. Monitoring (Enforcer)

Although not implemented in this academic setting, monitoring is critical for live trading. This section outlines key monitoring considerations for future practical applications of the developed models.

1. **Trading Costs:** In live trading, transaction costs (commissions, slippage, etc.) can significantly impact profitability. The potential impact of these costs on the achieved alpha will be analyzed and documented.

2. **Short-Selling Constraints:** Restrictions on short-selling, particularly in the small-cap market, can limit a strategy's ability to capitalize on downward trends. The potential impact of such constraints will be analyzed and documented.

3. **Unsuccessful Trade Evaluation:** A clear process for evaluating unsuccessful trades will be defined. This analysis will identify the causes of losses (e.g., market conditions, model limitations) and inform potential improvements to the model or trading strategy. This includes exploring confidence-based trade filtering.

4. **Stop-Loss Mechanism:** A stop-loss mechanism, while not implemented in this research, is crucial for live trading. Future implementations should incorporate a stop-loss to limit potential losses by automatically exiting positions when a predetermined loss threshold is reached.

5. **Risk-Based Weighting:** A risk-based weighting scheme, adjusting position sizes according to perceived risk, should be developed and implemented for live trading to potentially reduce portfolio volatility and maximize risk-adjusted returns. This is not implemented in the current research scope.

## IV. Deployment and Monitoring

This section addresses the practical considerations of deploying the model and monitoring its performance, including the impact of real-world trading constraints and the improvements offered by soft labeling. While the current project focuses on research and development, these aspects are crucial for transitioning the model to a live trading environment.

### A. Deployment Considerations

Although specific deployment steps are not fully detailed in the current planning stage, preparing for deployment requires considering the target environment (e.g., cloud platform, local server, backtesting framework), necessary infrastructure, and deployment procedures. This foresight will streamline the eventual transition to a live trading system. Furthermore, the implementation of soft labeling necessitates integrating these modifications into the main codebase and ensuring seamless execution within the existing training and evaluation pipelines. Documenting these steps enhances reproducibility and facilitates future adaptations of the technique.

### B. Monitoring and Evaluation

Effective monitoring is essential for evaluating the model's performance and identifying areas for improvement. This involves tracking key metrics during both development and simulated trading, with a particular focus on the impact of soft labeling and real-world trading constraints.

**Impact of Soft Labeling:**

- **Prediction Accuracy:** Monitor and compare key metrics such as precision, recall, F1-score, and the calibration of predicted probability distributions against a baseline model without soft labels to assess the effectiveness of this technique.
- **Probability Distribution:** Analyze the distribution of predicted probabilities across the discretized return bins. A well-calibrated model should exhibit appropriate uncertainty, reflected in the entropy of its predictions.
- **Discretization Effects:** Evaluate the impact of different discretization schemes (bin size and range) for the return space on the model's predictive power. Careful calibration of these parameters is crucial for optimal performance with soft labeling.
- **Model Output Stability:** Monitor the stability and convergence of the softmax output layer during training. This new layer, which generates a probability distribution over return bins, requires careful observation to identify potential issues.
- **Soft Label Generation:** Analyze the impact of the Gaussian kernel bandwidth used for converting hard labels to soft labels. An appropriate bandwidth is crucial to avoid overfitting (narrow bandwidth) or signal dilution (wide bandwidth).

**Real-World Trading Constraints:**

- **High Trading Costs:** Trading costs, including commissions and slippage, can significantly erode returns. The current strategy's potential for frequent rebalancing requires careful evaluation and potential mitigation strategies to minimize their impact. A 10% reduction in annual returns due to trading costs highlights the importance of this consideration.

By diligently monitoring these aspects, we can gain valuable insights into the model's performance, refine its parameters, and ensure its robustness in a live trading environment. This proactive approach will facilitate a smooth transition from research and development to practical deployment.

## IV. Deployment and Monitoring

This section details the deployment process and ongoing monitoring of the SCoVA agent after development and testing. The focus is on ensuring reproducibility by connecting experimental results to the codebase, and establishing robust monitoring procedures to maintain performance and manage risk in live or simulated market conditions.

### A. Deployment (Facilitator)

This subsection outlines the steps to connect the research and experimentation to the final codebase, facilitating reproducibility and further development.

1. **Connect Code to Experiments:** Establish clear traceability between the codebase and the conducted experiments, enabling straightforward verification and validation of results. This includes linking specific code segments to the corresponding experimental setups and results.

2. **Reproducibility (Optional):** Provide clear instructions or scripts to reproduce the developed models and experiments. This enhances transparency and allows for independent verification of the research findings.

3. **Focused Analysis (Optional):** Offer detailed analysis and documentation for specific aspects of the codebase or models, as required or requested.

### B. Monitoring (Enforcer)

Continuous monitoring is crucial for ensuring the long-term performance and stability of the deployed agent. The following key areas require ongoing attention:

1. **Trading Costs:** Continuously analyze the impact of trading costs (e.g., commissions, slippage) on overall performance (Jensen's Alpha). The observed discrepancy between pre-cost and post-cost alpha necessitates regular assessment and potential adjustments to the trading frequency, weighting scheme, and prediction filtering to mitigate the impact of these costs. While the +8.89% annual alpha achieved on the First North All-Share index (using the OMXS All-Share model) demonstrates potential, the +37.57% outperformance (compared to the benchmark return of -27.88%) must be considered in light of these costs.

2. **Short-Selling Constraints:** Monitor short-selling feasibility, particularly for small-cap stocks, as constraints on borrow availability can significantly impact performance in this potentially high-profit segment. This ongoing monitoring will help mitigate potential losses from unrealizable short positions.

3. **Unsuccessful Trades:** Implement a clear methodology for evaluating unsuccessful trades (where outcomes contradict model predictions). This includes investigating the reasons for discrepancies and incorporating these insights into future predictions and risk management strategies.

4. **Stop-Loss Mechanism:** Implement a dynamic stop-loss mechanism to mitigate potential losses from incorrect predictions. This involves exiting a position when it moves against the predicted direction by a predefined percentage. For example, a position with a predicted return of +3.0% could be exited if the stock drops by -2.5% within two days. This dynamic approach enhances risk management compared to a fixed holding period.

5. **Risk-Based Weighting:** Implement a risk-based weighting scheme for capital allocation. This scheme should consider factors such as prediction confidence, the inverse of historical volatility, and the signal-to-noise ratio of predicted versus actual returns observed during validation. This approach prioritizes trades with higher potential reward relative to their risk.

## IV. Deployment and Monitoring

This section outlines procedures for deploying the trading agent and monitoring its performance post-deployment. Initial deployment focuses on integrating the experimental codebase with the main project repository. Ongoing monitoring emphasizes performance analysis, adaptive risk management, and learning from trading outcomes.

### A. Deployment

1. **Code Integration:** Integrate the experimental code for rally time prediction (including multi-head CNNs and survival analysis models) into the main project repository. Clearly document integration points and dependencies to ensure traceability and reproducibility.

2. **Reproducibility Guide (Optional):** If required, create a guide outlining the steps to reproduce the experiments, including data preprocessing, model training, and evaluation procedures.

3. **Targeted Documentation (Optional):** Provide focused analysis and documentation for specific aspects of the repository or models related to rally time prediction upon request.

### B. Monitoring

Monitoring focuses on evaluating the performance of the rally time prediction component and its impact on trading decisions, including the effectiveness of dynamic capital allocation, position sizing based on time premium, and trade exit strategies. A core component of monitoring is learning from unsuccessful trades to enhance future performance.

1. **Trading Cost Analysis:** Continuously analyze the impact of trading costs (commissions, slippage, etc.) on the achieved alpha. This analysis should inform adjustments to trading frequency and order execution strategies.

2. **Short-Selling Constraint Analysis:** Evaluate the impact of short-selling constraints, particularly on small-cap stock performance. This assessment may necessitate adjustments to the trading strategy or portfolio allocation.

3. **Unsuccessful Trade Analysis:** Implement a clear methodology for analyzing unsuccessful trades, including understanding the reasons for losses and incorporating these learnings into the trading strategy.

4. **Stop-Loss Mechanism:** Implement and refine a stop-loss mechanism to mitigate potential losses during periods of high volatility or unexpected market movements. Carefully tune this mechanism to avoid premature exits from profitable positions. Further research into alternative algorithmic exit strategies is recommended.

5. **Risk-Based Weighting:** Implement a risk-based weighting scheme to allocate capital based on the perceived risk of each trade. This approach can enhance portfolio risk management and optimize returns. Incorporate learnings from unsuccessful trades to refine the risk weighting scheme.

## IV. Post-Training Analysis and Refinement

This section outlines procedures for model refinement and performance evaluation following the training phase. While this project doesn't involve traditional deployment to a production environment, the principles of iterative improvement and analysis remain crucial. The focus here is on analyzing model behavior, identifying weaknesses, and implementing strategies for continuous learning.

### A. Documentation and Reproducibility

This phase focuses on linking experimental results back to the codebase to facilitate future reproduction and analysis.

- **Code-Experiment Mapping:** Thorough documentation will link specific experiments and model configurations to the corresponding code sections. This is critical for understanding the model's evolution and interpreting results.
- **Model Reproduction (Optional):** While not strictly required, a clear guide for reproducing models and experiments is valuable for future research and validation.
- **Targeted Codebase Analysis (Optional):** Upon request, a focused analysis of specific aspects of the codebase or models can be provided, such as performance bottlenecks or particular architectural choices.

### B. Performance Monitoring and Refinement

This section details strategies for identifying potential issues and areas for improvement based on backtesting and performance evaluation. This process mirrors the monitoring of a live trading system, identifying areas needing refinement.

The following strategies, while not explicitly labeled "monitoring," provide a framework for ongoing analysis and improvement:

- **Sample Re-weighting:** During retraining, higher weights will be assigned to misclassified, high-loss trades. This prioritizes learning from costly mistakes. Backtesting will track losses associated with each trade, informing adjustments to the training data for subsequent retraining cycles.
- **Bootstrapping of Hard Cases:** A "hard sample bootcamp" will be created by isolating high-loss trades for targeted training. This addresses specific weaknesses identified through backtesting by focusing on challenging scenarios. High-loss trades will be identified and used to create specialized training datasets.
- **Meta-Model for Trade Review:** A second model will be developed to predict potential trade failures before execution. This meta-learner analyzes initial predictions, trade characteristics, and historical outcomes, acting as a proactive risk management layer. Its performance will be monitored to ensure effectiveness.
- **Reward Logic Enhancement:** The reward logic will be enhanced to consider the influence of preceding market states (using sequences of images). This allows the model to learn from sequential patterns and adapt to changing market dynamics. Backtesting will evaluate the effectiveness of the modified reward function.
- **Model Input Adjustment for Sequences:** The model will be adapted to handle sequences of images as input, enabling the reward logic enhancement and other future improvements. Performance will be monitored to ensure correct handling of sequential data.

### C. Further Model Development

The following enhancements are planned for future implementation:

- **Benchmark Comparison Experiment:** A benchmark comparison will assess the performance of different architectural approaches for handling temporal data (static picture, picture-pair, and sequence). This will inform the optimal method for incorporating temporal context and may impact future deployment architecture.

## IV. Deployment and Monitoring

This section outlines the process for deploying the developed models and establishing a robust monitoring framework. While immediate deployment to a live trading environment is not within the current project scope, this phase focuses on laying the groundwork for future deployment by emphasizing reproducibility, backtesting, and data preparation.

### A. Deployment (Facilitator)

The current deployment strategy prioritizes connecting the codebase to the conducted experiments and ensuring reproducibility, rather than deployment to a live trading system. This involves:

1. **Code-Experiment Linkage:** Clear documentation and organization of the codebase will directly link code implementations to specific experiments. This ensures easy identification and revisiting of the code for analysis or modification.

2. **Reproducibility (Optional):** A guide for reproducing the models will be developed if time and resources permit. This guide will detail environment setup, code execution, and replication of experimental results.

3. **Targeted Analysis (Optional):** Further investigation into specific aspects of the repository or models can be conducted on request, enabling in-depth examination of areas of particular interest.

### B. Monitoring (Enforcer)

Although live monitoring is not yet applicable, the backtesting phase establishes the foundation for a robust monitoring system. Key considerations for future monitoring include:

- **Backtesting Framework:** A robust backtesting framework will simulate real-market conditions and assess model performance using the implemented `Build the backtesting framework` task. This allows for evaluation of trading strategies based on generated image sequences.

- **Data Pipeline for Backtesting:** An effective data pipeline is essential for backtesting and comprises:

  - **Image Sequence Generation:** The `Build image sequence generator` task creates sequential candlestick chart images using a sliding window approach, providing suitable input for the models.
  - **Dataset Structure:** The `Design the dataset` task defines the dataset structure for training and backtesting, consisting of image sequences (t-2, t-1, and t charts) and corresponding labels (return value, rally time, and signal class).
  - **Feature Engineering:** The `Feature Engineering: Delta Features` task incorporates delta features (calculated via image or feature subtraction) from sequential candlestick windows to enhance the model's ability to capture temporal dependencies.
  - **Sequential Data Input:** The `Input Data: Sequential Candlestick Windows` task confirms that models utilize sequential candlestick windows as input, crucial for capturing market dynamics during backtesting.

- **Trading Cost Analysis:** The impact of trading costs (commissions, slippage) on overall performance, particularly Jensen's Alpha, will be analyzed to assess real-world applicability.

- **Short-Selling Constraints:** The limitations of short-selling constraints, especially on small-cap stock performance, will be investigated to evaluate strategy feasibility in different market segments.

- **Unsuccessful Trade Evaluation:** A clear method for evaluating and handling unsuccessful trades will be defined, including understanding the reasons for failure and potential model/strategy refinements.

- **Risk Management:** A stop-loss mechanism and risk-based weighting scheme will be implemented to mitigate potential losses and optimize the risk-return profile of the trading strategy.

## IV. Deployment and Monitoring

This section outlines considerations for deploying the developed models and establishing a comprehensive monitoring framework. While the current focus is on research and development, preparing for future deployment and ongoing model performance evaluation is crucial for validating the practical application of the research and facilitating a smooth transition to a live environment.

### A. Deployment (Facilitator)

Deployment, in this research context, refers to connecting the experimental code to the final model and ensuring reproducibility. This involves establishing a clear link between the codebase and experimental results, rather than deploying to a production environment. However, building reusable components now will streamline both future experiments and any eventual transition to a live trading system. This includes:

- **Reusable Components:** Develop modular and reusable components such as data loaders, the masking and padding logic for handling variable-length candlestick chart sequences (including integration of positional embeddings), and Vision Transformer (ViT) wrappers. This modularity will facilitate future experiments with financial time series data and simplify integration into a production environment, should the model be deployed for live trading.
- **Code Traceability and Reproducibility:** All experimental results should be clearly linked back to the specific code used to generate them, ensuring traceability and allowing for easy verification. Documenting the environment setup, data preprocessing steps, and model training procedures will further enhance reproducibility.

### B. Monitoring (Enforcer)

Effective monitoring goes beyond tracking performance metrics; it requires understanding the _why_ behind those metrics and proactively addressing potential issues. Given the novel approach of candlestick image prediction, monitoring should encompass both quantitative metrics and qualitative assessments:

- **Input Sequence Length:** Continuously monitor the distribution of input sequence lengths encountered in the live environment and compare it to the distribution observed during training. Significant deviations could indicate a need for model retraining or adjustments to the input pipeline. Also, track any performance degradation associated with specific input sequence lengths to identify potential model limitations. Remember that while the ViT can handle variable-length input, the maximum sequence length used during training should be respected during deployment.
- **Candlestick Image Prediction Evaluation:** Rigorously evaluate the efficacy of predicting candlestick images instead of direct return values. Track the accuracy of extracted returns from predicted images and identify any biases or inconsistencies. Compare the model’s interpretation of visual patterns with human trader interpretations to highlight areas for model improvement.
- **Conceptual and Theoretical Validation:** Continuously evaluate the underlying premise of predicting candlestick images. Analyze the realized benefits of the image-based prediction approach against its hypothesized advantages, such as richer representation of market dynamics and improved uncertainty modeling. This analysis will provide valuable insights into the strengths and weaknesses of the chosen architecture.
- **Performance Metrics:** Employ relevant performance metrics like Sharpe ratio, directional accuracy, mean squared error (MSE), and rally-time prediction accuracy to quantitatively assess model performance. A structured experiment comparing different memory management options (Options A, B, and C) for the ViT input pipeline will be conducted, evaluating trade-offs between flexibility, performance, and training efficiency.

## IV. Deployment and Monitoring

This section details the deployment considerations and ongoing monitoring requirements for the SCoVA project, ensuring the model remains effective and relevant after initial development and testing. A key focus is the relationship between visual pattern prediction accuracy and financial performance. While this section doesn't address infrastructure or system-level monitoring, it highlights crucial model-specific considerations for real-world deployment.

### A. Deployment

Deployment in this context goes beyond simply putting the model into operation; it emphasizes maintaining a clear link between the deployed code, experimental findings, and the generation of actionable trading signals. Specifically:

- **Connecting Code to Experiments:** Maintaining clear traceability between the deployed code and the experimental results is crucial for reproducibility and future analysis. This includes preserving links to specific experiment IDs, configurations, and model parameters validated through rigorous experimentation, ensuring consistency between research and application. The primary goal is deploying models demonstrably capable of generating actionable trading opportunities, not just visually appealing charts. This reinforces the importance of distinguishing between realistic visual outputs and their actual trading value.

### B. Monitoring

Effective monitoring for this project requires going beyond traditional performance metrics and incorporating the following model-specific considerations:

- **Model Accuracy vs. Financial Performance:** Continuously monitor the relationship between the model's accuracy in predicting visual candlestick patterns and the actual financial returns generated by those predictions. Investigate discrepancies where visually accurate predictions don't translate into profitable trades. This addresses both the need to correlate visual accuracy with financial outcomes and to manage expectations regarding the financial implications of realistic predictions.

- **Actionable Trading Decisions:** Evaluate the clarity and actionability of the model's output for informing trading decisions. The model should not only predict patterns but also provide clear signals indicating when a trade is warranted, considering the overall financial implications. This ensures the model's output directly supports practical trading strategies.

- **Model Evaluation Metric:** While financial return is a critical metric, it shouldn't be the sole measure of the model's effectiveness. Monitor the model's ability to generate visually plausible candlestick sequences that align with real-world market behavior. This broader evaluation helps prevent overfitting to specific market conditions and promotes generalizability.

- **Causal Grounding of Predictions:** Monitoring should investigate _why_ the model makes specific predictions. Understanding the driving factors behind the model's decisions through techniques like feature importance and attribution analysis can reveal potential shifts in market dynamics that might impact its efficacy. This proactive approach helps maintain the model's reliability and provides insights into the causal relationships between market conditions and predicted patterns.

## IV. Deployment and Monitoring

While this project primarily focuses on research and development, rather than deployment in a live trading environment, reproducibility and thorough documentation are crucial. "Deployment," in this context, refers to providing the necessary materials to reproduce the experiments and support the dissertation. "Monitoring" refers to tracking performance and limitations during experimentation.

### A. Deployment

This section outlines the steps for ensuring reproducibility and providing supporting materials for the dissertation.

1. **Connecting Code to Experiments:** All experimental results presented in the dissertation must be directly traceable to the provided codebase. Clear connections between the code and specific experiments are essential. Version control (e.g., Git) with clear commit messages referencing experiment numbers or descriptions is highly recommended. A comprehensive README file explaining the repository structure, dependencies, and how to run key experiments should be included.

2. **Reproducing Models (Optional):** A guide for reproducing the models and experiments, while optional, significantly strengthens the research. This guide should include detailed instructions on environment setup (including specific library versions), data preprocessing, model training, and result evaluation. Containerization (e.g., Docker) can simplify environment setup and ensure consistency.

3. **Focused Analysis (Optional):** If specific aspects of the repository or a particular model require deeper analysis, this section can be tailored to provide in-depth explanations and address specific concerns.

### B. Monitoring

This section details the requirements for documenting experimental performance, challenges, and limitations. This thorough documentation is essential for understanding the model's behavior and potential shortcomings.

1. **Complete Codebase:** A complete and well-organized codebase is required to support the dissertation. This should include all scripts and modules necessary to reproduce the experiments and results.

2. **Detailed Code Documentation:** Clear and comprehensive documentation within the codebase is crucial for understanding implementation details. This documentation should clearly explain the functionality and methodology used.

3. **Experimental Logging:** Detailed logging templates for each experiment are necessary. These templates should capture performance metrics, hyperparameter settings, and any other relevant information to facilitate reconstruction of the experimental setup.

4. **Performance Visualization:** Comparative performance charts and tables should be provided to illustrate the results and facilitate analysis. These visuals should clearly present key performance indicators and comparisons between different models or experimental settings.

## IV. Deployment and Monitoring

This section details the deployment and monitoring procedures for the visualization components, crucial for understanding and analyzing the model's behavior. While this differs from deploying a trading model into a live environment, it focuses on effectively presenting the visualization tools.

### A. Deployment

This subsection outlines the deployment of the visualization elements, encompassing data storage, visualization generation, and adherence to technical constraints.

- **Data Storage and Logging:** Planar coordinates (u, v) and the moving frame's orientation will be stored. Planar coordinates will be tracked using an arc length counter. Orientation data, essential for spiral path reconstruction, will be efficiently stored using quaternions or rotation vectors. The storage mechanism will be designed for efficiency while ensuring compatibility with visualization and rendering requirements, respecting memory and storage limitations.

- **Visualization Generation:** The system will generate 2D trace visualizations and 3D helix visualizations as charts and images to facilitate interpretation and analysis. The 2D trace visualization will prioritize clarity, potentially employing techniques like enhanced visual representation, tooltips, annotations, or a dedicated help section. The 3D helix visualization will depict the helix with the specified parameters (radius 1.0, pitch 0.5, and 4 turns), including axis labels (X, Y, and Z) and a title ("3D Helix in Laboratory Coordinates").

- **Technical Constraints:** Each plot (3D helix and 2D trace) will be generated as a separate figure, avoiding subplots, to ensure clear and independent visualization.

### B. Monitoring

Monitoring in this context focuses on the clarity and effectiveness of the visualizations, rather than traditional trading metrics like alpha decay or slippage.

- **Visualization Clarity:** The clarity of the 2D trace visualization will be continuously monitored. User feedback on interpretability issues will drive iterative improvements, ensuring the visualizations remain relevant and informative.

## IV. Deployment and Monitoring

While this project primarily focuses on research and model development, considering deployment and monitoring aspects is crucial for future practical applications.

### A. Deployment (Facilitator)

Facilitating future deployment involves ensuring the reproducibility of experiments and providing clear instructions for recreating the model.

- **Connecting Code to Experiments:** The code implementing the dynamic projection (PCA rotation and re-centering) must be clearly linked to the experiments evaluating its impact. This ensures reproducibility and enables accurate analysis.
- **Reproducing Models (Optional):** While not immediately required, documentation on reproducing the models incorporating the dynamic projection should be created. This includes details on the PCA implementation, dynamic re-centering method, and relevant hyperparameters.
- **Targeted Analysis (Optional):** Upon request, specific aspects of the repository or model related to the dynamic projection system can be analyzed in detail. This could involve examining its impact on specific asset classes or market conditions.

### B. Monitoring (Enforcer)

Effective monitoring is essential to evaluate the dynamic projection system's performance and identify potential issues. Given the computational intensity of the image processing pipeline, focusing on resource utilization and the impact of PCA is critical.

- **Computational Cost:** Track the computational resources (CPU, memory, time) consumed during model training and inference. This includes monitoring the impact of PCA calculations on overall processing time.
- **PCA Performance:** Analyze the efficiency of the PCA rotation implementation within the Vision Transformer/CNN pipeline. Evaluate its impact on prediction accuracy and its effectiveness in prioritizing relative local movement over absolute position.
- **Clarifying Dynamic Projection:** Clearly define the goal of the dynamic projection (e.g., prediction invariance or focusing on local movement). Precisely describe the rotation mechanism (mathematical transformation) and how the dynamic projection is applied to the trading model with each new candlestick or group of candlesticks. This includes explaining how the feature space is re-centered and rotated.

## IV. Deployment and Monitoring

While this project primarily focuses on research and model development, considerations for deployment and monitoring are relevant for ensuring reproducibility, understanding practical limitations, and informing future development. This section details how the dynamic snapshot generation process informs deployment and what aspects require monitoring during backtesting.

### A. Deployment (Facilitator)

Deployment, in this context, focuses on facilitating the reproducibility of experiments and connecting the experimental results to the codebase. This involves:

1. **Connecting Code to Experiments:** Explicitly reference relevant code sections or modules (e.g., the `RotatingSnapshotGenerator` module encapsulating the dynamic plane implementation) within the dissertation when discussing specific experiments or results. This enhances traceability and verification. A conceptual diagram visualizing the dynamic rotation operation on price movement sequences will further clarify the implementation.

2. **Reproducibility (Optional):** Providing a concise guide for reproducing models and experiments, potentially through a README file or a dissertation appendix section, can strengthen the research's impact.

3. **Detailed Documentation (Optional):** If the dissertation emphasizes specific repository or model aspects (e.g., the dynamic plane generator), provide detailed documentation for those components, including pseudocode, implementation details, usage examples, and input format (image-based or numerical features). Clearly state the generator's compatibility with the chosen model (ViT or CNN). For example, thoroughly document the pseudocode and Python implementation of the batch-optimized `RotatingSnapshotGenerator` designed to handle trading data and link it to relevant experimental results.

### B. Monitoring (Enforcer)

Monitoring focuses on evaluating the quality and consistency of dynamically generated snapshots during backtesting and analyzing performance limitations to guide future improvements. Key areas include:

1. **Data Integrity:** Monitor the input data feed for quality issues that might lead to incorrect snapshots, paying close attention to the "Volatility Jump Handling" mechanism.

2. **Snapshot Consistency:** Monitor generated snapshots for consistent axis scaling and the absence of rotation artifacts. Automated checks for these aspects should be implemented. Deviations could indicate problems in the dynamic snapshot generation process and impact model predictions.

3. **Performance Monitoring:** Track metrics like image generation time and resource utilization during backtesting to assess the system's ability to handle real-time demands and identify potential bottlenecks. This informs future optimization efforts.

4. **Backtesting Analysis:** Monitor backtesting results to understand the limitations of the current approach, considering factors like high trading costs, short-selling constraints, unsuccessful trades, stop-loss mechanisms, and risk-based weighting, which are detailed elsewhere in this document.

## IV. Deployment and Monitoring

This section outlines the deployment and monitoring process for the Vision Transformer (ViT) model trained with dynamic candlestick snapshots generated by the Rotating Dynamic Plane Generator. This includes integrating the generator into the training pipeline and visualizing the dynamic plane transformation to understand model behavior and performance.

### A. Deployment

Deployment focuses on integrating the Rotating Dynamic Plane Generator into the ViT training pipeline and providing visualizations for understanding its functionality.

1. **Integration with ViT Training:** The generator's code must be seamlessly integrated into the ViT training pipeline. This includes managing parameters like the window size, which controls the locality of the dynamic plane, and incorporating volume data as a third dimension (optional), as per the functional requirements. The implementation should ensure consistent alignment with the experimental design comparing static and dynamic frames.

2. **Model Reproduction (Optional):** A guide for reproducing the model training with the integrated dynamic plane generator can be provided for future reference. This documentation could include generator parameters, ViT training setup, and preprocessing steps for the dynamic candlestick snapshots.

3. **Visualization of Dynamic Plane Transformation:** Visualizations will be crucial for understanding the model's interpretation of input data. These visualizations include:

   - **Example Images of Candlestick Input:** Raw candlestick input images will be generated and stored as a baseline for comparison and input verification.
   - **Example Images of Transformed Candlestick Input:** Images of the transformed candlestick data after dynamic plane rotation and origin shift will be generated and stored to demonstrate the transformation's effect.
   - **Dynamic Redrawing Animation:** An animation illustrating the step-by-step redrawing process as new data points are added will be created. This animation will be accessible through a monitoring dashboard or as a standalone tool, providing insights into the model's adaptation to new information.
   - **Image Rendering Validation:** The deployment process will validate the Rotating Dynamic Plane Generator's correct rendering of the dynamic 2D plane, ensuring the transformed images are suitable for the ViT model.

4. **Handling Limited Data Points:** The deployment must account for the technical constraint requiring at least two data points for the dynamic plane generator. Error handling will be implemented to manage scenarios with insufficient data, such as displaying a placeholder during initial frames with only a single data point.

### B. Monitoring

Monitoring involves evaluating the ViT model's performance with dynamic candlestick snapshots and comparing it against the baseline model trained on static frames. The visualizations from deployment will be integrated into the monitoring process.

1. **Performance Comparison:** Central to the monitoring process is the comparison between models trained with static and dynamic frames. This comparison will consider the influence of varying window sizes and the inclusion/exclusion of volume data.

2. **Dynamic Plane Visualization in Monitoring Dashboard:** Integrating the dynamic plane animation into the monitoring dashboard allows real-time observation of the model's interpretation of market movements. This visual feedback helps identify potential issues like unexpected rotations or shifts.

3. **Alerting Based on Dynamic Plane Behavior:** Monitoring the dynamic plane's behavior enables the implementation of alerts triggered by predefined thresholds. For example, unusual rotations or rapid origin shifts could signal high market volatility and trigger an alert to review model performance and mitigate potential risks.

## IV. Deployment and Monitoring

This section outlines considerations for deployment and monitoring of the dynamic plane visualization, even though the current project focus is on model development and backtesting. These considerations lay the groundwork for a robust and informative animation that accurately reflects the underlying data and calculations.

### A. Deployment (Facilitator)

Deployment involves creating a standalone simulator for detailed inspection and validation of the visualization process.

- **Standalone Animation Simulator:** A standalone animation simulator will be developed (Task: Build Standalone Animation Simulator). This simulator will incorporate the dynamic plane calculations and display the visualization step-by-step, including the dynamic movement of points (Functional Requirement: Dynamic Point Movement) and the live rotation and re-centering of the frame (Functional Requirement: Live Frame Rotation and Recentering). To avoid instability in the principal component analysis (PCA) calculations due to limited initial data, rotation will be delayed until at least three points are available (Architectural Decision: Minimum Points for Rotation). Furthermore, to prevent errors related to insufficient data for plotting and rotation calculations, a delay mechanism will ensure these processes begin only when at least two valid data points are available (Addresses Task: Delay Rotation and Plotting and Technical Constraint: Minimum Points for Animation). Consistent data offset formatting will be maintained, especially with limited data, to prevent dimension mismatches during calculations and ensure correct animation behavior (Addresses Task: Format Offsets).

### B. Monitoring (Enforcer)

Monitoring focuses on ensuring the visual integrity and stability of the dynamic plane animation, particularly addressing potential issues arising from the instability of PCA calculations with limited initial data.

- **PCA and Rotation Stability:** The stability of PCA calculations will be monitored, especially in early frames where limited data points might collapse into a line and lead to errors in the dynamic plane rotation (Addresses Technical Constraint: PCA Instability). This includes monitoring for and mitigating unexpected dimension blowups and ensuring smooth transitions between frames to prevent jarring visual artifacts (Task: Smooth Rotation Matrices). Continuous monitoring will ensure the animation framework remains robust and error-free, especially during the initial visualization stages with scarce data points (Addresses Animation Framework Robustness).

## IV. Deployment and Monitoring

While this project primarily focuses on research and model development, considerations for deployment and monitoring inform design choices and highlight potential real-world applications. This section outlines the necessary visualizations and analyses to understand model behavior under various market conditions, and addresses practical considerations for future deployment.

### A. Deployment Considerations

Although live deployment in a trading environment is not the project's primary goal, ensuring reproducibility and connecting the code to experimental results are crucial for validating the research. Clear documentation will link specific code sections to the experiments. A guide for reproducing the models and experiments, including data acquisition, preprocessing, model training, and evaluation, will be developed if deemed necessary. Further analysis of specific repository or model aspects relevant to potential deployment scenarios may also be conducted and documented.

### B. Monitoring and Model Behavior Analysis

Monitoring, in this context, refers to analyzing the model's behavior under simulated market conditions. This analysis involves several key visualizations and investigations:

- **Market Regime Visualization and Analysis:** Visualizations will demonstrate how different market regimes (trend, reversal, sideways) appear within the dynamic plane representation, mirroring existing visualization methods. This helps understand how the dynamic plane transforms data under various market conditions. A single, comprehensive panel will present a side-by-side comparison of standard Heiken-Ashi charts with rotated dynamic plane charts for three distinct market regimes: Trend-Reversal-Recovery, Choppy Sideways Market, and a Strong Linear Uptrend/Sharp V-Shaped Recovery. This direct comparison highlights the impact of the dynamic plane transformation across diverse market behaviors. A detailed analysis of the model's behavior within each regime will be conducted, and the implications for model learning—specifically how the model might adapt to different market conditions—will be summarized for inclusion in dissertation drafts.

- **Simulated Market Conditions:** Two specific simulated scenarios are used to stress-test the model:

  - **Choppy/Sideways Market:** Choppy candlestick data, generated using the `generate_choppy_candlesticks(n=30)` function, simulates a volatile sideways market. A standard Heiken-Ashi chart (saved to `/mnt/data/standard_heiken_ashi_choppy.png`) and a rotated dynamic Heiken-Ashi chart, generated using the `dynamic_rotate_recenter_heiken` function (saved to `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`), provide a direct comparison to understand the impact of the transformation.
  - **Complex Price Patterns:** A realistic market simulation incorporating rallies, drops, and recoveries will be implemented for a more comprehensive evaluation than simpler price patterns.

- **Interpretability of Model Activations:** The dynamic rotation via PCA complicates direct interpretation of model activations in terms of Price, Time, and Volume. Further research is needed to project the model's focus back into the original Time-Price-Volume space for human-interpretable monitoring. This is a crucial consideration for future deployment scenarios.

## IV. Deployment and Monitoring

This section outlines the procedures for deploying the trained model and monitoring its performance in real-world or simulated trading environments. The following discussion integrates the model enhancements described in the checklist with their implications for deployment and monitoring, especially regarding error handling and adaptive behavior.

### A. Deployment

The deployment process must incorporate the error signal mechanisms and adaptive frame adjustments detailed below. This includes verifying the correct implementation and functionality of these features within the deployed model. Connecting the deployed model's code to the development environment's experimental setup facilitates a seamless transition and simplifies debugging. Reproducing the model within the deployment environment, while optional, provides an additional layer of verification. Deployment should prioritize specific repository or model aspects as required by stakeholders or dictated by the target environment. Crucially, the static error value previously used in PCA calculations should be removed before deployment, as this value is now dynamically calculated.

### B. Monitoring

Monitoring extends beyond tracking standard performance metrics like Alpha and Sharpe ratios and trading costs. It requires closely observing the model's adaptation to market dynamics and the effectiveness of its error correction mechanisms. The following elements require specific attention:

- **PCA Rotation Stabilization:** Continuously monitor and analyze the dynamic PCA axes to prevent erratic rotations due to market noise or small data windows. Implement stabilization techniques to ensure consistent model predictions.

- **Loss Function Refinement:** Continuously evaluate and refine the loss function within the dynamic PCA space. The loss function should prioritize learning meaningful movement patterns over spurious correlations.

- **Model Learning Focus:** Shift the model's focus from learning absolute price changes (e.g., "If price rises over time") to learning relational movements within the dynamic space (e.g., "If dominant movement along the local principal axis shows rising oscillations"). Monitor the model's learning behavior and adjust the architecture accordingly.

- **Error Signal Integration:** Monitor the performance of the integrated "Error Signal" component. This component, inspired by biological error correction, accounts for increasing uncertainty and delayed feedback in longer-term market predictions. Its parameters should be adjusted based on observed performance and changing market conditions.

- **Frame Confidence Correction and Prediction Error Memory:** Continuously monitor the alignment between predicted and actual market movements. Analyze the frequency and magnitude of recent prediction errors to inform adjustments to rotation weight and dynamic plane stability, enabling the model to learn from its mistakes.

- **Feedback-Driven Frame Smoothing:** Monitor for periods of high prediction errors and automatically trigger a reduction in the rotation speed of the dynamic plane. Continuously assess the effectiveness of this smoothing mechanism in stabilizing the model during volatile market conditions.

- **Dual-Frame Estimation:** Monitor the performance of both the "optimistic" and "stable" frames, and track their dynamic weighting. This provides insights into the model's balance between rapid adaptation and cautious correction, allowing for adjustments to the weighting strategy as needed.

These monitoring procedures, coupled with the implemented error signal and frame adaptation algorithms, provide a comprehensive approach to managing the deployed model and ensuring its robustness in dynamic market conditions. Integrating a stop-loss mechanism and a risk-based weighting scheme, while detailed elsewhere, are also crucial aspects of a robust deployment and monitoring strategy and require ongoing performance analysis.

## IV. Deployment and Monitoring

This section details the deployment process and the ongoing monitoring required to maintain the model's accuracy and responsiveness within a live trading environment. Given the dynamic nature of financial markets, continuous monitoring and potential adjustments are crucial.

### A. Deployment

While specific deployment details are outside the scope of this document, connecting the research code to practical experiments is essential for ensuring reproducibility and facilitating further development. This includes refining existing pseudocode into robust, production-ready code suitable for integration into a live trading system (Ref: Chat1.json message #114). The deployment process must also ensure proper initialization and integration of the prediction error buffer and associated algorithms within the live trading system. This facilitates the seamless integration of the monitoring components described below.

### B. Monitoring

Robust monitoring is critical for the dynamic plane algorithm and includes the following key components:

- **Rolling Frame Correction Algorithm and Associated Metrics:** This algorithm addresses potential misalignment between the dynamic PCA frame and market dynamics over time. Operating on a biological "wound healing" analogy, it makes small adjustments to correct for accumulating prediction errors. This algorithm comprises:

  - **Prediction Error Buffer:** A rolling buffer (suggested size: 5-10 steps, determined empirically) stores recent prediction errors, calculated as the difference between the predicted and realized movement within the rotated frame.

  - **Error Trend Detector:** Analyzing the prediction errors in the buffer, this component calculates their rolling mean and variance. A threshold, potentially based on a multiple (e.g., 1-2x) of the rolling standard deviation, triggers corrective action when consistently exceeded. This prevents overreaction to noise while maintaining responsiveness to significant trends. This detector should be enhanced beyond basic rolling statistics to incorporate the influence of price, time, and volume on prediction errors (Ref: Chat1.json message #115). Further enhancements should include analyzing deviations within a dynamic 2D frame, considering both distance and angular discrepancies between predicted and realized movements (Ref: Chat1.json message #116).

  - **Frame Correction Action and Healing Phase:** When triggered, small rotations or damping adjustments are applied to the dynamic PCA frame. These corrections are temporary and gradually reduced ("healed") as prediction errors subside below the threshold, allowing adaptation to new market regimes without permanent bias.

  - **Frame Intervention Metric:** A soft evaluation metric quantifying the frequency of frame intervention over a trading year is needed. This assesses system fluidity and adaptivity. A high intervention rate might indicate instability or over-sensitivity, requiring parameter tuning.

  - **Error Visualization:** Visualizing the "error spike → correction → healing decay" process over a simulated sequence is crucial for identifying potential issues and optimizing parameters like correction magnitude and decay rate. This visualization should also be incorporated into the live monitoring system.

- **Deviation Vector and Angular Error Tracking:** To provide a more nuanced understanding of model accuracy, the following metrics should be monitored:

  - **Deviation Vector Monitoring:** Track the vector difference between the predicted and actual movement vectors in the dynamic 2D plane (Ref: Chat1.json message #116).

  - **Angular Error Tracking:** Implement a mechanism to specifically track the angular error between predicted and realized movement vectors.

- **Peripersonal vs. Extrapersonal Gap Simulation:** Backtesting and ongoing monitoring should incorporate the proposed simulation method for the peripersonal vs. extrapersonal gap. This provides insights into performance within different proximity ranges and the effectiveness of the error signal integration, particularly in relation to the lagging rotation deactivation strategy. This analysis informs ongoing refinements to the error signal mechanism and enhances adaptability to varying market dynamics. A strategy for deactivating lagging rotation and returning to the normal state should be defined and implemented as part of the deployed model.

## IV. Deployment and Monitoring

While this project focuses on research and dissertation writing rather than deployment in a live trading environment, considerations related to error correction and frame consistency are crucial for evaluating the model's robustness and potential real-world applicability. This section outlines key aspects of error analysis within the dynamic plane framework, essential for assessing model performance and informing future development.

### A. Deployment Considerations

Although not deployed in a production environment, maintaining clear traceability between code and experiments is paramount for research reproducibility. This is implicitly addressed through the dissertation's codebase and documentation, further detailed in Section V. Before any hypothetical deployment, a thorough review of the dynamic plane implementation is necessary. This includes verifying the correct usage and intended behavior of the two rotational angles and two distance vectors. Any discrepancies must be documented and resolved to ensure the model functions as expected.

### B. Monitoring and Error Analysis

Robust error analysis is vital for understanding model performance and identifying areas for improvement. The following metrics and visualizations are crucial for evaluating the dynamic plane's effectiveness:

- **Error Metrics:** Two key error metrics are essential for quantifying prediction accuracy within the dynamic 2D plane:

  - **Distance Error:** The magnitude difference between the predicted and realized displacement vectors. This measures the accuracy of the model's magnitude prediction.
  - **Angular Error:** The angular difference between the predicted and realized direction vectors, calculated using the arccosine of the normalized dot product (Ref: Chat1.json message #116). This measures the accuracy of the model's directional prediction.
    Note: The initial frame creation rotation should be excluded from these calculations, focusing solely on the prediction error.

- **Error Visualization:** A diagram illustrating the two rotation layers (global frame transformation via PCA and local vector misalignment) and their relationship to the prediction error should be created. This visual aid will clarify the interaction of these components and facilitate error source analysis.

- **Composite Error Score (Future Development):** For future development, consider a composite error score incorporating both distance and angular error with tunable weighting factors. This would provide a more comprehensive evaluation of model performance than individual metrics. Furthermore, exploring new loss functions that penalize both scalar (magnitude) and angular (directional) drift during training could improve model robustness.

- **Dynamic Error Correction (Future Development):** Investigating a dynamic rolling error correction module to address accumulating vectorial misalignments over time is another valuable avenue for future research. A visual simulation demonstrating how small vectorial misalignments can compound over time, diverging significantly from simple price error accumulation, could further illustrate the importance of this module.

By implementing these monitoring and analysis strategies, the research provides a strong foundation for understanding the model's behavior and identifying potential improvements within the dynamic plane framework, ultimately contributing to the development of a more robust and accurate predictive model.

## IV. Deployment and Monitoring

This section outlines the deployment and monitoring strategy for the SCoVA agent, emphasizing the consistent handling of the dynamic PCA frame for accurate short-term (1-5 candlestick) predictions. Two primary techniques for managing the dynamic PCA frame are considered: "Freeze Frame" and "Reproject Realization." A central "Freeze and Correct" module ensures consistent error calculation regardless of the chosen technique.

### A. Deployment

Deployment focuses on integrating the "Freeze and Correct" module into the prediction pipeline. This module encapsulates the logic for preserving and applying the correct rotation matrix (derived from PCA) for consistent error calculations. Key deployment steps include:

1. **Code Integration:** Seamlessly integrate the "Freeze and Correct" module's code into the live prediction pipeline. This ensures that the chosen frame management technique is applied consistently.
2. **Documentation and Pseudocode:** Provide comprehensive documentation and pseudocode for the "Freeze and Correct" module, facilitating understanding and future modifications.
3. **Model Reproduction (Optional):** Offer a guide for reproducing the models and experiments, especially those related to the dynamic frame and projection techniques.
4. **Targeted Documentation (Optional):** Upon request, provide detailed analysis and documentation of specific data structures and algorithms used for frame alignment and comparison.

### B. Monitoring

Robust monitoring ensures the reliability and accuracy of short-term predictions by verifying the consistent application of the chosen frame management technique and the integrity of the "Freeze and Correct" module. Key monitoring aspects include:

1. **Frame Management Technique Verification:**

   - **Freeze Frame:** Verify that the PCA rotation matrix 'R' is frozen at the time of prediction and consistently used for projecting both predicted and realized data points.
   - **Reproject Realization:** Confirm that the correct historical 'R' matrix is retrieved and applied to the realized movement vector.

2. **"Freeze and Correct" Module Validation:** Monitor the correct execution of this module, focusing on its role in preserving and applying the appropriate rotation matrix for consistent comparisons and error calculations.
3. **Data Structure Integrity:** Monitor the integrity and efficient usage of the data structure storing the PCA basis (rotation matrix) for each time window.
4. **Visualizations:**

   - **Frame Visualization:** Integrate visualizations to display the "frozen" or "reprojected" frame, clearly differentiating it from a potentially shifted frame at t+1.
   - **Alignment Visualization:** Implement a visual simulation of the alignment process, demonstrating how predicted and realized movements are aligned using the chosen dynamic PCA frame management technique.

5. **Error Calculation Clarity:** Provide clear explanations and numerical examples illustrating how the chosen frame management technique impacts error calculations. Visualizations comparing prediction and realization paths in both the original and potentially shifted frames can enhance understanding and address potential confusion arising from comparisons with traditional error metrics based on static PCA planes.

## IV. Deployment and Monitoring

This section outlines the procedures for deploying the model and implementing ongoing monitoring to ensure its continued effectiveness. While full deployment in a live trading environment is beyond the scope of this project, the principles described here lay the groundwork for future implementation.

### A. Deployment

Connecting the codebase to the experimental results is crucial for reproducibility and understanding the model's behavior. This involves documenting the relationship between specific code versions, parameter settings, and the resulting performance metrics. A guide outlining the steps to reproduce the models and experiments will be provided. Further analysis focusing on specific aspects of the repository or model can be provided upon request.

### B. Monitoring

Continuous monitoring of the model's performance is essential for identifying potential issues and adapting to evolving market conditions. This monitoring will focus on key factors impacting performance, particularly the stability and reliability of the dynamic coordinate system (the PCA plane implementation):

- **Robust PCA Frame Management:** Continuously monitor the chosen PCA frame management approach (Freeze Frame or Reproject Realization) to ensure it effectively handles shifts in market data distribution and prevents model inaccuracies due to market fluctuations.

- **Dynamic Error Calculation:** Implement a comprehensive error calculation that accounts for the dynamic nature of the PCA planes. This calculation will encompass two key components:

  - **Vector Deviation Error:** Calculate the deviation between predicted and realized values within the current PCA frame.
  - **Frame Drift Error:** Quantify the difference between consecutive PCA frames using principal angles. In 2D, this involves calculating the angle between corresponding principal components (PCA1 and PCA2) at consecutive time steps (t and t+1). The Frame Error is calculated as a weighted sum: `Frame Error = α * Angle(PCA1_t, PCA1_{t+1}) + β * Angle(PCA2_t, PCA2_{t+1})`, where α and β are tunable weights. These angles will be normalized (e.g., mapping degrees to the range [0, 1]) to ensure consistent scaling with other error components.

- **Total Error and Pseudocode:** The Total Error combines the Vector Deviation Error and the Frame Drift Error: `Total Error = Vector Deviation Error + Frame Drift Error`. Clear pseudocode will be provided for this calculation to facilitate tracking and debugging. A small numerical example will demonstrate the total error calculation using initial default values for α and β, derived from trading intuition and physics principles.

- **Error Trend Detection and Healing Phase:** An Error Trend Detector will continuously monitor a rolling window of Total Errors. If errors accumulate too rapidly, a "Healing Phase" will be triggered. This phase involves applying a correction factor to the model's PCA frame construction, with this factor gradually decaying as errors decrease. If errors spike again during the Healing Phase, the system will re-enter a "Correction Mode" to recalibrate its parameters.

- **Adaptive Weighting and Parameter Tuning:** Continuously monitor the effectiveness of the chosen weights (α and β) for the Frame Drift Error. Analyze how adjustments to these weights affect trading outcomes and consider implementing an adaptive weighting scheme based on market conditions or model performance.

This dynamic monitoring and error management approach is critical for maintaining long-term model stability and performance in a live trading environment.

## IV. Deployment and Monitoring

This section details the deployment strategy for the error detection and healing system and the subsequent monitoring procedures necessary to maintain model performance in a live trading environment. While the primary focus of this project is research and model development, simulating the deployment and monitoring process within a backtesting framework is crucial for understanding system behavior and optimizing its parameters.

### A. Deployment

Deployment, in this context, involves integrating the error detection and healing system into the backtesting framework and visualizing its behavior.

- **Module Integration:** The error detection and healing system, encompassing its various components (correction factor decay, dynamic re-entry, etc.), will be integrated into the backtesting framework as a distinct module. This modular design promotes code clarity and maintainability.

- **Visualization:** The system's actions, particularly the activation and deactivation of the healing phase, will be visualized over a sequence of trading periods. This visual representation allows for a qualitative assessment of the system's behavior and effectiveness.

### B. Monitoring

Monitoring focuses on observing system behavior during backtesting, tuning parameters, and ensuring the system's robustness. Post-deployment, continuous monitoring and dynamic adjustments are essential for maintaining model performance in a live trading environment.

- **Backtesting Monitoring and Parameter Tuning:** Initial tuning values for key parameters, such as thresholds for triggering the correction mode and decay rates for the correction factor, will be established based on common market regimes and further optimized through backtesting observations. The implementation includes a correction factor decay mechanism (potentially exponential or linear, e.g., exponential decay with λ=0.95) during the healing phase, allowing the system to gradually return to normal operation. The system is designed for dynamic correction re-entry if errors resurge during the healing phase, ensuring adaptability to changing market conditions.

- **Live Trading Environment Monitoring:** The deployed model's performance will be continuously monitored using the following mechanisms:

  - **Prediction Correctness Tracking:** Predicted outcomes will be compared with actual market movements, using a simple scoring system (+1 for correct, 0 for incorrect) to track prediction correctness in real-time.

  - **Rolling Prediction Correctness Buffer:** A rolling buffer will store the prediction correctness scores for the last N timesteps. The rolling average of this buffer provides a running measure of prediction accuracy, used as input for dynamic model adjustments.

  - **Dynamic Decay Rate Adjustment:** The model's decay rate, accounting for past market disruptions, will be dynamically adjusted based on the rolling prediction correctness buffer. Higher accuracy leads to faster decay, reducing the influence of past disruptions when the model performs well.

  - **Performance-Based Healing:** The model's healing process will be directly tied to its performance. The correction factor will be reduced proportionally to the true predictive recovery as measured by the rolling prediction correctness buffer, enabling faster adaptation to improved market conditions.

## IV. Deployment and Monitoring

This section outlines the deployment process and the monitoring strategy for the model, with a focus on the "Healing-by-Correctness" system. This system dynamically adjusts a correction factor applied to the model's predictions based on its recent performance. While the provided checklist focuses primarily on data preprocessing and PCA implementation, these steps are integral to the deployed system's performance and thus inform the deployment and monitoring strategy.

### A. Deployment

The deployment pipeline must incorporate the following components:

1. **Data Transformation Integration:** The preprocessing steps, including handling unseen price and volume values, non-linear time representation, encoding time as fractional elapsed time, and transforming price into relative returns, must be seamlessly integrated into the deployment pipeline. This ensures consistency between the training and deployment environments.

2. **Healing-by-Correctness System Integration:** The core of the deployment involves integrating the Healing-by-Correctness system. This includes the logic for tracking prediction correctness, calculating the dynamic decay rate, and applying the correction factor to the model's output.

### B. Monitoring

Robust monitoring is crucial for maintaining the system's long-term stability and accuracy. The following aspects require continuous monitoring:

1. **Data Transformation Monitoring:**

   - **Input Data Distribution:** Monitor the distribution of incoming price and volume data for significant deviations from the training data distribution. This helps identify potential issues with scaling or the handling of unseen values.
   - **Time Representation Consistency:** Ensure consistent and expected behavior of the non-linear time representation.
   - **Relative Return Calculation:** Monitor the distribution of calculated relative returns for consistency with the training data.

2. **Healing-by-Correctness System Monitoring:**

   - **Prediction Correctness:** Track the rolling prediction correctness to assess the model's performance and the effectiveness of the healing mechanism.
   - **Dynamic Decay Rate:** Monitor the decay rate of the correction factor to ensure it responds appropriately to changes in prediction correctness.
   - **Correction Factor:** Track the applied correction factor to understand its impact on the model's output.

3. **System Health:** Monitor the overall health of the data pipeline and the model, including data throughput, latency, and error rates.

4. **Threshold Evaluation:** Continuously evaluate and refine the healing thresholds (e.g., 75-80% directional correctness) based on observed performance and market conditions.

By meticulously integrating the data transformations and the Healing-by-Correctness system into the deployment pipeline and implementing comprehensive monitoring, the deployed system can maintain accuracy and robustness, ensuring reliable predictions.

## IV. Data Preparation and Transformation

This section details the necessary data transformations for preparing price, volume, and time data for model input. These transformations ensure robustness, handle outliers, and normalize the data across different stocks and time periods, crucial for both model training and any future deployment.

### A. Core Transformations

These transformations are fundamental to preparing the data for both model training and subsequent analysis.

- **Time Transformation:** Timestamps are converted to fractional elapsed time within each window. This is calculated by subtracting the minimum timestamp from each timestamp and dividing by the total time elapsed within the window (the difference between the maximum and minimum timestamps). This accounts for irregular data intervals and preserves information about the rate of data arrival.

- **Price Transformation:** Price data is transformed into window-relative returns, either percentage return or log return, relative to the first price within the window. This anchors the price series to zero at the beginning of each window, enabling comparisons across different stocks and time periods by removing the influence of absolute price magnitude.

- **Volume Transformation:** A logarithmic transformation is applied to volume data to smooth out extreme values and make the data more suitable for analysis. This is followed by robust scaling using the median and interquartile range (IQR), which is less sensitive to outliers than traditional mean and standard deviation scaling.

- **3-Dimensional Matrix Construction for PCA:** A matrix is constructed with fractional elapsed time, relative return (or log return), and scaled volume as columns. This matrix serves as input for Principal Component Analysis (PCA), which identifies the most important underlying patterns and correlations among these key features.

- **Principal Component Analysis (PCA):** PCA is performed on the constructed 3-dimensional matrix. This decomposes the data into its principal components, which are uncorrelated linear combinations of the original features. These components capture the most significant variance in the data and can be used for dimensionality reduction or feature extraction.

### B. Normalization for Model Input

These additional transformations normalize the data to a specific range, which can be beneficial for certain models.

- **Time (T):** Fractional elapsed time (`time_frac`) is normalized to the [-1, 1] range using the formula: `(2 * time_frac) - 1`.

- **Price (P):** Log returns are further normalized. Two options are available:

  - **Scaling by Maximum Absolute Log Return:** Divide each log return by the maximum absolute log return observed within the data window.
  - **Min-Max Scaling:** Scale log returns to the [-1, 1] range using the minimum and maximum log return values within the data window.

- **Volume (V):** The previously log-transformed and IQR-scaled volume data is used directly. No further normalization is applied at this stage.

These transformations create a robust and meaningful representation of the data, suitable for model training, validation, and potential future deployment. While the subsequent sections on deployment and monitoring will address broader system integration and performance tracking, these data transformations are foundational for consistent and reliable results.

## V. Deployment and Monitoring Considerations

While this project's primary focus is backtesting and research, considering future deployment and monitoring requirements is essential. This section outlines key aspects relevant to these stages.

### A. Deployment Considerations

- **Data Pipeline Integration:** The data transformation pipeline described in Section IV must be seamlessly integrated into the deployment environment. This ensures data consistency between research and live trading.

- **Model Reproduction:** Clear documentation and procedures for reproducing the model are essential for deployment. This includes specifying software versions, dependencies, and parameter settings.

- **Code-Experiment Link:** Explicitly linking the codebase to the experimental results ensures traceability and facilitates debugging and future improvements.

### B. Monitoring Considerations

Effective monitoring is crucial for identifying and addressing potential issues in a live trading environment. Key areas to monitor include:

- **Trading Costs:** High trading costs can significantly erode alpha. Monitoring and optimizing transaction costs are essential for maximizing profitability.

- **Short-Selling Constraints:** Restrictions on short selling, particularly in small-cap markets, can impact performance. Monitoring the impact of these constraints and adapting the strategy accordingly is necessary.

- **Unsuccessful Trades:** Analyzing unsuccessful trades is crucial for understanding model weaknesses and refining the trading strategy. Clear criteria for classifying and evaluating unsuccessful trades should be established.

- **Risk Management:** Implementing risk management mechanisms like stop-loss orders and risk-based weighting schemes is crucial for protecting capital and managing downside risk. These mechanisms should be integrated into the deployed system and their performance carefully monitored.
  Image Generation for Model Input

This section details the generation of pre- and post-transformation image pairs to facilitate understanding and monitoring of the data pipeline. These image pairs allow for a visual comparison of raw and transformed data.

Pre-Transformation Candlestick Images: Candlestick images are generated from raw price and volume data. These represent the data before the dynamic plane transformation, simulating the input that a CNN might have received in a prior development stage.

Post-Transformation Normalized Images: These images are generated after applying the dynamic plane transformation (log returns, PCA rotation) and normalization to the [-1, 1] range. They represent the current CNN input, enabling direct comparison with the pre-transformation images.

Image Pair Generation and Display: A sequence of five image pairs is generated. Each pair consists of a pre-transformation candlestick chart with volume and a post-transformation dynamic plane image. These paired images are displayed individually, facilitating visual inspection and monitoring of the transformation pipeline's effects across multiple examples, each representing a distinct market pattern (uptrend with rising volume, downtrend with volume spikes, reversal, sideways chop, and breakout spike then stabilize). The use of single-day charts with 10-minute intervals for each example provides a standardized context for evaluation.

Monitoring Through Visualization: Visual inspection and analysis of the generated image pairs are crucial for monitoring the dynamic plane transformation's effects. By comparing the pre- and post-transformation images, one can identify potential issues, such as artifacts introduced by rotation or the impact of normalization. This visual monitoring helps ensure the transformed data accurately represents the intended market patterns and serves as a quality control measure for the model input.

## IV. Deployment and Monitoring

This section details the deployment process for the algorithmic trading bot and the subsequent monitoring system designed to track its performance, stability, and adaptability to evolving market dynamics.

### A. Deployment

Deployment encompasses the continuous generation of dynamic input images for the predictive model. This dynamic approach ensures the model consistently receives up-to-date market information. The process involves:

1. **Dynamic Frame Construction:** Normalized time, price, and volume data from a recent time window are analyzed using Principal Component Analysis (PCA). This identifies the two primary axes of correlated movement (PC1 and PC2), defining a dynamic 2D plane that adapts to current market conditions.

2. **Rotation and Refocusing:** Historical market data is projected onto this dynamic plane, then re-centered with the most recent data point at the origin (0,0). This focuses the model on the latest price action and its projected trajectory.

3. **Image Generation:** The transformed 2D plot of market flow is rendered into an image, potentially using candlestick or Heiken-Ashi charts. This image serves as the input for the predictive model (e.g., a Vision Transformer).

4. **Predictive Goals:** The model predicts a 2D movement vector (Δx', Δy') within the dynamic plane, representing the anticipated future price trajectory. It also predicts the "Rally Time," estimating the duration of this predicted movement. Connecting this experimental code to the final deployed model ensures traceability and facilitates future analysis. Reproducibility of the model is highly recommended for validation and further development.

### B. Monitoring

Continuous monitoring ensures long-term model performance and incorporates a self-correction mechanism:

1. **Self-Correction Mechanism:** A feedback loop utilizes a Total Error signal, combining Vector Deviation Error (distance and angular error between predicted and actual movement) and Frame Shift Error (discrepancy between predicted and actual dynamic plane shift).

2. **"Wound Detection":** A dynamic threshold, potentially based on the rolling standard deviation of the Total Error signal, identifies periods of reduced predictive accuracy ("wounds"). These indicate potential divergence between the model's understanding and actual market behavior.

3. **"Healing Phase":** Upon detecting a "wound," the system enters a "Healing Phase." Frame construction dynamism is gradually restored by increasing reliance on historical data and reducing the influence of potentially erratic recent movements. As accuracy recovers, the system gradually returns to full dynamic operation.

Beyond the core self-correction mechanism, monitoring also encompasses:

- **Market Context Integration:** The model integrates a multi-scale temporal model considering daily, weekly, monthly, quarterly, and yearly contexts, weighted and integrated to capture cyclical patterns and broader market influences. The weighting scheme for these timeframes should be optimized for predictive accuracy.

- **Cost and Constraint Analysis:** The impact of trading costs on achieved alpha and the effects of short-selling constraints, particularly on small-cap performance, are analyzed.

- **Trade Evaluation:** A clearly defined process for evaluating unsuccessful trades informs model refinements and risk management strategies.

## IV. Deployment and Monitoring

This section outlines the deployment and monitoring strategy for the SCoVA agent, considering both the current experimental setup and potential future real-world applications. While the immediate focus is on integrating the developed code with the experimental framework, the strategy also addresses the complexities of multi-timeframe analysis and the Dynamic Rotating Plane method for a potential live trading environment.

### A. Deployment

1. **Experimental Integration:** Seamlessly integrate the developed codebase with the experimental framework. This includes verifying data flow, model initialization, and result logging within the experimental environment. Clear documentation and instructions should be provided to ensure reproducibility of the experiments detailed in the dissertation.

2. **Future Application Considerations:** While not the primary focus, future deployment in a live trading environment should consider the following:

   - **Multi-Scale Data Processing:** The system architecture must efficiently handle market data across multiple timeframes (intraday, daily, weekly, monthly). This involves robust data pipelines for ingestion, processing, and storage of context images generated from the different frequencies. Resource allocation for storage and processing should be carefully planned.

   - **Dynamic Rotating Plane Implementation:** Integrate the logic for calculating principal axes using PCA on normalized Time, Price, and Volume data. This calculation must be performed efficiently and accurately for each new data point in real-time.

### B. Monitoring

1. **Experimental Monitoring:** Focus on verifying the correct functionality and performance of the implemented code within the experimental setup. This includes monitoring data flow, model performance metrics, and resource usage.

2. **Future Application Monitoring:** A robust monitoring strategy for a potential live trading application should consider the following:

   - **Data Pipeline Integrity:** Monitor the generation of datasets at each timeframe (intraday, daily, weekly, monthly) using the Dynamic Rotating Plane method, ensuring data quality, timely processing, and the absence of errors or delays.

   - **Model Performance and Attribution:** Continuously monitor model performance against a baseline intraday model. Analyze the contribution of different timeframes to predictions, especially during significant market events, to understand model behavior under stress. This analysis should include the impact of the weighting strategy employed.

   - **Resource Utilization:** Track resource usage, including computational resources, processing time, and potential bottlenecks, particularly within the multi-scale data processing pipeline and the Dynamic Rotating Plane implementation. This will help ensure efficient operation and identify areas for optimization.

   - **Non-Hierarchical Attention Model Behavior (Indirect):** If employing a Non-Hierarchical Attention Model, indirectly monitor its query patterns across different timeframes through resource usage and prediction latency. Unusual patterns could indicate data anomalies or issues with the model's understanding of market context.

   - **Risk Management:** Monitor the effectiveness of the stop-loss mechanism and risk-based weighting scheme in mitigating potential losses and optimizing portfolio performance.

3. **Existing Checklist Items:** Maintain and refine the existing monitoring strategy addressing trading costs, short-selling constraints, unsuccessful trades, stop-loss mechanisms, and risk-based weighting, adapting it to the chosen multi-timeframe architecture and the Dynamic Rotating Plane implementation.

## IV. Deployment and Monitoring

This section details the deployment considerations and monitoring requirements for the SCoVA project, specifically focusing on the Swaha trading application. Robust monitoring and a well-designed user interface (UI) are crucial for maintaining performance, managing risk, and ensuring the research findings translate effectively into real-world trading.

### A. Deployment

The Swaha trading application will be developed with a Python backend and a Progressive Web App (PWA) frontend for cross-platform compatibility. Connecting the experimental code to the deployed application is paramount. While full model reproduction instructions are optional, a guide focusing on relevant repository or model aspects should be provided upon request. The application's UI will facilitate managing different model versions and their deployment to the live trading environment, connecting experiment results with specific code versions for streamlined deployment and performance tracking. The core application functions include:

- **Initial Training & Periodic Retraining:** The ability to train the model initially and retrain it as needed.
- **Backtesting:** Functionality to backtest the trading strategy against historical data.
- **Live Trade Execution:** The capability to execute trades in a live market environment.
- **Trade Error Monitoring:** Real-time monitoring and logging of any trading errors.
- **Model Revision:** Tools to revise and update the model based on performance and market conditions.
- **Trade Ledger Reporting:** Access to historical trade data for analysis and reporting.

The UI design for the portrait iPad should incorporate tab-based or sidebar navigation for access to these core functions. The main dashboard should provide a high-level overview, including Swaha's status, market status, current P&L, a live portfolio snapshot, performance charts, recent trades, model confidence, and system logs, all clearly presented and easily accessible.

### B. Monitoring

Effective monitoring is essential. The following UI elements and procedures will provide real-time insights and facilitate risk management:

- **Model Management & Revision UI:** This section lists all trained models, including name/version, training date, backtesting metrics, and status (e.g., deployed, training). Users can deploy models for live trading, run new backtests, and fork existing models for retraining.

- **Live Trade Execution & Monitoring UI (Cockpit View):** This real-time "cockpit" displays open positions, orders, and risk metrics:

  - **Main View:** A detailed table of open positions, with expandable rows showing intraday price charts.
  - **Order Book & Trade Ledger:** Tabs displaying open, executed, and failed orders, along with a detailed trade log.
  - **Risk & Error Dashboard:** Visualizations for key risk metrics and potential errors, including a Frame Shift Error Plot, a Prediction Correctness Plot, and a Manual Override to pause trading or liquidate all positions.

- **Reports & Ledger UI:** This interface allows generating downloadable reports (PDF or CSV) of P&L summaries, trade ledgers, equity curves, and summary statistics for user-selected date ranges.

- **Swaha Status Indicator:** A pulsing circular icon visually represents Swaha's state: Live Trading, Backtesting, Training, Idle, or Error. Tapping the icon reveals further details.

- **Market Status:** A simple text display showing the National Stock Exchange (NSE) status: OPEN, CLOSED, or PRE-MARKET.

- **Current P&L (Today):** Displays the current day's profit/loss in absolute value (e.g., +₹12,450.75) and percentage return (e.g., +1.25%), color-coded for clarity (green for profit, red for loss, grey for flat).

- **Live Portfolio Snapshot:** A scrollable list of current holdings. (Further details of this element are needed and assumed to be provided elsewhere).

- **Performance Monitoring & Risk Mitigation:** Beyond the UI elements, the following procedures are essential:
  - **Trading Cost Analysis:** Regularly analyze the impact of trading costs on performance and alpha generation.
  - **Short-Selling Constraint Analysis:** Monitor how short-selling constraints affect small-cap stock performance.
  - **Unsuccessful Trade Evaluation:** Establish clear criteria for evaluating and learning from unsuccessful trades, informing future model revisions.
  - **Stop-Loss Mechanism:** Implement a stop-loss mechanism to mitigate potential losses.
  - **Risk-Based Weighting:** Implement a risk-based weighting scheme for portfolio allocation.

These UI elements and procedures ensure comprehensive monitoring, risk management, and informed decision-making in live trading. The Training & Backtesting module will use a consistent color scheme: solid blue for equity curves, dashed grey for benchmarks, and distinct colors/line styles for loss curves (e.g., blue and purple).

## IV. Deployment and Monitoring

This section outlines the design considerations for deploying and monitoring the SCoVA agent, focusing on the user interface (UI) and user experience (UX) for effective interaction and analysis on a portrait iPad. The current scope is limited to client-side UI/UX design; details regarding server-side deployment or automated monitoring are not included.

### A. Deployment (Facilitator)

Deployment primarily involves implementing the described UI/UX elements for local interaction with the application. The focus is on providing a user-friendly interface for running experiments and visualizing results directly within the application.

### B. Monitoring (Enforcer)

Monitoring relies on real-time performance tracking and visualization through a dedicated Monitoring Display within the Training & Backtesting Module UI. This display is designed with the following features:

- **Two-Panel Layout:** The portrait iPad screen is divided into two panels: one for experiment controls and the other for real-time monitoring visualizations.
- **Real-time Visualizations and Metrics:** The right-hand panel provides context-specific visualizations:
  - **Training Monitoring:** A line chart displays the training and validation loss curves, enabling immediate detection of potential issues like overfitting.
  - **Backtesting Monitoring:** An equity curve chart is displayed alongside a dashed grey line representing a benchmark (e.g., NIFTY 50). Key performance metrics such as Sharpe Ratio, Maximum Drawdown, Win Rate, and Total Profit & Loss are also presented.
- **Colorblind-Friendly Palette:** The UI utilizes a blue-orange color scheme instead of red-green for accessibility. Descriptive icons and text labels further enhance clarity.
- **Top Bar Status Icons:** Color-coded icons in the top bar indicate the current operating state: blue (Live Trading), yellow (Backtesting), purple (Training), grey (Idle), and orange (Error).
- **Dashboard Widgets:** Consistent color-coding in dashboard widgets provides at-a-glance understanding: blue for positive indicators, orange for negative indicators, and a blue-yellow-orange gradient for gauges representing nuanced performance levels.
- **Live Portfolio Snapshot:** This card provides a clear overview of current holdings. LONG positions have a blue background, SHORT positions have an orange background, and unrealized P&L uses the same blue/orange color scheme.
- **"Health" Gauge:** A blue-yellow-orange gradient represents both Frame Stability (blue for calm, orange for high instability) and Prediction Correctness (orange for low accuracy, blue for high accuracy).
- **Open Positions Overview:** A list of open positions provides a real-time overview. Each entry includes the ticker symbol, trade direction (LONG/SHORT), quantity held, average entry price, last traded price (LTP), and unrealized P&L. Tapping an entry displays a mini sparkline chart of the stock's price movement since the position was opened.
- **Recent Trades List:** A reverse-chronological list displays details of the last five closed trades, including ticker, action (buy/sell), P&L, and trade duration.
- **System Log Stream:** An auto-scrolling log displays non-critical system actions and interpretations, providing insights into the system's decision-making process.

## IV. Deployment and Monitoring

This section details considerations for deploying the SCoVA agent and monitoring its performance. While the current focus is on research and development, deployment considerations inform design choices and ensure a smooth transition to a live trading environment should it be required. This section emphasizes monitoring aspects relevant to the research context, particularly robust monitoring during backtesting for performance evaluation and drawing meaningful conclusions.

### A. Deployment (Facilitator)

Deployment in this context focuses on connecting the codebase to experimental results, ensuring reproducibility, and supporting ongoing analysis.

- **Connecting Code to Experiments:** Maintaining clear links between experimental runs and specific code versions is crucial for tracking progress and understanding the impact of code changes on performance. This includes meticulous version control of the code and detailed logging of experiment parameters.
- **Reproducing Models (Optional):** Documenting the steps to reproduce specific model configurations and experimental setups is beneficial for future work and validation. This can be achieved through comprehensive documentation and potentially automated scripts.
- **Targeted Analysis (Optional):** Depending on the specific research questions, deeper investigation into particular aspects of the repository or model may be required. This targeted analysis will be clearly documented if undertaken.

### B. Monitoring (Enforcer)

Effective monitoring during backtesting is crucial for identifying potential issues and evaluating trading strategy performance. Key aspects include configuring agent inputs, visualizing its behavior, and monitoring performance metrics:

- **Input Configuration and Validation:** Flexible configuration of the agent's inputs is essential for comprehensive testing across various market conditions and time periods. This includes selecting different asset universes (e.g., NIFTY 50, NIFTY 500, Custom Watchlist) and date ranges. Automated splitting of the date range into training/validation sets is required for efficient model training. Input validation within control panels will prevent logical errors, such as ensuring the end date is not before the start date.
- **Visualization:** Visualizing both the standard candlestick chart and the dynamically rotated and re-centered 2D plane image fed into the model provides crucial insights into the agent's perception of market data. Real-time updates with each new candlestick further illuminate the dynamic behavior of the agent's perception system. This includes configuring parameters such as the candlestick chart type, the local window size, and the features included in the dynamic plane (price, time, volume). Parameters related to the self-correction mechanism, such as the wound detection threshold and healing trigger, will also be configurable and monitored.
- **Performance Monitoring:** Real-time performance metrics are essential during training and backtesting. This includes visualizations of training/validation loss charts (for monitoring model convergence), equity curves (to track portfolio performance), and benchmark comparisons (to assess relative performance). Key performance indicators (KPIs) such as the Sharpe Ratio and Jensen's Alpha will provide insights into the model's effectiveness and its potential for real-world application. Profits/gains will be visualized in blue, and losses in orange throughout the module, including tables and LONG/SHORT tags within the Live Trade Execution module. The "Today's Performance" chart will use a solid blue line for "My Portfolio" and a dashed grey line for the benchmark (NIFTY 50). The Control Panel will offer collapsible sections for Data & Timeframe, Dynamic Plane Configuration, Model & Learning Architecture, and Error & Healing Mechanism, each with tailored input controls (dropdowns, calendars, sliders, etc.) for efficient experiment setup.

## IV. Deployment and Monitoring

This section outlines considerations for deploying the developed models and establishing robust monitoring procedures for ongoing performance evaluation and system health. Given the research-oriented nature of this project, traditional production deployment aspects are less critical than ensuring reproducibility and detailed documentation. However, considerations around error detection, healing mechanisms, and resource allocation are vital for robust operation and potential future applications.

### A. Deployment (Facilitator)

Deployment in this context focuses on ensuring reproducibility and clear linkage between experimental results and the codebase. The following steps are essential:

1. **Connect Code to Experiments:** Maintain clear traceability between experimental results presented in the dissertation and the corresponding code within the repository. This involves utilizing a robust version control system and detailed documentation linking specific experiments to code versions, hyperparameters, and results. This facilitates understanding and allows for future verification and extension of the work.

2. **Reproduce Models:** While not strictly required for this project phase, providing a clear guide for reproducing the trained models is highly recommended for future research and potential practical applications. This guide should detail the necessary steps, including data acquisition, preprocessing, model training parameters, and dependencies.

3. **Focus on Specific Repository/Model Aspects:** If the research focuses on a particular aspect of the model or repository, prioritize clear documentation and analysis of those areas. This may include specific modules, algorithms, or design choices.

### B. Monitoring (Enforcer)

Robustness and reliability are crucial for any trading system, even in a research context. This subsection details key aspects of monitoring for potential issues and implementing strategies to mitigate risk and maintain performance. The monitoring system should include both real-time performance tracking during backtesting and diagnostic tools for understanding model behavior.

1. **Comprehensive Monitoring Display:** A user-friendly interface will provide access to various performance metrics and diagnostic plots. This will be organized with tabs for efficient navigation and include visualizations for:

   - Learning Curve
   - Error Signal Diagnostics
   - Healing Mechanism Status
   - Backtesting Metrics (Sharpe Ratio, Sortino Ratio, Max Drawdown, Win Rate, Profit Factor, Average Trade Duration)
   - Trade Return Distribution

2. **Performance and Time-Based Healing Triggers:** Two distinct mechanisms will trigger a "healing" process: one based on a drop in predictive performance below a user-defined threshold and another based on error stabilization time. This two-pronged approach ensures responsiveness to both sudden and gradual performance degradation. The healing process may involve retraining the model on more recent data or adjusting model parameters.

3. **Total Error Metric:** A comprehensive "Total Error" metric, combining vector error and frame shift error with adjustable weights, will provide a nuanced evaluation of prediction accuracy.

4. **Multi-Scale Context Fusion:** The system will fuse information from different time scales (e.g., intraday, daily, weekly) using user-selectable fusion methods (Attention-Based, Concatenation, or Weighted Average) to enhance decision-making.

5. **Dynamic Capital Allocation:** A dynamic capital allocation strategy will distribute initial capital across trades based on a probability distribution derived from the model's predictions. This approach allows for adjusting capital allocation based on model confidence and market conditions.

6. **Dynamic Error Detection and Healing:** Beyond the triggered healing mechanisms, the system will incorporate dynamic (non-time-based) error detection and healing strategies. This includes mechanisms to identify and recover from unexpected events or errors, contributing to system stability and resilience.

## IV. Deployment and Monitoring

This section outlines the strategy for deploying the SCoVA agent and monitoring its performance. Given the project's current focus on research and development, deployment emphasizes reproducibility, experimental management, and performance analysis in simulated environments rather than live trading.

### A. Deployment (Facilitator)

Deployment in this research context centers on establishing a robust system for managing and reproducing experiments. This involves connecting the codebase to experimental results, ensuring traceability, and enabling further investigation. Key components include:

- **Experiment Management System (EMS):** A robust EMS will be implemented to handle the complexities of the research workflow. This system will track experimental runs, logging hyperparameters, model versions, data versions, performance metrics, and associated code commits. This centralized system will be essential for organizing, analyzing, and comparing the numerous experiments conducted throughout the project.

- **Reproducibility Guide:** Clear documentation and scripts will be provided to facilitate the reproduction of models and experimental results. This guide will include instructions on setting up the environment, preparing the data, and executing the models, ensuring that experiments can be easily replicated and validated.

### B. Monitoring (Enforcer)

Monitoring focuses on analyzing the results of various experiments and visualizing comparative performance. Unlike monitoring in a live trading environment, this stage emphasizes evaluating backtesting and simulated trading results. Key features include:

- **Experiment Builder Interface:** An intuitive Experiment Builder interface will be developed to define and execute complex permutations of experiments. This interface will facilitate setting up various experimental scenarios by allowing users to easily adjust hyperparameters, model configurations, and data inputs.

- **Comparative Analytics Visualization:** Interactive visualization tools will be developed to enable comparative analysis across numerous experiments. These tools will facilitate visual comparisons of performance metrics, enabling researchers to quickly identify trends, assess the impact of different parameters, and make informed decisions about model refinement.

- **Backtesting Interface:** A user-friendly interface will be developed for backtesting trading strategies. This interface will enable users to configure backtests, visualize results, and analyze performance metrics. Real-time visualizations of the trading bot's performance _during backtesting_ will provide immediate feedback on the effectiveness of strategies and parameters.

- **Data Management UI:** UI elements will be implemented for comprehensive data management, including functionality for uploading, downloading, preparing, visualizing, and managing data across different markets. This will support flexible experimentation and analysis.

These components will form a comprehensive system for deploying and monitoring the SCoVA agent within the research and development context, enabling efficient experimentation, rigorous analysis, and iterative model improvement.

## IV. Deployment and Monitoring

This section details the deployment and monitoring procedures for the SCoVA project. Deployment focuses on integrating with the Zerodha KiteConnect API and structuring the application architecture according to philosophical principles. Monitoring emphasizes both performance metrics and adherence to a dharmic mandate.

### A. Deployment

The deployment process leverages the Zerodha KiteConnect API for seamless integration with the brokerage platform. KiteConnect facilitates several key functionalities:

- **Data Acquisition:** KiteConnect downloads historical data for backtesting and model training. A websocket connection, also established via KiteConnect, provides live market data for real-time trading and continuous model learning.
- **Order and Portfolio Management:** KiteConnect manages all order placement, portfolio generation, and tracking, providing a unified interface for interacting with the brokerage.

Beyond technical integration, the application's architecture will be restructured based on the four paths to liberation (Gyaan, Bhakt, Karam, and Raaj) described in the Bhagavad Gita. This restructuring goes beyond superficial rebranding, involving a deep review of both the UI and underlying architecture to align the system's functionality and user experience with these philosophical principles. This includes ensuring clear traceability between the codebase and experimental results presented in the "Manana & Nididhyasana" and "Kurukshetra" dashboards. While optional, a guide for model reproduction will be considered to enhance transparency and collaboration.

### B. Monitoring

Monitoring encompasses both performance metrics and ethical considerations, ensuring the trading bot operates within defined ethical boundaries:

- **Dharmic Mandate Adherence:** Hard-coded, inviolable ethical rules, based on the principles of Satya (truthfulness), Shaucha (purity), and Santosha (contentment), will govern the bot's operation. These rules encompass restrictions against predatory trading strategies, maintaining immutable audit logs, utilizing rigorously cleaned data, and prioritizing stability alongside profit maximization. Continuous monitoring will ensure consistent adherence to these ethical boundaries.
- **Performance Analysis:** Regular assessment of transaction costs and their impact on portfolio performance (including brokerage fees and slippage) will be conducted. The effect of short-selling constraints, particularly on small-cap stock performance, will also be evaluated. A clear methodology for analyzing unsuccessful trades will be implemented, including examining the reasons behind incorrect predictions and adjusting the model or trading strategy accordingly. Finally, a stop-loss mechanism and a risk-based weighting scheme will be implemented to mitigate potential losses and manage portfolio risk.

## IV. Deployment and Monitoring

This section details the deployment process and ongoing monitoring strategy for the SCoVA project, emphasizing the integration of its philosophical underpinnings.

### A. Deployment (Facilitator)

SCoVA will be developed using VS Code with Gemini Code Assist Agent with a Python backend and a Progressive Web App (PWA) frontend. Deployment and management will be handled through Firebase, ensuring scalability and accessibility. The connection between the codebase and experiments within the Gyaan Shala (Data & Experiment Design module) will be clearly documented to ensure traceability and reproducibility. While model reproduction itself is optional, a guide will be provided to facilitate future research and development. More focused analysis on specific repository or model aspects can be provided upon request. A comprehensive blueprint detailing every screen within the application, including extensive technical descriptions, will be provided following guidelines given to Swaha.

### B. Monitoring (Enforcer)

Monitoring for SCoVA transcends traditional performance metrics, focusing on aspects that reinforce user trust and alignment with the system's operational philosophy. This is achieved through several key UI components:

- **Kurukshetra (Live Trading Dashboard):** Provides real-time monitoring, prioritizing the "Dharma Adherence Score" (accuracy of trade execution) over pure profit and loss. This reflects the focus on the quality of the bot's actions rather than just immediate financial outcomes.
- **Manana & Nididhyasana Dashboard:** Offers in-depth analysis tools, including the "Pattern Sangam Visualizer" (displays visual patterns related to trading outcomes) and the "Vairagya Score" (measures performance stability by assessing the consistency of gains).
- **Gyaan Shala (Experiment Designer):** Facilitates systematic experimentation and hyperparameter tuning, allowing users to explore different configurations and strategies for performance optimization.
- **Pratyahara Implementation (Wound & Noise Filtering):** A protective mechanism where the bot enters a conservative "Pratyahara" state when total error spikes, mitigating the impact of unreliable market data. The "Pranayama Gauge" visualizes the dominant timeframe influencing the bot's decision-making by reflecting the attention weights of the Vision Transformer (ViT) across different timescales.
- **Karma Ledger:** Replaces the traditional trade ledger. Each trade is tagged with a "Karma Type" (Sattvic, Rajasic, or Tamasic) reflecting its alignment with predefined principles and market conditions, providing qualitative assessment beyond simple profit and loss.
- **Darshan (Main Dashboard):** Provides a clear overview of the bot's current state and performance.
- **Shraddha Gauge (Faith/Trust Gauge):** Visualizes historical reliability and performance consistency to foster user trust and mitigate emotional reactions to individual trade outcomes.
- **Sankalpa (Intention/Will) and Trust Log:** Reframes the manual override button as "Sankalpa," signifying intention. An accompanying "Trust Log" documents the historical performance impact of manual overrides, encouraging trust in the data-driven process.
- **Swaha's Chants (System Log Stream):** Provides a user-friendly stream of operational logs, enhancing transparency and user connection with the bot's decision-making process.

## IV. Deployment and Monitoring

This section details the deployment process and ongoing monitoring strategy for the project. While the current focus is on model development and testing, deployment and monitoring considerations are essential for successful real-world application. This section outlines the key elements of both processes.

### A. Deployment

The deployment process leverages a user-friendly interface designed to facilitate reproducible experiments and streamline the transition from research to practical application. Key components include:

- **Experiment Designer Canvas UI:** A Flutter-based UI enables visual design of experiments. Users can drag and drop blocks representing different pipeline stages, configuring each with detailed parameter settings, including permutation ranges. This visual approach promotes flexibility and intuitive experiment management.

- **Campaign Runner UI:** A separate Flutter-based UI manages experiment campaigns. Users select experiment templates, automatically calculate the resulting permutations based on parameter ranges, choose different execution modes, and monitor job queue status via Firestore integration. This provides centralized control for running and tracking multiple experiments.

### B. Monitoring

Post-deployment monitoring is crucial for identifying and mitigating issues, ensuring consistent performance, and adapting to changing market conditions. The following key areas will be monitored:

- **Trading Costs Impacting Alpha:** Trading costs (transaction fees, slippage, etc.) will be continuously monitored and analyzed to assess their impact on overall alpha generation. This analysis will inform strategies for cost mitigation and optimization.

- **Short-Selling Constraints:** The impact of short-selling constraints, particularly on small-cap stock performance, will be closely monitored. This involves evaluating regulatory restrictions, borrowing costs, and other factors influencing short-selling strategy effectiveness.

- **Unsuccessful Trade Evaluation:** A clear methodology will be implemented for evaluating unsuccessful trades. This analysis will identify the root causes of these trades and guide improvements to the trading strategy.

- **Stop-Loss Mechanism:** A stop-loss mechanism will be implemented to limit potential losses and manage risk. Specific triggers, thresholds, and execution procedures will be defined and monitored.

- **Risk-Based Weighting:** A dynamic risk-based weighting scheme will be used to optimize portfolio allocation and manage overall portfolio risk. The factors, formulas, and adjustments within this scheme will be monitored and adjusted as needed.

## IV. Deployment and Monitoring

This section outlines the deployment strategy and ongoing monitoring plan for the SCoVA project. The deployment architecture leverages a federated learning approach, distributing computation across user iPads and a central server, to optimize performance and cost-efficiency while preserving user privacy. This hybrid approach combines the benefits of local processing with centralized coordination.

### A. Deployment

SCoVA will be deployed using a combination of Firebase, Google Cloud services, and a custom federated learning framework.

- **Federated Learning Framework:** This framework forms the core of the deployment, enabling distributed computation across iPads and a central server. iPads generate candlestick chart images and train local TensorFlow.js models using their GPUs. The server orchestrates training, distributes the initial model, and aggregates model updates from the iPads. This minimizes server load and maximizes the use of client-side resources.

- **Backend (Coordination Server):** Deployed on Google Cloud Run using containerized microservices orchestrated by Google Cloud Tasks, the backend primarily serves as a coordination hub for the federated learning process. It manages the distribution of models and aggregation of updates, ensuring asynchronous execution and reliable task management. The backend is developed in Python 3.11 or later.

- **Frontend (iPad App):** Deployed as a Progressive Web App (PWA) to Firebase Hosting, the frontend provides the user interface for interacting with the system. It handles local image generation, model training on the iPad, and communication with the backend server for model updates. The PWA is built using Flutter.

- **Development Environment:** IDX.google facilitates development with a cloud-based IDE optimized for Google Cloud services, streamlining the development and deployment workflow.

### B. Monitoring

Continuous monitoring is crucial for maintaining the stability, performance, and cost-effectiveness of the SCoVA system. Key areas of focus include:

- **Federated Learning Performance:** Monitoring the performance of the federated learning process is critical. This includes tracking the time taken for local training on iPads, the efficiency of model aggregation, and the overall convergence rate of the global model.

- **Cloud Computing Costs:** Regularly monitor cloud computing expenses, primarily focusing on the backend server costs associated with coordination and aggregation. This tracking helps evaluate the cost-effectiveness of the federated learning approach.

- **iPad Resource Utilization:** Monitoring resource utilization on the iPads, especially GPU usage and battery consumption during training, is important for optimizing the client-side experience and ensuring the feasibility of the approach.

- **Data Integrity and Security:** Monitoring mechanisms will be implemented to ensure the integrity and security of data exchanged between the iPads and the central server during the federated learning process. This is crucial for maintaining user privacy and the reliability of the trained models.

## IV. Deployment and Monitoring

This section details the deployment strategy for the SCoVA project, leveraging a Progressive Web App (PWA) deployed on iPads, and outlines the associated monitoring procedures. This approach utilizes client-side processing for model training and server-side orchestration for data management and model aggregation.

### A. Deployment

The deployment will utilize a client-server architecture with the SCoVA application implemented as a PWA. This allows for a streamlined deployment process and leverages the capabilities of modern web technologies. However, the feasibility of running computationally demanding tasks within a PWA environment, particularly on iPads, requires careful consideration.

- **PWA Suitability for GPU-Intensive Tasks:** A thorough investigation is required to determine the feasibility of running computationally intensive tasks, such as Vision Transformer training, within a PWA environment on iPads. This investigation must consider potential performance limitations, browser stability under heavy GPU load, and the potential for crashes.

- **Resource Intensive Task Limitations within a PWA:** This assessment should focus on the limitations and stability concerns related to running resource-intensive Vision Transformer training within a PWA on an iPad. A key consideration is the feasibility of long-term, uninterrupted operation, essential for robust training processes. Specific attention should be paid to memory management and resource utilization within the browser context.

- **Technical Challenges of GPU-Intensive Training in a PWA:** Addressing the core technical challenge of GPU-intensive training requires in-depth research on current TensorFlow.js performance and WebGPU capabilities. This research will inform the architectural viability of using a PWA and guide potential optimizations.

- **Mitigating PWA Crash Probability under GPU Load:** Strategies to mitigate the risk of browser crashes under heavy GPU load within the PWA environment are crucial. These may include optimized resource management, load balancing, and robust error handling within the application.

- **Alternative Deployment Strategies:** Contingency planning is essential. If PWAs prove unsuitable for reliably running the required GPU-intensive tasks, alternative solutions will be explored. These alternatives could include native iPadOS application development or leveraging cloud-based solutions for offloading the computational burden.

### B. Monitoring

Continuous monitoring is essential for maintaining performance, identifying potential issues, and ensuring the stability of the SCoVA system. This includes monitoring both trading performance metrics and system resource usage.

- **Trading Performance:** The impact of trading costs on Jensen's Alpha will be analyzed, and strategies to mitigate high trading costs will be investigated. The effects of short-selling constraints on small-cap stock performance will be evaluated, and alternative strategies considered if necessary. The handling and evaluation of unsuccessful trades will be refined for a more accurate performance assessment.

- **Risk Management:** A stop-loss mechanism will be implemented and optimized to mitigate potential large losses. A risk-based weighting scheme will be used to allocate capital across trades based on their assessed risk.

- **System Resource Usage:** Client-side resource utilization (CPU, memory, GPU) on the iPads will be monitored to identify performance bottlenecks and ensure smooth operation. Server-side resource usage will also be monitored to maintain system stability and responsiveness. This monitoring will inform potential optimizations and ensure the long-term reliability of the SCoVA system.

## IV. Deployment and Monitoring

This section details the deployment strategy for the SCoVA project, outlining the evolution from a hybrid Progressive Web App (PWA) approach to a native iOS Swift application. The rationale for each approach and associated monitoring considerations are also addressed.

### A. Deployment (Facilitator)

Initially, a hybrid deployment model was considered to balance the computational demands of model training with the interactive requirements of the application on iPads. This approach aimed to mitigate client-side resource limitations while enabling local fine-tuning and responsiveness.

- **Hybrid PWA Approach:**
  - **Server-Side Training (Google Cloud Run):** Computationally intensive tasks, such as initial model training and large-scale multi-permutation campaigns, were to be executed on Google Cloud Run jobs leveraging GPU acceleration (e.g., NVIDIA T4). The PWA would act as a remote control to initiate these jobs, monitor progress, and retrieve results.
  - **Client-Side Tasks (iPad):** Model fine-tuning (delta training), interactive backtesting, and live inference using the DynamicPlaneGenerator were intended for client-side execution. This approach aimed to enable quick iterations and interactive exploration of model performance. Weight updates generated from client-side fine-tuning would be sent back to the server to update the primary model.
  - **PWA UI Revisions:** The PWA's "Campaign Runner" UI would have incorporated an "Execution Target" dropdown menu, allowing users to select between "Cloud GPU (Full Training)", "On-Device (Fine-Tuning Only)", and "On-Device (Interactive Backtest)".

However, limitations inherent in the PWA environment, particularly regarding access to hardware resources for tasks like image generation and model training, prompted a shift to a native iOS Swift application.

- **Native iOS Application:** This transition allows for greater control over hardware resources, enabling client-side execution of computationally intensive tasks previously constrained by the PWA architecture. This approach necessitates a reassessment of the existing hybrid architecture to ensure compatibility with the iOS Swift frontend. Documentation of any required architectural changes will be provided. While full model reproduction on the client-side is not mandatory, a guide outlining the process will be provided for future development and analysis. Further analysis of specific repository or model aspects can be provided upon request.

### B. Monitoring (Enforcer)

Robust monitoring is crucial for both the hybrid PWA and native iOS deployments. While metrics related to trading performance (e.g., Jensen's Alpha, Sharpe Ratio) are addressed elsewhere, this section focuses on monitoring the deployment architecture itself.

- **Hybrid PWA Monitoring (Deprecated):** The initial plan involved monitoring PWA performance, feasibility, iPad GPU constraints, user concerns regarding browser crashes, and the stability of the hybrid training architecture. Specific metrics and implementation details were deferred.

- **Native iOS Application Monitoring:** Given the shift to a native application, monitoring will focus on the stability and performance of long-running tasks like image generation and model training within the iOS environment. This includes close monitoring of memory usage, GPU performance (especially for the DynamicPlaneGenerator, which performs PCA, rotations, and rendering of numerical data), and the overall stability of client-side intensive computations. Further investigation will address potential memory limitations and long-running task stability within iOS.

## IV. Deployment and Monitoring

This section details the deployment strategy and ongoing monitoring procedures for the SCoVA project, outlining the responsibilities of both the backend (Python) and client-side (iOS app) components.

### A. Deployment

The SCoVA project utilizes a hybrid architecture, balancing computational load between a lightweight Python backend and a more computationally intensive native iOS client application.

**Backend Responsibilities:**

- **Model Management:** Stores and serves the master versions of trained Core ML models, acting as the central repository. This includes managing different model versions and aggregating updates received from the iOS app after on-device training.
- **Experiment Management:** Manages experiment templates and stores high-level results from research campaigns, enabling efficient tracking and analysis. This also includes explicitly linking code implementations to specific experiments for traceability and reproducibility.
- **Data and API Management:** Serves the master database of raw numerical data to the iOS application. Handles secure authentication and connection with external APIs, such as the Zerodha Kite Connect API, centralizing API key management.

**Client-Side Responsibilities (iOS App):**

- **Core Functionality:** Performs computationally intensive operations, including on-device image generation, model training/fine-tuning using Core ML (leveraging the Apple Neural Engine and GPU via Metal), backtesting, and live inference.
- **Data Management:** Manages locally cached data received from the backend, optimizing performance and minimizing network requests.
- **Model Updates:** Sends lightweight model updates to the backend after on-device training, contributing to the global model's improvement.

Future deployment considerations include porting core functionality to other frontends (web and Android) once the model is sufficiently refined and the experimental setup is removed. This future implementation will likely involve minimal image processing, primarily for daily predictions and occasional retraining.

### B. Monitoring

Continuous monitoring is crucial to ensure the model's effectiveness and identify potential issues. Key monitoring areas include:

- **Client-Side Performance:** Monitor the performance of on-device model training, image generation (DynamicPlaneGenerator), and local data storage, tracking metrics such as processing time, resource utilization (CPU, GPU, memory), and storage capacity usage.
- **Model Accuracy and Stability:** Continuously evaluate the deployed model's accuracy and stability in real-world scenarios. This involves tracking prediction accuracy and identifying any potential drift in performance.
- **Trading Cost Impact:** Regularly analyze the impact of trading costs on the realized Alpha, ensuring profitability is not eroded by excessive fees.
- **Short-Selling Constraints:** Monitor the effects of short-selling restrictions, particularly on small-cap stock performance, and adapt the trading strategy accordingly.
- **Unsuccessful Trade Analysis:** Establish a clear process for analyzing unsuccessful trades to understand their root causes and refine the trading strategy.
- **Risk Management:** Implement and monitor a stop-loss mechanism and a risk-based weighting scheme to mitigate potential losses and manage portfolio risk effectively.

## IV. Deployment and Monitoring

This section details the deployment strategy and ongoing monitoring procedures for the SCoVA project. A decoupled architecture, employing a Python backend and various frontend implementations (PWA and native Android), facilitates portability across different platforms.

### A. Deployment (Facilitator)

The decoupled architecture allows the Python backend to handle data provisioning, model distribution, and model update aggregation, while the frontend manages computationally intensive tasks like data preprocessing (including Dynamic Plane Generation), inference, and on-device training. This supports deployment across various platforms, including Progressive Web Apps (PWAs) and native mobile applications.

The backend serves three primary functions:

1. **Data Provisioning:** Serving the raw numerical data required for model input.
2. **Model Distribution:** Serving the latest trained master model file to the frontend applications.
3. **Model Update Aggregation:** Receiving and aggregating model updates from various deployed instances, potentially contributing to a federated learning approach in future iterations.

Specific deployment requirements for each platform are outlined below:

- **Progressive Web App (PWA):** This implementation utilizes TensorFlow.js for on-device machine learning, a TypeScript/JavaScript re-implementation of the DynamicPlaneGenerator, and the HTML5 Canvas API for image rendering. Daily predictions involve fetching data from the Python backend, generating the dynamic plane image, and obtaining a prediction from the TensorFlow.js model. Re-tuning involves downloading the latest master model, fetching recent data, generating dynamic plane images locally, and executing the training process in a Web Worker. The trained model will be converted to the TensorFlow.js JSON format.

- **Native Android App:** This implementation leverages Kotlin, Jetpack Compose for the UI, TensorFlow Lite for on-device machine learning, and a Kotlin implementation of the DynamicPlaneGenerator. The app handles daily predictions and re-tuning using TensorFlow Lite. The trained model will be converted to the TensorFlow Lite (.tflite) format. Using appropriate libraries for matrix and linear algebra operations (e.g., ejml) is a key technical consideration.

### B. Monitoring (Enforcer)

Continuous monitoring is crucial for maintaining model performance and identifying potential issues. The primary focus is on error rate monitoring. A rise in the error rate beyond a defined threshold will trigger a re-tuning process leveraging the decoupled architecture to retrain on recent client-side data and aggregate updates on the server.

While this section focuses on technical monitoring related to error rates and re-tuning, the overall project monitoring also includes considerations like financial performance, risk management (including high trading costs and short-selling constraints), evaluation of unsuccessful trades, and the implementation of stop-loss and risk-based weighting mechanisms. These aspects will be addressed separately as outlined in the main project documentation.

## IV. Deployment and Monitoring

This section details the deployment strategy, including mobile integration and platform considerations, and outlines the approach to monitoring.

### A. Deployment

The deployment architecture aims to balance performance and cross-platform compatibility. The primary focus is on-device inference using a hybrid approach leveraging platform-specific machine learning frameworks within a Flutter application.

**Mobile Integration:**

- **TensorFlow Lite/Core ML Integration:** The trained model will be integrated using TensorFlow Lite for general compatibility and Core ML for optimized performance on Apple devices. A conditional compilation or runtime check will dynamically select the appropriate inference engine based on the platform.
- **Dynamic Plane Image Generation:** The `DynamicPlaneGenerator`, including normalization, PCA, and rotation, will be implemented in Dart using packages like `ml_linalg` and Flutter's `CustomPainter` API.
- **Local Data Management:** On-device storage using SQLite or Hive will manage raw market data for efficient access.
- **Flutter TFLite Bug Evaluation:** Any TensorFlow Lite integration issues will be documented, investigated, and potentially reported to the TFLite maintainers.

**Platform-Specific Implementations:**

- **iOS:** A Swift `CoreMLHandler` class will receive image data from Flutter via Platform Channels, process it using the integrated Core ML model (`*.mlmodel`), and return predictions.
- **Other Platforms:** TensorFlow Lite will be the primary inference engine. Platform-specific implementation details (e.g., Kotlin for Android) will be addressed as needed.

**Cross-Platform Communication:**

- **Platform Channels:** Platform Channels will facilitate communication between the shared Dart code and native (Swift/Kotlin) implementations for invoking machine learning operations and retrieving results.

### B. Monitoring

While the current checklist doesn't specify monitoring tasks, continuous monitoring is crucial for assessing model effectiveness and identifying potential issues in a live environment. This will include defining metrics and procedures for tracking performance, data quality, and resource usage. Specific monitoring details will be finalized as the project progresses and deployment requirements are solidified. Key considerations, informed by potential future deployment in a live trading environment, include:

- **Trading Costs:** Monitoring the impact of trading costs on realized alpha.
- **Short-Selling Constraints:** Assessing the effect of short-selling restrictions, particularly for small-cap stocks.
- **Unsuccessful Trade Evaluation:** Implementing a methodology for analyzing and learning from unsuccessful trades.
- **Stop-Loss Mechanism:** Implementing and tuning a stop-loss mechanism for risk management.
- **Risk-Based Weighting:** Implementing and monitoring a dynamic risk-based weighting scheme for portfolio optimization.

This deployment strategy prioritizes on-device performance and cross-platform flexibility. The monitoring plan will be developed to ensure the ongoing effectiveness and stability of the deployed model.

## IV. Deployment and Monitoring

This section details the deployment and monitoring strategy for the SCoVA project, encompassing model management, platform-specific implementations, and security considerations. While the project is primarily research-focused, a robust deployment strategy ensures consistent model performance and integrity across target platforms.

### A. Deployment (Facilitator)

The deployment process utilizes a centralized Python backend to manage a universal source model and facilitate conversion and distribution to various client platforms (iOS, Android, and potentially web). This approach ensures consistency and simplifies updates.

- **Universal Source Model:** A framework-agnostic model (using either PyTorch or TensorFlow) will serve as the single source of truth, residing on the Python backend. This centralizes model management and ensures consistency across all deployed instances.

- **Platform-Specific Conversion:** The backend will handle the conversion of the universal model into platform-specific formats: `.mlmodel` for iOS, `.tflite` for Android, and potentially `TensorFlow.js` for web. This conversion process will be integrated into the deployment pipeline.

- **Client-Side Integration:**

  - **iOS:** The iOS app will utilize Core ML to integrate the `.mlmodel` file. Communication with the shared Dart UI code will be handled via Platform Channels.

  - **Android:** The Android app will integrate the `.tflite` model using a Kotlin `TFLiteHandler.kt` class. The `.tflite` file will be included in the Android assets. Similar to iOS, communication with the Dart UI will occur through Platform Channels.

  - **Web (Potential):** If web deployment is pursued, `TensorFlow.js` will be used for client-side model integration.

- **Shared Dart Codebase:** The cross-platform user interface, business logic, and data management will reside within a shared Dart codebase. This streamlines development and maintenance. The Dart code will use Platform Channels to interact with the platform-specific model implementations.

- **Model Updates:** The Python backend will manage updates to the universal source model. A defined process will handle receiving updated weights from client devices (if applicable) and integrating them into the source model. An automated process will then re-convert the updated model to platform-specific formats and redistribute them to clients as needed.

### B. Monitoring (Enforcer)

While SCoVA is not deployed in a traditional production environment, monitoring focuses on model integrity, security, and potential performance discrepancies across platforms.

- **Security Measures:** Given the single-user nature of the application, appropriate security measures will be implemented to protect the model and associated data on both the app and backend. This includes secure single-user authentication to ensure authorized access.

- **Performance Monitoring:** Monitoring will track potential performance discrepancies between platform-specific deployments. This includes monitoring inference speed and resource utilization.

- **Model Drift Monitoring (Future Consideration):** If on-device training is implemented in the future, monitoring will also track potential drift in the model's accuracy after updates. This ensures the model continues to perform as expected over time.

## IV. Deployment and Monitoring

This section details the deployment process and ongoing monitoring of the SCoVA agent, focusing on security and authentication.

### A. Deployment

While connecting the code to experiments and reproducing models are important for research reproducibility (and documented elsewhere), this section focuses on security aspects critical for any deployment, including backend infrastructure and client-side security.

### B. Monitoring

Continuous monitoring is essential to ensure the agent's security and identify potential vulnerabilities or performance issues. The following security measures are critical:

- **Secure Authentication:** Implement robust authentication using Firebase Authentication with OAuth providers (Apple and Google). Integrate "Sign in with Apple" and "Sign in with Google" buttons within the client application. Securely store refresh tokens in the device's hardware-backed keystore. Implement server-side ID token verification and UID matching using the Firebase Admin SDK in the Python backend. A decorator should wrap every API endpoint to enforce authentication and authorization. Store the user's Firebase UID as a secure environment variable.

- **Data Encryption and Secrets Management:** Enforce TLS 1.3 for all communication between the client and Google Cloud services. All Zerodha Kite Connect API calls must use HTTPS. Securely store sensitive keys (Zerodha API Key/Secret, Firebase Admin SDK credentials, Firebase UID) in Google Secret Manager. Grant Cloud Run services least privilege access via the "Secret Manager Secret Accessor" IAM role.

- **Data Storage Security:** Implement robust security measures for data at rest and in transit. Encrypt all Parquet files and model artifacts stored in Google Cloud Storage. Control access to GCS buckets using granular IAM policies, granting least privilege access only to necessary service accounts (e.g., KiteConnect-Ingestor, ExperimentRunner). Implement strict Firestore Security Rules to restrict data access to the authenticated admin user.

- **Client-Side Security:** Implement certificate pinning within the native app to prevent man-in-the-middle attacks. Obfuscate the Dart (Flutter) codebase and implement runtime checks to detect tampering or jailbroken devices on iOS. Require Face ID/Touch ID authentication for app access. Implement a remote wipe functionality to protect sensitive data in case of device loss or compromise.

While this section focuses on security and authentication during deployment and monitoring, other important considerations such as trading costs, short-selling constraints, unsuccessful trades, stop-loss mechanisms, and risk-based weighting should be addressed elsewhere in the project documentation.

## IV. Deployment and Monitoring

This section outlines the deployment procedures and monitoring mechanisms for the project. While the primary focus isn't a deployable application in the traditional sense, this section details how experimental results are connected to the codebase and how the trading agent's performance and security are monitored.

### A. Deployment

Deployment, in this context, refers to integrating the developed models and scripts into the research environment and ensuring reproducibility.

- **Connecting Code to Experiments:** Clear documentation will link specific code commits and branches to corresponding experimental runs and results, including hyperparameters, data versions, and configuration settings. This ensures traceability and facilitates verification of reported performance metrics.
- **Model Reproduction (Optional):** A guide outlining the steps to retrain the models (data acquisition, preprocessing, architecture selection, and training procedures) will be provided upon request. Full model reproduction is not a primary project focus.
- **Detailed Code Analysis (Optional):** Deeper analysis of specific codebase or model aspects (e.g., module explanations, design justifications, performance benchmarking) can be provided upon request.

### B. Monitoring

Robust monitoring is crucial for ensuring optimal performance and identifying potential issues with the trading agent. The following aspects will be continuously monitored:

- **Trading Costs and Alpha:** Transaction costs will be monitored and analyzed for their impact on Jensen's Alpha. Strategies to minimize these costs (e.g., order aggregation, optimized trading frequency) will be explored. Excessively high costs may trigger a halt in trading activity, acting as a "kill switch."
- **Short-Selling Constraints:** The impact of short-selling restrictions on small-cap stock performance will be analyzed. Alternative strategies, such as inverse ETFs or put options (if applicable), will be considered to mitigate these constraints.
- **Analysis of Unsuccessful Trades:** Unsuccessful trades will be analyzed to understand their underlying causes (market conditions, model predictions, execution details) and identify areas for improvement. This analysis informs future strategy adjustments.
- **Stop-Loss Mechanism:** A stop-loss mechanism will automatically close positions when losses reach a predefined threshold, acting as a per-trade "kill switch" to prevent runaway losses.
- **Risk-Based Weighting:** A risk-based weighting scheme will allocate capital across assets based on their risk profiles, ensuring portfolio diversification and mitigating overexposure to high-risk assets.

The following security measures will also be implemented and monitored:

- **Kill Switch:** A "kill switch" flag (`isLockedOut`) within a protected Firestore document (`/app_config/security`) will permanently disable the trading application, even after unauthorized access and re-authentication.
- **Real-Time Monitoring:** A Firestore listener in the client-side application will monitor the `isLockedOut` flag. Activation triggers a wipe and lockdown sequence, halting all trading activities.
- **Emergency Lockdown Interface:** A dedicated Progressive Web App (PWA) will provide a clear and accessible interface for triggering the emergency lockdown, featuring a single, prominent "INITIATE EMERGENCY LOCKDOWN" button.
- **Recovery Process:** Lockdown recovery involves manually resetting the `isLockedOut` flag in the Google Cloud Console and re-authenticating with Firebase and Zerodha.
- **Secure Reset Function (Optional):** A secure Cloud Function (`resetEmergencyProtocol`) requiring separate credentials can be developed for resetting the `isLockedOut` flag, adding an additional layer of security.

## IV. Deployment and Monitoring

This section outlines the deployment and monitoring procedures for the secure lockdown and recovery functionality of the Progressive Web Apps (PWAs).

### A. Deployment

Deployment of the lockdown and recovery PWAs will prioritize secure authentication mechanisms. This involves integrating multi-factor authentication (MFA) as described in the following Monitoring section.

### B. Monitoring

Monitoring will focus on the security and reliability of the system, particularly the lockdown and restoration processes. Key aspects include:

- **Multi-Factor Authentication (MFA) for PWAs:** Robust MFA will be implemented for both the lockdown and recovery PWAs. This multi-layered approach includes:

  - **Device Verification:** Verify the user is accessing the PWA from a registered recovery device.
  - **Firebase Authentication:** Integrate Firebase Authentication (using Google or Apple sign-in) for initial user identity verification.
  - **PIN/Biometric Verification:** Implement a final layer of security using PIN entry or biometric authentication.

- **Independent MFA for Recovery:** The recovery process itself utilizes a separate, independent MFA flow distinct from the user's primary Google account. This includes:

  - **One-Time Passwords (OTP):** OTPs will be sent to a registered phone number via Firebase Phone Number Authentication.
  - **Recovery Passphrase:** A pre-set recovery passphrase will be required. This is accessed through the dedicated Secure Recovery Endpoint.

- **Secure Recovery Endpoint:** A dedicated webpage serves as the Secure Recovery Endpoint. This endpoint triggers the `resetEmergencyProtocol` Cloud Function and employs the independent MFA described above. A clearly labeled "DEACTIVATE LOCKDOWN & RESTORE SERVICE" button initiates the recovery process.

- **`resetEmergencyProtocol` Cloud Function Monitoring:** Monitoring of the `resetEmergencyProtocol` Cloud Function is crucial. The function logs all restoration events to the `security_events` collection in Firestore. Each log entry includes:

  - **IP Address:** The originating IP address of the restoration request.
  - **Timestamp:** The precise time of the restoration event.

- **Restoration Confirmation (Optional):** An optional confirmation message (via email or SMS) can be sent to the user upon successful system restoration.

- **Alternative Authentication Methods (Research):** Ongoing research will explore alternative authentication methods for the lockdown and recovery PWAs beyond the current methods. This research aims to balance robust security with a positive user experience.

## IV. Deployment and Monitoring

This section outlines the procedures for deploying the SCoVA agent and establishing robust monitoring practices. Given the sensitivity of financial modeling and the potential for exploitation, security is a paramount concern. While the specifics of deployment and monitoring will depend on the chosen infrastructure and tooling, this section highlights key considerations and best practices.

### A. Deployment

The deployment process should prioritize security, reproducibility, and maintainability. This includes:

- **Documentation:** Clear documentation linking the deployed code to specific experiments is crucial for reproducibility and understanding the context of the deployed model. This also aids in security auditing by enabling easier tracing of data flow and potential vulnerabilities. Version control systems (e.g., Git) should be used to track code changes and ensure consistent deployments.
- **Reproducibility:** A clear process for reproducing the deployed models is essential for both future development and security validation. This allows independent verification of results and ensures the system's integrity. Containerization technologies (e.g., Docker) can significantly enhance reproducibility by packaging the model and its dependencies into a portable unit.
- **Focused Analysis (Optional):** Deeper analysis of specific repository or model aspects can be conducted as needed, with an emphasis on identifying and mitigating potential security risks. This could involve code reviews, vulnerability scanning, and penetration testing.

### B. Monitoring

Ongoing monitoring is critical for identifying and mitigating potential security breaches, performance issues, and ensuring the model's continued effectiveness. This should include:

- **Performance Monitoring:** Track key performance indicators (KPIs) such as prediction accuracy, latency, and resource utilization. Establish alerts for significant deviations from expected performance levels.
- **Security Monitoring:** Implement security information and event management (SIEM) tools to detect and respond to suspicious activity. Regularly audit logs and access patterns to identify potential vulnerabilities.
- **Model Drift Monitoring:** Monitor the model's performance over time to detect and address model drift. This involves comparing the model's predictions against actual market behavior and retraining the model as necessary.

While the specific security measures mentioned in the source material (password length, brute-force cracking, etc.) may not be directly applicable to the deployment and monitoring of the SCoVA agent itself, they highlight the importance of a security-conscious approach. Adopting robust security practices from the outset is crucial for protecting the integrity and confidentiality of the SCoVA project. Future iterations of this document should include more specific deployment and monitoring procedures tailored to the chosen infrastructure and tooling.

## IV. Deployment and Monitoring

This section details the deployment and monitoring procedures for the SCoVA agent, focusing on cost-effective strategies and ensuring the on-device training and update mechanisms function correctly.

### A. Deployment

The deployment process incorporates a multi-stage approach designed to minimize cloud computing costs while thoroughly verifying system functionality:

1. **Unit/Integration Tests:** Initial testing uses mock data, including edge cases (spikes, flat periods, data gaps in CSV/JSON format) to validate individual components and their integration without incurring live data costs.

2. **End-to-End Dry Run:** An "Experiment Designer UI" dry run feature allows complete system simulation using mock data. This exercises the entire data pipeline and model execution without actual trading or financial implications.

3. **On-Device Smoke Tests:** A lightweight "dummy" neural network, mirroring the input/output of the actual ViT model, verifies Core ML training setup, loss calculation, backpropagation, and weight updates using a small amount of real data. The necessity of the current 1-epoch training run during this smoke test will be investigated; using the pre-trained dummy model directly may further optimize efficiency. This smoke test serves as a low-cost validation step before full-scale training.

4. **On-Device Training:** The primary deployment strategy implements on-device training directly on iOS, eliminating initial server-side training costs and enabling complete testing and debugging cycles without cloud computing expenses. Short, 1-epoch training sessions with minimal date ranges further reduce computational requirements and associated server costs during development. This single-epoch training is primarily a validation step, confirming the training and update mechanism's functionality, including data loading, training loop execution, and weight extraction.

The code implementing the on-device training loop is directly linked to the experimental setup for traceability and reproducibility. While not strictly required, documenting model reproduction steps and providing deeper analysis of specific repository/model aspects can be provided upon request.

### B. Monitoring

Ongoing monitoring is crucial for identifying and addressing potential issues:

1. **On-Device Training Evaluation:** Continuously evaluate the necessity and resource implications of the one-epoch on-device training, especially given the dummy model.

2. **Model Update Mechanism:** Clarify and monitor the Core ML model update mechanism, specifically how the backend universal model is updated and how the single-epoch training contributes to model improvement.

3. **Data Pipeline Verification:** Continuously verify the DynamicPlaneGenerator correctly feeds transformed image tensors into the Core ML training session on the iPad, ensuring data consistency and accuracy. Future monitoring strategies will maintain the principle of cost-effectiveness established during deployment.

## IV. Deployment and Monitoring

This section details the deployment procedures and ongoing monitoring strategy for the SCoVA agent, encompassing both reproducibility of research results and real-time "shocker event" detection.

### A. Deployment (Facilitator)

Deployment focuses on ensuring reproducibility and laying the groundwork for potential future deployment. This includes:

- **Connecting Code to Experiments:** The codebase should clearly link specific code segments to the conducted experiments, facilitating understanding and replication.

- **Reproducing Models (Optional):** If feasible, a guide for reproducing the trained models should be provided, including dependencies, data preprocessing steps, and training parameters.

- **Focused Analysis (Optional):** Upon request, a more detailed analysis of specific aspects of the repository or model can be provided.

For the Cognitive Threat Analysis Module (CTAM), deployment also involves deploying the lightweight CNN models and ensuring their proper integration with the DynamicPlaneGenerator.

### B. Monitoring (Enforcer)

Robust monitoring is crucial for understanding real-world performance and identifying areas for improvement. This includes both general monitoring tasks and those specific to real-time event detection and the CTAM.

**General Monitoring:**

- **Extract Weight Updates:** Verify the ability to access and extract updated model weights after training iterations for tracking model changes and potential deployment updates.

- **Synchronize with Backend (if applicable):** In a distributed training scenario, confirm the mechanism for sending weight updates back to the Python backend for maintaining a "universal model."

- **Technical Documentation:** A comprehensive technical document will be created, detailing design choices, implementation details, and testing strategies, including the three-stage testing strategy (Unit & Integration Testing with Local Mocks, End-to-End Pipeline Simulation in "Dry Run" Mode, and On-Device "Smoke Test"). This document will also include the "smoke test" debugging strategy.

**Real-Time Event ("Shocker Event") Detection and CTAM Monitoring:**

- **Real-time Event Detection:** The system should detect "shocker events" (volatility spikes, volume anomalies, rapid price changes) in real-time, leveraging computer vision techniques on equity charts and incorporating data from futures, options, and derivatives markets. Investigation into new data sources and processing methods for these additional market indicators may be required.

- **CTAM Performance and Integration:** Monitor the performance (computational efficiency and accuracy) of the lightweight CNN models within the CTAM and its integration with the DynamicPlaneGenerator. Specifically, track and evaluate the system's response to the CTAM's "Threat Level" score, including adjustments to the smoothing function or learning rate.

- **Shocker Event Response:** Monitor the system's overall response to identified "shocker events," evaluating its effectiveness in mitigating potential market risks. This includes evaluating the impact of trading costs, short-selling constraints, handling unsuccessful trades, stop-loss implementation, and risk-based weighting.

A three-stage testing protocol (Unit & Integration Testing, End-to-End Pipeline Simulation, and On-Device "Smoke Test") will ensure the reliability and cost-effectiveness of the real-time monitoring system.

## IV. Deployment and Monitoring

This section details the deployment and monitoring strategy for the SCoVA project, encompassing both the integration of the current models into the larger system and the proposed architectural enhancements necessary for long-term stability and profitability. While the current project focus doesn't include real-time trading and deployment, these considerations are crucial for future development and transition to a production environment.

### A. Deployment Strategy

The deployment strategy comprises two key phases: integrating the current model with existing systems and implementing the proposed dual-system architecture.

**Phase 1: Current Model Integration**

This phase focuses on integrating the Comprehensive Threat Assessment Model (CTAM) and its Systemic Threat Level (STL) output into the core trading systems. This integration will inform:

- **Proactive Pratyahara (Withdrawal):** Automated withdrawal strategies triggered by high STL scores to reduce exposure during periods of elevated systemic risk.
- **Dynamic Plane Adjustment:** Adaptive adjustments to the Dynamic Plane's smoothing factor based on the STL, enhancing responsiveness to market volatility.
- **Final Prediction Enhancement:** Using the STL as a context token for the final prediction module, providing valuable market environment information.

Documentation linking specific code sections to experiments will be provided to facilitate future development and analysis. Deeper analysis of specific repository/model aspects can be provided upon request. A comprehensive guide for reproducing the trained models, including data preprocessing, model architecture, and training parameters, while optional, is recommended for future development.

**Phase 2: Dual-System Architecture**

The current model, while functional, exhibits limitations due to its error-correction ("wound and healing") foundation. This approach introduces a mean reversion bias and vulnerability to "shock" events, hindering performance during extreme market volatility. To address these limitations, a dual-system architecture is proposed:

- **Flow Engine:** Handles normal market behavior using the existing error-correction mechanism within its "zone of comfort," focusing on exploiting predictable market patterns.
- **Threat Engine (CTAM):** Acts as a contextual override, detecting and reacting to "shock" events. This engine explores high-alpha opportunities outside the Flow Engine's normal operating parameters.

This architecture allows for both stability during normal market conditions and responsiveness to extreme volatility. Prioritizing this architectural enhancement is crucial for robust performance across diverse market conditions before deploying to a live trading environment.

### B. Monitoring Strategy

Continuous monitoring is essential for ensuring ongoing performance and stability. The following aspects are prioritized:

**Current Model Monitoring:**

- **Model Retraining Strategy:** Retraining based on "flow" and "shock" market regimes to maintain sensitivity to both gradual trends and sudden movements, preventing performance plateauing.
- **Specialized Anomaly Detection:** Monitoring the frequency and accuracy of the specialized CNNs for threat detection in equities (gaps, volume spikes) and derivatives (options chain heatmaps).
- **Fusion Model Performance:** Evaluating the stability and reliability of the STL score generated by the fusion model, adjusting the fusion mechanism as needed.

**Future Deployment Monitoring:**

- **Profit Maximization:** Focusing on key performance indicators related to profitability.
- **Adaptive Strategy Weighting:** Monitoring the STL and individual model performance under different market conditions to inform the dynamic weighting between the Flow Engine and a potential future "Shockwave Prediction Model" (SPM) using a "seesaw" mechanism.
- **Opportunistic Threat Response:** Monitoring the effectiveness of the Threat Engine in identifying and capitalizing on high-alpha opportunities during volatile market periods.

This comprehensive deployment and monitoring strategy will ensure the SCoVA project delivers reliable and robust performance in both experimental and future real-world trading scenarios.

## IV. Deployment and Monitoring

This section outlines the deployment and monitoring procedures for the SCoVA project, emphasizing the Facilitator and Enforcer roles in managing the system after model development and testing. A robust deployment and monitoring framework is crucial given the project's complexity and reliance on components like the DynamicPlaneGenerator and Multi-Scale Vision Transformer. This framework adheres to the four pillars of Continuity, Enforcement, Facilitation, and Specialization.

### A. Deployment (Facilitator)

The Facilitator manages the deployment process, ensuring seamless transition from the experimental codebase to a production or near-production environment. This includes:

- **Connecting Code to Experiments:** Maintaining clear and detailed documentation linking experimental results to specific code versions ensures traceability and facilitates understanding of model evolution and performance.

- **Reproducible Models (Optional):** Upon request, the Facilitator will provide clear instructions for reproducing models, including details on environment setup, data acquisition, and model training procedures.

- **Focused Analysis (Optional):** The Facilitator can provide in-depth analysis of specific aspects of the repository or models upon request, supporting focused investigation and understanding.

### B. Monitoring (Enforcer)

The Enforcer is responsible for monitoring the deployed system, managing resources, overseeing process execution, and handling error detection and prevention. Key monitoring tasks include:

- **Trading Cost Impact on Alpha:** Continuously monitor and analyze the impact of trading costs (commissions, slippage, etc.) on overall alpha to ensure profitability is not eroded.

- **Short-Selling Constraints and Small-Cap Performance:** Analyze the impact of short-selling constraints, particularly on small-cap stock performance, to identify potential limitations and areas for improvement.

- **Unsuccessful Trade Evaluation:** Establish clear procedures for evaluating unsuccessful trades, including analyzing the reasons for losses and incorporating these learnings into the trading strategy.

- **Stop-Loss Mechanism:** Implement and maintain a stop-loss mechanism with predefined thresholds to automatically exit positions and mitigate potential losses.

- **Risk-Based Weighting:** Implement and manage a risk-based weighting scheme to allocate capital across trades based on their perceived risk levels, optimizing portfolio risk and return.

## IV. Deployment and Monitoring

This section details the deployment process and ongoing monitoring strategy for the SCoVA project. The current implementation focuses on connecting experimental results with the codebase and monitoring key performance indicators within a single repository. However, the design anticipates future scalability and modularity, with provisions for a more service-oriented architecture.

### A. Deployment (Facilitator)

The deployment process prioritizes traceability and reproducibility by linking the implemented algorithms and models to the experiments conducted. This includes documenting parameters, datasets, and results. Future development envisions a "Specialist Broker Service" to mediate communication between system components, further decoupling the codebase and facilitating independent development and maintenance. This service will be instrumental in managing the anticipated API Gateway and Orchestrator, which will streamline access to various functionalities and manage complex workflows.

### B. Monitoring (Enforcer)

Monitoring is crucial for evaluating performance, identifying potential risks, and ensuring the long-term stability of the trading strategy. Current monitoring efforts focus on:

- **Trading Costs and Alpha:** Analyzing the impact of trading costs on achieved alpha to assess real-world viability and profitability.
- **Short-Selling Constraints:** Evaluating the impact of short-selling constraints on small-cap stock performance to understand strategy limitations and identify potential improvements.
- **Unsuccessful Trade Analysis:** Clearly defining and monitoring the handling and evaluation of unsuccessful trades to inform model refinements.
- **Stop-Loss Mechanism:** Implementing a stop-loss mechanism to mitigate potential losses and ensure long-term stability.
- **Risk-Based Weighting:** Implementing a risk-based weighting scheme to manage portfolio risk effectively and optimize the risk-reward profile.

Future monitoring functionalities, integrated with planned specialist services, include:

- **Normalization Service Integration:** Monitoring and adjusting normalization parameters in the data preprocessing pipeline based on market conditions (Normalization_Service).
- **PCA Service Integration:** Monitoring principal components of input data to identify shifts in market dynamics and potential anomalies (PCA_Service).
- **Coordinate Rotation Service Integration:** Monitoring the effectiveness and stability of the dynamic plane implementation and tracking changes in the rotated coordinate system (Coordinate_Rotation_Service).
- **Model Inference Service Integration:** Continuously evaluating real-time model predictions to identify potential degradation or retraining needs (Model_Inference_Service).

These preemptive monitoring services, coupled with future pre- and post-flight validation and analysis services, will contribute to the smooth, efficient operation of the system and facilitate continuous learning.

## IV. Deployment and Monitoring

This section details the deployment strategy and ongoing monitoring procedures for the SCoVA project. The system leverages a microservices architecture, promoting modularity, scalability, and independent updates. Distinct roles and responsibilities for each service ensure secure and efficient resource management.

### A. Deployment (Facilitator)

Deployment involves establishing independent, specialized services, each responsible for a specific task within the overall workflow. This modular approach is facilitated by the `Workflow_Broker` service, which orchestrates the interaction between components. The following specialist services are defined:

- **`NormalizeWindow`**: This service normalizes raw numerical array data based on a provided configuration dictionary. Designed for portability, it operates independently of external data sources and libraries, excluding `google-cloud-storage`, `google-cloud-firestore`, and `requests`.
- **`ComputePrincipalComponents`**: This service performs dimensionality reduction by calculating the top two principal component vectors from a normalized array. Its dependency on only `numpy` enhances portability and minimizes potential conflicts.
- **`ProjectToPlane`**: Using the basis vectors from `ComputePrincipalComponents`, this service projects the original data onto a 2D plane, simplifying visualization and subsequent analysis.
- **`TrainOneEpoch`**: This service encapsulates a single epoch of model training. It receives model artifact bytes, training data tensors, and a configuration, returning updated model artifact bytes. Its agnostic approach to data origin and destination promotes modularity and reusability.

The `UI_Gateway` facilitator service acts as the primary API gateway for the frontend application, translating user requests into internal workflows and coordinating the execution of the specialist services listed above. This centralized gateway streamlines communication between the frontend and backend. Connecting the experimental code to the deployed system is crucial for reproducibility and facilitates a smooth transition from research to production. While full model reproduction is optional, clear guidance should be provided to support this process if needed.

### B. Monitoring (Enforcer)

Continuous monitoring is critical for maintaining the long-term performance and stability of the deployed system. While specific trading performance metrics are addressed elsewhere, this section focuses on overall system health and potential issues impacting model accuracy and trading efficacy. The `Workflow_Broker`'s central role allows for monitoring resource usage, performance degradation, and error handling across all services. Key monitoring areas include:

- **Resource Usage:** Tracking and managing resource utilization to ensure efficient allocation and prevent performance bottlenecks.
- **Performance Degradation:** Monitoring key performance indicators (KPIs), such as prediction latency and service response times, to identify potential issues early.
- **Error Handling:** Logging errors, implementing recovery mechanisms, and providing alerts for critical failures.

Although the provided design doesn't explicitly detail monitoring implementation, the modular architecture and the central role of the `Workflow_Broker` provide a robust foundation for implementing comprehensive monitoring strategies. Considerations like high trading costs, short-selling constraints, handling unsuccessful trades, stop-loss mechanisms, and risk-based weighting require specific implementation within specialist components and should be integrated into the monitoring plan.

## IV. Deployment and Monitoring

This section details the deployment strategy and ongoing monitoring procedures for the SCoVA project. The microservice architecture, categorized by the four pillars of Continuity, Enforcement, Facilitation, and Specialization, promotes decoupling, modularity, and role purity, leading to enhanced security, testability, and scalability. However, inherent challenges of distributed systems, such as network performance, require careful mitigation strategies.

### A. Deployment (Facilitator)

The deployment process leverages this microservice architecture. Connecting deployed code to experimental results ensures traceability and reproducibility. While full model reproduction is optional, a guide will be provided to facilitate this process if needed. Targeted analysis and review can be supported by focusing on specific repository/model aspects upon request. All deployed services inherit from one of the four base classes (ContinuityService, EnforcementService, FacilitationService, and SpecialistService), ensuring a consistent design and operational framework.

### B. Monitoring (Enforcer)

Continuous monitoring is essential to ensure system performance and stability. Given the microservice architecture, network performance and trading performance will be closely monitored.

**Network Performance Monitoring:**

- **Mitigating Network Bottlenecks:** Continuous monitoring of network performance will identify potential bottlenecks. Solutions such as service discovery, load balancing, and optimized API design will be employed. Asynchronous communication via Google Cloud Pub/Sub will be explored for non-critical requests to reduce latency.
- **Asynchronous Communication:** Google Cloud Pub/Sub will be the primary method for inter-service communication. This asynchronous approach minimizes latency and improves system responsiveness, crucial for a real-time trading system and mitigating the overhead introduced by network hops between microservices.
- **Performance Testing:** End-to-end performance tests, implemented from project inception, will continuously monitor latency, identify bottlenecks early, and ensure consistent real-time performance.

**Trading Performance Monitoring:**

- **High Trading Costs Impacting Alpha:** The impact of trading costs on achieved alpha will be analyzed, and mitigation strategies will be explored.
- **Short-Selling Constraints Impacting Small-Cap Performance:** The impact of short-selling constraints on small-cap stock performance will be analyzed, particularly concerning the chosen trading strategy.
- **Unsuccessful Trade Evaluation:** A clear methodology for evaluating unsuccessful trades will be defined and implemented, informing future model refinements.
- **Stop-Loss Mechanism Implementation:** A stop-loss mechanism will be implemented to mitigate potential losses and protect capital.

A dedicated `Post-flight_Analytics_Service` (a ContinuityService) will handle post-campaign analysis. This service will collect data via EnforcementServices, perform calculations, and report summarized findings through the `State_Enforcer`, ensuring consistent and automated performance evaluation. Further pseudocode and specifications for these base classes will be drafted to guide implementation.

## IV. Deployment and Monitoring

This section details the deployment strategy and monitoring plan for the SCoVA agent, focusing on establishing a robust and reproducible system for analysis and potential future deployment in a live trading environment. Distributed tracing is a core component of this strategy, enabling detailed performance analysis and optimization of the interacting services (Facilitator, Enforcer, and Continuity).

### A. Deployment (Facilitator)

Deployment in this context prioritizes establishing a clear connection between the experimental code and the research findings, laying the groundwork for potential future deployment in a live trading system. A microservice architecture is employed, allowing for flexible inter-service communication, supported by the distributed tracing system.

- **Connecting Code to Experiments:** Clear links will be established between specific code versions and experimental results. This will involve tagging commits in version control, documenting experiment parameters within the code, and creating detailed experiment logs referencing specific code sections. This ensures reproducibility and facilitates future analysis.

- **Reproducibility Guide (Optional):** A comprehensive guide for reproducing specific models will be provided if required. This guide will detail the necessary data, preprocessing steps, model architecture, training parameters, and evaluation metrics.

- **Targeted Analysis (Optional):** Upon request, the analysis and documentation can focus on specific aspects of the repository or model for more in-depth examination.

### B. Monitoring (Enforcer)

While not yet deployed in a live trading environment, comprehensive monitoring is crucial for evaluating model performance and identifying potential issues. OpenTelemetry is utilized for distributed tracing, providing detailed insights into system behavior.

- **Distributed Tracing:** All microservices (Continuity, Enforcement, Facilitator, Specialist) will be instrumented with the OpenTelemetry Python SDK. Context propagation will be ensured using OpenTelemetry's mechanisms for HTTP headers and Pub/Sub metadata, enabling seamless tracking of requests across the entire system.

- **Visualization:** A visualization tool (Google Cloud Trace, Jaeger, or Grafana Tempo) capable of generating Gantt chart visualizations will be selected to effectively display hierarchical traces and pinpoint performance bottlenecks.

- **Performance Impact Analysis:** Analysis will focus on the impact of high trading costs on Jensen's Alpha, the influence of short-selling constraints on small-cap performance, and the handling of unsuccessful trades. This analysis will inform necessary adjustments to the trading strategy.

- **Risk Management Monitoring:** The implemented stop-loss mechanism and risk-based weighting scheme will be continuously monitored through the distributed tracing system to assess their effectiveness and make any necessary adjustments.

## IV. Deployment and Monitoring

This section details the deployment procedures and ongoing monitoring strategies crucial for the success of the SCoVA project. The project utilizes a client-side heavy lifting approach with a lightweight backend orchestrator.

### A. Deployment (Facilitator)

The deployment process centers around distributing the trained model to client devices. A central "universal source model" is maintained on a lightweight backend, built using technologies like Cloud Run/Functions with Firebase. This backend acts as a facilitator, managing the model and handling API interactions. The model is converted to platform-specific formats optimized for on-device use within the native app frontend (e.g., Swift/Core ML or Flutter/hybrid native ML). Connecting the experimental results back to the codebase is critical for reproducibility and understanding the project's evolution. This involves meticulous documentation linking specific experiments to the corresponding code commits. While full model reproduction is not a primary focus at this stage, a concise guide outlining the steps for rebuilding the models will be provided for future reference or extension of the project.

### B. Monitoring (Enforcer)

The lightweight backend also acts as an enforcer, monitoring critical performance metrics and potential issues. This monitoring will cover both financial performance and technical health, encompassing the following areas:

**Financial Performance Monitoring:**

- **Trading Costs Impacting Alpha:** The backend will track trading costs and analyze their impact on the realized alpha. This analysis will inform adjustments to the trading strategy and potential cost optimization measures.
- **Short-Selling Constraints and Small-Cap Performance:** Performance for small-cap stocks will be monitored separately, paying close attention to how short-selling constraints might be affecting returns.
- **Unsuccessful Trade Analysis:** The system will rigorously evaluate unsuccessful trades, investigating the reasons behind them and adjusting the model or trading logic accordingly. A clear definition of "unsuccessful trade" will be established and tracked.

**Technical Health Monitoring:**

- **Advanced Error Signal:** A comprehensive error monitoring system employing a "Total Error" signal will be implemented. This signal comprises two key components: _Vector Deviation Error_ and _Frame Shift Error_. These metrics provide insights into model health and deviations from expected behavior.
- **Performance-Based Healing:** Corrective actions ("healing") will be directly tied to the model's prediction accuracy. Instead of fixed intervals, corrective procedures will be initiated based on real-time performance monitoring.
- **Multi-Scale Periodicity Analysis:** Data from multiple timeframes (intraday, daily, and weekly) will be ingested and analyzed to enhance the model's understanding of cyclical patterns and improve predictions.
- **"Rally Time" Prediction:** The model will be enhanced to predict not only the direction and magnitude of price movements but also the expected duration (measured in candlesticks) for the predicted movement.
- **Distributed Tracing:** System-wide distributed tracing using OpenTelemetry will enable visualization of the entire workflow, from data ingestion to trade execution, facilitating identification of performance bottlenecks and optimization.

This combined approach to financial and technical monitoring, facilitated by the distributed architecture, ensures robust oversight and continuous improvement of the SCoVA model.

## IV. Deployment and Monitoring

This section details the deployment process and ongoing monitoring strategy for the SCoVA project. While the current development phase focuses on research and backtesting, the following considerations are crucial for future deployment to a live trading environment.

### A. Deployment

Transitioning from research to a live trading environment requires a robust deployment process focused on reproducibility and maintainability. Key considerations include:

- **Connecting Code to Experiments:** Maintaining clear traceability between the codebase and the conducted experiments is essential. This ensures the deployed model accurately reflects the research findings and facilitates future analysis.
- **Reproducing Models:** Clear instructions and documentation for reproducing the models should be provided. This allows for auditing, verification, and ongoing development of the model.
- **Documenting Key Repository/Model Aspects:** Thorough documentation of critical aspects of the codebase and model architecture should be maintained. This facilitates understanding, collaboration, and future modifications. Specific areas of focus can be prioritized based on project needs and stakeholder requests.

### B. Monitoring

A comprehensive monitoring strategy is crucial for identifying potential issues, evaluating performance, and ensuring the long-term success of the trading strategy. This includes both performance monitoring and explainability for regulatory compliance.

- **Performance Monitoring:** Continuous monitoring of key performance indicators is essential for optimizing the trading strategy. Specific areas of focus include:

  - **Trading Costs:** Analyze the impact of trading costs, such as commissions and slippage, on overall performance (Alpha). Excessive trading costs can significantly erode profits.
  - **Short-Selling Constraints:** Monitor the impact of short-selling constraints, particularly in the small-cap space, on trading opportunities and portfolio performance.
  - **Stop-Loss Mechanism:** Implement and monitor a stop-loss mechanism to mitigate potential large losses and protect capital.
  - **Risk-Based Weighting:** Implement and monitor a risk-based weighting scheme to optimize portfolio allocation and manage overall risk.

- **Explainability and Compliance:** Maintaining a clear understanding of the model's decision-making process is crucial for both performance analysis and regulatory compliance. This involves leveraging the `Narrative_Generation_Service` and integrating it with other key components:
  - **Narrative Generation:** The `Narrative_Generation_Service` should automatically generate human-readable explanations for each trade executed, incorporating insights from the feature store and attribution methods.
  - **Feature Store Integration:** The service should integrate with the feature store to retrieve relevant versioned input features and system state information used in generating trade rationales.
  - **Attribution Methods:** Employ both model-agnostic attribution methods (e.g., LIME, SHAP) and model-specific methods (e.g., attention maps) to pinpoint factors influencing the model's decisions.
  - **Karma Ledger Integration:** Integrate with the Karma Ledger to store generated narratives, creating a comprehensive audit trail for all trading activities.
  - **Explanation AI Integration:** Explore the potential of integrating an "explanation AI" to further enhance transparency and interpretability of the model's predictions.
  - **Feature Store for Reporting and Compliance:** Utilize the feature store for broader reporting and compliance needs, ensuring transparency and the ability to justify trade decisions.

These monitoring and explanation components are crucial for ensuring the deployed system remains performant, compliant, understandable, and adaptable to changing market conditions.

## IV. Deployment and Monitoring

This section details the deployment process, bridging the gap between backtesting and live trading by incorporating live market data, derived order book features, and a robust paper trading environment. Monitoring strategies for data usage and potential anomalies are also outlined.

### A. Deployment

Prior to live trading, a comprehensive paper trading environment will be implemented to mitigate risk and refine the trading strategy. This environment will integrate with live market data feeds and simulate realistic order execution.

- **Paper Trading Environment:** A new service, `Paper_Brokerage_Simulator`, emulating the Zerodha Kite Connect API, will be created. This simulator will handle order execution and portfolio management within the paper trading environment, persisting its state in Firestore. The API endpoints will mirror those of the `Live_Execution_Enforcer` for seamless switching between paper and live trading modes via a toggle in the Live Trading Dashboard UI. The `Live_Execution_Enforcer` will be modified to route trade requests based on this toggle.

- **Live Market Data Integration:** The `Paper_Brokerage_Simulator` will integrate a live tick data feed from Zerodha via WebSockets, enabling realistic fill simulation based on live bid/ask prices, volume, and potential partial fills. This real-time data simulates market conditions, network latency, and realistic bid-ask spreads, bridging the gap between historical backtesting and live trading.

- **Order Book Feature Integration:** A new `DeriveOrderBookFeatures` service will process raw market depth data to generate features like Order Book Imbalance (OBI), Weighted Average Price (WAP), and Bid-Ask Spread. These features will be added as dimensions to the data window used by the `Normalization_Service` and `PCA_Service` before being passed to the `DynamicPlaneGenerator`, enabling the model to consider real-time supply and demand pressures. The `Workflow_Broker` will be modified to incorporate data from this new service.

- **Enhanced Paper Brokerage Simulation (Market Depth):** The `Paper_Brokerage_Simulator` will be enhanced to utilize live market depth data for order filling. Market orders will simulate "walking the book" to realistically model price slippage, while limit orders will be simulated within the order book, filling based on Last Traded Price (LTP) and available quantity.

### B. Monitoring

Robust monitoring will ensure the proper functioning and data integrity of the system.

- **Market Depth Data Usage Review:** A comprehensive review will document all system components using market depth data, clarifying its influence and aiding in debugging and future development.

- **Market Depth Anomaly Detection:** A new `MarketDepthAnomalyDetector` service, employing a CNN model (details to be provided), will be implemented to detect anomalies within the market depth data.

This deployment and monitoring strategy leverages real-time market data and derived features within a simulated environment, providing a robust and realistic platform for testing and refining trading strategies before live deployment. The monitoring framework ensures system integrity and facilitates efficient identification of potential issues related to market depth data.

## IV. Deployment and Monitoring

This section details the deployment and monitoring procedures for the enhanced SCoVA model, incorporating the Order Book Imbalance (OBI) and Market Depth Heatmap Anomaly Detector. These enhancements address limitations identified in the Zerodha market depth data and contribute to a more robust and responsive trading agent.

### A. Deployment (Facilitator)

Deployment of the enhanced SCoVA model requires integrating the new components into the existing data pipeline and codebase. The following steps outline the necessary adjustments:

1. **Order Book Imbalance Integration:** Integrate the `CalculateOrderBookImbalance` service. This includes verifying the correct input of the market depth dictionary and the incorporation of the normalized OBI value as a fourth dimension in the Dynamic Plane input data. Thoroughly document this integration within the codebase and highlight its connection to prior experiments demonstrating OBI's effectiveness.

2. **Market Depth Heatmap Integration:** Incorporate the `GenerateDepthQuantityHeatmap` service and the `MarketDepthAnomalyDetector` CNN. This involves establishing the data flow for generating heatmaps from the Zerodha market depth data and feeding these to the CNN. Document the CNN's training and evaluation process, linking and explaining any related experiments.

3. **Spread Calculation Removal:** Remove all code related to spread calculations, given the limitations of the Zerodha market depth data. Document this removal and its rationale.

### B. Monitoring (Enforcer)

Continuous monitoring is crucial to ensure the stability and effectiveness of the enhanced model. The following aspects require specific attention:

1. **Order Book Imbalance Monitoring:** Monitor the `CalculateOrderBookImbalance` service for correct functionality and data integrity. Verify OBI values fall within the expected range (-1.0 to +1.0) and correlate with market activity as anticipated.

2. **Market Depth Heatmap Monitoring:** Monitor the heatmap generation process and the `MarketDepthAnomalyDetector`'s performance. Evaluate the effectiveness of anomaly detection in relation to actual market shocks and trading performance.

3. **Impact on Existing Metrics:** Monitor existing performance metrics (Jensen's Alpha, Sharpe Ratio) after deployment to assess the impact of the new components. Adjust trading strategies as needed based on observed changes.

4. **Trading Costs and Constraints:** Continue monitoring trading costs, including commissions and slippage, and any short-selling constraints. Evaluate their impact on overall performance, especially in light of strategy adjustments made after incorporating OBI and heatmap analysis.

5. **Unsuccessful Trades and Risk Management:** Analyze unsuccessful trades and refine stop-loss and risk-based weighting mechanisms, leveraging insights gained from OBI and market depth analysis. Document any refinements made to these risk management strategies.

## IV. Deployment and Monitoring

This section outlines the deployment and monitoring procedures for the SCoVA agent in a live trading environment. Given the complexities of real-time order book analysis, the deployment focuses on connecting the research codebase to the experimental findings and establishing a robust monitoring system for error detection and model adaptation.

### A. Deployment

The deployment process emphasizes connecting the developed code with the documented experimental results to ensure the deployed model accurately reflects the validated research. While full model reproduction isn't the primary objective at this stage, specific aspects of the repository or model can be highlighted for deeper analysis if required.

1. **Connect Code to Experiments:** Establish clear links between the codebase and the experimental results, including documentation of specific code versions and instructions for reproducing the experimental setup.
2. **Reproduce Models (Optional):** A guide for reproducing the trained models can be provided if necessary. Due to the constraints of real-time order book analysis, this primarily serves for verification of the research process, not direct deployment in a live trading environment.
3. **Focus on Specific Repository/Model Aspects (Optional):** Provide detailed documentation and analysis for any specific components of the repository or model requiring further investigation (e.g., the "anxiety level" indicator).

### B. Monitoring

The monitoring strategy focuses on detecting potential errors and triggering model adjustments based on real-time market conditions and performance feedback. This involves analyzing execution quality, enhancing order book analysis, and incorporating an "anxiety level" indicator for dynamic risk management.

1. **Model Profitability and Tracking Enhancement:** Model profitability will be accurately tracked by incorporating a "Price Improvement Rate" into calculations. A dedicated service, `CalculatePriceImprovementRate`, will track the rolling average price improvement. This rate will then be integrated as a feature within the Vision Transformer predictive models to leverage its relationship with subsequent market movements.

2. **Execution Quality Feedback Loop:** A feedback loop within the Self-Correction & Healing Controller will monitor slippage and price improvement. Deteriorating execution quality (increasing slippage, declining price improvement) will trigger an increase in the `CorrectionFactor`, making the Dynamic Plane perception more conservative and mitigating potential prediction errors.

3. **Enhanced Order Book State Analysis:** The `ComputeOrderBookState` specialist will now produce a feature vector containing Order Book Imbalance (OBI) and a new "Book Resilience Score." This score quantifies the relationship between order quantities at different levels of the order book, strengthening signal conviction from the Vision Transformer, particularly when combined with bullish patterns identified by the Flow Engine.

4. **"Anxiety Level" Indicator for Error Detection:** A real-time "anxiety level" indicator based on market depth will assess the potential for model errors or the need to switch between trading modes (e.g., "flow" and "shock" modes).

5. **Trading Cost Analysis:** The impact of trading costs on overall performance (Jensen's Alpha) will be analyzed to inform adjustments to the trading logic and the "anxiety level" indicator's thresholds.

6. **Short-Selling Constraint Analysis:** The impact of short-selling constraints, especially on small-cap performance, will be evaluated and integrated into the trading strategy and risk management.

7. **Unsuccessful Trade Evaluation:** A clear methodology for evaluating unsuccessful trades will be established to refine the "anxiety level" indicator and improve overall system performance.

8. **Stop-Loss Mechanism:** A stop-loss mechanism will mitigate potential losses during periods of high volatility or when the "anxiety level" indicator signals high risk.

9. **Risk-Based Weighting:** A risk-based weighting scheme will be implemented for trade allocation, considering the "anxiety level" indicator to reduce exposure to potentially risky trades during periods of high uncertainty.

## IV. Deployment and Monitoring

This section details the deployment and monitoring strategy for the integrated trading system, encompassing the SCoVA (Snapshot Computer Vision Algorithm), the DynamicPlane algorithm, and the Anxiety Model.

### A. Deployment

Deployment involves integrating SCoVA, the DynamicPlane algorithm, and the Anxiety Model into the live trading environment. This includes:

- **SCoVA Integration:** Connecting the SCoVA code to the live market data feed and ensuring the trained model loads and executes correctly. Documentation of this connection is crucial for reproducibility and troubleshooting. While full model reproduction isn't initially required, a guide facilitating this process should be available. Specific repository or model aspects can be analyzed on a need-basis. The deployment should adhere to the defined project scope and SCoVA's architectural design.
- **DynamicPlane and Anxiety Model Integration:** Connecting the Anxiety Model's output ("Anxiety Level") to the DynamicPlane algorithm's Error Detector and Weight Shifter modules. This includes implementing and integrating the 'flow' and 'shock' trading modes, with the Anxiety Model controlling the switching logic based on market conditions.

### B. Monitoring

Monitoring focuses on system performance and stability in the live trading environment. Key monitoring tasks include:

- **Anxiety Model Performance:**

  - **Data Integrity and Feature Calculation:** Verify the Anxiety Model receives correct market depth data and accurately calculates features like Order-to-Quantity Ratio, Rate of Change of Order Book Imbalance (OBI), Level 1 Dominance, and Book "Flicker" Rate.
  - **Anxiety Level Responsiveness:** Ensure the Anxiety Level (0.0 to 1.0) responds appropriately to changing market conditions.
  - **Integration with Control Mechanisms:** Confirm the Anxiety Level correctly influences the Correction Factor within the Error Detector and the weight distribution between the DynamicPlane's Flow Engine and Shockwave Prediction Model. This may involve log analysis, visualization, and simulations.

- **DynamicPlane Performance:** Monitor the Total Error (Vector Deviation + Frame Shift) of the DynamicPlane algorithm. This metric serves as the target variable for the Anxiety Model's ongoing training.

- **SCoVA Performance:** Analyze the integration between SCoVA-generated signals and subsequent trading actions. This includes investigating unsuccessful trades to identify potential improvements in the model or trading logic.

- **Overall Trading System Performance:**

  - **Trading Costs:** Track the impact of trading costs, particularly on Alpha, and adjust strategies accordingly.
  - **Short-Selling Constraints:** Observe the effect of short-selling constraints, especially on small-cap stock performance.
  - **Risk Management:** Evaluate the effectiveness of risk management measures, including the Stop-Loss Mechanism and Risk-Based Weighting scheme.

- **Post-Hoc Analysis and Anxiety Model Training:** Continuously analyze historical order book data and correlate it with the algorithm's actions (stored in a feature store). This analysis further trains and refines the Anxiety Model using the DynamicPlane's Total Error as the target variable and high-frequency market depth data as input features. This continuous training enhances the Anxiety Model's ability to anticipate and mitigate potential errors in the DynamicPlane algorithm.

This comprehensive monitoring process enables continuous refinement and improvement of the deployed system, ensuring robustness and long-term profitability.

## IV. Deployment and Monitoring

This section details the deployment and monitoring strategy, focusing on reproducibility, performance analysis, and the integration of new components like the `AsymmetricFeatureEngine` and the `IdentifyAsymmetricRegime` service.

### A. Deployment

This project prioritizes reproducibility and understanding the impact of code changes, even without a traditional production deployment. The deployment process involves:

1. **Connecting Code to Experiments:** Maintaining clear links between the codebase, specific experiments, and their associated results. This includes version control and parameter tracking, allowing for precise reproduction of experiments and analysis of code modifications.

2. **Model Reproduction (Optional):** A guide outlining the steps to reproduce the trained models, including data preprocessing, training parameters, and model architectures, will be created if needed.

3. **AsymmetricFeatureEngine Integration:** This component will be integrated into the primary workflow. The `Workflow_Broker` will call the `AsymmetricFeatureEngine` to generate a feature vector used as a context token for the Vision Transformer (ViT). The ViT will be adapted to accept and process this additional context token. This direct integration balances enhanced asymmetry understanding with minimizing future pipeline modifications.

4. **IdentifyAsymmetricRegime Service Deployment:** This service, encapsulating the offline-trained unsupervised clustering model (GMM or Self-Organizing Map), will accept the asymmetric feature vector and output a Regime ID, also used as a context token for the ViT. Connecting the training code and deployment of this service to the experimentation pipeline is essential for reproducibility.

### B. Monitoring

Monitoring focuses on performance, architectural stability, and potential information loss due to design choices. Key areas include:

1. **Performance Impact of Context Vector Integration:** Monitor the computational cost and performance impact of generating and utilizing the context vector within the ViT. This ensures the added complexity provides tangible benefits without undue overhead.

2. **Impact of Terrain-Based Input:** Continuously evaluate the performance impact of using binned terrain categories as input to the ViT. Compare the prediction accuracy against using individual environmental metrics to assess potential information loss due to simplification. Any significant performance drop will trigger a reevaluation of the terrain categorization strategy.

3. **General Model Performance:** Track key performance indicators relevant to the specific model tasks, including metrics such as Sharpe ratio and maximum drawdown where applicable. Ongoing observation for unexpected behavior or performance degradation is essential.

4. **Future Enhancements Monitoring:** Any future enhancements for asymmetry understanding will be carefully monitored to ensure they contribute positively without over-complicating the system, maintaining a balance between analytical depth and maintainability.
   Deployment

This section outlines the necessary steps to deploy the developed models and connect the codebase to the experimental results, ensuring reproducibility and facilitating further analysis.

**Connecting Code to Experiments:** A clear link between the codebase and the experiments is crucial. This includes documenting specific code versions, parameters, data paths (including `stock_data/train/` and `stock_data/test/` for calculated returns), and the 5-day windowing logic as described in the data model. The integration of the return label calculation (`Return(t+h) = (Close(t+h) - Open(t+1)) / Open(t+1)` where `t` is the last date of the chart image and `h` is the holding period) with the candlestick chart generator should be clearly documented and traceable. Filename conventions for generated graphs should be consistent, specifying whether the holding period (`h`) is encoded in the filename (e.g., `YYYY-MM-DD__YYYY-MM-DD__3.25__h=3__ABB.ST.png`) or stored separately. The data sources (OHLCV data from Yahoo Finance for OMXS All-Share and S&P 500 stocks from 2016 to 2023, adjusted for splits but not dividends, along with index data from Nasdaq and Yahoo Finance), model training procedures (using a mean squared error loss function and early stopping with a validation set consisting of the last 30% of the training data from July 1, 2020, to December 31, 2021), and evaluation metrics should align with the experimental setup.

**Reproducing Models (Optional):** While not strictly required, a guide outlining the steps for model reproduction can be beneficial. This includes environment setup, required libraries, data acquisition and preprocessing, model training parameters, and the trade execution logic using actual t+1 to t+5 prices.

**Focusing on Specific Repository/Model Aspects (Optional):** Upon request, a deeper analysis of specific repository or model aspects, such as the 5-day windowing or return label integration, can be provided.

### A. Deployment (Facilitator)

This section outlines the steps required to deploy the project and facilitate further analysis and reproduction of the experimental results.

**Connecting Code to Experiments:** A crucial aspect of deployment is ensuring a clear and traceable link between the codebase and the conducted experiments. This involves meticulous documentation of the code versions, parameters, and datasets used for each experiment, enabling validation and facilitating future analysis. For example, explicitly link the code responsible for generating the 5-day candlestick charts from OHLC data and calculating corresponding return labels to the training results of the CNN model. This clarifies how changes in data preprocessing impact model performance.

**Reproducing Models (Optional):** While not strictly required for deployment, providing a comprehensive guide for reproducing the models is highly beneficial for validation and future research. This guide should include detailed instructions on:

- **Environment Setup:** Specify the required software and libraries, including versions, addressing any potential dependency conflicts. For instance, document the use of `matplotlib` for candlestick chart generation and its configuration.
- **Data Acquisition and Preprocessing:** Detail the process of acquiring the OHLC data (e.g., from Yahoo Finance), constructing the 5-day candlestick charts, and calculating the associated return labels. Clearly state the input data format to the CNN, confirming it correctly represents the OHLC data and the 5-day time window. Explain how edge cases at the beginning and end of the dataset are handled.
- **Model Training:** Provide the training scripts and instructions for executing them. This includes specifying the model architecture (CNN), hyperparameters, and training procedure.
- **Evaluation:** Explain how to evaluate the trained model and reproduce the reported results.

**Focused Analysis (Optional):** Upon request, further documentation can be provided focusing on specific aspects of the repository or models. This could include detailed explanations of particular modules, justification for specific design choices (e.g., the use of a CNN for image-based financial data), or in-depth analysis of experimental results. This targeted documentation caters to specific needs and facilitates a deeper understanding of the project.

## A. Deployment (Facilitator)

This section details how to connect the experimental results with the codebase to ensure reproducibility and facilitate further investigation. While full model reproduction is optional, establishing a clear link between the code and the reported experiments is crucial for validation and future development.

- **Connecting Code to Experiments:** Thoroughly document the relationship between the codebase and the conducted experiments. This includes specifying the relevant code sections for each experiment, the parameters used, and the corresponding results. This connection is essential for verifying the findings and enabling others to understand the implementation. Consider using tagged commits in the version control system or a dedicated log file mapping experiment identifiers to commit hashes.

- **Reproducing Models (Optional):** Provide a clear and concise guide for reproducing the trained models. This guide should include detailed instructions on setting up the environment, preparing the data, and executing the training scripts. This allows for independent verification of the results and enables others to build upon the work.

- **Focusing on Specific Repository/Model Aspects (Optional):** If requested, provide a focused analysis of specific parts of the code repository or aspects of the model. This might involve a deep dive into a particular module, a discussion of design choices, or an explanation of specific implementation details. This targeted exploration facilitates a deeper understanding of the system.

While not directly part of the deployment process, the following prototypes related to risk management and portfolio optimization should be considered for future development and integration:

- **Prototype Stop-Loss Module:** Develop a prototype stop-loss module designed to exit trades when they move against the predicted direction by a certain percentage. For example, a stop-loss could be triggered if a stock drops 2.5% within a specified timeframe (e.g., two days) after a predicted return of +3.0%. This prototype will facilitate testing and refinement of the stop-loss implementation through backtesting and exploration of various stop-loss levels.

- **Prototype Dynamic Trade Filtering Layer:** Develop a prototype dynamic trade filtering layer that leverages predicted return volatility or Historical Prediction Error Profiling (HPEP) data to selectively allow trades based on their predicted risk and potential return. This layer acts as a gatekeeper, enhancing portfolio management by filtering out potentially unfavorable trades.

### A. Deployment (Facilitator)

This section details how the experimental results connect to the codebase, enabling reproducibility and facilitating further investigation. While not traditional software deployment, this section focuses on deploying the research artifacts for review and future development.

- **Connecting Code to Experiments:** Clear connections between the codebase and the conducted experiments are crucial. This requires meticulous documentation and code organization, ensuring that each experiment's setup, parameters, and results are easily traceable to the corresponding code segments. This traceability ensures the integrity and reproducibility of the research.

- **Reproducing Models (Optional):** A guide for reproducing the models is highly recommended. This guide should include detailed instructions on setting up the development environment, acquiring and preprocessing the data, training the models with specific hyperparameters, and evaluating the results. This enhances the research's transparency and validity.

- **Focused Analysis (Optional):** Upon request, a more focused analysis of specific repository or model aspects can be provided. This may involve deeper dives into particular modules, algorithms, or experimental results, tailored to the requester's needs.

## A. Deployment (Facilitator)

This section details how to reproduce the experimental results and provides resources for further analysis of the model and codebase. While not a traditional production deployment, it focuses on ensuring transparency and reproducibility of the research.

**Key Considerations:**

- **Connecting Code to Experiments:** Clear traceability between the experimental results and the code is crucial. This includes detailed documentation within the codebase, potentially referencing specific experiment runs, configurations, or a separate mapping document. For the Vision Transformer (ViT) implementation, this involves documenting specific configurations and hyperparameters used for each experiment, including the number of input candlestick images (N).

- **Reproducing Models (Optional):** A guide for reproducing the trained models can be provided, detailing the required environment, dependencies, data preprocessing, and training parameters. For the ViT, this includes instructions on implementing positional embeddings, masking, and padding for dynamic-length input, training with variable-length sequences, and conducting experiments with different numbers of input images (N=3, 4, and 5). It also addresses potential input image count constraints for the training layer.

- **Focusing on Specific Repository/Model Aspects (Optional):** Deeper analysis of specific aspects of the codebase or model can be provided upon request. For the ViT, this might include a deeper dive into the architecture choices for positional embeddings, the rationale behind the maximum sequence length and padding strategy, or details on the experimental setup for evaluating the performance with different input image counts (N).

## A. Deployment (Facilitator)

This section details how to connect the experimental results presented in the dissertation with the underlying codebase, ensuring transparency and facilitating reproducibility. While full model reproduction is optional, clear documentation linking the dissertation's findings to the implementation is crucial.

- **Connecting Code to Experiments:** The codebase must clearly reflect the experiments discussed in the dissertation. This includes thorough documentation and comments within the code that link specific sections to corresponding experimental setups, parameters, data versions, and results. This traceability is essential for verifying the findings and understanding the implementation details.

- **Reproducing Models (Optional):** A concise guide for reproducing the trained models can be provided to enhance transparency and enable others to build upon the research. This guide should include instructions on setting up the necessary environment, acquiring and preprocessing data, training the models (including architecture specifications, training procedures, and hyperparameter settings), and evaluating the results.

- **Focusing on Specific Repository/Model Aspects (Optional):** Upon request, targeted analysis and documentation of specific aspects of the code repository or models can be provided. This may involve a deeper exploration of particular modules, algorithms, design choices, or experimental setups relevant to the dissertation's core arguments. This focused analysis offers greater clarity and detail for areas of particular interest.

## A. Deployment

This section details how to connect the developed code to the conducted experiments to ensure reproducibility and facilitate further analysis. While broader deployment considerations are outside the current project scope, this section provides a foundation for future extension.

**Connecting Code to Experiments:** Clear documentation links the code implementation to the experiments outlined in the dissertation. This includes references to specific commits, branches, or tagged releases within the code repository. This facilitates verification of experimental results and demonstrates the practical application of the developed models.

**Reproducing Models (Optional):** A concise guide outlining the steps to reproduce the trained models can be included. This guide should cover setting up the development environment, acquiring the necessary data, and executing the training scripts. This enhances transparency and strengthens the research validity.

**Focused Analysis (Optional):** Upon request, focused analyses of particular aspects of the code repository or specific models can be provided. This could include deeper explanations of specific algorithms, design choices, or performance characteristics, offering tailored insights based on audience needs.
Deployment

This section details considerations for deploying the project and connecting the codebase to experimental results. While full production deployment isn't the immediate focus, reproducibility and clear visualizations are crucial for understanding and validation.

**Connecting Code to Experiments:**

Clear traceability between the experimental results and the codebase is essential. This includes thorough documentation within the code, README files, or accompanying materials that explicitly link code sections to specific experiments.

**Reproducibility (Optional):**

If feasible, provide a guide for reproducing the trained models. This could involve instructions on environment setup, data preprocessing, and running training scripts. This facilitates validation and enables others to build upon the work.

**Focused Analysis (Optional):**

Depending on stakeholder needs, dedicate resources to analyzing and documenting specific aspects of the repository or model in greater detail. This might involve profiling specific modules, exploring alternative implementations, or providing in-depth explanations of critical design choices.

**Clarifications Regarding the Dynamic Projection System:**

A clear understanding of the dynamic projection system is crucial for deployment and result interpretation. The following points require clarification:

- **Interaction with Trading Model:** How does the dynamic 2D projection system interact with the trading model, particularly how new candlestick data influences the re-centering and rotation of the feature space before prediction generation?

- **Rotation Mechanism:** What specific mathematical transformation (e.g., affine transformation, PCA rotation, learned rotation) underlies the "rotation" within the dynamic projection system?

- **Projection Goal:** What is the primary objective of the dynamic projection? Is it to achieve prediction invariance to the previous trend direction or to prioritize relative local movement over absolute positioning?

**Technical Implementation Details:**

- **PCA Rotation (If Applicable):** If PCA rotation is used, its implementation should emphasize relative local movement over absolute position within the model.

- **Dynamic PCA Integration:** If applicable, detail how dynamic re-centering and rotation of candlestick image sequences using PCA is integrated into the Vision Transformer or CNN pipeline. The deployed model should operate within a locally optimized frame of reference, focusing on relative price behavior.

### A. Deployment (Facilitator)

This section outlines the deployment process, focusing on connecting the implemented codebase to the experimental results presented in the dissertation. This connection is crucial for validating the research findings and ensuring the practical applicability of the dynamic plane concept.

**Connecting the Dynamic Plane Implementation to Experiments:**

The implementation of the dynamic plane, specifically the `RotatingSnapshotGenerator` module, is directly linked to the experimental setup. This module applies the dynamic transformations to the price data, enabling the model to adapt to changing market conditions. Key aspects of this connection include:

- **Data Preprocessing:** The `RotatingSnapshotGenerator` integrates the calculations for local movement vectors, rotation matrices, and origin shifts into the data preprocessing pipeline. This ensures consistent transformations across training and testing datasets.
- **Model Input:** The model architecture is designed to accept the transformed data produced by the `RotatingSnapshotGenerator`. The input layer is configured to handle the rotated and translated price data.

**Reproducing the Dynamic Plane Implementation (Optional):**

To facilitate reproducibility and further investigation, the following steps detailing the `RotatingSnapshotGenerator`'s functionality can be provided upon request:

- **Local Movement Vector Calculation:** The module calculates the local movement vector, `v = P_current - P_previous`, where `P_current` and `P_previous` represent the current and previous price and time coordinates. This vector defines the direction of recent price movement.
- **Rotation Matrix Calculation:** The module determines the rotation angle (θ) using `θ = arctan(v_y / v_x)` and constructs the rotation matrix `R(θ) = [[cos(θ), sin(θ)], [-sin(θ), cos(θ)]]`. This matrix rotates the coordinate system to align with the current price trend.
- **Dynamic Origin Shift:** The module translates the origin to the current price point (0,0) after rotation. This centers the price action, allowing the model to focus on oscillations around the current trend.

**Focusing on Specific Repository/Model Aspects (Optional):**

Upon request, more detailed documentation and analysis can be provided on specific aspects of the dynamic plane implementation, including:

- **Dynamic Origin Implementation:** A deeper explanation of how the dynamic origin functions within the 2D plane analogy, clarifying the dynamic refocusing behavior.
- **Dynamic Rotation Implementation:** A detailed breakdown of the dynamic rotation of the coordinate system, explaining how the local movement vector aligns the rotated axes and the subsequent translation to the current price point creates a dynamically adjusted snapshot of price action. This will clarify how the `RotatingSnapshotGenerator` produces the transformed data used by the model.

### A. Deployment and Reproducibility

This section outlines how the experimental code connects to the final models, enabling reproducibility and facilitating focused analysis. While full deployment in a production environment isn't the primary goal, these steps ensure the research is well-documented and can be readily reproduced or extended.

**Connecting Code to Experiments:** The codebase should clearly link specific modules and functionalities to the experiments. This includes referencing experiment parameters, data versions, and model configurations within the code (e.g., through comments, configuration files, or dedicated documentation). This traceability simplifies understanding how the code generated the reported results and specifically how the Rotating Dynamic Plane Generator integrates into the Vision Transformer training pipeline. This documentation should clarify the parameters used for the generator, such as the window size and optional volume inclusion.

**Reproducibility (Optional):** A guide for reproducing the experiments and models can be provided, including detailed instructions on environment setup, dependency installation, data access, and code execution. This facilitates further investigation and validation of the research findings.

**Focused Analysis (Optional):** Upon request, specific aspects of the repository or model can be analyzed in greater depth. This could include profiling, performance benchmarking, sensitivity analysis, or detailed explanations of particular modules, parameter choices, or experimental configurations. This targeted approach enables in-depth exploration of specific areas of interest, such as the dynamic 2D plane generator. Providing a conceptual diagram illustrating the dynamic rotation operation on price movements and the reduction of three axes (time, price, volume) to two rotational axes on a dynamic 2D plane would be particularly helpful. This explanation should cover the dynamic redrawing of the plane, the refocusing of the origin, and the handling of the parabolic curve of price movement within this simplified 2D representation.

**Visualizing Data Transformations (Optional):** To clearly demonstrate the dynamic plane implementation, the following visualizations can be included:

- **Example Input:** Example images of the raw candlestick data _before_ transformation.
- **Example Transformed Input:** Example images of the candlestick data _after_ the dynamic plane transformation.
- **Dynamic Redrawing Animation:** An animation visualizing the step-by-step evolution of the dynamic plane as new data points are added, demonstrating the dynamic recalculation and redrawing process.

## Deployment

This section outlines the deployment and accessibility of the project's visualization component, specifically the dynamic plane animation. While not a traditional software deployment, it focuses on ensuring the reproducibility of experiments and providing a standalone animation simulator.

### Connecting Code to Experiments

Clear traceability between the codebase and the conducted experiments is crucial. Documentation should detail how specific code modules relate to the experimental setup and the results presented. This includes specifying code versions, parameters, and data used for each experiment.

### Reproducing Models (Optional)

A concise guide for reproducing the trained models can be provided upon request. This guide should outline the necessary steps, including data acquisition, pre-processing, model training, and evaluation.

### Focused Analysis (Optional)

Targeted documentation and explanations can be provided for specific aspects of the repository or models requiring in-depth analysis. This is driven by requests and may include deeper dives into particular algorithms, data transformations, or model configurations.

### Standalone Animation Simulator

The core deliverable is a standalone animation simulator visualizing the dynamic plane, point movement, and plane rotation. Key features include:

- **Delayed Rotation:** Rotation is delayed until at least three stable data points are available to mitigate PCA instability with fewer points.
- **Smoothing of Early Plane Formation:** Smoothing techniques ensure a visually coherent transition as initial data points are added.
- **Full Step-by-Step Visualization:** Provides a complete visualization of the dynamic plane, including dynamic point movement, live frame rotation, and re-centering.
- **Smooth Rotation Matrices:** Smoothing or stabilization techniques prevent dimensional errors and ensure smooth transitions in the animation.
- **Handling Limited Data Points:** The simulator gracefully handles scenarios with fewer than two data points, displaying a placeholder instead of attempting to animate and preventing errors. Properly formatted offsets further ensure accurate calculations, especially in early animation stages.

This simulator fulfills the following functional requirements:

- **Dynamic Point Movement:** Clearly displays the dynamic movement of points over time.
- **Live Frame Rotation and Re-centering:** Shows real-time frame rotation and re-centering as the animation progresses.

### Heiken-Ashi Chart Deployment

The project also includes the deployment of visualization and transformation tools related to Heiken-Ashi candles. This involves saving generated standard Heiken-Ashi charts for subsequent analysis and integration within the broader project.
Deployment

This section details how to generate visualizations and analyses of the Dynamic Plane implementation for inclusion in the dissertation. This is not related to deploying a trading system to a live environment.

**Visualization and Analysis Tasks:**

1. **Standard Heiken-Ashi Charts:** Generate standard Heiken-Ashi candlestick charts and save them as PNG images at `/mnt/data/standard_heiken_ashi.png`.

2. **Rotated Dynamic Plane Heiken-Ashi Charts:** Generate dynamically rotated and recentered Heiken-Ashi charts and save them as PNG images at `/mnt/data/rotated_dynamic_plane_heiken_ashi.png`.

3. **Simulated Market Conditions:** Simulate realistic market conditions, including rallies, drops, and recoveries, to test the model's resilience to complex market dynamics.

4. **Choppy/Sideways Market Simulation:** Simulate a chaotic, choppy, sideways market to test the dynamic plane's robustness under extreme volatility. Use the `generate_choppy_candlesticks(n=30)` function to produce the simulated data.

5. **Choppy Market Visualization:** Generate and save visualizations for both standard and rotated Heiken-Ashi charts based on the choppy market data:

   - Standard Heiken-Ashi: `/mnt/data/standard_heiken_ashi_choppy.png`
   - Rotated Dynamic Heiken-Ashi: `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`

6. **Connecting Code to Experiments:** Clearly link the code generating the visualizations to the specific experiments and data used for reproducibility.

7. **Comprehensive Market Regime Comparison:** Generate a single, cohesive visual panel comparing standard Heiken-Ashi charts alongside their corresponding rotated Dynamic Plane charts across three distinct market regimes:
   - Trend-Reversal-Recovery
   - Choppy Sideways
   - A simulated third regime (e.g., a strong linear uptrend or a sharp V-shaped recovery). This requires simulating this third regime.

**Optional Tasks:**

- **Reproducing Models:** Provide a concise guide for reproducing the Dynamic Plane model and visualizations.
- **Focused Analysis:** Provide a deeper analysis on specific requested aspects of the Dynamic Plane implementation, codebase, or associated experiments. This could include:
  - A written analysis summarizing how different market regimes (trend, reversal, sideways) appear within the dynamic plane representation and the implications for model learning.
  - Visualizations showcasing how the three defined market regimes are represented on the dynamic plane.

This streamlined process ensures that all visualizations and analyses are readily available for inclusion in the dissertation.

### A. Deployment (Facilitator)

This section details how the experimental results connect to the developed codebase, ensuring research reproducibility and facilitating further investigation. While full deployment is not the primary focus here, this section emphasizes traceability and accessibility.

- **Connecting Code to Experiments:** All experimental results and configurations must be clearly linked to the corresponding code within the repository. This includes documenting specific code versions, parameter settings, and data sources used for each experiment. This ensures transparency and allows for easy verification and reproduction of findings.

- **Reproducing Models (Optional):** Providing a clear guide for reproducing the trained models is highly recommended. This should include detailed instructions on environment setup, data preprocessing, model training procedures, and evaluation metrics. This facilitates future research, validation, and extension of the project.

- **Focusing on Specific Repository/Model Aspects (Optional):** Specific aspects of the repository or models requiring particular attention can be highlighted and documented here. This targeted approach allows for a deeper understanding of particular model components, design choices, or performance characteristics, and enables tailored explanations based on specific stakeholder needs.

## A. Deployment (Facilitator)

This section outlines how to connect the experimental code to the final model, facilitating reproduction and targeted analysis. While full deployment in a live trading environment isn't the primary focus, this section ensures the codebase is well-organized, understandable, and reproducible for future research and potential expansion.

Key deployment considerations include:

- **Connecting Code to Experiments:** Meticulous documentation is crucial for traceability. Each code section should clearly link to the corresponding experiment, specifying commit hashes, experimental parameters, and relevant results. This ensures a clear connection between the experimental findings and the code that generated them.

- **Reproducing Models (Optional, but Recommended):** A comprehensive guide for reproducing the trained models should detail the necessary steps: data acquisition and preprocessing, model training parameters, and required environment setup. This promotes transparency and enables others to validate the results and build upon the work.

- **Targeted Analysis (Optional):** If requested, provide focused analysis on specific aspects of the codebase or models. This could include deep dives into particular modules, explanations of design choices, or investigations of specific model behaviors. This allows for tailored insights into areas of particular interest.

It's important to note that the provided checklist items primarily address model development and training (e.g., error calculation, PCA frame management). While crucial for overall reproducibility, these items don't directly translate into specific deployment steps. Therefore, this deployment section focuses on general best practices for connecting code to experiments, enabling model reproduction, and facilitating deeper analysis, rather than providing specific instructions derived from the checklist.

### A. Deployment (Facilitator)

This section outlines the process of connecting the experimental results with the codebase and facilitating the reproduction of the models. While not strictly deployment in a production environment, this focuses on ensuring the research is reproducible and the connection between the dissertation and the supporting code is clear. Furthermore, it details the implementation of dynamic performance monitoring and adaptation within the deployed model.

**Connecting Code to Experiments:**

A clear mapping between the experimental results presented in the dissertation and the corresponding code sections is crucial for reproducibility. This can be achieved through:

- **Clear Code Comments:** Reference specific experiment numbers or configurations within the codebase.
- **Supplementary Documentation:** Provide a document explicitly linking results with code sections.
- **Direct Code Links:** Numerical examples demonstrating error calculations (checklist item `cdb894dd-f944-4aba-89a8-8676600925c0`) and the pseudocode for error computation (`877ad5e6-921e-4e7c-ac75-2ee4ac9e7410`) should have clearly referenced corresponding code implementations.
- **Default Parameter Documentation:** Clearly document within the code the default weights for α, β, and γ (checklist item `6571b058-573a-4955-87fc-651a97f1e3ef`) where they are initialized and used.

**Reproducing Models (Optional):**

A concise guide for reproducing the models and experiments can enhance the research's impact. This guide should detail:

- Software dependencies
- Data acquisition steps
- Model training parameters
- Evaluation procedures
- Angle unit normalization (checklist item `2db5640b-b017-4543-8b78-3eb1b1d88a3b`) and how they are handled within the codebase.

**Focus on Specific Repository/Model Aspects (Optional):**

Upon request, a focused analysis of specific aspects, such as the error trend detection and healing phase design (checklist item `f6d505c0-106b-4ddd-bd48-2bcc3d4fab98`), can be provided. This analysis should explain the self-regulating system's code implementation and the rationale behind the chosen correction mechanisms.

**Performance-Based Healing and Decay Rate Adjustment:**

The deployed model should incorporate dynamic performance monitoring and adaptation through a "Healing-by-Correctness" system:

1. **Prediction Correctness Tracking:** Continuously monitor and score predictions (+1 for correct, 0 otherwise), irrespective of frame-related issues.
2. **Rolling Prediction Correctness Buffer:** Maintain a buffer of the last N prediction correctness scores to calculate the mean prediction correctness.
3. **True Prediction Value Tracking:** Record the actual prediction values for decay rate adjustment and healing.
4. **Dynamic Decay Rate Adjustment:** Adapt the lag compensating for previous disruptions based on the rolling buffer's mean prediction correctness.
5. **Performance-Based Healing:** Tie the correction factor directly to the mean prediction correctness, ensuring faster recovery during periods of high accuracy.

**Data Model Enhancements for Robustness and Visualization (Optional):**

While not strictly deployment, the following enhancements facilitate practical application:

- Further details on these enhancements were not provided in the checklist items. This section can be expanded upon once those details are available.

## A. Deployment (Facilitator)

This section details the process of deploying the experimental results for analysis and dissertation purposes, specifically focusing on visualizing the data transformation pipeline. This is **not a production deployment**, but rather a deployment of experimental findings for review and reproducibility.

**Visualizing the Data Transformation Pipeline:**

The key objective is to generate visualizations that illustrate the effects of the data transformations on candlestick and volume data. This involves creating a series of before-and-after images showcasing the transformation process.

**Image Generation and Display:**

Five pairs of images will be generated, each demonstrating the transformation of raw market data into the format used by the Convolutional Neural Network (CNN).

1. **Input (Before):** Each "before" image will display a candlestick chart representing raw price and volume data at 10-minute intervals over a single day.

2. **Output (After):** The corresponding "after" image will visualize the same data after applying the following transformations:
   - Calculation of log returns from raw prices.
   - Application of Principal Component Analysis (PCA) rotation.
   - Normalization of time, log return, and log volume data to the range [-1, 1].

This paired presentation allows for a direct visual comparison of the raw data and its transformed representation, effectively illustrating the impact of each step in the transformation pipeline and demonstrating the data format ultimately inputted into the CNN.

**Reproducibility and Further Analysis:**

While the primary focus is on visualizing the transformations, supplemental materials can be provided upon request:

- **Reproducibility (Optional):** A guide for reproducing the model training and data transformation process can be provided if needed.

- **Focused Analysis (Optional):** More in-depth analysis of specific aspects of the repository, model, or transformation pipeline can be provided upon request, allowing for a more granular examination of particular components.
  This section outlines considerations for deploying the developed model and ensuring reproducibility, bridging the gap between research and practical application.

1. **Connecting Code to Experiments:** Thorough documentation linking the codebase to the experiments is crucial for reproducibility and understanding how the code supports the research findings. This includes specifying code versions, parameters, and data used in each experiment.

2. **Reproducing Models (Optional):** A clear guide outlining the steps to reproduce the trained models, including environment setup, data preparation, and training scripts, facilitates further research and validation.

3. **Focused Analysis (Optional):** Upon request, targeted analysis and documentation of specific repository or model aspects can be provided for a more in-depth understanding.

## A. Deployment (Facilitator)

This section details how the developed models connect to the experimental results and provides guidance for reproducing the research. While deploying a fully functional trading application is beyond this project's scope, the focus here is on reproducibility and clear documentation, enabling others to understand and build upon the work.

**Connecting Code to Experiments:** A clear link between the codebase and the conducted experiments is crucial for traceability and verification. This can be achieved through various methods, such as tagging commits with experiment identifiers, maintaining a detailed log of experiment parameters within the codebase, or utilizing a dedicated experiment tracking platform. This ensures that all results presented can be directly traced back to the specific code and configuration used.

**Reproducing Models (Optional):** A comprehensive guide for reproducing the trained models is highly recommended, facilitating independent verification and future research. This guide should include details on:

- Required dependencies and environment setup.
- Data preprocessing steps, including the Dynamic Rotating Plane method and generation of intraday, daily, weekly, and monthly timeframes.
- Model architectures, including the hierarchical attention model.
- Training parameters and hyperparameter tuning.
- Evaluation metrics and baseline model comparisons, especially during significant market events.

While reproducing the complete training process, including computationally intensive multi-scale data preparation, might be optional given its complexity, providing clear instructions for each step is essential.

**Focusing on Specific Repository/Model Aspects (Optional):** Upon request, more in-depth analysis and documentation can be provided for specific aspects of the repository or models. This allows for targeted investigation and addresses specific questions. Examples include:

- Deep dives into the multi-scale data processing techniques, including different timeframes and the Dynamic Rotating Plane implementation using PCA on normalized Time, Price, and Volume data.
- Analysis of the hierarchical attention model and its dynamic querying of different timeframes.
- Examination of specific model configurations, modules, or performance characteristics.
- Exploring alternative architectural approaches, such as ensemble methods combining predictions from specialist models trained on various timeframes or multi-input transformer models processing all timeframes simultaneously, along with their associated weighting strategies (static, dynamic, or learned). Documenting these explorations provides valuable insights for future development.
- Analysis of the data pipeline complexity and potential processing overhead from combining multiple datasets, which is crucial for future optimizations and deployment strategies.
  Deployment

This section details how the research code and models can be accessed and understood, ensuring reproducibility and facilitating further investigation. While this project primarily focuses on research and model development, deploying the model in a simplified environment for reproducibility is a key aspect. This section addresses this type of deployment, rather than deployment in a live trading system.

**Connecting Code to Experiments:** The codebase is meticulously documented to connect it directly to the experiments presented in the dissertation. Clear comments within the code explain the purpose of different sections and how they relate to specific experimental results. This traceability ensures transparency and allows for verification of the findings.

**Reproducing the Models (Optional):** A comprehensive guide for reproducing the trained models can be provided. This guide will outline the necessary steps, including setting up the environment, installing dependencies, acquiring the datasets, and executing the training scripts. While not required, this significantly enhances the research's transparency and allows others to validate the results.

**Focusing on Specific Aspects (Optional):** Upon request, focused documentation and analysis can be provided for specific aspects of the repository or models. This allows for a more in-depth understanding of particular components, modules, parameter choices, or design decisions. This targeted approach caters to specific inquiries and highlights interesting or unique elements of the project.

## A. Deployment (Facilitator)

This section outlines the process of connecting the experimental results with the codebase and facilitating the reproduction of the models. This ensures the research is reproducible and the connection between the code and the dissertation is clear. While not a deployment to a production environment, it addresses practical usability and future development.

The following steps facilitate this process:

- **Connecting Code to Experiments:** A clear and traceable link must be established between the codebase and the experiments conducted. This includes documenting the specific code versions, parameters, and data used for each experiment. This ensures reproducibility and allows for easy verification of the results. Version control and experiment tracking tools can be beneficial for managing this information.

- **Reproducing Models (Optional):** Providing a clear guide for reproducing the models is highly beneficial for validation and future research. This guide should detail the steps involved in setting up the environment, acquiring the data, preprocessing steps, and training the models. Including specific commands and scripts will further enhance reproducibility.

- **Focusing on Specific Repository/Model Aspects (Optional):** Upon request, a focused analysis of specific aspects of the repository or models can be provided. This could include deep dives into particular modules, algorithms, design choices, or performance characteristics. This targeted analysis can offer valuable insights for understanding the system's inner workings and potential areas for improvement. Examples of such analyses include hyperparameter permutation testing, exploring context-aware periodicity, evaluating transfer learning between different markets, or examining PCA analysis using dynamic plane projections.

### A. Deployment (Facilitator)

This section outlines the deployment process for the SCoVA project, focusing on connecting the codebase to experimental results and integrating with the Zerodha KiteConnect API. While full model reproduction isn't a primary focus, this section provides guidance on setting up the necessary environment and addressing key deployment considerations.

**Zerodha KiteConnect Integration:**

Seamless integration with the Zerodha KiteConnect API is crucial for deployment. This involves:

1. **Data Acquisition:** The KiteConnect API will be used to download historical market data and acquire live training data via a websocket connection, ensuring real-time data feeds for model training and updates.

2. **Websocket Connection:** A persistent websocket connection via the KiteConnect API will facilitate real-time data streaming for live trading and model updates, enabling the system to respond dynamically to market fluctuations.

3. **Order and Portfolio Management:** The KiteConnect API will manage order execution, portfolio tracking, and performance monitoring, providing a unified interface for trading activities.

**Dharmic Mandate Implementation:**

The deployed system must adhere to pre-defined ethical rules ("Dharmic Mandates") based on principles of Satya, Shaucha, and Santosha. These non-violable rules ensure ethical operation and may include:

- Immutable transaction logs for transparency and accountability.
- Rigorously validated and cleaned data sources.
- Balancing profit maximization with portfolio stability and risk management.

**Bhagavad Gita's Four Paths to Liberation:**

The application's architecture, including the deployment strategy, should reflect the philosophical principles of the Bhagavad Gita's four paths to liberation (Gyaan, Bhakt, Karam, and Raaj Yoga). Consideration will be given to how these principles influence the user interface and underlying architecture. Further documentation on this integration can be provided upon request.

**Reproducibility and Focused Analysis (Optional):**

While not mandatory for this phase, instructions for full model reproduction can be provided upon request. Similarly, in-depth analysis of specific repository or model aspects during deployment can be facilitated as needed. This allows for flexible deployment strategies based on specific project requirements.

**Connecting Code to Experiments:**

Clear linkage between the codebase and the experiments conducted is essential for traceability and reproducibility. Thorough documentation and version control within the repository will ensure that results can be directly traced back to the code that generated them.

**Firebase Deployment (Optional):**

While full deployment details are outside the scope of the current research phase, the project is intended to be developed with Gemini agentic help on `VS Code with Gemini Code Assist Agent` and deployed to Firebase. This information is provided for future reference and planning.

## A. Deployment (Facilitator)

This section outlines the deployment process, focusing on connecting the codebase to the experimental results and facilitating the reproduction of models. Thorough documentation is prioritized to minimize ambiguity and ensure reproducibility. The deployment process assumes a cloud-based experimental platform, though the instructions can be adapted for other environments.

**1. Connecting Code to Experiments:**

This subsection details the relationship between the codebase and the conducted experiments, providing explicit links between specific code versions and experimental runs.

- **Version Control:** The project utilizes [Specify Version Control System, e.g., Git]. Specific code versions associated with each experiment are identified using [Specify Method, e.g., commit hashes, tags, or branch names]. Example: `git checkout <commit_hash>` for a specific commit.
- **Experiment Tracking:** [Specify Tracking System, e.g., MLflow, Weights & Biases, or a custom solution] is used to track experiment parameters, metrics, and artifacts. [Provide details on how to access experiment data within the tracking system].
- **Mapping Code and Results:** Clear instructions on how to locate the code used for each experiment are provided, including directory structures, filenames, and specific function calls. Example: "Experiment A uses the `train_model.py` script in `experiments/A/` with configurations defined in `config.json` within the same directory."
- **Data Management:** Paths and versions of all datasets (training, validation, and testing) used in each experiment are documented, including file formats and preprocessing steps.

**2. Reproducing Models (Optional):**

This section provides a detailed guide for reproducing the trained models.

- **Environment Setup:** Instructions for setting up the required development environment, including specific software versions (e.g., Python 3.9, TensorFlow 2.10), library dependencies (using a `requirements.txt` file: `pip install -r requirements.txt`), and hardware requirements are provided.
- **Data Preparation:** Steps to reproduce data preprocessing and preparation, including data cleaning, transformations, and feature engineering are detailed.
- **Training Procedure:** Comprehensive instructions for executing the training scripts, including command-line arguments, configuration files, and hyperparameter settings are provided. Example: `python train.py --config config.json --epochs 100 --batch_size 32`.
- **Model Checkpoints:** Instructions on accessing and utilizing saved model checkpoints for resuming training or performing inference are included.

**3. Addressing Specific Repository/Model Aspects (Optional):**

Upon request, further analysis and documentation can be provided for specific parts of the repository or models. This can include:

- **Targeted Documentation:** Detailed explanations of specific modules, functions, or algorithms, including their purpose, implementation, and usage examples.
- **Code Walkthroughs:** Step-by-step explanations of complex code segments.
- **Performance Analysis:** Analysis of performance characteristics such as runtime, memory usage, and scalability.

## Deployment

This section details the deployment strategy, encompassing frontend and backend components, data management, and the implementation of key modules within a distributed architecture leveraging federated learning on iPads.

**Frontend Deployment:**

The Progressive Web App (PWA), built with Flutter, will be deployed to Firebase Hosting. This offers simplified deployment, fast and secure hosting via a global CDN, and offline capabilities for enhanced user experience.

**Backend Deployment:**

The backend, composed of Python 3.11+ containerized microservices, will be deployed to Google Cloud Run. Google Cloud Tasks will orchestrate these services, providing scalability, flexible resource utilization, and robust workflow management.

**Data Storage and Management:**

- **Google Cloud Firestore:** This NoSQL document database will store metadata, configuration data, and real-time application state, leveraging its flexibility and scalability.
- **Google Cloud Storage:** Cost-effective and scalable storage for large datasets, including historical market data (Parquet files) and trained model artifacts.

**Key Modules and Tasks:**

- **Interactive Results Dashboard:** This module provides an intuitive interface for visualizing backtesting results, performance metrics, and other key insights.
- **Live Trading Engine:** This module connects to brokerage APIs and executes trades based on model predictions. Detailed technical specifications will ensure safe and reliable trading operations.
- **Karma Ledger:** This module tracks resource usage and other relevant metrics. Its integration within the overall system will be detailed further.

**Federated Learning Deployment Strategy:**

This project utilizes a federated learning approach, leveraging iPads for on-device computation. The server acts as a coordinator, orchestrating the distributed training process.

- **Server-Side Orchestration:**

  - **Data Storage and Serving:** The server stores and serves raw market data to the iPads.
  - **Global Model Management:** The server maintains the master model, aggregating knowledge from all client devices.
  - **Training Coordination:** The server distributes the global model and training instructions to connected iPads, ensuring synchronization.

- **Client-Side Processing (iPad):**

  - **Data Fetching:** iPads fetch the necessary market data from the server.
  - **Local Model Training:** Using TensorFlow.js to harness the iPad's GPU, each iPad trains a local model based on the fetched data and the current global model. Web Workers enable background processing to maintain iPad interactivity.
  - **Model Update Transmission:** iPads send optimized model updates (small update packages) back to the server. Caching mechanisms on the iPad enhance efficiency.

- **Federated Learning Framework:**
  - TensorFlow.js is integrated for on-device model training.
  - Efficient data transfer and aggregation mechanisms maximize iPad potential and streamline server interaction.
  - Local image generation leverages iPad hardware capabilities.

This distributed architecture, with client-side heavy lifting on iPads, minimizes server load, improves system efficiency, and maintains data privacy. Further details on individual components and technical specifications will be provided in subsequent documentation.

## A. Deployment (Facilitator)

This section details the deployment strategy, focusing on a hybrid architecture that leverages both server-side and client-side (iPad) resources. This approach balances the computational demands of Vision Transformer training with the need for interactive features and acknowledges potential client-side resource limitations.

The deployment process involves the following key considerations:

- **Client-Side (iPad):** The iPad utilizes TensorFlow.js for on-device model training and image generation using the Canvas API or other suitable JavaScript graphics libraries. Web Workers handle background processing to maintain UI responsiveness. Raw market data is fetched from the server, and trained model updates are sent back to the server after local training.

- **Server-Side:** The server provides API endpoints for serving raw market data to the iPads, distributing the global model, and receiving client-side model updates. It implements federated averaging to aggregate these updates and update the global model.

- **UI/UX Integration:** The UI for both the Campaign Runner and Experiment Designer will be updated to reflect the distributed training architecture. New status indicators will provide feedback on data downloading, image generation, local training progress, and model update uploads. Local resource monitoring will also be integrated to provide insights into the iPad's performance.

- **PWA Considerations:** Although a Progressive Web App (PWA) was initially considered, the computational demands of Vision Transformer training within a PWA, particularly on an iPad, present significant technical challenges. Thorough investigation into PWA suitability for GPU-intensive tasks, including potential browser crashes and performance limitations, indicates that a PWA is likely not a viable deployment strategy for this project. Alternatives such as native iOS applications are being prioritized due to improved access to hardware resources and performance stability.

- **Connecting Code to Experiments:** The codebase maintains a clear link between experimental setups and findings through meticulous documentation, version control, and clear delineation of server-side and client-side components. This ensures reproducibility and facilitates analysis of results.

- **Reproducibility (Optional):** A guide for reproducing the trained models, including details on the training environment, data preprocessing steps, model parameters, and the hybrid training approach, can be provided upon request.

- **Targeted Deep Dives (Optional):** Upon request, focused analysis can be provided on specific aspects of the code repository, model, or hybrid architecture, addressing questions related to server-client processing trade-offs, data partitioning strategies, and performance characteristics within the native iOS application. This analysis can include evaluations of Swift libraries for tensor computations and websocket communication.

## A. Deployment

This section details the deployment strategy, encompassing both client-side (iOS) and server-side (Python backend) components, and emphasizes the connection between the experimental codebase and the deployed application.

**Client-Side (iOS):**

The iOS application leverages on-device capabilities for model training, inference, and data visualization. Key technologies and functionalities include:

- **On-Device Training and Fine-tuning:** Using Core ML, the app supports both training new models and fine-tuning existing ones directly on the device, leveraging hardware acceleration provided by the Apple Neural Engine (ANE) and GPU.
- **Core ML Integration:** Models developed in frameworks like PyTorch or TensorFlow are converted to the `.mlmodel` format for seamless integration with Core ML.
- **Metal Framework for GPU Computation:** The Metal framework handles computationally intensive tasks like image generation and rendering within the DynamicPlaneGenerator component, optimizing performance through direct GPU access.
- **Local Data Storage:** Raw OHLCV data from the server is stored locally using a high-performance database solution (e.g., Core Data or Realm) for efficient offline access.
- **DynamicPlaneGenerator:** This native Swift module leverages Metal for PCA, rotations, and rendering numerical data into images or tensors suitable for Core ML.

**Server-Side (Python Backend):**

The Python backend serves a crucial role in data management, API interaction, model versioning, and overall orchestration:

- **Data Serving:** The backend manages the primary database of raw numerical data and serves it to the iOS app on demand.
- **Authentication and API Management:** Securely handles authentication and communication with external APIs, such as the Zerodha Kite Connect API.
- **Model Versioning and Updates:** Stores and serves different versions of trained models and aggregates model updates from the iOS app.
- **Orchestration:** Coordinates the flow of information between the client and various services. The backend primarily facilitates client-side operations by providing necessary resources and managing external communication.

**Connecting Code to Experiments and Reproducibility:**

The backend manages Experiment Templates and stores high-level results, providing a crucial link between the codebase and experimental outcomes. Documentation will clearly outline this connection to ensure traceability and facilitate analysis of the relationship between code changes and experimental results. While a comprehensive guide for reproducing the trained models is optional, it can be provided upon request. This guide would detail data preprocessing, model architecture, training parameters, and other relevant information. Similarly, focused documentation on specific repository or model aspects can be provided upon request.

**Frontend Support and Deployment Preparation:**

The deployed application will support both web and Android frontends. Before deployment, the model will undergo thorough fine-tuning for optimal performance and generalization. Any experimental setup or scaffolding code will be removed to streamline the deployed application. In the deployed environment, image processing will be primarily focused on generating predictions for daily trading and occasional model re-tuning triggered by increased error rates.

## Deployment

This section outlines the deployment strategy for the SCoVA project. The strategy prioritizes efficiency and maintainability while ensuring broad accessibility through web and mobile interfaces. A decoupled architecture, with a Python backend and separate frontends, enables portability across different platforms. The primary deployment targets are a Progressive Web App (PWA) and a native Android application. However, the feasibility of using Flutter for cross-platform development will be investigated.

### Architecture

The system employs a decoupled architecture with the following responsibilities:

- **Python Backend:**

  - Serves the raw numerical data required for model input.
  - Serves the latest trained master model file.
  - Receives and aggregates model updates from the frontend instances.

- **Frontend (PWA/Android/Potentially Flutter):**
  - Performs daily predictions using the downloaded model and data.
  - Executes occasional re-tuning of the model using recent data.
  - Handles computationally intensive tasks, leveraging native app permissions and resources where applicable.

This decoupling allows the frontend to handle computationally intensive tasks, providing greater flexibility and portability.

### Platform-Specific Implementations

**Progressive Web App (PWA):**

- **Technologies:** TensorFlow.js (machine learning), TypeScript/JavaScript (core logic), HTML5 Canvas API (rendering), Web Workers (background processing).
- **Dynamic Plane Generator:** Implemented in TypeScript/JavaScript, leveraging libraries like ndarray or a lightweight matrix math library for PCA calculations.
- **Model Format:** TensorFlow.js JSON.

**Native Android App:**

- **Technologies:** Kotlin (primary language), Jetpack Compose (UI), TensorFlow Lite (on-device machine learning).
- **Dynamic Plane Generator:** Implemented in Kotlin, utilizing libraries like ejml for matrix operations and native Android graphics libraries for rendering.
- **Model Format:** TensorFlow Lite (.tflite).

**Flutter (Under Investigation):**

The feasibility of using Flutter for both web and mobile frontends is being explored. This investigation includes:

- **Flutter/Python Integration:** Researching suitable data transfer mechanisms, authentication protocols, and API design considerations.
- **Client-Side Processing:** Evaluating Flutter's capabilities for high-performance machine learning (potentially using TensorFlow Lite), custom GPU graphics rendering, background processing, and on-device training.
- **Replacing Swift:** Assessing the feasibility of replacing Swift with Flutter, if applicable.

### Model Management and Reproducibility

- **Model Conversion and Integration:** Trained models will be converted to the appropriate format for each platform (TensorFlow.js for PWA, TensorFlow Lite for Android).
- **Connecting Code to Experiments:** Clear documentation will link the codebase to specific experiments, detailing code versions, parameters, configurations, and outcomes. This ensures traceability and reproducibility of results.
- **Reproducing Models:** A guide for reproducing the trained models will be provided.

### Error Handling and Monitoring

Error rate monitoring will be implemented across all platforms. When the error rate exceeds a predefined threshold, a re-tuning process will be triggered, ensuring the model maintains acceptable performance.

## Deployment

This section details the strategy for deploying trained machine learning models, emphasizing cross-platform compatibility, performance, and a streamlined model management process. The approach leverages a "Universal Source Model" and platform-specific optimizations.

**Universal Source Model:**

A framework-agnostic model (using either PyTorch or TensorFlow) will be implemented on the Python backend. This model serves as the single source of truth for model weights and architecture, ensuring consistency across all deployments and simplifying updates.

**Model Conversion and Synchronization:**

A clearly defined process will convert the Universal Source Model into platform-specific formats:

- **iOS:** Conversion to Core ML's `.mlmodel` format for optimal performance.
- **Android:** Conversion to TensorFlow Lite's `.tflite` format for efficient on-device inference.
- **Web:** Conversion to TensorFlow.js format for browser-based deployment (if applicable).

This conversion process will be automated where possible to minimize manual intervention and ensure synchronization between the Universal Source Model and deployed models. Versioning will be implemented to track model updates and facilitate rollback if necessary.

**Mobile Deployment (iOS and Android):**

The mobile deployment strategy utilizes a shared Dart codebase for the user interface and platform channels for native model execution.

- **Shared Dart Code:** The Flutter application's Dart code manages the user interface, data handling (using local storage solutions like SQLite or Hive), dynamic image generation (using `DynamicPlaneGenerator`, `ml_linalg`, and `CustomPainter`), and business logic.

- **Platform Channels:** A `MethodChannel` in Dart invokes platform-specific methods (e.g., `runPrediction`) to execute the model on the native side. This bridges the Dart code with the native model implementations.

- **Native Model Execution:**

  - **iOS:** A Swift class (`CoreMLHandler.swift`) interacts with the Core ML model (`.mlmodel`), receiving image data via the platform channel, performing inference, and returning predictions to Dart.
  - **Android:** A Kotlin class (`TFLiteHandler.kt`) performs analogous functions using the TensorFlow Lite model (`.tflite`). The model will be included in the Android assets.

**Further Considerations:**

- **TFLite Troubleshooting:** Any bugs encountered during TFLite implementation in Flutter will be documented thoroughly, including logs, reproduction steps, and potential reports to the TFLite maintainers. Alternatives like Core ML on iOS will be explored for potential performance gains, using conditional compilation or runtime checks to select the appropriate inference engine.

- **Model Portability Investigation:** The project will analyze the portability of Core ML models to other platforms. This includes examining format, dependencies, and potential conversion strategies to ensure cross-platform compatibility if required. This investigation will inform long-term model training and deployment strategies.

## A. Deployment (Facilitator)

This section outlines the deployment process for the SCoVA project, encompassing model deployment, connection to experimental code, reproducibility, and crucial security considerations. It addresses both on-device and server-side deployments.

**Model Deployment:**

Models will be deployed to their respective client devices (e.g., mobile devices, edge devices). After on-device training or fine-tuning, a synchronization mechanism will transmit updated model weights (or weight deltas) back to the central server, ensuring the universal source model remains up-to-date.

**Connecting Code to Experiments:**

A critical aspect of deployment is establishing clear traceability between the experimental results and the corresponding code. This involves meticulous documentation of parameters, data versions, and specific code commits used for each experiment. This traceability ensures the validity and verifiability of the results, facilitating deeper understanding and future analysis.

**Reproducibility:**

Providing a clear and concise guide for reproducing the trained models is highly recommended. This guide should encompass environment setup instructions, data acquisition procedures, and detailed steps for executing the training scripts. Reproducibility is crucial for validating findings, enabling further research, and ensuring long-term project maintainability.

**Specific Repository/Model Aspects (Optional):**

Upon request, focused analysis and documentation can be provided for specific aspects of the code repository or the implemented models. This could include detailed explanations of particular modules, algorithms, or design choices, offering a deeper understanding of specific functionalities or components.

**Security Considerations:**

Security is paramount throughout the deployment process. The following best practices will be implemented:

- **Secure API Access:** Backend API access will be secured using JSON Web Tokens (JWT) for authentication and authorization. These tokens will be validated using the appropriate SDK (e.g., Firebase Admin SDK if applicable). Rate limiting and input validation will mitigate potential attacks. API access will be restricted by verifying tokens and granting access only to authorized services.

- **Secure Token Storage and Verification:** Tokens will be securely stored on the client-side, and robust backend verification mechanisms will be implemented. Hardcoding sensitive information, such as Firebase UIDs, will be avoided. Dynamic authorization methods will be employed.

- **Data Security:** Data security will be ensured both at rest and in transit. HTTPS will be used for all data communication. Existing encryption mechanisms (e.g., Firebase's built-in encryption at rest) will be leveraged, and additional security rules and IAM policies will be implemented for enhanced data protection.

- **User API Request Verification:** Every user API request will be verified to ensure authorized access. JWT validation will be performed. Scalable and flexible authorization mechanisms will be prioritized for production deployments.

- **Device-Bound Authentication (Where Applicable):** For mobile deployments, device-bound authentication, potentially combined with multi-factor authentication, will be considered to enhance security.

By integrating these security measures into the deployment process, the SCoVA project will ensure the confidentiality, integrity, and availability of its data and services.

### A. Deployment (Facilitator)

This section details deploying the SCoVA project, focusing on connecting the research code to a deployable format, facilitating reproducibility, and outlining the deployment of the Secure Recovery mechanism. While the provided checklist primarily focuses on security aspects, this section expands on those considerations to provide a more comprehensive deployment overview.

**Connecting Research Code to Deployable Artifacts:**

1. **Code-Experiment Mapping:** Clearly document the relationship between the codebase and the experiments conducted in the dissertation. This includes references to specific scripts, configuration files, and data files used for each experiment, along with instructions for execution and result reproduction.

2. **Model Reproduction (Optional):** Provide a streamlined process for reproducing trained models if feasible. This could involve providing pre-trained model weights or detailed instructions for retraining models from scratch using the provided code and data. This step is optional due to the potential size of models and datasets.

3. **Targeted Analysis (Optional):** Offer focused analysis and documentation for specific aspects of the repository or models if requested. This could include detailed explanations of specific model architectures, training procedures, or data preprocessing techniques, tailored to the needs of the dissertation committee or other stakeholders.

**Secure Recovery Deployment:**

The Secure Recovery mechanism is deployed as a Progressive Web App (PWA) and associated cloud function:

- **PWA Hosting:** The Secure Recovery Endpoint is hosted on a dedicated Firebase Hosting site (e.g., `Swaha-recovery.web.app`) for easy access.

- **Cloud Function Deployment:** The `resetEmergencyProtocol` Cloud Function, triggered by the Secure Recovery Endpoint, resets the `isLockedOut` flag in the `/app_config/security` Firestore document. Crucially, it logs restoration events (including IP address and timestamp) in a `security_events` collection for auditing and can optionally send confirmation notifications.

- **Endpoint Security:** The Secure Recovery Endpoint implements robust security measures, including multi-factor authentication (MFA) independent of the user's primary Google account. This MFA utilizes Firebase Phone Number Authentication (OTP) and a pre-set recovery passphrase. A clearly labeled "DEACTIVATE LOCKDOWN & RESTORE SERVICE" button initiates the recovery process.

While the checklist doesn't explicitly detail deploying the core model training and backtesting code, the connection between research experiments and the deployed Secure Recovery Endpoint remains critical for traceability and future analysis. Although Gmail account creation, mentioned in the source material, is a related setup and security protocol, it's not covered here as it falls outside the scope of this deployment section. Future streamlining of the recovery process should consider the complexities of this manual process.
Deployment (Facilitator)

This section details how to connect the Swaha system's codebase, particularly concerning the multi-factor authentication implementation for lockdown/recovery PWAs, to the experimental setup and ensure reproducibility. While the initial checklist focused on security features, this section addresses the practical aspects of making the developed system understandable and potentially replicable for others, particularly in an academic context.

1. **Connecting Code to Experiments:** Clearly document the relationship between the codebase and the experiments performed. This includes:

   - Specifying which code components relate to each experiment involving multi-factor authentication, including device registration, verification, and biometric/PIN authentication.
   - Detailing any specific configuration parameters or settings used during these experiments.
   - Explaining how the experimental results, such as success rates or performance metrics for the authentication flows, were generated from the code.

2. **Reproducing the Setup (Optional):** Provide a concise guide for reproducing the key aspects of the Swaha system related to multi-factor authentication. This could involve:

   - Documenting the steps to set up the PWAs, including any dependencies and required software versions.
   - Explaining how to integrate Firebase authentication and configure the biometric/PIN authentication methods.
   - Outlining the process of simulating lockdown and recovery scenarios within the experimental environment.

3. **Addressing Specific Aspects (Optional):** If specific aspects of the system require further explanation or analysis, address them here. This could include:

   - Detailed analysis of the UI/UX design choices for the authentication flows.
   - Security considerations and potential vulnerabilities of the implemented multi-factor authentication mechanisms.
   - Performance analysis of the authentication system under different conditions.

This documentation will ensure that the Swaha system's implementation, specifically concerning the multi-factor authentication for lockdown/recovery PWAs, is transparent, understandable, and potentially replicable for future research or development.

### A. Deployment (Facilitator)

This section outlines the process for deploying the developed models and connecting the experimental results with the codebase. The primary focus is establishing a clear link between the code and the experiments conducted, ensuring the implemented code corresponds directly to the reported results. This also facilitates future analysis and potential reproduction.

**Connecting Code to Experiments:** Documentation should clearly link the codebase to the experiments performed, specifying which code modules correspond to which experimental setups. This includes referencing specific experiment IDs, parameters used, and the resulting performance metrics.

**Reproducing Models (Optional):** A guide for reproducing the trained models can be provided. This guide should outline the necessary steps, dependencies, data preprocessing steps, model architecture, and training parameters required to replicate the experimental setup.

**Focusing on Specific Repository/Model Aspects (Optional):** Focused analysis can be performed on specific aspects of the repository or models as needed. This allows for a deeper investigation into particular areas of interest, such as performance bottlenecks, specific model components, or design choices. This section can be populated with details of any required deep dives.

While considerations like environment setup, model serving frameworks, data pipelines, integration, and post-deployment testing are important for a production deployment, they are outside the scope of this section, which focuses specifically on connecting the final codebase to the experimental results and facilitating reproducibility. Those aspects are presumably addressed elsewhere in the documentation.

## A. Deployment (Facilitator)

This section outlines the process of connecting the experimental results with the codebase and ensuring reproducibility for future development and validation. The primary focus is on validating the on-device training and update mechanism, rather than achieving optimal inference performance. While full model reproduction isn't strictly required, a guide is recommended for transparency and future research.

**Connecting Code to Experiments:** Clear traceability between experimental results and the codebase is essential. This involves meticulous record-keeping and documentation to demonstrate how specific code changes relate to observed experimental outcomes. For instance, clearly indicate which code modules were used to generate specific figures or tables in the dissertation.

**Reproducing Models (Optional):** A high-level guide outlining the steps to reproduce key models and results is recommended. This guide should detail data acquisition, preprocessing, training, and evaluation procedures. This facilitates independent verification of results and supports future research.

**On-Device Training Verification:** A key aspect of deployment is validating the on-device training process. This involves a single-epoch training run on a dummy model, primarily as a smoke test for the training and update mechanism, _not_ for inference. This validation includes:

- **Data Loading:** Confirm correct data loading into the training environment.
- **Training Loop Execution:** Ensure the training loop runs without errors, including a full forward pass, loss calculation, and backpropagation within the Core ML framework.
- **Weight Extraction:** Verify successful extraction of model weights after training.
- **Data Pipeline Connection:** Confirm the `DynamicPlaneGenerator` correctly feeds transformed image tensors into the Core ML training session on the iPad.
- **Model Update Mechanism:** Clarify how the single-epoch training relates to model improvement, specifically how Core ML updates the universal model on the backend.
- **On-Device Training Necessity:** Evaluate the necessity of performing one epoch of training on the device with a dummy model, considering performance and resource implications.

**Focus on Specific Repository/Model Aspects (Optional):** Upon request, provide focused analysis on specific aspects of the repository or models, such as deep dives into modules, performance bottlenecks, or architectural choices.

## A. Deployment (Facilitator)

This section outlines the deployment process, facilitated by the designated Facilitator. While this project doesn't involve traditional software deployment, the Facilitator ensures the trained models are readily accessible, reproducible, and easily integrated into future research or applications. This includes connecting the developed models to the experiments conducted, providing clear documentation, and offering the capacity for focused analysis.

The Facilitator is responsible for the following:

1. **Connecting Code to Experiments:** Meticulously document the relationship between specific code implementations and the experiments they were used for. This includes clear references to experiment parameters, datasets used, and the resulting outcomes. This documentation enables transparent tracking of the model development process and simplifies reproducing results.

2. **Reproducing Models (Optional):** If required, provide a comprehensive guide for reproducing the trained models. This guide should include detailed instructions on setting up the environment, acquiring and preparing the necessary data, and executing the training scripts. This allows for independent verification of the results and facilitates future research built upon the existing work.

3. **Focused Analysis (Optional):** Upon request, the Facilitator can conduct a focused analysis of specific aspects of the code repository or trained models. This could involve examining particular modules, exploring alternative implementations, or providing deeper explanations of the design choices made. This offering allows for targeted investigations based on specific research questions or areas of interest.

4. **API Gateway & Orchestrator (Future Consideration):** While not currently implemented, a future enhancement involves developing an API Gateway and Orchestrator. This component will serve as the central access point for all requests related to the project, streamlining access and managing complex workflows involved in training, backtesting, and data retrieval. This will further simplify interaction with the system and provide a unified interface for managing different components.

## Deployment (Facilitator)

This section outlines the facilitator's role in orchestrating the deployment and execution of the system, ensuring reproducibility and managing the interaction between components. While this project doesn't involve deployment to a traditional production environment, the focus is on establishing a robust and reproducible workflow for connecting the codebase to experimental setups. This includes configuring the necessary infrastructure and managing interactions between specialist services.

A key aspect of the facilitator's role is managing the connection between the code and executed experiments. This involves using tools and documentation to track experiment parameters and their corresponding code versions, ensuring reproducibility and facilitating analysis. The facilitator oversees the `ExperimentRunner`, which coordinates the execution of individual training/backtesting runs, ensuring its correct interaction with specialist services (model training, data retrieval, etc.) while adhering to communication restrictions. Optionally, the facilitator may also develop a process for reproducing specific models, including documenting dependencies, environment setup, and retraining steps. Furthermore, targeted analysis of specific repository or model aspects can be provided upon request.

The facilitator also plays a crucial role in upholding the following architectural principles during both deployment and ongoing operation:

- **Communication Restrictions:** The facilitator enforces strict communication restrictions between specialists. Specialists may not directly interact with anything outside the internal code, including external storage or other specialists. All communication must be mediated by the facilitator. This fundamental principle shapes the system's structure and interaction with its environment.

- **Data Access Control:** The facilitator ensures that specialists do not directly access databases or storage buckets. Access to these resources falls under the enforcer's domain, and specialists must request data through the appropriate enforcer.

- **Code Conventions:** The facilitator ensures consistent application of the chosen function naming convention throughout the codebase, promoting code clarity and maintainability.

- **Functional Pillar Clarity:** The facilitator maintains a clear understanding of the defined functional pillars (Continuity, Enforcement, Facilitation, and Specialization), which is essential for coordinating the different components.

A primary responsibility of the facilitator is managing the `UI_Gateway` service. This service serves as the central entry point for interacting with the system and orchestrates the execution of various specialist services. The `UI_Gateway` facilitates communication between the user interface and backend services, managing the flow of data and requests. For example, when a user initiates a new experiment through the UI, the `UI_Gateway` receives the request and coordinates the necessary steps, including data preprocessing, model training, and result visualization, by calling the appropriate specialist services, such as `NormalizeWindow`, `ComputePrincipalComponents`, `ProjectToPlane`, and `TrainOneEpoch`. This orchestrated approach ensures a structured and controlled execution of experiments.

## A. Deployment (Facilitator)

This section outlines the deployment strategy for the project, encompassing the necessary steps to connect the experimental results with the codebase, facilitate reproducibility, and ensure robust system monitoring. While a full microservice architecture isn't explicitly defined, the deployment emphasizes modularity and decoupling for future scalability and maintainability.

**Connecting Code to Experiments:** The deployed system maintains clear traceability between the codebase and the conducted experiments. This involves meticulous documentation of code versions, parameters, and datasets used for each experiment, ensuring transparency and facilitating validation of results. This connection is crucial for understanding the context of specific code modules and how they relate to the research findings.

**Reproducibility (Optional):** To facilitate reproducibility, a comprehensive guide can be provided, outlining the steps, dependencies, and data requirements for recreating the experimental environment and rerunning the models. This allows others (and future you) to verify the results and build upon the existing work.

**Focused Repository/Model Analysis (Optional):** Upon request, specific aspects of the repository or models can be analyzed in more detail, providing targeted insights into particular areas of interest. This might involve detailed documentation for specific modules, highlighting critical code sections, or explaining design decisions.

**System Monitoring and Logging:** A comprehensive logbook monitors system performance and component interactions. This logbook tracks:

- **Function Execution Time:** The execution time of each function, including facilitator wait times for specialist service responses.
- **Network and API Calls:** All network interactions and API calls made by all services, enabling analysis of communication patterns.
- **Facilitator-Specialist Interactions:** The pausing and resuming time for facilitators during interactions with specialist services, highlighting potential bottlenecks.

The logbook provides visualizations to aid in understanding the execution flow, especially for asynchronous and parallel services. These visualizations depict:

- **Branching Threads:** Clearly illustrating the creation and execution of separate threads.
- **Loopbacks:** Showing the return of control to the facilitator after receiving responses from specialists.

Integration with existing IDE logging capabilities or adaptable tools will be investigated to streamline log analysis. Furthermore, mitigating potential network bottlenecks will be addressed through strategies like leveraging Google Cloud Pub/Sub for asynchronous communication between critical components and performing rigorous performance testing to preemptively identify and address bottlenecks, ensuring optimal performance throughout the project lifecycle.

## Deployment Considerations for Facilitators

This section outlines considerations for deploying the Facilitator component within the Four Pillars System Architecture. This project primarily focuses on research and dissertation writing, so this section addresses bridging the gap between experimental code and a demonstrable, shareable artifact rather than full production deployment. It focuses on reproducibility and potential future development.

**Connecting Code to Experiments:** Clear documentation and organization are essential for linking the codebase to the experiments conducted in the dissertation. This facilitates verification and validation of results. Ensure the codebase clearly reflects the experimental setup. Specific instructions for reproducing experimental results should be included where feasible.

**Reproducing Models (Optional):** A concise guide for reproducing the trained models can be beneficial. This may include instructions on setting up the environment, data preprocessing, and running training scripts. Due to potential system complexity, full reproduction might be resource-intensive.

**Focused Analysis (Optional):** If specific parts of the codebase or particular model aspects are of primary interest, prioritize detailed explanations and documentation for those sections. This targeted approach streamlines review and understanding.

**Architectural and Functional Requirements Informing Future Deployments:**

While not directly part of the current deployment scope, the following architectural and functional requirements should be considered during development as they inform future deployments:

- **Distributed Tracing:** System-wide distributed tracing using OpenTelemetry is crucial for visualizing complex data flows and identifying performance bottlenecks during model training and execution. This will be essential for understanding and optimizing system behavior in any future deployment. This implementation will include instrumentation of all microservices (Continuity, Enforcement, Facilitator, Specialist) with the OpenTelemetry Python SDK, context propagation via appropriate mechanisms (e.g., HTTP headers, Pub/Sub message metadata), and visualization using a tool like Google Cloud Trace, Jaeger, or Grafana Tempo, preferably with a Gantt chart view. Inter-service communication restrictions will be lifted to allow for greater modularity and scalability, and this tracing mechanism will ensure these interactions are fully understood and monitored.
- **Advanced Error Signal:** An advanced error signal incorporating Vector Deviation Error and Frame Shift Error will be implemented to monitor model health and performance.
- **Performance-Based Healing:** The system's self-correction should be tied to model prediction accuracy rather than fixed timers, enabling dynamic adaptation to changing market conditions.
- **Multi-Scale Periodicity:** The system should incorporate data from multiple timeframes (intraday, daily, weekly) to capture and leverage cyclical market patterns.
- **"Rally Time" Prediction (Enhancement):** Ideally, the model will be enhanced to predict the expected timeframe (in candlesticks) for a predicted price movement.

**Logbook for Bottleneck Identification and Refactoring:** The logbook will be a valuable tool for identifying performance bottlenecks, particularly within Facilitators. These insights will inform refactoring efforts, potentially by decomposing complex Facilitators into smaller, more manageable components, promoting a modular and maintainable architecture.

## A. Deployment (Facilitator)

This section outlines the deployment process, focusing on establishing reproducibility, connecting the codebase to experimental results, and laying the groundwork for future transition to a live trading environment. While this phase doesn't involve live trading, it emphasizes thorough documentation and preparation for robust, explainable deployment. The deployment process will incorporate a paper trading phase to mitigate risk before transitioning to live trading.

**Connecting Code to Experiments:** Rigorous documentation will link the codebase to the conducted experiments. This includes specifying the code version, parameters, and resulting performance metrics for each experiment, ensuring traceability and validation of results.

**Reproducibility:** A comprehensive guide for reproducing the trained models will be created. This guide will detail environment setup, data acquisition, preprocessing steps, and model training procedures. This ensures the research is replicable and facilitates future development.

**Focused Analysis (Optional):** Upon request, specific aspects of the repository or models can be documented in greater detail. This might include in-depth explanations of algorithms, data structures, design choices, or specific modules, catering to the needs of stakeholders and facilitating deeper understanding.

**Risk Mitigation with Paper Trading:** Before deploying to a live environment, the trading strategy will be rigorously tested using a simulated brokerage environment powered by the `Paper_Brokerage_Simulator` service. This allows for refinement and validation without risking capital. This paper trading infrastructure will be integrated as a new Enforcer.

**Explainability and Transparency:** The deployed system, even in the paper trading phase, will prioritize explainability and transparency. It will integrate with a feature store, leverage explanation AI techniques (such as LIME, SHAP, and potentially attention maps), and utilize a `Narrative_Generation_Service` to produce human-readable explanations for each trade. This service will access the feature store, apply explanation AI methods, and potentially leverage a large language model (LLM) to generate comprehensive narratives. These narratives will be recorded in a "Karma Ledger" for auditing, compliance, and reporting purposes. This ensures a clear understanding of the rationale behind every trade decision, even in simulated trading.

## Deployment

This section outlines the deployment process, focusing on integrating the developed trading strategies into a live market simulation environment and ensuring the research is reproducible. This involves connecting the experimental code to the live trading system, simulating realistic order execution, and providing documentation for future research.

**Live Trading Simulation:**

To facilitate a smooth transition from backtesting to live trading, a dual-mode trading system will be implemented. This system will allow users to seamlessly switch between live trading using the `Zerodha Kite Connect API` and simulated paper trading.

- **Paper Trading Mode:** A `Paper_Brokerage_Simulator` will emulate the `Zerodha Kite Connect API`, providing a realistic paper trading experience. The simulator will maintain its internal state (cash balances, open positions, order statuses) within a dedicated Firestore collection named `paper_portfolio`. It will simulate realistic order fills using live tick data (bid/ask prices and volume) from a Zerodha data feed, accurately reflecting real-market execution dynamics. Live WebSocket tick data will be used to simulate network latency and realistic bid-ask spreads, bridging the gap between backtesting and live trading.

- **Dual-Mode Integration:** The `Live_Execution_Enforcer` will be modified to support both live and paper trading modes, controlled by a toggle in the Live Trading Dashboard UI. Trade requests will be routed to the appropriate API based on the selected mode. The Live Trading Dashboard will also display paper portfolio performance.

**Connecting Research to Live Systems:**

A clear link between the experimental code and the final deployed models is essential. This includes documentation that maps specific code sections to corresponding experiments, ensuring traceability and facilitating understanding of the development process.

**Reproducibility and Future Analysis (Optional):**

- **Model Reproduction:** A concise guide for reproducing the trained models, including details on data preprocessing, model architecture, training parameters, and dependencies, is highly recommended.
- **Focused Analysis:** Guidance on specific areas of the repository or models requiring in-depth analysis (e.g., specific modules, functionalities, or performance characteristics) can be provided as needed.

**Market Depth Data Integration (Future Enhancement):**

The following features are planned for future development and are not included in the immediate deployment plan:

- **Market Depth Data Usage Review:** A comprehensive review of current market depth data usage will be conducted.
- **Order Book Feature Derivation:** A dedicated `DeriveOrderBookFeatures` service will process raw market depth data to generate derived features (e.g., Order Book Imbalance, Weighted Average Price, Bid-Ask Spread) for the `DynamicPlaneGenerator`.

## A. Deployment (Facilitator)

This section details how the experimental code connects to the final model implementation and provides resources for reproducibility and targeted analysis. While full production deployment isn't currently within scope, this section ensures traceability and facilitates future work.

- **Connecting Code to Experiments:** The codebase should clearly link to the experimental findings. This includes referencing specific commits, branches, or tags corresponding to each experiment, enabling straightforward tracking of the model's development and validation. Code comments and documentation should clearly connect implementation details to the relevant experimental results and parameters.

- **Reproducing Models (Optional):** A guide for reproducing the trained models, though optional for this project, promotes transparency and future collaboration. This guide should outline the necessary steps, including environment setup, dependency installation, data acquisition, and execution of training scripts.

- **Focusing on Specific Repository/Model Aspects (Optional):** Upon request, focused analysis of specific codebase components or model architecture elements can be provided. This allows stakeholders to investigate areas of interest or concern, such as specific feature implementations or performance bottlenecks.

### A. Deployment (Facilitator)

This section details how the experimental setup is reflected in the codebase, enabling reproducibility and facilitating further investigation. While full production deployment isn't the primary focus of this research project, ensuring a clear link between the code and experiments is crucial for transparency and potential future applications.

**Connecting Code to Experiments:** The codebase directly mirrors the experimental design. The `AsymmetricFeatureEngine` generates a feature vector that's used as a context token by the `Workflow_Broker` and passed to the `Model_Inference_Service` along with the Dynamic Plane image tensor. The ViT model is modified to process this additional context token. This direct integration ensures the deployed model accurately represents the experimental findings. Specific code versions, parameters, and data used for each experiment are documented within the codebase and associated documentation.

**Reproducing Models (Optional):** The integration of asymmetry features into the ViT uses a simplified vector-based approach. This design choice prioritizes ease of reproduction and minimizes potential pipeline disruptions in future development. While a dedicated reproduction guide can be provided upon request, the inherent simplicity of this architecture facilitates reproducibility.

**Focus on Specific Repository/Model Aspects (Optional):** While the current implementation utilizes a straightforward vector-based input for asymmetry features, potential future enhancements could improve the model’s understanding of market asymmetry and volatility. However, such enhancements will be carefully considered to balance potential benefits against the risk of over-engineering and future pipeline modifications. Detailed analysis of specific repository or model aspects can be provided upon request.

### B. Monitoring (Enforcer)

- **High Trading Costs Impacting Alpha:** Analyze the impact of high trading costs on alpha generation.
- **Short-Selling Constraints Impacting Small-Cap Performance:** Analyze the impact of short-selling constraints on small-cap stock performance.
- **Unsuccessful Trade Evaluation:** Investigate and clarify the handling and evaluation of unsuccessful trades.

### B. Monitoring (Enforcer)

This section details the key aspects of monitoring the live trading system and analyzing its performance to identify potential issues and areas for improvement. Understanding the impact of market realities and constraints on the trading strategy is crucial.

- **Performance Evaluation:** Calculate the annualized Jensen's Alpha and Sharpe Ratio for each portfolio, using the equally weighted index as the benchmark and the 3-month Swedish Krona Short Term Rate (SWESTR) as the risk-free rate. Critically, deduct transaction costs of 20 basis points per round trip trade from portfolio returns _before_ performance calculations. This rigorous evaluation provides a clear picture of the strategy's risk-adjusted returns.

- **Trading Cost Analysis:** Analyze the impact of trading costs, including commissions and slippage, on the achieved Alpha. Quantify the reduction in profitability due to these costs and explore potential mitigation strategies. This analysis should consider the frequency of trades generated by the strategy and its associated costs.

- **Short-Selling Constraint Analysis:** Investigate how short-selling constraints might affect performance, particularly for small-cap stocks. This involves understanding the limitations and costs associated with shorting these stocks and assessing their impact on potential profits. Identify any restrictions on shorting specific small-cap stocks and quantify their potential effect on overall portfolio returns.

- **Unsuccessful Trade Analysis:** Define and document a clear process for evaluating unsuccessful trades. This includes documenting the reasons for losses, analyzing patterns in unsuccessful trades, and identifying potential areas for improvement in the trading strategy. This analysis will inform adjustments to the model or trading rules.

- **Stop-Loss Mechanism:** Implement a stop-loss mechanism to mitigate potential large losses on individual trades. Define and monitor the specific parameters of the stop-loss, such as the percentage or absolute price drop that triggers a sell order.

- **Risk-Based Weighting:** Implement a risk-based weighting scheme for portfolio construction. This involves assigning weights to different assets based on their perceived risk levels, aiming to optimize the portfolio's risk-return profile. This scheme should be continuously evaluated and adjusted.

### B. Monitoring (Enforcer)

Monitoring the performance of the trading system after deployment is crucial for identifying potential issues and ensuring optimal performance. This section outlines key monitoring tasks and considerations.

- **Trading Costs and Alpha:** Continuously analyze the impact of trading costs (commissions, slippage, etc.) on the overall alpha generated by the system. Quantify the reduction in alpha attributable to these costs and explore mitigation strategies, such as minimizing trading frequency or optimizing order execution.

- **Short-Selling Constraints and Small-Cap Performance:** For strategies involving short selling, regularly assess the impact of short-selling constraints, particularly on small-cap stocks. These constraints can limit profitability and may disproportionately affect small-cap stocks due to lower liquidity. Monitor instances where restrictions prevent trade execution and estimate their potential performance impact. Consider alternative strategies or adjustments to the investment universe if these constraints significantly hinder performance.

- **Analyzing Unsuccessful Trades:** A detailed analysis of unsuccessful trades is essential for system improvement. This goes beyond simply recording losses and involves investigating the specific contributing factors, such as inaccurate predictions, market volatility, or slippage. These insights should inform adjustments to the model, risk management parameters, or trading strategy.

- **Stop-Loss Mechanism Performance:** Implement and actively monitor a stop-loss mechanism to manage risk. Track its effectiveness, including the frequency of triggering, the average loss mitigated, and any potential adverse effects, such as premature exits from profitable positions. Regularly review and adjust stop-loss thresholds based on market conditions and volatility.

- **Risk-Based Weighting Performance:** Monitor the performance and risk profile of the portfolio under the implemented risk-based weighting scheme. Ensure that position sizes are dynamically adjusted based on the perceived risk of each trade, and regularly evaluate the effectiveness of this approach in optimizing risk-adjusted returns. Refine the risk assessment model and weighting scheme as needed based on performance data and market observations.

## Monitoring (Enforcer)

This section details the monitoring procedures necessary to ensure the ongoing performance and stability of the trading strategy, focusing on cost analysis, trading constraint analysis, and the evaluation of unsuccessful trades. Furthermore, preemptive measures to enhance model robustness and reliability during development are discussed to minimize the need for extensive reactive monitoring.

**Cost Analysis and Mitigation:**

High trading costs significantly erode overall alpha generation. While some portfolios show positive alpha before costs, profitability diminishes or disappears after incorporating trading expenses. This indicates that while the predictive model may generate valid signals, the trading implementation requires optimization. Specifically, frequent rebalancing (every 5 days), uniform weighting within deciles, and the absence of smart trade filtering contribute to excessive costs. A deeper analysis is required to quantify the impact of these factors.

Notably, applying the OMXS All-Share model to the First North All-Share (small-cap) index yielded a positive alpha of +8.89% annually after transaction costs, outperforming the benchmark return of -27.88% by +37.57% annually. This divergent result requires further investigation.

**Trading Constraint Analysis:**

Short-selling constraints, particularly limited borrow availability for certain small-cap stocks, likely hamper performance within this segment. Addressing these constraints is crucial for maximizing returns, given the identified potential profitability of the small-cap segment.

**Unsuccessful Trade Evaluation and Risk Management:**

A clear methodology for evaluating unsuccessful trades—where actual outcomes deviate from model predictions—is critical. Currently, the absence of a stop-loss mechanism raises concerns about potential losses. A detailed analysis of how the referenced paper addresses unsuccessful trades and risk management is necessary, focusing on identification and impact assessment. Implementing a stop-loss mechanism should be considered to mitigate downside risk.

**Preemptive Model Enhancements:**

Addressing the following during development will improve model robustness and reduce the need for extensive reactive monitoring:

- **Trading Accuracy Improvement:** Continuous monitoring of trading accuracy, including analysis of influencing factors and subsequent adjustments, is crucial. Refining alternative labeling strategies, exploring soft labels instead of hard labels, and optimizing model parameters and training procedures can enhance accuracy.

- **Implementing Soft Labeling for CNN Predictions:** Instead of single scalar values, the CNN should output a probability distribution over discretized return bins for more nuanced risk management. This involves:
  - **Discretizing the Return Space:** Dividing the range of possible returns into discrete bins (e.g., -5% to +5% with 0.5% increments).
  - **Changing the Model Output Layer:** Replacing the current output layer with a fully connected layer followed by a softmax activation function to generate the probability distribution.
  - **Converting Labels to Soft Labels:** Transforming hard labels into soft labels using a Gaussian kernel centered around the actual return to improve generalization and address market prediction uncertainty. This approach provides a smoother representation of the target variable and enhances the model's learning capabilities.

By focusing on these preemptive enhancements and continuous monitoring procedures, the model can be made more robust, reliable, and less prone to unexpected behavior, ultimately leading to improved performance and reduced risk.

### B. Monitoring (Enforcer)

This section details the monitoring procedures necessary to ensure the ongoing effectiveness of the trading model and identify areas for improvement. It focuses on analyzing the impact of real-world constraints and learning from trading outcomes, both successful and unsuccessful.

- **Trading Costs and Alpha:** Continuously analyze the impact of trading costs (commissions, slippage, and taxes) on the overall performance and profitability of the trading strategy. Quantify the erosion of alpha due to these costs and identify potential strategies for cost mitigation, such as reducing trade frequency or targeting larger price movements.

- **Short-Selling Constraints and Small-Cap Performance:** Monitor the impact of short-selling constraints, particularly for small-cap stocks. Assess limitations imposed by borrow availability, costs, and regulations, and their effects on portfolio returns and risk. Explore alternative strategies for small-cap stocks if short-selling constraints significantly hinder performance.

- **Analysis of Unsuccessful Trades:** Implement a rigorous methodology for analyzing unsuccessful trades. Investigate the specific reasons for losses, including inaccurate predictions, unforeseen market events, or operational issues. This analysis will provide valuable insights for model refinement and risk management.

- **Stop-Loss Mechanism:** Integrate a dynamic stop-loss mechanism into the trading strategy to limit potential losses. Define stop-loss levels based on risk tolerance, market volatility, and individual trade characteristics. Regularly review and adjust the stop-loss parameters based on performance and market conditions.

- **Risk-Based Portfolio Weighting:** Implement and monitor a risk-based weighting scheme for portfolio allocation. Assign weights to different assets based on their perceived risk levels, using metrics like volatility, beta, or drawdown potential. Regularly evaluate the weighting scheme's impact on the overall portfolio risk and return to ensure optimal performance.

## B. Monitoring (Enforcer)

This section details the ongoing monitoring and improvement strategies implemented after deployment. These strategies focus on refining exit logic, learning from mistakes, and enhancing the reward mechanism to improve the model's long-term performance and adaptability.

**Exit Logic Refinement:**

Post-deployment, the effectiveness of the implemented exit strategies will be continuously monitored and adjusted as needed. Key performance indicators (KPIs) like win rate, average win/loss ratio, and maximum drawdown will be tracked for each exit strategy. This data will be used to optimize the parameters of each strategy (e.g., ATR multiplier for volatility-aware exits, time threshold for time-based exits, divergence threshold for prediction divergence exits, and performance comparison period for portfolio contextual exits). Furthermore, the interaction and potential redundancy between exit strategies will be analyzed to ensure optimal performance.

**Learning from Mistakes:**

A robust learning process is essential for continuous model improvement. The following strategies will be employed to analyze and learn from unsuccessful trades:

- **Error Analysis and Sample Re-weighting:** Misclassified trades, particularly those with significant losses, will be analyzed to identify recurring patterns in market conditions or technical indicators. These "hard cases" will be assigned higher weights during subsequent model retraining to improve the model's ability to handle similar scenarios in the future.
- **Bootstrapping of Hard Cases:** A dedicated dataset of these difficult-to-predict scenarios will be created and used for specialized training cycles to enhance the model's robustness and ability to generalize to challenging market conditions.
- **Meta-Model for Trade Review:** A secondary "meta-learner" model will be trained to predict the likelihood of a trade being unsuccessful _before_ execution. This meta-learner will analyze the initial trade prediction, features of the chart/trade setup, and historical trade outcomes to provide an additional layer of risk assessment and potentially prevent unfavorable trades.

**Reward Logic Enhancement:**

The reward mechanism will be refined to incorporate temporal dependencies and market context:

- **Sequential Analysis of Market States:** The reward calculation will be expanded to consider the sequence of candlestick patterns leading up to the current state, rather than just the current pattern in isolation. This will enable the model to learn from market dynamics and transitions.
- **Model Input Adaptation:** The model's input layer will be adapted to handle sequences of images, allowing it to process and learn from these temporal relationships. This will require adjustments to the model architecture and training pipeline.

This comprehensive monitoring and improvement framework will ensure the trading system remains adaptable, robust, and profitable in the face of evolving market conditions.

## B. Monitoring (Enforcer)

This section details the monitoring procedures crucial for ensuring the continued performance and efficacy of the trading model after deployment. While the model is designed to generate realistic charts and predict market behavior, continuous monitoring is essential to identify and address potential issues in live trading.

Monitoring encompasses several key areas:

**1. Performance Monitoring:** This involves tracking key performance indicators (KPIs) such as Jensen's Alpha, Sharpe Ratio, maximum drawdown, and trading costs. Integration with trading platforms or market data providers may be necessary to obtain real-time portfolio valuations.

**2. Alerting:** A robust alerting system is crucial for notifying stakeholders of significant deviations from expected performance or unusual trading behavior. This could include alerts for large losses, excessive trading costs, or violations of risk limits.

**3. Data Quality:** Continuous monitoring of incoming data is necessary to ensure the model receives accurate and reliable information. This involves checks for missing data, outliers, and inconsistencies.

**4. System Health:** Monitoring the overall health of the trading system is essential, including infrastructure performance (e.g., latency, throughput), connectivity, and resource utilization.

**5. Model Drift:** Tracking the model's performance over time is crucial for identifying signs of model drift or degradation. This may necessitate periodic retraining, implementing adaptive learning techniques, or adjusting model parameters.

**6. Model Accuracy vs. Financial Performance:** Investigating the relationship between the model's predictive accuracy (e.g., accuracy in predicting visual candlestick patterns) and its actual financial performance is vital. High predictive accuracy doesn't guarantee profitability. This analysis should identify any disconnect between accurate predictions and profitable trades.

**7. Actionable Trading Decisions:** The model's output must provide clear and actionable trading decisions. Monitoring should focus on whether the model's predictions effectively inform trading decisions, translating into concrete buy/sell signals, rather than just generating visually accurate but financially ambiguous outputs.

**8. Trading Necessity vs. Pattern Presence:** Monitoring should evaluate the _necessity_ of a trade, not just the presence of a predicted pattern. This involves considering factors like potential profit, risk, and transaction costs to determine if a trade is justified.

**9. Causal Grounding of Predictions:** Understanding the _why_ behind the model's predictions is crucial for long-term performance and adaptability. Analyzing feature importance and employing interpretability tools can provide insights into the model's decision-making process and help identify potential overfitting to historical data.

**10. Robustness of Training and Evaluation:** Monitoring should address potential fragilities in the training and evaluation process, including:

- **Ground Truth Alignment:** Ensuring consistency between predicted and actual market movements.
- **Loss Function Effectiveness:** Validating the effectiveness of loss functions used during training for image fidelity or other relevant metrics.
- **Mode Collapse:** Detecting and mitigating instances where the model generates limited or repetitive outputs, hindering its ability to capture diverse market behaviors.

This multifaceted monitoring approach ensures the model's predictions are both visually accurate and financially relevant, leading to robust and profitable trading outcomes. This section will be further refined and updated as the project progresses and specific deployment details are finalized.

### B. Monitoring (Enforcer)

This section details the ongoing monitoring procedures necessary to ensure the trading strategy's continued performance and identify potential issues. While the provided checklist information focuses on model development rather than live trading, it highlights the importance of rigorous analysis for evaluating performance. The following points, while not explicitly mentioned in the checklist, are crucial for robust performance assessment and should be addressed in the dissertation:

- **Analysis of Unsuccessful Trades:** A thorough analysis of unsuccessful trades is essential. This involves documenting the specific conditions leading to losses, quantifying those losses, and exploring potential mitigation strategies. This analysis provides valuable insights into the model's limitations and informs future refinements.

- **Stop-Loss Mechanism Implementation and Evaluation:** A stop-loss mechanism is crucial for risk management. Monitoring should include tracking its effectiveness in limiting losses during periods of high volatility or unexpected market movements. The implementation details and performance analysis should be documented.

- **Risk-Based Weighting Performance:** If a dynamic allocation strategy with risk-based weighting is employed, its performance must be monitored. This includes analyzing the correlation between assigned weights and actual trade outcomes to assess the effectiveness of the weighting scheme.

- **Trading Cost Analysis:** Monitoring trading costs, including commission fees, slippage, and other transaction costs, is critical for assessing their impact on overall profitability and the strategy's practical feasibility.

- **Impact of Short-Selling Constraints:** If the strategy involves short-selling, particularly in small-cap stocks, the impact of short-selling constraints on profitability and performance metrics should be tracked and analyzed.

These points represent essential considerations for comprehensive performance evaluation, even if not explicitly addressed in the provided model development checklist. They underscore the need to go beyond basic performance metrics and delve into the underlying drivers of those metrics. This level of detail is crucial for a robust and insightful analysis of the trading strategy's performance.

### B. Monitoring (Enforcer)

This section details the monitoring procedures necessary to ensure the continued performance and stability of the trading system after deployment. It addresses both the operational aspects of trading, such as costs and risk management, and the technical performance of the underlying model, specifically the dynamic plane implementation.

**Operational Monitoring:**

This system requires ongoing monitoring for several key operational metrics, including:

- **High Trading Costs Impacting Alpha:** Regularly assess the impact of trading costs (commissions, slippage, etc.) on overall portfolio performance. Investigate and address any instances where excessive costs erode alpha generation.
- **Short-Selling Constraints Impacting Small-Cap Performance:** Evaluate the impact of short-selling constraints, particularly on small-cap stock performance. Explore alternative strategies or adjustments to minimize negative impacts.
- **Unsuccessful Trades:** Analyze unsuccessful trades to identify patterns and potential areas for improvement in the trading strategy or its execution.
- **Stop-Loss Mechanism:** Continuously monitor the effectiveness of the stop-loss mechanism and adjust parameters as needed to mitigate potential losses.
- **Risk-Based Weighting:** Verify the ongoing efficacy of risk-based weighting within the portfolio and adjust as market conditions change.

**Technical Monitoring (Dynamic Plane Implementation):**

Given the dynamic nature of the dynamic plane implementation and its reliance on planar coordinates and transformations, the following technical monitoring measures are crucial:

- **Data Storage and Logging:** Implement a robust logging system to store the planar coordinates (u, v), orientation of the moving frame (using efficient representations like quaternions or rotation vectors), and an arc length counter. Ensure this logging respects memory and storage constraints. This data is essential for debugging, understanding model behavior, and potential future model improvements.
- **Data Visualization:** Generate clear and informative charts and images to visualize the logged data. This allows for visual inspection of the dynamic plane's behavior and potential anomalies, particularly in coordinate transformations. Specifically:
  - **3D Helix Visualization:** Generate a 3D helix plot (with appropriate labels and title) showing the trajectory in laboratory coordinates.
  - **2D Trace Visualization:** Produce a separate 2D trace visualization. Maintain visual clarity by presenting these as separate figures rather than combined subplots. Improve clarity by using intuitive visual representations, informative tooltips or annotations, and potentially a dedicated help section or tutorial explaining the visualizations.
- **Computational Performance (PCA):** Continuously monitor the computational resources used by the Principal Component Analysis (PCA) component. If PCA introduces significant overhead impacting real-time performance, explore alternative dimensionality reduction techniques or optimizations.

This multifaceted monitoring strategy will help ensure the trading system's long-term stability and performance by addressing both operational and technical risks.

### B. Monitoring (Enforcer)

This section outlines the monitoring procedures crucial for maintaining performance and identifying potential issues after deployment. It focuses on practical risk management and performance analysis within a live trading environment. Key monitoring areas include:

- **Transaction Cost Analysis:** Continuously monitor and analyze the impact of transaction costs (commissions, slippage, fees) on overall portfolio performance and alpha generation. High transaction costs can significantly erode profitability. If necessary, adjust the trading strategy (e.g., increase minimum trade thresholds, reduce trading frequency) to mitigate their impact.

- **Short-Selling Constraint Analysis:** Monitor the impact of short-selling constraints, particularly on small-cap stock performance. Restrictions can limit profitability from declining prices. If constraints are significantly impacting performance, consider adjustments to portfolio construction or target asset selection.

- **Unsuccessful Trade Analysis:** Implement a robust methodology for analyzing unsuccessful trades. This involves identifying the root causes of losses (e.g., inaccurate predictions, market volatility) to inform model refinements and improve future trading decisions.

- **Stop-Loss Mechanism:** Implement and monitor a stop-loss mechanism to limit potential losses on individual trades. Carefully calibrate stop-loss parameters based on historical volatility and risk tolerance to balance loss mitigation with the potential for profitable trades.

- **Risk-Based Weighting:** Implement and monitor a risk-based weighting scheme for capital allocation. Assign weights based on perceived risk, allocating less capital to higher-risk trades. This diversifies the portfolio and manages overall risk exposure. Consider factors like volatility, market capitalization, and sector concentration when determining position sizes. Regularly review and adjust the weighting scheme as market conditions change.

### B. Monitoring (Enforcer)

This section outlines monitoring requirements to ensure the dynamic plane visualization functions correctly and provides meaningful insights. These checks focus on data integrity, visualization stability, and the accurate representation of market dynamics.

**Data Integrity and Visualization Stability:**

- **Minimum Data Points for Rotation:** The dynamic plane rotation should only commence after at least three data points are available. This threshold ensures stability in the Principal Component Analysis (PCA) calculation, preventing potential errors and misleading visualizations that can arise from underdetermined systems with fewer points.

- **Smooth Rotation Transitions:** Rotation matrices used in the visualization should be monitored and smoothed to prevent jarring transitions and potential visual artifacts. This ensures a smooth and coherent representation of the data's evolution.

- **Single-Point and Two-Point Handling:** The visualization should handle scenarios with fewer than three data points gracefully. For single-point frames, a placeholder visual or an empty canvas should be displayed. Two-point frames should trigger the delayed rotation logic but avoid PCA calculations until a third point is available.

- **Data Offset Verification:** Data offsets should be consistently formatted and validated to prevent dimension mismatches, which can lead to inaccurate calculations and animation errors, especially with limited data points.

**Visualization Accuracy and Market Representation:**

- **Standalone Animation Simulator:** A standalone simulator will be developed to aid in monitoring and debugging the dynamic plane visualization. This simulator will incorporate the delayed rotation logic, smoothing algorithms, and provide step-by-step visualization capabilities for detailed inspection.

- **Dynamic Point Tracking:** The animation must clearly depict the dynamic movement of individual data points within the rotating plane, allowing for visual tracking of their trajectories and relative positions over time.

- **Real-Time Rotation and Recentering:** The visualization should display the frame rotating and recentering in real-time, providing an intuitive understanding of how the dynamic plane adapts to new data points and maintains its orientation relative to the evolving data structure.

- **Simulated Market Condition Validation:** Two simulated market scenarios—a complex price pattern (rally, drop, recovery) and a choppy sideways market—will be used to validate the visualization's ability to represent diverse market dynamics accurately. Visual comparisons between standard Heiken-Ashi charts and rotated dynamic Heiken-Ashi charts generated from these simulations will be performed.

## B. Monitoring and Refinement

This section outlines the monitoring and refinement procedures crucial for ensuring the ongoing performance, stability, and adaptability of the dynamic plane trading model. This encompasses both high-level performance monitoring and low-level algorithmic refinements driven by prediction error feedback.

**Performance Monitoring**

Continuous monitoring of key performance indicators (KPIs) and risk management metrics is essential to identify and address any deviations from expected behavior. The following areas require ongoing scrutiny:

- **Trading Costs:** Regularly analyze the impact of transaction costs (commissions, slippage, and fees) on realized alpha. Explore strategies to minimize these costs, such as order type optimization and minimizing market impact.
- **Short-Selling Constraints:** Monitor the frequency and impact of short-selling restrictions, particularly on small-cap stock performance. Consider alternative strategies like inverse ETFs or put options if short-selling is consistently problematic.
- **Post-Trade Analysis:** Conduct thorough post-trade analyses of unsuccessful trades to identify systematic errors and inform model refinements. Examine the reasons for trade failures, including incorrect predictions, market volatility, and unexpected news events.
- **Stop-Loss Mechanism:** Implement a dynamic stop-loss order to limit potential downside. Adjust the stop-loss level based on market conditions and volatility.
- **Risk-Based Weighting:** Employ a risk-based weighting scheme to diversify capital allocation across trades. Consider factors like volatility, correlation, and expected return to ensure a balanced portfolio.

**Dynamic Plane Algorithm Refinements**

The dynamic plane algorithm itself requires continuous refinement based on prediction error feedback. The following mechanisms are crucial for this adaptive learning process:

- **Error Signal Integration:** Incorporate an error signal mechanism within the dynamic rotating plane algorithm. This allows the model to learn from prediction errors by adjusting its understanding of market structure based on the difference between predicted and actual market movements.
- **Frame Confidence Correction:** Compare actual market movements against predicted movements after each prediction. Discrepancies should trigger corrections in the rotational frame assumptions, allowing the model to learn the reliability of its dynamic plane projections and refine future rotations.
- **Prediction Error Memory:** Maintain a rolling memory of prediction errors across recent dynamic frames. Consistent misalignments between PCA frame rotations and realized market structure should trigger adjustments to the rotation's weighting, enhancing the dynamic plane's stability and adaptability.
- **PCA Impact on Feature Representation:** Continuously monitor how PCA affects the representation of Price, Time, and Volume on generated graphs. Verify whether the emphasized features vary with different input patterns. This helps understand how PCA interacts with market dynamics and ensures it consistently captures the desired information.
- **Model's Relational Understanding:** Ensure the model learns relationships and structures within the data rather than relying on fixed interpretations of price, time, and volume. Confirm the model recognizes patterns in the dynamic coordinate system created by PCA and isn't overly sensitive to absolute feature values. Evaluate its ability to generalize across varying market conditions.
- **Geometric Pattern Recognition:** Monitor the model's focus on identifying geometric shapes and flows within the normalized PCA space. Prioritize evaluating its effectiveness in capturing and interpreting these patterns over traditional technical indicators that may be less relevant in this context.
- **Interpretability Projection:** Validate the effectiveness of the mechanism used to project the model's focus back into the original Time-Price-Volume space for human interpretation and analysis of its decision-making process. This is crucial given the dynamic axis rotation introduced by PCA.
- **PCA Parameter Optimization:** Continuously monitor and adjust the PCA window size and any smoothing or stability thresholds to mitigate the risk of noise-driven rotations and overfitting. Pay close attention to the impact of these parameters, especially in scenarios with small window sizes or high market noise.

**Visualization for Analysis**

To facilitate analysis and understanding of the dynamic plane's behavior, generate visualizations comparing standard and dynamic plane charts across various market regimes, including Trend-Reversal-Recovery, Choppy Sideways, and a simulated strong linear uptrend or sharp V-shaped recovery. Combine these visualizations into a single panel for direct comparison. This visual analysis will highlight the strengths and weaknesses of the dynamic plane approach across different market conditions.

## B. Monitoring and Adaptation

This section details the monitoring procedures and algorithmic adjustments required to maintain the model's performance and adaptability in dynamic market conditions. These procedures include tracking system behavior, visualizing responses to prediction errors, quantifying corrective interventions, and mitigating potential performance plateaus.

**Error Detection and Correction:**

A rolling frame correction algorithm, inspired by the biological process of wound healing, will be implemented to detect and correct prediction errors. This algorithm comprises three key components:

- **Error Detection:** A rolling buffer (5-10 steps) will store recent prediction errors, defined as the difference between the predicted and realized movement within the rotated dynamic PCA frame. An error trend detector will analyze the rolling mean and variance of these errors, triggering a correction when errors consistently exceed a predefined threshold (e.g., 1-2x the rolling standard deviation).

- **Frame Correction:** Upon triggering, the algorithm applies small rotations or damping adjustments to the dynamic PCA frame to realign it with actual market dynamics.

- **Healing Phase:** Following correction, the algorithm gradually removes the applied adjustments as prediction errors subside, smoothly returning the system to its normal state and preventing over-correction.

**Performance and Stability Monitoring:**

Several metrics and visualizations will be used to monitor system performance and stability:

- **Frame Intervention Metric:** A metric quantifying the frequency and magnitude of frame interventions over a trading year will be developed. This metric will provide insights into the system's fluidity and adaptability, with higher intervention rates potentially indicating data instability or algorithmic issues.

- **Error Visualization:** A visualization tool will illustrate the "error spike → correction → healing decay" process. This visual representation will aid in understanding the system's response to errors and facilitate parameter tuning for correction magnitude and decay rate.

**Addressing Specific Challenges:**

The following monitoring tasks address specific performance challenges:

- **Trading Costs and Alpha:** The impact of high trading costs (transaction fees and slippage) on Jensen's Alpha will be analyzed. This analysis will inform potential adjustments to the trading strategy or cost optimization measures.

- **Short-Selling Constraints:** The impact of short-selling constraints on small-cap stock performance will be investigated by analyzing cases where desired short positions could not be executed and quantifying potential lost returns. Mitigation strategies will be explored.

- **Unsuccessful Trade Analysis:** A clear methodology will be established for evaluating unsuccessful trades, categorizing failures (e.g., stop-loss triggered, incorrect prediction), and analyzing contributing factors to identify potential model weaknesses.

- **Risk Management:** A stop-loss mechanism with dynamically adjusted thresholds and a risk-based weighting scheme for capital allocation will be implemented. Their impact on portfolio risk and return will be continuously monitored.

- **Dual Frame Plateau Mitigation:** Strategies will be explored to address potential performance plateaus arising from the interaction of the dual frames, particularly the tendency of the long-term frame to lag and become static.

- **Frame Coincidence Correction:** A rolling frame coincidence correction mechanism, analogous to the error correction algorithm, will be implemented to prevent the dynamic and static frames from becoming overly aligned and hindering adaptability. This mechanism will gradually return the system to its normal state after applying corrections.

These monitoring and adaptation mechanisms are crucial for ensuring the long-term effectiveness and robustness of the model in dynamic market conditions.

## B. Monitoring and Error Analysis

This section details the monitoring and error analysis procedures crucial for maintaining and improving model performance. This goes beyond simply tracking profit and loss, focusing on understanding the nature of prediction errors to guide further model refinement.

**Error Metrics and Trend Detection:**

- **Deviation Vector Monitoring:** Continuous monitoring of deviation vectors within the dynamic 2D plane is critical. This involves calculating the vector deviation between the predicted movement vector and the realized movement vector. This difference provides insights into the accuracy of the model's predictions in terms of both direction and magnitude.

- **Angular Error Tracking:** The system will track the angular error between the predicted and realized vectors in the rotated 2D plane. This will be calculated using the arccosine of the dot product of the vectors divided by the product of their magnitudes. This provides a measure of directional accuracy independent of the magnitude of the movement.

- **Composite Error Score:** A composite error score will be developed, integrating both distance (magnitude) and angular (directional) errors in predictions. This score, incorporating weighting factors (alpha and beta) to adjust the relative importance of distance and angular errors, will provide a more nuanced understanding of prediction inaccuracies than simply looking at price differences and allow customization based on specific trading strategies.

- **Enhanced Error Trend Detector:** An enhanced Error Trend Detector will be implemented, utilizing the composite error score and incorporating rolling window analysis to identify systematic error trends over time. This will help pinpoint specific market conditions or model weaknesses contributing to recurring prediction errors.

**Dynamic Plane Analysis and Correction:**

- **PCA Plane Consistency:** The consistency of the 2D plane constructed using Principal Component Analysis (PCA) will be investigated. Differences in price and volume data between prediction and reality matrices may lead to variations in PCA axes. The need for a transformation or alignment step to ensure comparability between predicted and realized movements will be determined.

- **Dynamic Frame Freezing for Prediction:** The dynamic PCA frame generated at time 't' will be frozen for short-term predictions (e.g., 1-3 candlesticks into the future) to simplify error calculation and maintain a consistent frame of reference. This approach assumes minimal market structure shifts within this short timeframe.

- **Error Correction within the 2D Dynamic Plane:** A robust error correction mechanism will be implemented within the dynamic 2D plane, calculating Distance Error (magnitude difference between predicted and realized displacement vectors) and Angle Error (orientation difference between predicted and realized direction vectors). The initial frame creation rotation will be excluded from this calculation to isolate errors specifically related to the prediction mechanism. This correction will be integrated into the existing `DynamicPlaneGenerator` class as a modular function.

- **Frame Correction Action:** When the error trend detector identifies a need for correction, a frame correction action will be applied. This involves a small rotation adjustment or damping applied to the PCA frame, reweighting the principal axes to shift away from pure PCA and towards greater stability (potentially discounting minor eigenvectors). The optimal magnitude of this adjustment requires further determination.

- **Healing Phase Logic:** Once the error magnitude returns to an acceptable range, a healing phase will gradually reduce the correction magnitude (e.g., using exponential decay) to smoothly transition back to relying primarily on PCA. The appropriate decay rate needs to be defined and calibrated.

**Visualization and Future Enhancements (Potential):**

- **Visualization of Rotations and Prediction Error:** A diagram will be created to visually represent the two layers of rotation (global frame transformation using PCA and local vector misalignment) and how the calculated prediction errors relate to these rotations.

- **Dynamic Rolling Error Correction Module:** The enhanced Error Trend Detector could be further developed into a dynamic rolling error correction module, automatically adjusting model predictions based on detected error trends.

- **Visual Simulation of Vectorial Misalignments:** A visual simulation could demonstrate the cumulative effect of small vectorial misalignments in price predictions.

- **New Loss Functions for Model Training:** New loss functions could be designed that explicitly penalize both scalar and angular drift in predictions during model training.

- **Clarification of Rotational Angles and Distance Vectors:** The current implementation and documentation regarding the number of rotational angles and distance vectors used in the dynamic plane will be reviewed for consistency and accuracy.

### B. Monitoring and Refinement

This section details the post-deployment monitoring and refinement procedures necessary to ensure the long-term effectiveness and stability of the trading strategy within a dynamic market environment. This includes specialized monitoring of the chosen Principal Component Analysis (PCA) frame management technique, transaction costs, short-selling limitations, and risk management measures.

**PCA Frame Management Monitoring:**

Given the dynamic nature of financial markets, the PCA frame (defined by the principal components) can shift over time. To address this, a "Freeze Frame" or "Reproject Realization" method is employed. Rigorous monitoring of the chosen method is crucial to ensure accurate and consistent comparisons between predicted and realized movements.

- **Rationale and Comparison to Traditional Methods:** Traditional error calculations, such as distance and angular error, assume a static PCA plane. The "Freeze Frame" and "Reproject Realization" methods explicitly address the challenge of a shifting PCA plane by either freezing the frame at the time of prediction or reprojecting realized movements back to the initial frame. Detailed documentation will highlight these differences and ensure appropriate application.

- **Freeze Frame Implementation:** This method freezes the PCA rotation matrix (R) at the time of prediction (t). Both predicted and realized data points at t+1 are then projected using this frozen matrix, providing a consistent frame of reference for comparison, even if the underlying data distribution shifts.

- **Reproject Realization Implementation:** This method reprojects the realized movement vector from t+1 back to the original PCA frame at t using the initial rotation matrix (R). This ensures consistency by comparing results within the original prediction frame, mitigating the impact of market shifts.

- **Error Checking within the Dynamic Frame:** Error calculation must account for the dynamic PCA frame. Instead of calculating error directly between predicted and realized vectors, the monitoring system will calculate the deviation errors for both predicted and realized values along each principal component axis (PCA1 and PCA2). The total error will then encompass both this vector deviation error and the frame shift error (change between PCA axes from t to t+1).

- **Visualization and Numerical Examples:** Numerical examples and visualizations will be provided to demonstrate the chosen PCA frame management technique. These will include predicted vs. realized movement values, error calculations within the frozen/reprojected frame, and plots visualizing prediction and reality paths in both the initial and shifted frames.

- **Pseudocode and Data Structures:** Robust pseudocode for the "Freeze and Correct" module, encapsulating the frame management logic, will be developed and maintained. A lightweight data structure will store the PCA rotation matrix (R) for each time window to facilitate efficient reprojection and analysis.

**Trading Strategy Performance Monitoring:**

- **Transaction Cost Analysis:** Continuous monitoring of transaction costs (brokerage fees, slippage, etc.) and their impact on portfolio performance is crucial. Strategies to mitigate costs, such as adjusting trading frequency or order execution methods, will be explored as needed.

- **Short-Selling Constraint Mitigation:** The impact of short-selling restrictions, particularly in small-cap markets, on portfolio performance and risk exposure will be monitored. Alternative strategies (e.g., inverse ETFs, put options) will be considered if these constraints significantly hinder performance.

- **Unsuccessful Trade Analysis:** Clear criteria for classifying and evaluating unsuccessful trades will be established. Analyzing the reasons for trade failures (e.g., incorrect predictions, unfavorable market conditions) will inform future model refinements and risk management strategies.

- **Stop-Loss Mechanism:** A dynamic stop-loss mechanism will be implemented and continuously monitored. Stop-loss parameters will be adjusted based on market volatility and overall risk tolerance.

- **Risk-Based Weighting:** A risk-based weighting scheme will be implemented to allocate capital across trades, considering factors like volatility, prediction confidence, and market conditions. The scheme's effectiveness in managing portfolio risk will be continuously monitored and adjusted.

## B. Monitoring and Self-Correction

This section details the monitoring and self-correction mechanisms implemented to ensure the trading model's stability, accuracy, and adaptability to evolving market dynamics. This system continuously analyzes prediction errors, triggering corrective actions when necessary and gradually returning to normal operation as performance improves.

### Error Calculation and Weighting

Accurate error calculation is crucial for effective monitoring. The total error is a composite metric derived from two primary components: Vector Deviation Error and Frame Shift Error.

- **Vector Deviation Error:** This measures the difference between predicted and actual vector values within the current frame. It incorporates both distance (d<sub>vec</sub>) and angular (θ<sub>vec</sub>) deviations:

  `Vector Error = α₁⋅d<sub>vec</sub> + α₂⋅θ<sub>vec</sub>`

- **Frame Shift Error:** This quantifies the drift between successive dynamic coordinate frames using principal angles between corresponding principal components (PCA1 and PCA2):

  `Frame Shift Error = β₁⋅θ<sub>PCA1</sub> + β₂⋅θ<sub>PCA2</sub>`

- **Total Error:** These components are combined using weighting factors to form the total error:

  `Total Error = γ₁⋅Vector Error + γ₂⋅Frame Shift Error`

The weights (α₁, α₂, β₁, β₂, γ₁, γ₂) allow fine-grained control over the error calculation. Initial default values will be derived from trading intuition and principles from physics and subsequently refined through ongoing monitoring and optimization. A numerical example demonstrating this calculation will be provided. Angle units (e.g., degrees) will be normalized to the range [0, 1] for consistent scaling across all error components. Formal pseudocode will be developed for this multi-weight error computation process to ensure clarity and facilitate rigorous testing.

### Error Trend Detection and Healing Phase

A rolling error buffer storing the total error over a defined time window (e.g., 5-10 periods) will continuously monitor model performance. The rolling mean and standard deviation of these errors will be calculated to identify significant deviations from expected behavior.

- **Wound Detection:** A "Wound Phase" is triggered when the rolling mean error exceeds a predefined threshold, specifically when it surpasses _k_ times the rolling standard deviation (e.g., _k_=2).

- **Correction Mode:** Upon entering the Wound Phase, the system transitions into "Correction Mode." A correction factor is applied to the PCA rotation or smoothing operations used in constructing input data frames. This intervention aims to mitigate the source of the increased error. The specific implementation of this correction factor will be further detailed.

- **Healing Phase:** Continuous monitoring of the rolling mean error determines the start of the "Healing Phase." This phase begins when the mean error drops below a healthy threshold, defined as a value below a certain multiple of the rolling standard deviation (e.g., 1-1.5 times). The system then exits Correction Mode, and the correction factor is gradually reduced to allow a smooth return to normal operation. If another error spike occurs during the Healing Phase, the system re-enters Correction Mode.

A small-scale simulation will be created to illustrate vector deviation, PCA frame drift, and the impact of these errors on model performance. The potential of using Frame Drift Error as a confidence indicator for trading decisions will also be explored. This could provide an additional layer of risk management.

## B. Monitoring (Enforcer)

This section outlines the monitoring procedures necessary to ensure the model's ongoing performance and facilitate dynamic adjustments based on predictive accuracy. These procedures will enable the system to adapt to changing market conditions and mitigate performance degradation.

**Prediction Correctness Tracking and Dynamic Decay:**

The system will continuously monitor and record the correctness of each prediction, assigning a score of 1 for correct predictions and 0 for incorrect predictions. This binary scoring focuses solely on the accuracy of the predicted direction and magnitude, independent of framing effects. These scores will be stored in a rolling buffer of the last _N_ timesteps to provide a dynamic view of recent performance. The mean correctness within this buffer will drive a dynamic decay rate adjustment for the correction factor applied during the healing phase. Consistent correct predictions will accelerate the decay, reducing the influence of the correction factor and allowing the model to operate more independently. Conversely, a lower mean correctness will slow the decay, maintaining the corrective influence.

**Healing by Correctness:**

The model's healing process is directly tied to its predictive accuracy through a "Healing by Correctness" system. This system uses the mean prediction correctness over the last _N_ timesteps to dynamically adjust the decay rate of the correction factor. A function, `dynamic_decay_rate(mean_correctness)`, will calculate the decay rate as follows: `Decay Rate = 1 - (mean_correctness - healing_threshold)`. This formula ensures rapid decay when correctness is high (e.g., approaching 100%) and slower decay when correctness is just above the defined `healing_threshold`.

**Implementation Details and Validation:**

To ensure transparency and facilitate future modifications, formal, modular pseudocode will be developed to document the entire Healing-by-Correctness system, including prediction correctness tracking, the dynamic decay rate function, and the correction factor application. A toy example simulation will be created to demonstrate the entire healing process, from initial performance degradation (the "wound") to the application of corrective measures and the eventual "healing" through regained accuracy. This simulation will validate the logic and help identify potential issues. Initial healing thresholds, representing the minimum acceptable mean prediction correctness, will be proposed based on reasonable trading expectations and backtesting results. These thresholds might range from 75% to 80% directional correctness, depending on the specific trading strategy and market conditions, providing a starting point for the dynamic adjustment of the correction factor. Additionally, the system will track the actual predicted values to better understand model behavior over time and further refine the healing process.

**Data Preprocessing for Robustness:**

To enhance the robustness of the model and its monitoring system, specific data preprocessing steps will be implemented. Raw price values will be replaced with relative returns (percentage change or log returns) calculated relative to the first price in the rolling window. This anchoring minimizes the impact of extreme price values and focuses on relative changes. Time will be encoded as a fractional value between 0 and 1, representing the elapsed time within the window. This preserves chronological order and facilitates normalization while handling uneven time intervals. The current linear time representation will be replaced with a more suitable non-linear representation that maintains chronological order while providing more meaningful values for normalization and analysis.
Principal Component Analysis (PCA) offers valuable insights into market dynamics and trading behavior by monitoring transformed features. This section details the data preparation for PCA and the subsequent monitoring procedures.

**Data Transformation and Preparation for PCA:**

The following steps outline the data preparation process for PCA, enabling the monitoring of key performance indicators and potential issues like high trading costs or the impact of short-selling constraints:

1. **Fractional Elapsed Time:** Time stamps are converted to represent the fraction of total elapsed time within a specific window. This is calculated by subtracting the minimum timestamp from each timestamp and then dividing by the total time difference within the window (maximum timestamp minus minimum timestamp). This preserves information about the rate of data arrival and accounts for any gaps or periods of inactivity. Monitoring this feature can reveal patterns in trading activity.

2. **Window-Relative Returns:** Prices within a window are transformed into percentage or log returns relative to the first price in that window. This anchors the price series to zero at the beginning of the window, normalizing prices across different stocks or time periods. This normalization is crucial for PCA, mitigating the influence of absolute price magnitude. Monitoring these returns provides a standardized view of price movements.

3. **Robustly Scaled Volume:** To handle potential spikes and outliers in trading volume, a log transformation is applied. Subsequently, the volume data is normalized using median and interquartile range (IQR) scaling instead of the more traditional mean and standard deviation approach. This robust scaling makes the volume data more resistant to outliers. Monitoring scaled volume provides a clearer picture of trading intensity.

4. **PCA Input Matrix:** A 3-dimensional matrix is constructed with columns representing fractional elapsed time, relative return (or log return), and scaled volume. This matrix serves as the input for Principal Component Analysis.

5. **PCA Application:** PCA is performed on the constructed matrix. The resulting principal components capture the correlations between the three input features. Monitoring these components helps understand the underlying dynamics driving trading performance and identify potential issues. For instance, a high weighting on the volume component combined with high trading costs might indicate the need for optimization. Similarly, limitations in short-selling might be reflected in return-related principal components, especially for small-cap stocks.

Monitoring these transformed features and the resulting principal components provides a robust and insightful understanding of the factors influencing trading performance. This informs strategies for mitigating issues like high trading costs and short-selling constraints, ultimately improving investment outcomes.

### B. Monitoring and Data Integrity

This section details the monitoring procedures and data transformations necessary for accurate model input and post-deployment performance evaluation. While data normalization is a preprocessing step, revisiting it during monitoring helps ensure data integrity and consistency, indirectly impacting the reliability of trading results and facilitating debugging.

**Impact of Normalization on Trading Decisions:**

Monitoring should consider the impact of price and volume normalization on trading signals. For example, if extreme price movements or volume spikes (even after normalization) lead to erroneous trades, the normalization strategy may require adjustment:

- **Price Scaling Review:** Evaluate the chosen price clamping/scaling method (e.g., dividing by the maximum absolute value, min-max scaling) for its effectiveness in preventing outlier-driven trades.
- **Volume Scaling Impact:** Monitor how the suppression of extreme volume spikes influences the model's behavior. If relevant information is lost due to aggressive scaling, adjust the scaling parameters.

**Time Normalization and Backtesting:** Carefully consider the normalization of time data as a fractional elapsed time within the [-1, 1] range during backtesting. Ensure the backtesting framework correctly handles this normalization when evaluating performance over different periods and market conditions.

**Data Drift Detection:** Regularly examine the distributions of the normalized time, price, and volume data. Significant shifts (data drift) compared to the training data could indicate changing market dynamics and degrade model performance. Monitoring for data drift can trigger alerts or initiate model retraining with more recent data.

**Data Transformations and Preprocessing for Model Input:**

- **Volume Transformation:** Apply a logarithmic transformation (`np.log1p(volume)`) to normalize the volume distribution. Clip extreme outliers at the 5th and 95th percentiles to mitigate the impact of extreme spikes. Finally, min-max scale the resulting values to the [-1, +1] range.
- **Min-Max Scaling:** Apply min-max scaling to all features (fractional time, log return of price, and transformed volume) within each window to the [-1, +1] range. This ensures consistent input representation for PCA and prevents data leakage.
- **Price Clipping:** Clip extreme outliers in the log return of price at the 5th and 95th percentiles before applying min-max scaling to the [-1, +1] range, further enhancing data robustness.
- **PCA Application:** After normalizing and clipping, combine the features into a matrix and perform PCA. Consider centering the matrix before PCA, although this might be redundant if the data is already zero-mean.
  Data Preparation and Verification

Five distinct examples illustrating different price and volume scenarios will be created to verify data transformations. Each example will represent a single-day chart with 10-minute intervals and include:

- The original candlestick image.
- The transformed candlestick image after normalization to the [-1, +1] range.

These pre- and post-transformation images will be displayed individually for clear comparison and analysis, allowing for visual verification of the data preprocessing pipeline and identification of potential issues. This normalization aligns the candlestick data with the symmetric [-1, +1] scaling used in the model, potentially improving efficiency and performance.

### B. Monitoring (Enforcer)

This section details the monitoring procedures necessary to ensure the ongoing performance and stability of the deployed algorithmic trading bot. While a fully deployable system with a monitoring dashboard is the ultimate goal, this section focuses on specific monitoring aspects crucial for identifying potential issues and areas for improvement within the trading strategy.

**Performance Monitoring and Self-Correction:**

The model incorporates a "self-healing" mechanism designed to adapt to changing market conditions and correct for prediction errors. This mechanism relies on continuous monitoring and feedback based on a composite error signal.

- **Total Error Signal:** A comprehensive error signal, calculated from several key components, drives the self-correction mechanism:

  - **Vector Deviation Error:** This measures the discrepancy between the predicted movement vector (Δx', Δy') and the actual realized movement. It is further decomposed into:
    - **Distance Error:** The magnitude difference between the predicted and actual movement vectors.
    - **Angular Error:** The directional difference between the predicted and actual movement vectors.
  - **Frame Shift Error:** This quantifies the error introduced by the dynamic rotation and refocusing of the coordinate system, measuring how the predicted movement is affected by inaccuracies in the dynamic frame construction.

- **Wound Detection:** The system continuously monitors the Total Error signal for significant deviations, indicating potential performance issues. "Wounds" are identified when the Total Error exceeds a dynamic threshold, typically calculated as a multiple of the rolling standard deviation of the error signal. This adaptive threshold allows the system to adjust to varying market volatility.

- **Healing Phase:** Upon detecting a "wound," the system enters a "healing phase," reducing the dynamism of the frame construction. It relies more on historical data and less on recent, potentially noisy price movements. As predictive accuracy improves and the Total Error falls below the threshold, the system gradually restores full dynamism, enabling adaptation to the current market regime.

**Trading Strategy Monitoring:**

Beyond the self-correcting mechanism, the following aspects of the trading strategy will be monitored:

- **High Trading Costs:** Transaction costs (commissions, slippage, etc.) will be continuously analyzed to assess their impact on Jensen's Alpha. This will identify if excessive costs are eroding profitability and necessitate adjustments to the trading strategy.
- **Short-Selling Constraints:** Given the potential limitations and increased costs of short-selling small-cap stocks, the performance of these trades will be closely monitored. This includes analyzing the frequency and profitability of short positions in small-cap equities to determine if adjustments to position sizing or the use of alternative instruments are warranted.
- **Unsuccessful Trades:** A clear methodology will evaluate unsuccessful trades to identify systematic weaknesses in the trading strategy. This involves investigating the reasons for losses (e.g., incorrect prediction, adverse market movement, slippage) and determining if adjustments to the model or risk management parameters are necessary.
- **Stop-Loss Mechanism:** A stop-loss mechanism will be implemented and monitored to mitigate potential losses. This involves defining and enforcing predetermined exit points for trades. Monitoring will assess the mechanism's effectiveness in protecting capital and identify optimization opportunities, such as adjusting stop-loss levels or using trailing stops.
- **Risk-Based Weighting:** A risk-based weighting scheme will allocate capital across trades based on their perceived risk levels, considering factors like volatility, predicted confidence, and market conditions. Monitoring will focus on the effectiveness of this scheme in managing overall portfolio risk and ensuring optimal capital allocation.

## B. Monitoring (Enforcer)

This section details the monitoring procedures necessary to ensure the continued performance and stability of the trading model. This involves tracking key performance indicators (KPIs), system health metrics, and potential risk factors. The monitoring strategy addresses both the technical infrastructure and the model's predictive accuracy within the dynamic market environment.

**Model Performance and Stability:**

- **Multi-Scale Data Quality:** Continuous monitoring of data quality and consistency across all timeframes (intraday, daily, weekly, monthly) is crucial. Discrepancies or gaps in any timeframe could significantly impact predictive capabilities and will trigger alerts.
- **Dynamic Rotating Plane Stability:** The dynamic 2D plane, derived from PCA on normalized Time, Price, and Volume data, requires close monitoring. Unexpected shifts or instabilities in its orientation, potentially indicating anomalous market behavior, will be detected by tracking eigenvalues, eigenvectors, and the re-centering process. These anomalies will trigger further investigation.
- **Non-Hierarchical Attention Effectiveness:** The hierarchical attention mechanism's efficiency and focus will be assessed by analyzing queried timeframes in relation to prevailing market conditions. This provides insights into the model's decision-making process and highlights potential areas for improvement.
- **Model Evaluation and Attribution:** The multi-scale model's performance will be continuously evaluated against a baseline intraday model using relevant metrics. Attribution analysis techniques, such as Grad-CAM or SHAP, will be employed to understand the model's reliance on different timeframes. Performance during major market events will be specifically analyzed to ensure robustness in volatile conditions.
- **Periodicity Weight Optimization:** The impact of different weighting schemes for daily, weekly, monthly, quarterly, and yearly data will be continuously monitored and adjusted to optimize predictive accuracy as market conditions evolve.

**System Health and Resource Management:**

- **Data Processing Overhead:** Data processing time and resource utilization will be monitored to ensure real-time performance. Excessive overhead, especially given the integration of multiple datasets for the multi-scale model, will trigger alerts and optimization efforts.
- **Data Pipeline Integrity:** The complex data pipeline, integrating various timeframes (10-minute, daily, weekly, monthly) using the Dynamic Rotating Plane method, will be continuously monitored for breaks, inconsistencies, and performance bottlenecks.
- **Ensemble and Transformer Model Performance:** The individual performance of specialist models within the ensemble and the overall combined predictions will be tracked. Similarly, the multi-input transformer model's performance will be monitored and compared to guide model selection and optimization.
- **Weight Assignment Strategy Optimization:** The impact of different weight assignment strategies (static vs. dynamically learned using attention mechanisms) on overall model performance will be monitored to identify optimal configurations.

**Risk Management and Trading Constraints:**

- **Trading Cost Impact on Alpha:** Transaction costs and their impact on portfolio performance will be closely monitored. Accurate price data, accounting for these costs, is crucial for data normalization and model performance. This aligns with the log-return normalization of Price data (ID: `027cc35d-e351-479e-a550-3706d5d5d1a0`).
- **Short-Selling Constraints:** The impact of short-selling constraints, particularly in the small-cap space, will be monitored, and the strategy adjusted accordingly to mitigate potential limitations on capitalizing on downward trends.
- **Unsuccessful Trade Analysis:** A robust framework for analyzing unsuccessful trades will be developed and implemented. Understanding the reasons behind these trades is critical for refining the model and improving its overall performance. This analysis will consider market conditions, model predictions, and execution details.

## B. Monitoring and Mitigation

This section details the monitoring of the trading system's performance and the mitigation strategies for potential issues. This encompasses both real-time monitoring through a dedicated user interface and ongoing analysis of trading performance against market conditions and inherent limitations.

**Real-time Monitoring and System Health:**

A comprehensive user interface provides real-time insights into the system's operation and performance. This interface is organized into the following sections:

- **Live Trade Execution & Monitoring:** This "cockpit" view provides a real-time overview of the system's performance. An expandable table displays Open Positions with intraday price charts. Dedicated tabs provide access to the Order Book (open, executed, and failed orders) and the complete Trade Ledger. The Risk & Error Dashboard visualizes key metrics like Frame Shift Error and Prediction Correctness, enabling immediate anomaly detection. Manual Override functionality allows for pausing trading or liquidating all positions in emergencies.

- **Model Management & Revision:** This section facilitates the oversight of different model versions. It displays a list of trained models with details like version, training date, backtesting metrics, and current status. Users can deploy models for live trading, run new backtests on existing models, and fork/retrain models to create variations based on previous versions.

- **Reports & Ledger:** This section provides a simple interface for generating downloadable performance reports (PDF/CSV) with data like P&L summaries, detailed trade ledgers, equity curves, and summary statistics.

- **Training & Backtesting Module:** This module employs a consistent color scheme for clear visualization. Equity curves are displayed in solid blue, benchmark lines in dashed grey, and loss curves are differentiated using distinct colors (e.g., blue and purple) or varying line styles.

**Key Status Indicators:**

Several key indicators provide at-a-glance system status and performance information:

- **Swaha Status Indicator:** A pulsing circular icon indicates Swaha's operational state (Live Trading, Backtesting, Training, Idle, or Error). Interacting with the icon provides further details, such as the model version during live trading or navigation to system logs for errors.

- **Market Status:** A text display indicates the current market status (e.g., NSE: OPEN, NSE: CLOSED).

- **Current P&L (Today):** Displays the current day's profit and loss in absolute value (e.g., +₹12,450.75) and percentage return (e.g., +1.25%), color-coded for at-a-glance performance monitoring (green for profit, red for loss, grey for flat).

- **Live Portfolio Snapshot:** A scrollable list of open positions, including ticker, trade direction (LONG/SHORT), quantity, average price, last traded price (LTP), and unrealized P&L. Expanding an entry reveals a mini sparkline chart of the stock's price movement since position opening.

- **Model Cognitive State - Frame Stability:** A visual representation (e.g., horizontal bar or seismograph) displays the Total Error in real-time, color-coded for stability (green for stable, yellow/red for unstable).

**Ongoing Performance Analysis and Mitigation:**

Beyond real-time monitoring, the following areas require continuous analysis and potential mitigation strategies:

- **Trading Cost Analysis:** Analyze the impact of trading costs (commissions, slippage) on performance and alpha generation. Explore trade execution optimization strategies to minimize costs.

- **Short-Selling Constraints:** Investigate the impact of short-selling constraints, particularly on small-cap performance. Consider alternative strategies or adjustments to mitigate these limitations.

- **Unsuccessful Trade Evaluation:** Define how unsuccessful trades are handled and evaluated in backtesting and live trading, ensuring proper accounting for losses within performance metrics.

- **Stop-Loss Mechanism:** Implement and monitor a configurable stop-loss mechanism to mitigate potential large losses on individual trades. Regularly evaluate and adjust stop-loss levels as needed.

- **Risk-Based Weighting:** Implement and monitor a risk-based weighting scheme for portfolio allocation, adjusting position sizes based on factors like volatility and model confidence to optimize risk-adjusted returns. This scheme should be adjusted based on market conditions and model performance.

## B. Monitoring (Enforcer)

This section details the monitoring procedures necessary to ensure the ongoing performance and stability of the trading system. Key areas of focus include evaluating trading costs, constraints on short selling, handling unsuccessful trades, and implementing risk management mechanisms such as stop-loss orders and risk-based weighting. Additionally, future enhancements will incorporate automated healing mechanisms triggered by performance degradation or persistent errors.

**Current Monitoring Procedures:**

- **Trading Cost Analysis:** Regularly analyze the impact of trading costs (commissions, slippage, and fees) on overall performance and profitability. High trading costs can significantly erode alpha and should be carefully monitored and optimized.

- **Short-Selling Constraint Analysis:** Evaluate the impact of any restrictions on short selling, particularly on the performance of small-cap stocks. Limitations on short selling can hinder the ability to profit from declining prices and may skew overall portfolio performance.

- **Unsuccessful Trade Evaluation:** A clear methodology for evaluating unsuccessful trades is essential. This involves investigating the reasons behind losses, including analyzing market conditions, model predictions, and any external factors. This analysis is crucial for refining the trading strategy and improving future performance.

- **Stop-Loss Mechanism:** A stop-loss mechanism will be implemented to limit potential losses on individual trades. This crucial risk management tool helps protect capital during periods of adverse market movements. The stop-loss thresholds will be carefully calibrated to balance risk mitigation with profit potential.

- **Risk-Based Weighting:** A risk-based weighting scheme will be implemented for portfolio allocation. This approach assigns weights to different assets based on their perceived risk levels (e.g., volatility, market capitalization, correlation) to optimize the risk-return profile of the overall portfolio.

**Future Enhancements:**

- **Automated Healing Mechanisms:** To maintain long-term model stability and performance, automated healing mechanisms will be implemented. These mechanisms will trigger model adjustments and retraining based on two criteria:

  - **Performance-Based Trigger:** Activated when the model's predictive performance falls below a user-defined threshold.
  - **Time-Based Trigger:** Activated when the model's error remains above a specified threshold for a certain duration, addressing persistent performance issues.

- **Comprehensive Error Metrics:** Future development will include a more granular error analysis, incorporating a total error metric to provide a comprehensive view of model performance and identify areas for improvement. Further details on this metric will be provided in subsequent documentation.

### B. Monitoring (Enforcer)

This section outlines the monitoring procedures crucial for maintaining the performance and stability of the SCoVA project after deployment. These checks will identify potential issues and highlight areas for improvement. The following functionalities will be continuously monitored and analyzed:

- **Context Awareness:** The system will continuously monitor and adjust the optimal number of candlesticks per frame and the total number of frames. This dynamic adjustment will consider changing market conditions and data characteristics.

- **Transfer Learning Effectiveness:** Model performance will be monitored across different markets (US-US, US-India, and India-India) to assess the effectiveness of transfer learning and identify potential improvements to the transfer learning strategy.

- **Context-Aware Periodicity:** The weighting assigned to predictions based on daily, weekly, monthly, quarterly, and yearly periodicities will be monitored and adjusted. Optimal configurations for different stock categories (market caps, sectors, share price bins) will be continuously evaluated and refined based on observed performance, considering the number of frames, candles per frame, and the specific stock category.

- **PCA Analysis on Dynamic Plane:** Model performance using a 2 PCA axes dynamic plane will be compared to the original method to assess its effectiveness. This involves repeating the entire model development and evaluation process on the dynamic plane.

- **Hyperparameter Permutation Testing:** An automated system will periodically test all hyperparameter permutations for a single epoch, using defined training and testing date ranges. This establishes a performance baseline, monitored for potential hyperparameter tuning improvements.

While the initial checklist focused on model development and architecture, subsequent checklists addressing post-deployment monitoring will inform the implementation and specifics of these monitoring procedures. Future updates to this section will detail the metrics, thresholds, and alerting mechanisms used for each monitoring function. These updates will also integrate aspects of dynamic capital allocation, hyperparameter tuning analysis, and experiment management, drawing on information from upcoming checklist reviews related to UI/UX components for those functionalities. Finally, any monitoring requirements related to trading costs, short-selling constraints, and unsuccessful trades identified in other sections of the document will be incorporated here to provide a comprehensive overview of the post-deployment monitoring strategy.

## B. Monitoring (Enforcer)

This section details the monitoring procedures crucial for maintaining the system's health and efficacy post-deployment. It focuses on specific technical aspects related to performance, risk management, and understanding trading outcomes. The following checks and procedures will be implemented with detailed configurations and actions:

1. **Trading Cost Analysis:** A detailed analysis of how trading costs affect the Jensen's Alpha will be conducted. This includes a breakdown of commission fees, slippage, and other transaction costs. The analysis will quantify the impact on overall profitability and identify strategies for mitigating these costs, such as order type optimization, trading frequency adjustments, or exploring alternative brokerage options. Specific metrics for evaluating the impact, such as cost-to-profit ratios, will be defined and tracked. This analysis will be integrated into the Live Trading Dashboard (described below) to provide real-time feedback on trading costs.

2. **Short-Selling Constraint Analysis:** This involves a thorough examination of how short-selling limitations affect the performance of small-cap stocks within the portfolio. Specific constraints, including borrowing fees, share availability, and regulatory limitations, will be documented. The analysis will quantify the impact on returns, considering factors like reduced market neutrality and missed opportunities. Strategies for mitigating these constraints, such as focusing on long-only strategies for small-caps or exploring alternative instruments, will be developed. These findings will also be integrated into the Live Trading Dashboard to provide alerts and visualizations related to short-selling constraints.

3. **Unsuccessful Trade Analysis:** A rigorous process for analyzing unsuccessful trades will be defined and implemented. This involves categorizing losses based on their root causes (e.g., incorrect predictions, market volatility, slippage), calculating the financial impact of each category, and defining specific actions for improving future performance based on these insights. Detailed logging and reporting mechanisms will track and review unsuccessful trades, and a clear process will incorporate these findings into model refinements. This analysis will be a key component of the Live Trading Dashboard, providing insights into the causes of unsuccessful trades.

4. **Stop-Loss Mechanism:** A stop-loss mechanism will be implemented with clearly defined parameters. This includes specifying stop-loss order types (e.g., fixed-price, trailing stop), triggers (e.g., percentage drawdown, volatility thresholds), and management logic for different market conditions. Backtesting results demonstrating the effectiveness of the stop-loss mechanism and its impact on portfolio performance will be documented. Specific considerations for different asset classes and market scenarios will be addressed. The Live Trading Dashboard will provide visibility into the activation and performance of the stop-loss mechanism.

5. **Risk-Based Weighting:** A comprehensive risk-based weighting scheme will be implemented. This includes defining the risk metrics used (e.g., volatility, beta, drawdown), the calculation methodology for determining asset weights, and the process for dynamically adjusting weights based on changing market conditions. The rationale behind the chosen weighting scheme will be justified, and its alignment with the overall investment strategy and risk tolerance will be explained. Backtesting results demonstrating the impact of risk-based weighting on portfolio performance and risk metrics will be presented. The Live Trading Dashboard will display the current asset allocation based on the risk-based weighting scheme.

6. **Live Trading Dashboard:** A Live Trading Dashboard will provide real-time monitoring of trading activities, system health, and key performance indicators (KPIs). This dashboard will include real-time visualization of trading activity (open positions, executed trades, pending orders), system health monitoring (data feeds, execution platforms, model status), and performance monitoring and alerting (KPIs like Jensen's Alpha and Sharpe Ratio, with alerts for significant deviations). The insights from the analyses described in points 1-5 will be integrated into this dashboard, providing a comprehensive overview of the system's performance and potential issues.

### B. Monitoring (Enforcer)

This section details the monitoring procedures essential for ensuring the stability, performance, and cost-effectiveness of the trading model. These procedures address both client-side (iPad) and server-side components.

**Client-Side (iPad) Monitoring:**

The iPad handles data fetching, image generation, local model training, and model uploading. Monitoring on the client-side will focus on resource utilization, task completion, and user experience:

- **Data Acquisition:** Monitor the progress and status of market data downloads, including download speed and any errors encountered.
- **Image Generation:** Track the status and completion rate of candlestick image generation. This includes monitoring for errors or exceptions during image creation via JavaScript libraries and the Canvas API, as well as the time taken for image generation.
- **Model Training:** Monitor the TensorFlow.js training process, including metrics like loss, accuracy, and training speed. Track resource usage, such as memory and CPU utilization, to identify potential bottlenecks.
- **Model Uploads:** Monitor the progress and status of model updates uploaded to the server, including upload speed and any communication errors.
- **Resource Monitoring:** Continuously monitor available resources on the iPad, including storage space, memory, and battery life. This helps anticipate potential issues and optimize resource allocation.

**Server-Side Monitoring:**

The server stores and serves market data, maintains the global model, coordinates training, and performs federated averaging. Server-side monitoring concentrates on:

- **Data Storage:** Monitor available storage capacity and perform regular data integrity checks to ensure the reliability of market data.
- **Model Integrity:** Verify the integrity and availability of the global model. Implement alerts for any inconsistencies or unexpected changes.
- **API Performance:** Monitor the performance and availability of all API endpoints, including those responsible for serving data, serving the global model, and receiving model updates. Track response times, error rates, and request volumes.
- **Federated Averaging:** Monitor the federated averaging process, including the number of participating clients, aggregation speed, and the impact of aggregated updates on the global model's performance.

This comprehensive monitoring strategy allows for proactive identification and resolution of potential issues, optimizing both client and server performance and ensuring the trading model's reliability and efficiency. Regular review and analysis of the collected monitoring data will inform ongoing system improvements and refinements.

### B. Monitoring (Enforcer)

This section details the monitoring procedures necessary to maintain the performance and stability of the deployed trading system. It addresses both backend and client-side considerations and incorporates best practices for real-world deployment.

**Backend Monitoring:**

Given the backend's role as an orchestrator and data provider, monitoring focuses on key interactions and data flow between the backend and the iOS client.

- **Data Serving:** Monitoring ensures the reliable and efficient delivery of raw numerical data to the iOS app. This includes tracking data access times, verifying data integrity, and monitoring the backend's responsiveness to client data requests.
- **API Management (Zerodha Kite Connect):** Monitoring focuses on the health and stability of the connection to the Zerodha Kite Connect API, including authentication successes and failures, API request latency, and any errors returned by the API. This ensures the proper functioning of trading functionalities.
- **Data Request Patterns:** Analyzing client data requests helps identify potential bottlenecks or areas for backend optimization.
- **API Usage:** Monitoring the frequency and types of API calls made through the backend to Zerodha Kite Connect is crucial for identifying potential issues and ensuring compliance with API usage limits.

**Client-Side Monitoring (iOS App):**

- **Model Performance:** Regularly monitor key performance indicators like Jensen's Alpha and Sharpe Ratio to ensure the model continues to perform as expected in live trading. Deviations from expected performance should trigger investigations.
- **Error Rate Monitoring:** Track the model's prediction error rate over time. An increasing error rate could indicate concept drift or other issues requiring model retraining. Automated alerts should be configured for error rate thresholds.
- **Resource Utilization:** Monitor CPU usage, memory consumption, and battery drain of the iOS app during model execution to identify potential optimizations and ensure a smooth user experience. Address the 50MB cache limit’s impact and explore Web Workers for stability improvements, particularly for intensive computations. If a hybrid architecture is implemented, monitor its effectiveness in mitigating browser crash risks and memory constraints from batch processing.
- **Data Quality:** Monitor the quality of the data received from the backend. Anomalies or inconsistencies could negatively impact model predictions.
- **Image Processing Efficiency:** Ensure the image processing pipeline is optimized for efficiency. Limit processing to generating daily predictions and occasional model re-tuning triggered by a higher error rate or performance degradation. Remove any experimental setups after model fine-tuning to streamline the deployment and reduce resource consumption.

**General System Monitoring:**

- **System Health:** Monitor the health and availability of all system components, including the data pipeline, the model serving infrastructure, and any trading interfaces. Automated alerts should notify administrators of system failures or performance degradations.
- **Trading Costs:** Analyze the impact of trading costs, including brokerage fees and slippage, on overall portfolio performance and profitability.
- **Short-Selling Constraints:** Evaluate the impact of any short-selling constraints, particularly on small-cap performance.
- **Unsuccessful Trades:** Analyze unsuccessful trades to identify potential patterns or areas for improvement in the trading strategy.
- **Risk Management:** Monitor the effectiveness of implemented risk management measures, such as stop-loss orders and risk-based weighting.

This comprehensive monitoring strategy provides insights into the overall health and performance of the trading system, enabling proactive identification and resolution of any issues. This is essential for maintaining the stability, reliability, and profitability of the system.

### B. Monitoring (Enforcer)

This section details the key areas requiring continuous monitoring after the trading system is deployed. Effective monitoring allows for identifying potential issues and optimizing the system for long-term profitability and stability.

- **High Trading Costs Impacting Alpha:** Transaction costs (commissions, slippage, and exchange fees) can significantly erode returns. This monitoring process will analyze the impact of these costs on the achieved Jensen's Alpha. Regular monitoring, optimizing trading frequency, and exploring alternative brokers can mitigate this. Calculations will quantify the precise impact of trading costs.

- **Short-Selling Constraints Impacting Small-Cap Performance:** Restrictions on short selling can limit the strategy's ability to profit from declining small-cap stocks. This monitoring process will track instances where short-selling constraints prevent the execution of trades and analyze their potential impact on overall performance. Alternative strategies for managing downside risk in small-cap holdings will be explored.

- **Unsuccessful Trade Evaluation:** A clear process is needed for evaluating unsuccessful trades. This includes categorizing the reasons for failure (e.g., inaccurate predictions, market volatility, slippage) and quantifying their impact. This analysis will inform refinements to the model and trading strategy.

- **Stop-Loss Mechanism Implementation:** A stop-loss mechanism is crucial for managing risk and preventing significant losses. This will involve defining specific stop-loss criteria (e.g., percentage decline, moving average crossover) and implementing automated triggers to execute sell orders when these criteria are met. The effectiveness of the stop-loss mechanism will be continuously monitored and adjusted as needed.

- **Risk-Based Weighting:** Implementing risk-based weighting involves adjusting the portfolio allocation based on the perceived risk of individual assets. This requires developing a robust risk assessment model and integrating it into the portfolio construction process. The performance of the risk-based weighting strategy will be monitored to ensure it effectively manages portfolio risk.
  Constraints on short selling, particularly in small-cap stocks due to liquidity or regulatory limitations, can hinder profitability from downward trends. This section analyzes the impact of these constraints and explores mitigation strategies. Specifically, it investigates whether the model disproportionately identifies short opportunities in small-cap stocks and whether adjusting the universe of tradable assets could improve performance.

Crucially, the system incorporates several risk management features:

- **Stop-Loss Mechanism:** A stop-loss mechanism is implemented to mitigate potential losses. This section details the chosen strategy—whether fixed percentage loss, trailing stops, or a time-based exit—and presents backtesting results demonstrating the impact of the stop-loss on overall portfolio performance.

- **Risk-Based Weighting:** This section describes the risk management framework used for position sizing. This may involve allocating capital based on volatility, model confidence, or other relevant risk factors. The chosen weighting scheme and its rationale are explained.

- **Evaluation of Unsuccessful Trades:** Recognizing that unsuccessful trades are inevitable, this section outlines how these trades are analyzed to refine the trading strategy. Losing trades are categorized by their cause (e.g., incorrect prediction, unexpected market event) to reveal patterns and potential weaknesses in the model. This detailed analysis provides valuable insights for continuous improvement.

Addressing these monitoring points allows the trading system to be continuously refined and adapted to changing market conditions, ensuring its long-term viability and performance.

## B. Monitoring (Enforcer)

This section details the ongoing monitoring processes crucial for maintaining the trading system's effectiveness and identifying potential issues requiring intervention. This includes monitoring both the financial performance of the trading strategy and the technical health of the on-device training and model update mechanisms.

**Performance Monitoring:**

This subsection focuses on key performance indicators (KPIs) and risk management procedures to ensure the trading strategy operates as expected and delivers the target alpha.

- **Trading Cost Analysis:** Continuously monitor and analyze the impact of trading costs (commissions, slippage, and taxes) on the portfolio's Jensen's Alpha. High trading costs can significantly erode returns. This analysis will inform adjustments to trading frequency or strategies to mitigate cost impacts.
- **Short-Selling Constraint Analysis:** Regularly assess the limitations imposed by short-selling restrictions, particularly their impact on small-cap stock performance. This analysis will inform potential mitigation strategies, such as seeking alternative investment vehicles or adjusting the target universe.
- **Unsuccessful Trade Analysis:** Maintain a rigorous process for evaluating unsuccessful trades, including identifying the root causes of losses and categorizing them as systematic errors or market fluctuations. This analysis will drive refinements to the trading rules and risk management parameters.
- **Stop-Loss Mechanism:** A dynamic stop-loss mechanism will be implemented to mitigate potential large losses. Stop-loss levels will be determined based on individual asset volatility, market conditions, and overall portfolio risk tolerance. The effectiveness of the stop-loss mechanism will be continuously monitored and adjusted as needed.
- **Risk-Based Portfolio Weighting:** A risk-based weighting scheme will be implemented for portfolio allocation. This scheme will consider individual asset volatility, correlations, and other relevant risk factors to optimize portfolio diversification and risk-adjusted returns. The weighting scheme will be reviewed and adjusted periodically based on market conditions and portfolio performance.

**Technical Monitoring (On-Device Training):**

This subsection outlines the monitoring procedures for the on-device training and model update mechanisms, ensuring their correct functionality and efficient operation.

- **On-Device Training Necessity Evaluation:** A thorough evaluation will determine the necessity of performing one epoch of training on the device, considering the trade-off between potential performance gains and increased resource utilization. This evaluation will leverage the pre-trained dummy model to assess the potential benefits of on-device training.
- **Model Update Mechanism:** The model update mechanism, including the interaction between on-device training and the backend universal model updates via Core ML, will be thoroughly documented and monitored. Key metrics to track include update frequency, data transfer volume, and update success rate.
- **Smoke Test Validation:** The purpose of the single-epoch training on the dummy model during the smoke test will be clearly documented. This test focuses on validating the training and update mechanism, including data loading, training loop execution, and weight extraction, _not_ inference performance. This provides a low-cost way to confirm pipeline functionality before full-scale training.
- **Data Pipeline Verification:** The connection between the `DynamicPlaneGenerator` and the Core ML training session on the iPad will be rigorously verified. This includes confirming the correct format and delivery of transformed image tensors to the training process. Automated checks will be implemented to ensure data integrity and prevent pipeline disruptions.
- **On-Device Training Loop Execution:** Regularly execute a full training step on the device, encompassing the forward pass, loss calculation, and backpropagation. This will confirm the continued functionality of the Core ML framework for on-device training and identify any potential issues related to resource constraints or software updates.

## B. Monitoring and Risk Management

Robust performance monitoring and proactive risk management are crucial for maintaining the effectiveness and stability of the predictive model. This section outlines the key monitoring procedures and risk mitigation strategies, including the development of a Cognitive Threat Analysis Module (CTAM) to address "shocker events" and strategies to manage trading costs and portfolio risk. While the current project scope doesn't encompass a fully realized CTAM, the principles outlined here inform the future scalability and robustness of the system.

**Shocker Event Detection and Response:**

To enhance the model's responsiveness to unexpected market events, a CTAM will be developed and integrated. This module will focus on the detection and analysis of "shocker events" – significant market disruptions characterized by volatility spikes, anomalous trading volume, and rapid price changes. This proactive approach complements the model's existing reliance on historical data analysis.

- **CTAM Development:** A dedicated CTAM will be developed to operate independently from the DynamicPlaneGenerator, enabling specialized processing of shocker events. This module will leverage lightweight CNN models to analyze financial charts for patterns indicative of these events, balancing accuracy with the real-time speed requirements.

- **Visual Analysis with Computer Vision:** Computer vision techniques will be explored to analyze equity charts, futures, options, and derivatives data for patterns and anomalies that could presage a shocker event. This broader perspective helps identify potential systemic risks not readily apparent from equity data alone.

- **CTAM Integration and Response:** The CTAM's integration will involve defining how the DynamicPlaneGenerator reacts to the CTAM's calculated "Threat Level" score. Potential responses include modifying the data preprocessing smoothing function or adjusting the model's learning rate in real-time, enabling dynamic adaptation to changing market conditions.

- **Synthetic Shocker Event Implementation:** Synthetic shocker events will be injected into the data stream during training and testing. This allows for robust evaluation of the system's responsiveness and proactive preparation for unforeseen real-world scenarios.

**Ongoing Performance Monitoring and Risk Mitigation:**

Beyond shocker event detection, continuous monitoring of key performance indicators and potential risk factors is essential.

- **Trading Cost Analysis:** The impact of trading costs (commissions, slippage) on overall performance and profitability (Alpha) will be analyzed and monitored. Understanding the relationship between trading frequency and profitability is critical for optimization.

- **Short-Selling Constraint Monitoring:** The impact of short-selling constraints on small-cap stock performance will be monitored. Strategies to mitigate the effects of these constraints, such as adjustments to trading logic or portfolio construction, will be explored.

- **Unsuccessful Trade Analysis:** A clear methodology for evaluating unsuccessful trades will be implemented, going beyond simple loss calculation to identify the root causes, including market volatility, incorrect model predictions, and slippage. This analysis will inform model refinement and risk management.

- **Stop-Loss Mechanism:** A stop-loss mechanism will be implemented to limit potential losses. The specific stop-loss parameters (e.g., percentage drop, trailing stop) will be carefully tuned and monitored.

- **Risk-Based Portfolio Weighting:** A risk-based weighting scheme will be implemented for portfolio allocation, assigning weights based on perceived risk. This approach diversifies the portfolio and enhances overall risk management.

By addressing these monitoring and risk management aspects, the project establishes a foundation for a more comprehensive and robust system, aligned with the potential future integration of a fully realized CTAM. This proactive approach prioritizes not only performance optimization but also the system's resilience in the face of unexpected market events.

### B. Monitoring (Enforcer)

This section details the monitoring procedures crucial for maintaining the health and performance of the deployed trading system. Robust monitoring enables proactive identification and resolution of issues, ensuring consistent performance and risk management.

- **Real-time Process Monitoring:** A dedicated service continuously monitors the health and resource consumption of active training and backtesting jobs. This includes tracking CPU and memory usage, disk I/O, and other relevant metrics. The monitor will also detect potential issues such as training stagnation, enabling prompt intervention to address resource bottlenecks or algorithmic inefficiencies.

- **Execution State Control:** This service manages the state of all running processes, providing control over execution flow. It handles pausing, resuming, and gracefully terminating processes. This functionality is vital for managing resources effectively and preventing unintended consequences from interrupted processes.

- **Transaction Cost Analysis:** The Enforcer analyzes and monitors the impact of transaction costs on overall alpha. This involves tracking commission fees, slippage, and other transaction costs to ensure they don't erode profitability. Investigating and addressing unexpectedly high costs is a key responsibility.

- **Short-Selling Constraint Monitoring:** The Enforcer monitors the impact of short-selling constraints, particularly on small-cap stock performance. This might involve tracking borrow rates, availability of shares for shorting, and regulatory limitations. Strategies to mitigate the impact of these constraints will be explored and implemented as needed.

- **Unsuccessful Trade Analysis:** A clear process for evaluating unsuccessful trades is essential. The Enforcer analyzes why trades underperform, identifies potential flaws in the model's predictions or execution logic, and recommends adjustments. This ensures continuous learning and improvement of the trading system.

- **Stop-Loss Mechanism Monitoring and Adjustment:** The Enforcer monitors and adjusts the stop-loss mechanism to mitigate potential losses. This involves defining and regularly reviewing appropriate stop-loss levels based on risk tolerance and market conditions.

- **Risk-Based Weighting Management:** The Enforcer manages and adjusts the risk-based weighting scheme for portfolio allocation. This involves assigning weights to different assets based on their risk profiles, ensuring diversification and managing overall portfolio risk. This requires continuous monitoring and adjustment based on market dynamics.

## B. Monitoring (Enforcer)

This section details the monitoring responsibilities of the Enforcer, crucial for maintaining the stability, performance, and reliability of the trading system. The Enforcer manages data access, evaluates performance, and ensures system integrity through continuous monitoring and proactive issue identification.

The Enforcer's monitoring activities encompass two key areas:

**1. Data Storage Access and Integrity:** The Enforcer monitors all communication with internal databases and storage buckets, focusing on data integrity, access patterns, and storage capacity. This proactive monitoring helps identify potential data corruption, bottlenecks, or capacity issues early on, aligning with the Enforcer's role in upholding the system's rules and constraints related to data management.

**2. Performance Evaluation and Risk Management:** The Enforcer oversees performance metrics, investigates specific trade outcomes, and manages risk through the following mechanisms:

- **Trading Cost Analysis:** The Enforcer analyzes the impact of trading costs (commission fees, slippage, etc.) on portfolio alpha, ensuring that transaction expenses do not unduly erode returns.
- **Short-Selling Constraint Analysis:** Specifically for small-cap stocks, the Enforcer analyzes how short-selling constraints (borrowing costs, share availability, regulatory limitations) affect performance.
- **Trade Outcome Evaluation:** The Enforcer maintains a consistent and accurate assessment of trading performance, including the evaluation of unsuccessful trades, to provide a complete picture of trading activity.
- **Stop-Loss Mechanism:** The Enforcer implements and monitors a stop-loss mechanism, defining and enforcing rules for automatically exiting positions when they reach a predetermined loss threshold to mitigate potential losses.
- **Risk-Based Portfolio Weighting:** The Enforcer implements and monitors a risk-based weighting scheme for portfolio allocation, assigning weights to different assets based on their perceived risk levels to ensure the portfolio remains within the desired risk tolerance.

The architecture of the monitoring system itself adheres to key principles from the Universal Checklist, leveraging the Enforcer's role in resource access and control:

- **Controlled Resource Access:** All access to performance data, trading logs, and other relevant monitoring resources is channeled through designated Enforcer components, ensuring secure and controlled access to sensitive information.
- **Isolation of Monitoring Modules:** Specialized monitoring modules (Specialists) handle monitoring tasks, isolated from the core trading logic to prevent interference and improve system stability.
- **Asynchronous Communication:** Asynchronous communication (e.g., pub/sub model) facilitates information flow within the monitoring system, addressing potential latency issues and ensuring monitoring processes do not impede real-time trading operations.
- **Inheritance-Based Code Structure:** The monitoring system's codebase utilizes inheritance to structure services, enforcing a strongly typed and well-defined separation of concerns for improved maintainability and extensibility.

This structured approach, combined with the modular design inherent in the system's specialist services (`NormalizeWindow`, `ComputePrincipalComponents`, `ProjectToPlane`, `TrainOneEpoch`), facilitates granular monitoring and targeted debugging. By monitoring individual modules, data quality, and model training progress, the Enforcer can quickly identify and address performance bottlenecks, data issues, or unexpected behavior, ensuring the ongoing effectiveness of the SCoVA trading strategy. This modularity allows for targeted performance monitoring, data quality checks within each stage, and detailed observation of model training progress.

## B. Monitoring (Enforcer)

This section details the monitoring procedures and metrics crucial for maintaining the performance and stability of the trading system, focusing on the Enforcer's role. The Enforcer, comprised of the `Live_Execution_Enforcer` and `State_Enforcer` services, is responsible for upholding trading rules and risk management. Monitoring informs the logic within these services and ensures their effectiveness. Given the modular, microservice-based architecture, monitoring will address both individual service performance and inter-service communication.

**Enforcement Logic Monitoring:**

The following metrics will be monitored to ensure the correct and efficient operation of the Enforcer's core logic:

- **Transaction Cost Impact on Alpha:** The `Live_Execution_Enforcer` will track and analyze the impact of transaction costs (fees and slippage) on portfolio alpha. This data will inform order execution strategies, potentially optimizing broker selection or order types. The `State_Enforcer` will record and aggregate these costs for ongoing analysis.

- **Short-Selling Constraint Impact on Small-Cap Performance:** The `Live_Execution_Enforcer` will monitor how short-selling constraints affect small-cap stock performance. This data will dynamically adjust short-selling strategies, including position sizing and instrument selection.

- **Unsuccessful Trade Analysis:** The `State_Enforcer` will implement a standardized process for evaluating unsuccessful trades, tracking specific outcomes to identify patterns and improve future trading strategies.

- **Stop-Loss Mechanism Effectiveness:** The `Live_Execution_Enforcer` will implement and monitor the effectiveness of stop-loss mechanisms, ensuring timely exits from positions when predefined thresholds are breached. Metrics tracked will include the frequency of stop-loss triggers, the magnitude of losses averted, and the timeliness of executions.

- **Risk-Based Weighting Performance:** The `Live_Execution_Enforcer` will monitor the performance of the risk-based weighting scheme, ensuring alignment with investment goals and risk tolerance. The `State_Enforcer` will provide the necessary data for dynamic adjustments to portfolio allocations.

**System Performance Monitoring:**

In addition to the enforcement logic, the following system-level aspects will be continuously monitored, especially considering the microservice architecture:

- **Service Performance:** End-to-end performance tests will monitor latency and throughput for each service, including the `Live_Execution_Enforcer` and `State_Enforcer`. This proactive approach allows for early identification of bottlenecks and ensures real-time performance.

- **Inter-service Communication:** Google Cloud Pub/Sub, used for asynchronous communication between services, will be monitored for message delivery latency and throughput. This is crucial for maintaining responsiveness and ensuring the timely execution of trading decisions.

- **Resource Utilization:** Resource usage (CPU, memory, network) for each microservice will be tracked to identify potential resource constraints and optimize resource allocation.

This comprehensive monitoring strategy, encompassing both enforcement logic and system performance, ensures the robustness, stability, and adaptability of the trading system. The use of a modular, microservice-based architecture allows for targeted monitoring and performance optimization within each service and across the entire system.

## B. Monitoring (Enforcer)

The Enforcer role is responsible for maintaining system stability, performance, and the ongoing health and efficacy of the trading system. This involves monitoring resource usage, mitigating latency and network bottlenecks, identifying and addressing real-world trading challenges, and ensuring the long-term viability of the SCoVA project. Monitoring is implemented through a combination of comprehensive logging, distributed tracing, and post-deployment performance analysis.

**1. System-Wide Logging and Visualization:**

A comprehensive system-wide logbook meticulously tracks the performance and execution flow of all SCoVA services. This facilitates granular insights into system behavior and enables proactive identification of potential bottlenecks. Key logged data points include:

- **Function Execution Time:** Recording the execution time of each function, including pausing and resuming times for Facilitator-Specialist interactions, provides granular performance insights.
- **Network and API Calls:** Logging all network and API calls made by any service (Facilitator or Specialist) enables analysis of communication overhead and potential latency issues.
- **Visualization for Parallel and Asynchronous Services:** Visualizations depicting the execution flow of asynchronous and parallel services, including branching threads and loopbacks during Facilitator-Specialist communication, clarify complex interactions.

To streamline integration, the project will investigate leveraging or adapting existing IDE logging functionality. The logbook will be instrumental in identifying performance bottlenecks within Facilitator services, guiding refactoring efforts to decompose complex Facilitators into smaller, more efficient components.

**2. Distributed Tracing:**

A distributed tracing system using OpenTelemetry provides comprehensive insights into the interactions between SCoVA microservices (Continuity, Enforcer, Facilitator, and Specialist). This system comprises:

- **Trace and Span Generation:** OpenTelemetry's Python SDK, integrated into all microservices, generates traces and spans, providing detailed visibility into the execution flow.
- **Context Propagation:** OpenTelemetry manages context propagation, ensuring that trace and span IDs are correctly passed between services via HTTP headers and Pub/Sub message metadata, maintaining trace continuity across communication channels.
- **Visualization:** A visualization tool (Google Cloud Trace, Jaeger, or Grafana Tempo) with Gantt chart capabilities will be selected to visualize trace hierarchy and pinpoint performance bottlenecks.

To enhance system flexibility and observability, all service categories (Facilitators, Enforcers, and Continuity services) will be permitted to interact with each other, with distributed tracing providing comprehensive monitoring of these interactions.

**3. Post-Deployment Performance and Risk Management:**

Following deployment, ongoing monitoring focuses on identifying and mitigating real-world trading challenges and managing risk:

- **Trading Cost Analysis:** The impact of trading costs (commissions and slippage) on Jensen's Alpha will be analyzed to identify optimization opportunities.
- **Short-Selling Constraint Analysis:** The impact of short-selling restrictions, particularly in the small-cap market, will be investigated, including assessing the effect on portfolio returns when desired short positions cannot be executed.
- **Unsuccessful Trade Evaluation:** A clear methodology will be defined for evaluating unsuccessful trades, categorizing the reasons for losses, and informing future model refinements.
- **Stop-Loss Mechanism:** A stop-loss mechanism will be implemented, with parameters defined based on volatility and risk tolerance, to limit potential losses on individual trades.
- **Risk-Based Weighting:** A dynamic risk-based weighting scheme will be implemented for portfolio allocation, adjusting position sizes based on the perceived risk of each trade to manage overall portfolio risk.

### B. Monitoring (Enforcer)

This section details the monitoring procedures crucial for ensuring the continued performance and stability of the live trading system. These procedures address key risk management aspects and provide insights into the system's behavior, drawing upon concepts outlined in the project checklist.

**Performance and Risk Monitoring:**

- **Trading Cost Analysis:** Regularly analyze trading costs, including commissions, slippage, and market impact, to understand true profitability and identify optimization opportunities. High trading costs can significantly erode alpha. This analysis should be integrated with the backtesting engine for realistic cost simulations, directly supporting the need for a "Robust Backtesting Engine."

- **Short-Selling Constraint Monitoring:** Continuously monitor the impact of short-selling constraints, particularly on small-cap stock performance. Restrictions on short-selling can limit profitability and disproportionately affect small-cap-focused strategies. This monitoring should be integrated with the portfolio construction algorithm to ensure compliance.

- **Unsuccessful Trade Analysis:** Rigorously track and analyze unsuccessful trades, investigating the underlying reasons for poor performance. This analysis should inform model refinements and improvements to predictive accuracy, aligning with the need to "Clarify evaluation of unsuccessful trades." Insights gained will be used to enhance the "Adaptive Seesaw Blending" process.

- **Stop-Loss Monitoring:** Monitor the effectiveness of the stop-loss mechanism by analyzing trigger frequency, loss mitigation, and potential opportunity costs. This is essential for understanding the impact of this risk management tool on portfolio performance and should be integrated with the "Portfolio Risk Management" service.

- **Risk-Based Weighting Performance:** Regularly evaluate the performance of the risk-based weighting scheme, analyzing its effectiveness in managing portfolio risk and its impact on overall returns. This evaluation should be incorporated into the "Robust Backtesting Engine" to assess the long-term viability of the weighting strategy and ensure the "Portfolio Risk Management" service functions as intended.

**Explainability and Narrative Generation:**

To ensure transparency and facilitate compliance, a dedicated `Narrative_Generation_Service` will provide human-readable explanations for each executed trade. This service will leverage:

- **Feature Store Integration:** Accessing versioned input features and system state information from the Feature Store to ensure explanations are based on the precise data used for decision-making.

- **Attribution Methods:** Implementing both model-agnostic (LIME and SHAP) and model-specific (attention maps) attribution methods to understand feature influence on trade decisions.

- **LLM Integration:** Utilizing a Large Language Model (LLM) to synthesize information from the Feature Store and attribution methods into coherent narratives.

- **Karma Ledger Integration:** Integrating generated narratives into the Karma Ledger to provide a centralized, auditable record of the rationale behind each trade.

### B. Monitoring (Enforcer)

This section details the monitoring procedures and infrastructure required for a smooth transition from paper trading to live trading, focusing on risk mitigation and performance evaluation. This staged approach allows for iterative refinement and validation before deploying to live markets.

**Paper Trading Environment:**

A robust paper trading environment will be implemented to mirror live market conditions as closely as possible. This includes:

- **`Paper_Brokerage_Simulator`:** This dedicated service emulates the Zerodha Kite Connect API, handling order execution and portfolio management within the paper trading environment. It persists its internal state in Firestore for analysis and exposes the same API endpoints as the `Live_Execution_Enforcer` for seamless switching between environments. Fills are simulated based on real-time market data, incorporating realistic bid/ask prices, volume, and the possibility of partial fills. Live WebSocket tick data is used to introduce realistic network latency and bid-ask spreads.

- **Live Tick Data Integration:** The `Paper_Brokerage_Simulator` integrates live tick data from Zerodha, ensuring the system is tested against real-time market conditions and providing a more accurate representation of actual trading scenarios. This data is crucial for accurate performance evaluation and realistic fill simulation.

- **Market Depth Data Processing:** While the Zerodha API has limitations in market depth precision (increments of ₹0.05 between -₹0.25 and +₹0.25), the system leverages available order quantity and count data at each of the five fixed bid/ask levels. Monitoring will verify the correct acquisition and utilization of this data in derived features. The `DeriveOrderBookFeatures` service processes this raw data to generate features like Order Book Imbalance (OBI), Weighted Average Price (WAP), and will be monitored to ensure accurate calculation and effective usage by the `DynamicPlaneGenerator`. Monitoring should confirm that spread-based features (initially planned but discarded due to API limitations) are not being used.

**Live Trading Dashboard Integration:**

The Live Trading Dashboard facilitates seamless switching and comparison between paper and live trading:

- **Trading Mode Toggle:** A clearly labeled toggle within the Dashboard UI allows users to easily switch between `LIVE` and `PAPER` trading modes. The `Live_Execution_Enforcer` uses this toggle to route trade requests to either the Zerodha Kite Connect API (for `LIVE`) or the `Paper_Brokerage_Simulator` (for `PAPER`).

- **Portfolio State Management:** The `Paper_Brokerage_Simulator` manages and persists the paper trading portfolio state (cash balance, open positions, order status) in a dedicated Firestore collection (`paper_portfolio`) for analysis and performance tracking.

This comprehensive monitoring framework allows for continuous evaluation and refinement of the trading system, minimizing risk and maximizing the likelihood of successful deployment to live markets. The focus on realistic data and simulated environments provides valuable insights into system behavior under real-world constraints.

### B. Monitoring (Enforcer)

This section details the monitoring procedures crucial for maintaining and improving the trading system's performance in live trading. Monitoring focuses on key performance indicators (KPIs), market conditions, and the health of the underlying services. These procedures inform adjustments to the system's behavior through feedback loops and dynamic parameter tuning.

**Market Data and Service Health:**

- The `ComputeOrderBookState` service will be monitored to ensure it correctly calculates Order Book Imbalance (OBI) from the market depth dictionary. The resulting OBI values should consistently fall within the expected range of -1.0 to +1.0.
- The `GenerateDepthQuantityHeatmap` service will be monitored for correct heatmap generation. The heatmaps should accurately represent order book quantities across recent time steps and be effectively utilized by the `MarketDepthAnomalyDetector`.
- The `MarketDepthAnomalyDetector`'s performance in identifying visual patterns indicative of market shocks will be analyzed. This analysis will focus on the correlation between detected anomalies and actual market movements.

**Performance and Profitability:**

- **Model Profitability and Tracking Enhancement:** Model profitability calculations will be continuously monitored and enhanced for accuracy. This includes incorporating price improvement metrics to provide a more realistic representation of trading returns.
- **High Trading Costs Impacting Alpha:** Trading costs, including commission fees, slippage, and other transaction expenses, will be continuously analyzed to determine their impact on alpha generation.
- **Short-Selling Constraints Impacting Small-Cap Performance:** The impact of short-selling constraints on strategy performance, particularly within the small-cap space, will be monitored.
- **Clarify evaluation of unsuccessful trades:** A clear methodology will be established and implemented for evaluating unsuccessful trades, focusing on understanding the reasons for losses and identifying potential areas for improvement in the trading strategy.

**Dynamic Adjustments and Feedback Loops:**

- **Implement Execution Quality Feedback Loop:** A feedback loop within the Self-Correction & Healing Controller will continuously monitor rolling average execution quality, including slippage and price improvement. Deteriorating execution quality (e.g., increasing slippage and decreasing price improvement) will automatically increase the `CorrectionFactor`, making the Dynamic Plane perception more conservative and mitigating the risk of accumulating prediction errors.
- **Develop `CalculatePriceImprovementRate` Specialist Service:** A dedicated service, `CalculatePriceImprovementRate`, will track the rolling average price improvement on executed trades over a defined time window (e.g., N minutes or hours), outputting a "Price Improvement Rate" feature.
- **Add Price Improvement Rate as Feature to Predictive Models:** The "Price Improvement Rate" will be integrated as a new feature (context token) within the Vision Transformer predictive models, allowing them to learn and exploit potential correlations between high rates of price improvement and specific market outcomes.
- **Enhance `ComputeOrderBookState` with Book Resilience Score:** The `ComputeOrderBookState` service will be enhanced to include a "Book Resilience Score" in its feature vector. This score, calculated as the ratio of quantity at Level 1 to the quantity at Levels 2-5, provides insights into order book stability. A low resilience score, combined with a bullish pattern from the Flow Engine, will increase the conviction of Vision Transformer signals.
- **Implement Stop-Loss Mechanism:** A stop-loss mechanism with defined thresholds will be implemented to automatically close positions and mitigate potential losses.
- **Implement Risk-Based Weighting:** A risk-based weighting scheme will adjust position sizes based on perceived risk, optimizing capital allocation and potentially reducing portfolio volatility.

**Anxiety Model Monitoring:**

- **Anxiety Model Training and Integration:** An "Anxiety Model" will predict the behavior of the DynamicPlane algorithm and output an "Anxiety Level" based on real-time market depth data, including Order-to-Quantity Ratio, Rate of Change of OBI, Level 1 Dominance, and Book "Flicker" Rate. This "Anxiety Level" will adjust the Error Detector and Weight Shifter. The model will be trained post-hoc using backtest data, targeting the Total Error from the main algorithm.
- **Post-Hoc Analysis for Anxiety Model Refinement:** Post-hoc analysis will correlate algorithm actions with historical order book data to inform Anxiety Model training and enable it to recognize patterns suggesting potential errors or necessary model adjustments.

### B. Monitoring (Enforcer)

This section details the monitoring procedures crucial for evaluating the SCoVA project's performance and stability in a live trading environment. Monitoring encompasses both systemic factors affecting trading performance and model-specific behaviors.

**Market and Trading Mechanics Monitoring:**

- **Trading Costs:** Analyze the impact of trading costs (commissions, slippage, and exchange fees) on overall investment performance (Alpha). High trading costs can significantly erode profitability, requiring careful monitoring and potential adjustments to the trading strategy.
- **Short-Selling Constraints:** Evaluate the impact of short-selling restrictions, particularly on the performance of small-cap stocks. Limited short-selling opportunities can hinder the model's ability to profit from declining prices, necessitating adjustments to position sizing or the target universe.
- **Unsuccessful Trade Analysis:** Establish a clear methodology for evaluating and learning from unsuccessful trades. This includes investigating why trades deviated from expectations and identifying potential areas for model or strategy improvement.
- **Stop-Loss Mechanism:** Implement and monitor a stop-loss mechanism to limit potential losses on individual trades. This involves setting a predetermined price level for automatic position closure to prevent excessive losses in adverse market conditions.
- **Risk-Based Weighting:** Implement and monitor a risk-based weighting scheme to manage portfolio risk effectively. This involves allocating capital based on perceived risk levels, potentially reducing exposure to highly volatile instruments.

**Model Behavior Monitoring:**

- **Anxiety Model and Market Depth:** Continuously monitor live market depth data to inform the Anxiety Model's dynamic assessment of error potential and adjust trading algorithm weights accordingly. The model's switching between "flow" and "shock" trading modes based on market conditions should also be monitored.
- **Asymmetric Risk Management:** The Portfolio_Risk_Manager (Enforcer) will employ asymmetric risk management strategies, applying different risk tolerances to short and long positions. Monitor the effectiveness of these asymmetric limits, particularly the tighter drawdown limits likely applied to short positions.
- **Asymmetric Self-Correction (HealingController):** Monitor the HealingController's asymmetric self-correction mechanism, ensuring it reacts more aggressively to large, unexpected losses than to equivalent gains. Track the magnitude and direction of deviations from expected performance and the controller's response.
- **Risk-Averse Loss Function:** Monitor the performance of the custom risk-averse loss function, ensuring it continues to penalize the underestimation of losses appropriately without becoming overly conservative. Track the ratio and magnitudes of overestimated to underestimated losses.
- **State-Dependent Attention Mechanism:** Monitor the functionality of the state-dependent attention mechanism, ensuring it adapts appropriately to different market regimes. Analyze attention weights across various market states and correlate them with relevant market indicators.
- **Asymmetric Feature Impact:** Monitor the impact of any implemented asymmetric features (e.g., Upside vs. Downside Volatility, Volatility Skewness, Accumulation/Distribution Ratio) on model predictions and overall trading outcomes. This monitoring should assess the effectiveness of these features in capturing market asymmetries.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on compiling the research and code into a comprehensive dissertation and a well-documented, reproducible codebase. It incorporates best practices from the CVSM-15 reference regarding data handling and model training.

### A. Dissertation Writing (Continuity)

The dissertation should clearly articulate the project's methodology and findings. Crucial details to include are:

- **Data Acquisition and Preprocessing:** Explain the process of acquiring OHLC and index data from Yahoo Finance and Nasdaq, respectively, covering the period from 2016 to 2023. Detail the adjustment of closing prices for stock splits (excluding dividends) and the equal weighting applied to all index data.
- **Return Label Calculation:** Explicitly define the return label formula: `Return(t+h) = (Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` represents the last date of the input chart image and `h` represents the holding period (1 to 5 days). This formula is central to the model's training and evaluation.
- **Model Training and Validation:** Describe the training process for the CNN models, including the use of mean squared error loss and the implementation of early stopping based on validation loss. Specify the use of an out-of-sample, last-block validation strategy with the last 30% of the training data (July 1, 2020, to December 31, 2021) serving as the validation set. Clarify that the model with the lowest validation loss is selected for testing.
- **Filename Convention:** Explain the implemented filename convention for generated graphs, detailing how the holding period (`h`) is encoded within the filename (e.g., `YYYY-MM-DD__YYYY-MM-DD__3.25__h=3__ABB.ST.png`) or if this information is stored separately (e.g., in a CSV file). This ensures reproducibility and clarifies the link between generated images and their respective holding periods.

### B. Codebase Finalization (Facilitator)

The final codebase should be well-structured, documented, and aligned with the methodologies described in the dissertation.

- **Code Clarity and Documentation:** Provide clear and concise documentation for all code modules, particularly those related to data preprocessing, return label calculation, model training, and graph generation. This will facilitate understanding and potential replication of the project.

This comprehensive documentation ensures the project's transparency and reproducibility, which are vital aspects of academic research.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on integrating the research, code, and results into a cohesive and reproducible dissertation. This includes finalizing the codebase for submission alongside the dissertation.

### A. Dissertation Writing

This subsection details the required content and structure for the dissertation. The dissertation should not only present the results but also provide a clear and comprehensive explanation of the methodology, including the connection between the code and the experiments performed. This requires:

- **Structure and Content:** The dissertation should follow a standard structure, including chapters on introduction, literature review, methodology, results, discussion, and conclusion. All relevant findings, analyses, and results obtained throughout the project lifecycle must be incorporated. The dissertation should also include a detailed explanation of the thought process behind design choices and the evolution of the project. This will contribute to a richer and more insightful dissertation.
- **Connecting Code and Experiments:** It is paramount to link the code in the repository to the specific experiments described in the dissertation. This ensures a clear connection between implementation and results and facilitates reproducibility. The dissertation should prepare the reader for in-depth discussions about the research and the associated GitHub repository, enabling them to understand complex concepts and insights derived from the project analysis.

### B. Codebase Finalization

This subsection outlines the steps required to finalize the codebase for submission alongside the dissertation. A well-structured and documented codebase is crucial for reproducibility and understanding. This requires:

- **Code Organization and Documentation:** The codebase should be clean, well-documented, and functional. All unnecessary code or experimental branches should be removed. Detailed documentation within code modules is essential, explaining the functionality and purpose of different sections of the code. This includes clear documentation of hyperparameters such as the learning rate (0.001) and batch size (32) used in training, as well as performance evaluation metrics (Jensen's Alpha and Sharpe Ratio).
- **Data Handling and Processing:** The code should include clear file paths for data access, ideally with scripts for automated data downloading. The implementation of the 5-day windowing logic for both input features (candlestick chart images) and output labels (return calculation) must be clearly documented within the code. The `trade.py` script should be thoroughly commented to illustrate the real-world return calculation process during backtesting, mirroring the training methodology. The integration process between the return label calculation and the candlestick chart image generator should also be clearly documented.
- **Model Implementation and Functionality:** Provide well-documented code modules demonstrating the core functionality of the developed models and algorithms. This includes clear documentation of the model input (5-day candlestick chart) and how the CNN transforms the visual input into numerical outputs, specifying what these values represent (e.g., entry/exit signals, predicted returns). The structure and format of the input data, including the representation of candlestick data and the handling of the time window, should be clearly defined, including the size _n_ of the data points. Instructions on how to navigate and understand the code structure should be provided, along with a step-by-step guide on how to reproduce the models and results.

By adhering to these guidelines, the final dissertation and accompanying codebase will be comprehensive, reproducible, and demonstrate a deep understanding of the project's technical intricacies.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on documenting the research and finalizing the codebase for submission alongside the dissertation. It emphasizes reproducibility and clarity for future readers and researchers.

### A. Dissertation Writing

The dissertation will provide a comprehensive narrative of the SCoVA project, encompassing methodology, results, and discussion. Key elements to be incorporated include:

- **Data Preprocessing and Model Input:** Document the data preprocessing steps, including creating candlestick chart images from five consecutive trading days of OHLCV data from Yahoo Finance. These images, incorporating candles, volume bars, and a moving average line, serve as input to the CNN. Explain the calculation of 5-day future returns using the formula `((Close(t+5) - Open(t+1)) / Open(t+1)) * 100`, which forms the target output for the CNN model. Address edge cases for the last _n_ data points to ensure complete dataset handling.

- **Model Training and Architecture:** Detail the CNN model training process using the candlestick chart images and corresponding 5-day future return labels. Discuss the exploration of combining CNNs with LSTMs or Transformers to capture temporal dependencies and the potential inclusion of fundamental/macroeconomic data for broader market context. Describe the scaling of the training data beyond the initial small dataset to a larger dataset of similar candlestick chart images for improved generalization.

- **Model Evaluation and Validation:** Clearly define the evaluation metric used (e.g., MSE, RMSE) for assessing the model's predictive ability. Explain the rolling walk-forward validation process employed to rigorously test out-of-sample performance and adaptability to dynamic market conditions.

- **Methodological Considerations and Limitations:** Emphasize the methodological rigor and address the model's limitations. Explicitly state that the CNN was not trained on dates, real prices (using scaled visual bars, not actual values), or ticker symbols (unless inadvertently leaked). Discuss the implications of model uncertainty and potential estimation methods (e.g., Monte Carlo Dropout, Bayesian CNNs, Ensemble Models) for future research. Incorporate insights from exploring trading strategies based on maximum prediction accuracy (instead of decile ranking) and the use of Historical Prediction Error Profiling.

- **Dissertation Title Refinement:** Refine the title to accurately reflect the core research contributions, including novel techniques like Historical Prediction Error Profiling or alternative trade selection criteria. Consider keywords related to model interpretability and stress testing.

### B. Codebase Finalization

The finalized codebase, submitted with the dissertation, will provide a clear and reproducible representation of the project's technical implementation.

- **Code Structure and Documentation:** Organize the code into well-defined, documented modules corresponding to data acquisition, preprocessing, model training, and backtesting. Thoroughly document the code implementing candlestick chart generation with `matplotlib`, CNN architecture, and input data handling. Include inline comments explaining the code logic and docstrings for functions and modules. Document the initial challenge with the missing `mplfinance` library and the subsequent solution using `matplotlib` directly.

This combined approach ensures that the dissertation and codebase offer a comprehensive and accessible record of the project, facilitating understanding and potential future research extensions.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on compiling the research, finalizing the codebase, and ensuring comprehensive documentation for both. Key aspects include justifying design choices, explaining the model training process, and demonstrating the implementation of confidence-based filtering and hyperparameter tuning.

### A. Dissertation Writing

The dissertation should clearly articulate the entire research process, from data acquisition and preprocessing to model development, training, evaluation, and backtesting. Ensure the following points are addressed:

- **Model Architecture:** Explicitly state and justify the use of a 5-candlestick input window, referencing Jiang et al. (2023) for its empirical effectiveness in financial time series pattern recognition. Acknowledge that the 5-day prediction window (output window) lacks similar empirical validation and explain the rationale behind this choice (e.g., symmetry with the input window, practical trading considerations).
- **Training Process:** Detail the training of separate Convolutional Neural Networks (CNNs) for each holding period from 1 to 5 days. Compare their performance using metrics like validation loss, Sharpe ratio, alpha, and Mean Squared Error (MSE).
- **Historical Prediction Error Profiling (HPEP):** Thoroughly document the HPEP strategy, highlighting its impact on key performance indicators and explaining the chosen confidence metric and its implementation. Discuss the improvements gained through confidence-based trade filtering.
- **Title:** Refine the dissertation title to accurately reflect the core focus of the research, including confidence-based filtering and its impact on trading performance.

### B. Codebase Finalization

Prepare the codebase for submission alongside the dissertation, including clear instructions and all necessary scripts and data files for reproducibility. Ensure comprehensive documentation, specifically addressing the following:

- **HPEP Implementation:** Clearly document the HPEP module within `test_model.py` (HPEP map generation after training) and `trade.py` (HPEP-based trade filtering during backtesting). Include a working prototype script with dummy data for demonstration. Document the post-training confidence profile generation process, explaining the binning strategy (e.g., -5% to -3%) and the calculation of accuracy and any additional metrics (e.g., average error magnitude, Sharpe ratio) within the HPEP map. Detail how predicted values are mapped to HPEP bins and how the accuracy threshold determines trade execution.
- **Hyperparameter Tuning:** Document the implementation of hyperparameter tuning strategies, including the `confidence_threshold` representing the minimum accuracy for trade execution. Detail the tuning process for this threshold using methods like grid search or Bayesian optimization. Document the exploration of joint hyperparameter optimization, considering `holding_days` alongside other hyperparameters. If included, document the exploration of bin width optimization.
- **Code Modularity:** Provide well-documented modules for key components, including data preprocessing, model training, backtesting, HPEP implementation, and confidence-based filtering/ranking logic. This will enhance readability and facilitate future extensions of the research.

## V. Dissertation and Documentation

This section details the finalization of the dissertation and codebase, ensuring comprehensive documentation of the research process, implemented changes, and performance analysis. It emphasizes the integration of the soft labeling approach and its impact on model performance and trading strategy.

### A. Dissertation Writing

The dissertation should provide a complete account of the research, from data acquisition and preprocessing to model development, evaluation, and trading strategy implementation. The following aspects related to soft labeling and performance analysis require detailed documentation:

- **Soft Labeling Implementation and Rationale:** Explain the theoretical basis and motivation for adopting a soft labeling approach. Detail the implementation process, including:
  - **Return Discretization:** Describe the discretization of the return space into bins, specifying the chosen ranges and justification.
  - **Model Output Modification:** Document the changes to the CNN's output layer to accommodate a probability distribution over the discretized returns. Explain the architecture (e.g., fully connected layer with softmax activation) and its role in probability generation.
  - **Label Conversion:** Explain the process of converting hard labels into soft labels using a Gaussian kernel, justifying the chosen kernel parameters.
- **Loss Function Modification:** Detail the change in the loss function from Mean Squared Error (MSE) to a more appropriate function for probabilistic outputs (e.g., Categorical Crossentropy or KL-Divergence). Explain the rationale and impact on model training and performance.
- **Probabilistic Trading Strategy:** Describe the development and implementation of a trading strategy that leverages the probabilistic outputs of the soft label CNN. Include:
  - **Trading Logic:** Explain how trading decisions are made based on confidence thresholds applied to the predicted probability distributions.
  - **Backtesting Results:** Present the results of backtesting the probabilistic trading strategy, comparing its performance to the baseline approach. Analyze trading behavior and insights gained.
- **Trading Cost Analysis and Mitigation:** Address the impact of trading costs on overall alpha generation, specifically the observed reduction in annual returns. Analyze contributing factors such as rebalancing frequency, transaction friction, uniform weighting, and short-selling constraints. Discuss potential mitigation strategies and their feasibility.
- **Performance on First North All-Share Index:** Highlight the achievement of a +8.89% annual alpha (after transaction costs) when applying the model to the First North All-Share (small-cap) index. Analyze the potential reasons for this improved performance compared to other indices.
- **Uncertainty-Aware Architectures (Optional):** If explored, document the investigation of uncertainty-aware architectures (e.g., Monte Carlo Dropout, Deep Ensembles, Bayesian CNNs) and their impact on quantifying prediction uncertainty and trading decisions.

### B. Codebase Finalization

The finalized codebase should be clear, well-documented, and easily reproducible. Ensure the following aspects are addressed:

- **Soft Label Implementation:** The code should clearly reflect the modifications for soft labeling, including return discretization, output layer changes, and label conversion. Provide comprehensive comments and documentation within the code.
- **Loss Function and Model Training:** Include the updated loss function and training procedures for the soft label CNN.
- **Probabilistic Trading Strategy:** Provide the code implementation of the probabilistic trading strategy, including the logic for making trading decisions based on confidence thresholds.
- **Backtesting Framework:** Include the backtesting framework used to evaluate the soft label strategy.
- **Trading Cost Mitigation:** Include any code modifications implemented to address the impact of trading costs.
- **Uncertainty-Aware Architectures (Optional):** If implemented, include and document the code for any uncertainty-aware architectures.
- **5-Day Input Window and Labeling Approach:** Ensure the code reflects the chosen 5-day input window and the final labeling approach (hard or soft). Provide clear documentation within the code to explain these choices.

By addressing these points, the dissertation and codebase will effectively communicate the research process, implementation details, and performance analysis, ensuring reproducibility and facilitating future extensions of the work.

## V. Dissertation and Documentation

This section details the final project stages, focusing on compiling the research, findings, and code into a comprehensive dissertation and preparing the codebase for submission. The development and implementation of the predictive models for trend direction, reward magnitude, and rally time, while not explicitly addressed in this section, form the core of the research and will be extensively documented and analyzed within the dissertation.

### A. Dissertation Writing

The dissertation will provide a comprehensive record of the entire project lifecycle, from initial project setup and data acquisition to model development, training, evaluation, backtesting, and any deployment and monitoring strategies. It will follow a standard academic structure. Central to the dissertation's content will be the rationale for choosing the specific prediction targets (trend direction, reward magnitude, and rally time), the model architectures, and a thorough performance analysis. Implementation challenges and their solutions will also be discussed. Furthermore, the dissertation will address the limitations of the initial trading strategy (fixed holding period and indiscriminate trade execution) by incorporating the following risk management strategies and analyzing their impact:

- **Stop-Loss Mechanism:** A stop-loss mechanism will be implemented to mitigate losses by exiting trades when they move against the predicted direction by a defined percentage. The analysis will explore the impact of different stop-loss levels on overall portfolio performance, addressing the limitations of the fixed 5-day holding period.

- **Prediction Confidence Thresholding:** A filter will be implemented to execute trades only when the prediction magnitude or historical accuracy (using HPEP) exceeds a predefined threshold. The analysis will examine the impact of various threshold levels on trading frequency and profitability, addressing the initial strategy's indiscriminate trade execution.

- **Risk-Based Weighting:** A risk-based weighting scheme will be implemented, assigning position weights based on factors like prediction confidence, inverse historical volatility, and the signal-to-noise ratio. This allows for more nuanced portfolio management. The rationale behind the chosen weighting factors and their effects on portfolio diversification and risk-adjusted returns will be discussed.

The dissertation will also incorporate the following analyses and discussions:

- **OMXS All-Share Performance:** The significant outperformance (+37.57% annually) compared to the benchmark return (-27.88%) will be highlighted, demonstrating the approach's potential in specific market segments.

- **Trading Strategy Inefficiencies:** The discrepancy between pre-cost and post-cost alpha (positive for 6 out of 8 portfolios pre-cost, but mostly negative or marginally positive post-cost) will be analyzed. This will be attributed to inefficiencies stemming from high turnover (trades every 5 days), uniform weighting, and the inclusion of low-confidence predictions due to the absence of smart trade filtering.

- **Alpha Enhancement Strategies:** Potential strategies for enhancing alpha generation will be explored, including a breakdown of long versus short position contributions, a table of per-index Sharpe ratios (pre- and post-transaction costs), and concrete suggestions for modifying portfolio construction logic. These suggestions will address the identified weaknesses (high turnover, uniform weighting, lack of smart trade filtering) and justify their potential impact on performance.

- **Short-Selling Constraints:** The potential impact of short-selling constraints on small-cap portfolio performance, particularly the likelihood of infeasible short positions due to borrow availability, will be addressed, emphasizing the significance of this constraint given the identification of small-cap stocks as the most profitable segment.

- **Handling Unsuccessful Trades:** The methodology for evaluating unsuccessful trades (where outcomes deviate from predictions) will be explained, detailing how such scenarios are handled without a stop-loss mechanism in the referenced paper.

- **Refined Dissertation Title:** The dissertation title will be revised to accurately reflect the enhanced scope of the research, including the incorporation of risk management strategies.

### B. Codebase Finalization

The accompanying codebase will be clean, well-documented, and reflect the final implementation, including the integrated risk management strategies. It will be organized into well-defined and documented modules for clarity and reproducibility. Specifically, this includes:

- **Codebase Submission:** The complete and documented codebase will be provided as a supplement to the dissertation.
- **Modular Code Structure:** The code will be organized into well-defined, documented modules.
- **Risk Management Modules:** Well-documented modules for the stop-loss mechanism, prediction confidence thresholding, and risk-based weighting will be provided, including the rationale behind design choices, parameter settings, and performance impact. Prototyping modules for the stop-loss and dynamic trade filtering (using concepts like HPEP and volatility) will also be included to demonstrate the development process and allow for further exploration.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on documenting the research and preparing the accompanying codebase. This involves refining the dissertation, ensuring the code's clarity and reproducibility, and providing comprehensive documentation for both.

### A. Dissertation Writing

This subsection details the process of finalizing the dissertation, ensuring a clear, logical structure and comprehensive presentation of the research.

1. **Structure and Write Dissertation:** This encompasses drafting, revising, and finalizing the dissertation. The document should be structured logically, with clear explanations of the methodologies, effective presentation of results, and meaningful conclusions drawn from the findings. A dedicated section should address the research and implementation of rally time prediction, as detailed below.

2. **Refine Dissertation Title:** The dissertation title should be concise and descriptive, accurately reflecting the research conducted and the core contributions of the project. Key aspects, such as the predictive models developed and their application to financial markets, should be incorporated.

3. **Research on Predicting Rally Time:** This section should thoroughly investigate the credibility and feasibility of predicting rally time, discussing potential challenges and limitations, and exploring the chosen methodologies. It should address whether predicting rally time is a viable strategy enhancement.

4. **Label Generation for Rally Time:** This section must clearly explain the process of generating labels for rally time, detailing the lookahead window (t+1 to t+N) and the specific criteria used to determine the time-to-target label (k). It should also describe how censored data is handled when the target price isn't reached within the lookahead window (k = N+1).

5. **Model Architecture for Rally Time Prediction:** This section should describe the chosen architecture for predicting rally time. If a multi-head neural network is used, details should include its structure, the use of EfficientNet features, and separate linear layers for return regression and rally time prediction. The loss function and the weighting of MSE losses for each head should be explained. If survival analysis models are employed, the rationale for this choice and the specific implementation (e.g., DeepSurv, DeepHit, or Weibull Time-To-Event models) should be justified. The explanation should emphasize how the chosen model predicts the probability distribution of reaching the target price over time, addresses the challenge of censored data, and expresses uncertainty.

6. **Trade Decision Logic Enhancement based on Rally Time:** This section should explain how the predicted rally time informs trade decision logic, including how it enables dynamic capital allocation and position sizing based on the predicted time to reach the target return. It should also address how the system handles situations where the predicted rally time window fails.

### B. Codebase Finalization

This subsection outlines the steps for preparing the codebase for submission alongside the dissertation.

1. **Provide Codebase for Dissertation:** The complete and finalized codebase, including all scripts and data processing pipelines, should be submitted with the dissertation. Clear instructions on executing the code and reproducing the results must be provided. A specific branch or tag within the version control system should be used to represent the final version of the code associated with the dissertation.

2. **Finalize Codebase:** Before dissertation submission, the codebase should be thoroughly reviewed, cleaned, and commented to ensure clarity and maintainability. Unnecessary code and temporary files should be removed. All dependencies should be clearly documented.

3. **Provide Detailed Code Modules:** Comprehensive documentation should be provided for all code modules, including their purpose, inputs, outputs, and relevant implementation details. This documentation should enable others to understand and potentially reproduce the experiments. This specifically includes documentation for:
   - **Dataset Generation for Rally Time:** The code used to generate the labeled dataset for rally time from existing stock data.
   - **Code Modification for Multi-Head Model (or Survival Analysis Model):** The modifications made to the codebase to incorporate the chosen prediction model, including details on the architecture, rationale, and specific changes.
   - **Metrics for Rally Time Prediction Accuracy:** The code implementing the metrics used to evaluate the accuracy of rally-time predictions, including clear definitions and explanations of their implementation.

By incorporating these details into the dissertation and codebase, the research, implementation, and evaluation related to rally time prediction will be comprehensively documented and readily accessible.

## V. Dissertation and Documentation

This section details the project's final stages, focusing on documenting the research, finalizing the codebase, and ensuring alignment between the dissertation and the implemented system. The dissertation should provide a comprehensive account of the project's development, implementation, and evaluation, encompassing both the core trading logic and the advanced enhancements explored. The codebase should be thoroughly documented and clearly reflect the described methodologies.

### A. Dissertation Writing

The dissertation should cover the following key areas:

- **Sophisticated Exit Strategies:** Dedicate a section to the implemented exit strategies, explaining their rationale and impact on performance. This section should detail:

  - **Volatility-Aware Exit Thresholds:** Explain the use of Average True Range (ATR) to dynamically adjust exit thresholds. Describe the ATR calculation during the simulation and how exit tolerances are scaled based on ATR.
  - **Time-Based Confidence Decay:** Discuss the time-sensitive nature of predictions and the implementation of a time-based exit strategy. Explain how this mechanism improves the model's temporal awareness, including the specific logic used to implement the decay and its influence on trading decisions.
  - **Prediction Divergence Exit Logic:** Detail the mechanism for tracking prediction divergence after model retraining. Explain the threshold used to trigger exits based on this divergence and its impact on mitigating model drift.
  - **Portfolio Contextual Exit Logic:** Describe the method for monitoring peer trades and exiting underperforming trades within a prediction group. Explain how this introduces internal attribution to the system and how it influences portfolio-level risk management.

- **Learning from Mistakes:** Address the process of learning from incorrect predictions. This section should cover:

  - **Error Map Creation:** Explain the construction of the error map during validation, detailing how predicted and actual returns are tracked and the criteria for classifying "bad trades."
  - **Error Analysis and Future Training:** Discuss the analysis of patterns in incorrect predictions and how these patterns can inform future training iterations, including potential techniques such as sample re-weighting, bootstrapping of hard cases, and meta-model development for trade review.

- **Reward Logic Enhancement:** Detail the exploration and implementation of a more nuanced reward logic that considers preceding candlestick images. Explain the rationale, potential impact on performance, and how this approach incorporates temporal dependencies or sequential analysis into the reward calculation.

- **Model Input Adjustment for Sequences:** Clearly explain the adjustments made to the model's input layer to accommodate image sequences, contextualizing the architectural changes related to incorporating temporal dependencies. Describe the benefits of using image sequences and their impact on the model's ability to learn temporal transitions.

### B. Codebase Finalization

The finalized codebase should reflect the enhancements and modifications discussed in the dissertation, adhering to best practices for readability and maintainability. Ensure the following:

- **Modular Implementation:** Implement each exit strategy (Volatility-Aware Thresholds, Time-Based Confidence Decay, Prediction Divergence, and Portfolio Contextual Exit Logic) in well-defined and documented modules.
- **Error Map Generation:** Include functionality for generating and storing the error map during validation, as described in the dissertation.
- **Reward Logic Implementation:** Clearly implement and document the enhanced reward logic incorporating candlestick sequences.
- **Input Layer Modifications:** Document the modifications to the input layer for handling image sequences.
- **Comprehensive Documentation:** Thoroughly document all code, including clear comments explaining the logic and parameters used for each exit strategy, error mapping process, reward logic enhancements, and input layer modifications.

This combined approach to dissertation writing and codebase finalization will ensure a cohesive and comprehensive representation of the research, facilitating reproducibility and future development.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on documenting the research and finalizing the codebase. This documentation must provide sufficient detail for a reader to understand the motivations, implementation, and results of all experiments.

### A. Dissertation Writing (Continuity)

This subsection focuses on structuring the dissertation, incorporating the findings from the model development, testing, and evaluation stages. Key elements derived from the project that should be documented include:

- **Data Model Design:** The dissertation should thoroughly explain the data model design choices, particularly focusing on the sequential nature of the input data and the incorporation of temporal context. This includes:

  - **Sequential Candlestick Windows:** Explicitly describe the use of sequential candlestick windows as input to the models. Justify the chosen window length and how it captures temporal dependencies.
  - **Delta Feature Engineering:** Detail the implementation of delta features, explaining whether image subtraction or feature subtraction from sequential candlestick windows was used, and provide the rationale for this choice.
  - **Dataset Design:** Explain the creation of the image sequence dataset using the sliding window approach. Specify the composition of each sequence (e.g., charts from t-2, t-1, and t) and the associated labels (return value, rally time, signal class). Further, explain the rationale behind using static images, paired images, and image sequences as model input.

- **Benchmark Comparison:** Document the comparative analysis of using static pictures, paired pictures, and image sequences as input to the model. Discuss the insights gained regarding the most effective method for incorporating temporal context.

- **Model Architectures:** Provide detailed explanations of the chosen model architectures, including design choices, implementation details, and performance evaluation. This includes:

  - **CNN + LSTM Hybrid:** Articulate the rationale behind combining CNNs for feature extraction with LSTMs for temporal modeling.
  - **Vision Transformer (ViT):** Describe both the EfficientNet encoding approach and the patch embedding approach for the ViT model, including the rationale behind choosing the final implementation. Analyze and discuss the ViT model's performance and its ability to capture temporal dependencies.

- **Backtesting Framework:** Document the development and implementation of the backtesting framework. Describe the chosen backtesting methodology and its relevance to evaluating the trading strategy in real-world market scenarios.

### B. Codebase Finalization (Facilitator)

This subsection details the finalization and documentation of the codebase to ensure reproducibility and clarity. The provided codebase should be clean, well-commented, and correspond directly to the experiments and results presented in the dissertation. Key components include:

- **Image Sequence Generator:** Provide well-documented code for the image sequence generator, explaining its implementation of the sliding window approach and usage for creating datasets with various parameters.
- **Paired Image Dataset Generator:** Provide well-documented code for the paired image dataset generator and explain how it facilitates training models leveraging temporal transitions.
- **Model Implementations:** Include well-documented code modules for the CNN + LSTM hybrid model and the ViT implementation.
- **Backtesting Framework Code:** Include the complete and documented code for the backtesting framework, detailing its usage and the interpretation of its outputs.

By addressing these points, the dissertation and codebase will accurately and comprehensively represent the conducted research, ensuring clarity, reproducibility, and a strong conclusion.

## V. Dissertation and Documentation

This section details the final project stages: completing the dissertation and finalizing the codebase. These tasks are intrinsically linked, with the dissertation providing the theoretical underpinnings and experimental findings reflected in the code. This section also identifies potential future research directions.

### A. Dissertation Writing

The dissertation should comprehensively document the entire project lifecycle, from data acquisition and preprocessing to model development, training, evaluation, and deployment. Beyond standard dissertation structure, the following key architectural decisions, evaluations, and experimental findings related to the Vision Transformer (ViT) architecture require detailed documentation:

- **Input Image Count (N):** Include a discussion on the ViT's input image constraint, if any. Justify the chosen value of N (number of input candlestick images), referencing supporting experiments and results. Document the experiments conducted with N=3, 4, and 5, presenting the results and analysis leading to the final selection.
- **Positional Embeddings:** Detail the implementation and impact of positional embeddings on the model's ability to interpret the sequential nature of candlestick data.
- **Masking and Padding:** Explain the implementation of masking and padding for handling variable-length input sequences, specifying the maximum sequence length and rationale.
- **Variable-Length Sequence Training:** Describe the training process with variable-length sequences, referencing the specific techniques used (masking or dynamic positional encodings) and the results achieved.
- **Candlestick Image Prediction:** Thoroughly analyze the core concept of predicting future candlestick images instead of directly predicting returns. This analysis should encompass:
  - **Conceptual Soundness:** Compare the model's image interpretation to how human traders visually analyze candlestick patterns to establish the approach's validity.
  - **Theoretical Advantages:** Discuss the potential advantages, including the richness of visual representation, ability to model uncertainty, potential for improved causal reasoning, enhanced training supervision through visual patterns, improved interpretability of predictions, and potential for generative flexibility.
- **Alternative Return Prediction Evaluation:** Evaluate predicting the subsequent candlestick _pattern_ as an image instead of a numerical return. Analyze whether this approach improves prediction accuracy and acknowledge the potential increase in coding complexity.
- **Memory Management:** Document the evaluation of the different memory management options (A, B, and C), including the experimental framework, evaluation metrics (Sharpe ratio, directional accuracy, MSE, and rally-time prediction accuracy), and the rationale behind the final selection.
- **ViT Input Pipeline:** Clearly explain the development, design choices, and impact on model performance of the ViT input pipeline.

### B. Codebase Finalization

The finalized codebase should be clear, well-organized, reproducible, and aligned with the dissertation. This includes:

- **Integration of Chosen Solutions:** Seamlessly integrate the chosen memory management solution (A, B, or C) and the finalized ViT input pipeline (designed for N candlestick image inputs). Thoroughly test and document these implementations.
- **Comprehensive Documentation:** Provide clear and comprehensive documentation within the code (comments and docstrings) and a detailed README file explaining the various modules, functionalities, and usage. Focus on documenting the core components: the memory management solution, the ViT input pipeline, data loaders, masking logic, and ViT wrappers to ensure reusability and extensibility for future research.

## V. Dissertation and Documentation

This section details the requirements for the dissertation and the finalization of the supporting codebase. The dissertation should thoroughly document the project, focusing on the model's design, evaluation, and practical application in trading decisions. The codebase should be complete, well-documented, and easily reproducible.

### A. Dissertation Content

The dissertation must address the following key aspects of the research:

- **Model Design and Rationale:** Describe the prototype design for the image-to-image candlestick forecaster, specifying the chosen architecture (ViT or U-Net based) and its implementation details. Justify the selection of this architecture and explain the process of extracting open, high, low, and close values (and subsequently, returns) from the generated candlestick images. Address the accuracy and reliability of the extraction method.

- **Model Evaluation:** Evaluate the model's performance using multiple perspectives:

  - **Visual Accuracy:** Assess the model's ability to generate visually plausible candlestick sequences that align with established market patterns. Discuss the importance of this visual fidelity alongside traditional financial metrics.
  - **Financial Performance:** Evaluate the model's financial performance using appropriate metrics such as RMSE, SSIM/LPIPS, and backtested profit performance. Detail the experimental design for validation, including the training process, post-processing steps, and comparison with baseline models (e.g., scalar regression and probabilistic return models). The comparison should consider aspects such as output type, supervisory signal, link to trading strategies, richness of learned structure, interpretability, risk of ambiguity, data requirements, and modeling complexity.
  - **Accuracy vs. Profitability:** Investigate the relationship between the model's visual predictive accuracy and its financial performance. Analyze the correlation (or lack thereof) between these two aspects.

- **Trading Implications:** Clearly articulate how the model's predicted visual patterns translate into actionable trading decisions. Move beyond pattern identification and explain how the model's output informs specific trade entries and exits.

- **Risks and Drawbacks:** Critically assess the inherent risks and drawbacks associated with image-based prediction. Discuss the challenges of indirect evaluation, potential error compounding, the lack of direct reward supervision, and the ambiguity in translating image-based predictions to concrete financial implications.

- **Causal Grounding:** Explain not only _what_ patterns the model predicts but also _why_ these patterns occur. Connect the model's architecture and training process to a deeper understanding of market dynamics and justify its potential to generalize beyond the training data.

- **Comparison with Existing Models:** Thoroughly compare the proposed model with existing financial forecasting models, highlighting its advantages and disadvantages.

### B. Codebase Finalization

The finalized codebase should be thoroughly documented, clean, and well-commented. Include clear instructions (e.g., a README file) explaining how to execute and reproduce the experiments. Provide well-documented modules for all key components, including image generation, return extraction, backtesting procedures, and the implementation of the chosen evaluation metrics. The codebase will serve as a valuable resource for reproducibility and future research. Ensure its completeness and functionality before dissertation submission.

## V. Dissertation and Documentation

This section details the final stages of the SCoVA project, encompassing the compilation of research findings into a cohesive dissertation and the preparation of the accompanying codebase.

### A. Dissertation Writing

This section outlines the requirements for structuring and writing the dissertation. A well-defined structure, including chapters, abstract, and accurate citations, is crucial for effective communication of the project's methodologies and findings. Critically, the dissertation must provide comprehensive literature framing for each methodological innovation, supporting the research arguments and enhancing the academic rigor of the work. Furthermore, the current working title, "Automating Technical Analysis in Intraday Trading by using image snapshots of price action and making predictions using computer vision: a case study of Indian equity markets," requires refinement to a more concise and academic style while retaining core concepts like "snapshot" and "computer vision." Finally, the dissertation should clearly articulate the difference between generating realistic-looking charts and generating charts with actual trading value. While visual realism is desirable, the ultimate goal is identifying actionable trading opportunities. The evaluation should focus on identifying clear entry and exit points. The dissertation must also acknowledge and address potential challenges related to training and evaluating image-based prediction models, such as ground truth alignment, image fidelity loss functions, mode collapse, pattern overfitting, and the difficulty of evaluating generated images for trading relevance. The methods employed to mitigate these challenges should be thoroughly discussed.

### B. Codebase Finalization

This section details the preparation of the codebase for submission alongside the dissertation. The codebase should be well-documented, organized into clear modules, and aligned with the dissertation's content to ensure reproducibility and transparency.

- **Code Structure and Documentation:** The final codebase should reflect the architectural choices and experimental protocols described in the dissertation. If a dual-module framework (chart generator + trade evaluator) is employed, clear documentation explaining the interaction between these modules and how actionable trading signals are derived is necessary. The code implementing the experimental protocol comparing scalar vs. image-based prediction should also be included and thoroughly documented, including the metrics used for evaluation.
- **Supporting Materials:** The codebase submission should include a clear and concise prompt outlining the necessary code elements, which served as a guide for development. Furthermore, experimental logging templates, clearly outlining the information captured during experimentation (e.g., hyperparameters, evaluation metrics, runtime information), should be provided. Finally, comparative performance charts and tables generated during the project should be included to visually demonstrate the model's performance and provide a concise summary of results. While a theoretical comparison of 2D vs. 3D frameworks and the potential implementation of a 2D motion representation prototype are encouraged, they are not required for the codebase submission.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on compiling the research and findings into a dissertation and finalizing the associated codebase. The dissertation should provide a comprehensive theoretical framework for the dynamic 2D plane representation, connect it to relevant mathematical concepts, and detail its practical implementation and potential applications. The finalized codebase should be well-documented and clearly linked to the dissertation's content.

### A. Dissertation Writing

The dissertation should encompass the following key areas:

- **Introduction and Motivation:** Establish the context for the research, highlighting the challenges of representing 3D data in a 2D framework and the potential advantages of the dynamic 2D plane approach.
- **Mathematical Formulation of the Dynamic 2D Plane:** Provide a rigorous mathematical description of the dynamic 2D plane, including its coordinate system, transformation equations, and the role of rotations. Clearly define the relationship between the dynamic 2D plane and the original 3D space. Discuss the mathematical principles underpinning the approach, such as coordinate transformations, manifolds, and the concept of a moving frame on a 1-manifold.
- **Encoding 3D Information in the 2D Plane:** Detail how 3D data, specifically exemplified by a parabola along the z-axis, is encoded within the dynamic 2D plane through the rotation of its axes. Explain the information balance and degrees of freedom within this representation, considering the 2D coordinates and the three Euler angles (or rotation matrix) defining the frame's orientation.
- **Connection to Established Formalisms:** Explore the connections between the dynamic 2D plane and related concepts in mathematics and physics, including parallel transport, affine connections, Frenet-Serret frames, the SE(3) group, and local inertial frames in general relativity.
- **Application to the Three-Body Problem:** Describe the application of the dynamic 2D plane to the three-body problem. Detail the chosen representation for the three bodies within the 2D framework (e.g., pairwise relative coordinates, barycentric frame, or shape space) and discuss the challenges and solutions encountered. Relate the approach to recent advances in the three-body problem, such as machine learning approximations and new periodic solutions.
- **Technical Evaluation and Limitations:** Critically analyze the dynamic 2D plane representation, including its advantages, limitations, and potential applications in areas like robotics, computer graphics, navigation, and data compression. Discuss potential issues such as path dependence, singularities, and computational overhead.
- **Future Directions:** Outline potential future research directions, including formulating an explicit rotation law, investigating curvature invariants, and generalizing the concept to surfaces.

### B. Codebase Finalization

The finalization of the codebase should focus on clarity, completeness, and alignment with the dissertation:

- **Complete and Documented Code Modules:** Ensure the codebase is complete and includes detailed documentation for all modules related to the dynamic 2D plane creation, transformations, and data projection. This documentation should facilitate understanding and reproducibility.
- **Code Alignment with Dissertation:** Establish a clear link between the codebase and the dissertation. Code examples and references should be integrated within the dissertation to illustrate the practical implementation of the theoretical concepts. This allows for verification of the implementation and strengthens the connection between theory and practice.

## V. Dissertation and Documentation

This section outlines the final steps for documenting the research and preparing the codebase for inclusion with the dissertation. A comprehensive and well-documented dissertation is crucial for conveying the project's theoretical underpinnings, practical implementation, and potential impact.

### A. Dissertation Content and Structure

The dissertation should encompass not only the practical implementation and results of the SCoVA project but also a thorough exploration of the theoretical justifications for the chosen methodologies. This includes addressing the following key topics:

1. **2D Chart Representation of 3D Market Dynamics:** Discuss the challenges and feasibility of representing three-dimensional market movements (price, volume, time) within a two-dimensional candlestick chart. Explain how the dynamic plane acts as a form of state information, capturing the evolving context of market activity. Analyze the implications of not recording orientation information and the potential difficulties in reconstructing past trajectories solely from candlestick data.

2. **Cognitive Parallels: Human Navigation and Market Interpretation:** Explore the parallels between human navigation and the model's interpretation of market data. Discuss how humans utilize both egocentric and allocentric frames of reference in spatial navigation and relate this to the 2D + orientation representation employed in the SCoVA project. Ground this discussion with relevant neuroscience literature, specifically referencing O'Keefe's work on place and grid cells within the hippocampus.

3. **Neurophysiological Basis of Spatial Representation:** Expand on the neurophysiological evidence for egocentric and allocentric reference frames in the brain, highlighting the respective roles of the parietal/premotor cortex and hippocampal formation. Incorporate a discussion of head-direction cells and how parallax contributes to spatial understanding in biological systems, drawing parallels to the model's function.

4. **Benefits and Limitations of the Dynamic Plane Abstraction:** Analyze the benefits and limitations of using the dynamic plane abstraction, particularly in the context of financial time series data represented as candlestick charts. Discuss scenarios where translational and rotational symmetries within this representation might be redundant and the impact of tracking orientation data derived from price movements.

5. **Singularity and Chaos Considerations:** Investigate potential singularities or points of instability within the dynamic plane implementation, particularly considering the continuous nature of price data and the potential for abrupt market shifts. Discuss the behavior of the dynamic coordinate system near these points and how the model handles such scenarios. Address the potential impact of market chaos on the system, especially given the project's focus on short-term prediction.

6. **Feature Space Coverage and Transformation Justification:** Demonstrate that the dynamic plane approach allows for comprehensive coverage of the relevant feature space derived from price and volume data. Justify that the transformation doesn't restrict the model's ability to capture relevant patterns.

7. **Computational Cost Analysis of the Moving Frame:** Analyze the trade-offs of the dynamic plane implementation regarding memory usage and computational costs. Compare the costs of storing 2D representations with orientation data (derived from rotations and translations) versus storing the original 3D representation of price, volume, and time. Detail the specific computational costs associated with maintaining and applying orientation information, focusing on the chosen implementation (e.g., rotation matrices). Quantify and justify any efficiency gains or losses associated with this approach.

8. **Human Decision-Making Analogy:** Explore potential analogies between the dynamic plane implementation and aspects of human decision-making in trading. Discuss whether anchoring the coordinate system based on recent price movements can be compared to how traders focus on recent trends and volatility when making decisions.

Finally, refine the dissertation title to accurately reflect the core focus of the research, incorporating elements of dynamic plane representation and potentially referencing the neuroscientific inspiration behind the approach.

### B. Codebase Documentation and Submission

The final codebase should be thoroughly documented and submitted as a supplement to the dissertation to ensure reproducibility and transparency. This includes:

1. **Complete Codebase Submission:** Submit the entire codebase, ensuring it is well-organized and easy to navigate.

2. **Comprehensive Code Comments:** Provide clear and concise comments throughout the codebase, explaining the purpose and functionality of each section, function, and module.

3. **Detailed Documentation of Key Modules:** Pay particular attention to documenting the following modules:

   - **Numerical Conditioning of Moving Frame Coordinates:** Include commentary on the implementation choices made to ensure numerical stability, potentially referencing research on the advantages of body-fixed frames in robotics and UAV trajectory optimization. Compare and contrast this approach with using global coordinates (x, y, z).
   - **Stability During Rapid Rotations:** Detail the methods used to maintain stability during rapid market fluctuations (analogous to rotations), explaining any implementation of angular-momentum conserving integrators (Lie-group methods). Compare this approach to simpler integration methods such as Euler and RK4, justifying the chosen methodology.

By addressing these theoretical and practical aspects, the dissertation and accompanying codebase will provide a comprehensive and insightful overview of the SCoVA project, its underlying principles, and its potential contributions.

## V. Dissertation and Documentation

This section details the requirements for the dissertation and the finalization of the project codebase, emphasizing clear visualizations and comprehensive documentation.

### A. Dissertation Writing

The dissertation must include clear and informative visualizations to effectively communicate the project's findings. These visualizations should be created using `matplotlib` and `numpy`; the use of `seaborn` is prohibited. Each visualization should be a standalone figure, avoiding the use of subplots. Ensure visualizations are designed with clarity and audience comprehension in mind.

The following visualizations are required:

- **Data Visualization:** Charts and images representing the collected and analyzed data.
- **3D Helix Visualization:** A 3D plot of a helix with a radius of 1.0, a pitch of 0.5, and 4 turns, labeled with X, Y, and Z axes and titled "3D Helix in Laboratory Coordinates."
- **Improved Planar Trace Visualization:** An enhanced visualization of the planar trace in the moving chart, potentially incorporating tooltips, annotations, or a separate help section to explain its significance.
- **2D Unfolded Helix Visualization:** A 2D chart illustrating the unfolding of the helix arc, demonstrating how a straight path along the helix translates to a straight line within the moving coordinate frame.
- **3D Helix with Moving Frames Visualization:** A 3D visualization of the helix with moving coordinate frames (represented by tangent, normal, and binormal vectors) at various points along the curve, illustrating the frame's orientation changes.
- **Improved Moving Chart Explanation:** A detailed graphic or visual aid explaining the helical nature of the moving chart, clearly distinguishing it from simple circular motion and emphasizing the upward movement along the z-axis. The previous description of "marching forward while the personal coordinate frame slowly spins" is insufficient.
- **Helix with Tangent Vectors Visualization:** A 3D plot of the helix with tangent vectors at several points, demonstrating how forward movement along the tangent contributes to both horizontal and vertical displacement.
- **Synchronized Global and Local Trajectory Plots:** Two synchronized plots illustrating the relationship between a traveler's perceived circular motion within a rotating frame and their actual helical trajectory in a global frame. Note: Animation is not feasible for this visualization.

While animation with frame snapshots is a potential method for visualizing the helix and its 2D representation, carefully consider its performance implications and choose the most efficient and effective approach. Each call to a charting function should produce a single chart. Therefore, representing the 3D helix and its unfolded 2D representation might require separate charts or a carefully considered alternative.

### B. Codebase Finalization

The finalized codebase must be clear, reproducible, and well-documented. This includes:

- **Data Storage and Logging:** Robust data storage for planar coordinates (u, v using an arc length counter) and orientation (using a quaternion or rotation vector) of the moving frame. This data should facilitate spiral path reconstruction while respecting memory and storage constraints and remaining compatible with visualization and rendering requirements. Detailed logging is essential for reproducibility.
- **Documented Code Modules:** Provide well-documented code modules to facilitate understanding and future use.

The complete codebase, along with the dissertation, should be submitted as final deliverables.

## V. Dissertation and Documentation

This section details the final project stages, encompassing dissertation writing and codebase finalization. The dissertation and codebase should function synergistically, providing a comprehensive and reproducible account of the research.

### A. Dissertation Writing

The dissertation serves as the primary project documentation, detailing the theoretical underpinnings, implementation, and results. It should clearly address the following key aspects:

- **Image Input Justification:** Justify the use of images (e.g., candlestick or Heiken Ashi charts) as input. This should include a discussion of visually recognizable patterns within these charts and how the chosen model architecture captures this information.

- **Rotational Axis Paradigm:** Thoroughly explain the final design and integration of the rotational axis paradigm, including a precise definition of the rotating axes' behavior at each refocus point.

- **Dynamic PCA Explanation and Justification:** Provide a comprehensive discussion of Dynamic Principal Component Analysis (PCA), including its potential benefits and associated challenges. This discussion should encompass:

  - **Challenges:** Potential overfitting within small windows, computational overhead, integration complexities, loss of the absolute reference frame, instability and jitter, model dependency on the transformation, explanatory challenges, potential edge cases, and ensuring fair baseline comparisons.
  - **Comparison Strategy:** Detail the chosen baseline and comparison strategy, including comparisons against: no PCA, static PCA, simpler transforms, and alternative data-driven focus methods.
  - **Ablation Study:** Include an ablation study analyzing the individual contributions of re-centering and PCA by evaluating performance with each component independently disabled.

### B. Codebase Finalization

The final codebase must be submitted alongside the dissertation to ensure reproducibility and verifiability.

- **Complete and Documented Codebase:** The submitted codebase should be complete, thoroughly documented, and easily navigable. This includes clear comments and documentation for all modules and functions, particularly those related to the dynamic projection system (including PCA rotation, integration into the Vision Transformer/CNN pipeline, and the re-centering mechanism).

- **Dynamic PCA Implementation:** The code should clearly reflect the implemented dynamic PCA methods, including preprocessing, model integration, and any solutions for backpropagation challenges. Explicitly link the code implementation with the descriptions provided in the dissertation.

- **Reproducibility:** Ensure the code enables straightforward reproduction of the reported results. Provide clear instructions for execution and data preparation.

## V. Dissertation and Documentation

This section details the required documentation and dissertation content related to the dynamic plane implementation, a central concept of this project.

### A. Dissertation Content

The dissertation must thoroughly document the dynamic plane implementation, including its conceptual basis, algorithmic details, and impact on model performance. The following aspects require particular attention:

- **Conceptual Diagram:** Include a clear visual representation illustrating the evolution and shifts of the dynamic 2D plane with each market movement. This diagram will clarify the dynamic coordinate system.
- **Pseudocode:** Provide and explain detailed pseudocode for the dynamic plane generation algorithm. This should cover data input, local window definition, movement vector calculation, PCA application, coordinate system rotation, and 2D plane reconstruction.
- **Local Frame Definition using PCA:** Explain how Principal Component Analysis (PCA) defines the local frame of reference at each time step. Describe the process of using local windows of previous data points (e.g., candlesticks), defining movement vectors in time-price-volume space, and using the principal components to form the axes of the dynamically redrawn 2D plane.
- **Dynamic Coordinate System:** Elaborate on the dynamic redrawing of the coordinate system at each time step. Explain the rationale for deriving the 2D axes from the principal components of local price movements, integrating time, price, and volume into the local frame. Clearly differentiate this approach from simpler rotations based on trend or windowed price variance. Emphasize that the model receives this dynamically redrawn plane as input, effectively operating within a locally relevant, reconstructed 2D space.
- **Angle Theta Calculation:** Discuss the recalculation of the angle theta within the context of the dynamic 2D plane, incorporating the dynamic origin and rotational axes. Explain the transition from a simpler price difference-based calculation to a more sophisticated method that accounts for the dynamic coordinate system.

### B. Codebase Finalization

The accompanying codebase must be complete, well-documented, and clearly linked to the dissertation.

- **Code Completeness and Documentation:** The codebase should be fully functional, thoroughly documented, and include a clear `README` file with instructions for running the experiments, particularly those related to the dynamic plane implementation.
- **Modular Implementation:** A modular `RotatingSnapshotGenerator` class or function should encapsulate the dynamic plane logic, handling the rotation, redrawing, and refocusing of the 2D plane. This will facilitate integration into the dissertation experiments and ensure reproducibility.
- **Conceptual Diagram in Code:** The codebase should include or generate the conceptual diagram described above, visualizing the operation of the dynamic rotation on a sequence of price movements. This diagram should illustrate:
  - The reduction from three dimensions (time, price, volume) to two rotational axes on the dynamic 2D plane.
  - The dynamic recalculation and redrawing of the 2D plane and axes with each price or time change.
  - The dynamic shifting of the 2D plane's origin to the new data point.
  - How the rotation of the two axes on the 2D plane effectively captures the price movement represented by the parabolic curve in the original 3D model, demonstrating the dimensionality reduction without significant information loss.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on documenting the research, findings, and implementation details within the dissertation and finalizing the accompanying codebase. A central focus will be the development and integration of the Rotating Dynamic Plane Generator.

### A. Dissertation Writing

The dissertation should comprehensively detail the project's methodology, including a thorough explanation of the Rotating Dynamic Plane Generator's design, implementation, and integration within the chosen deep learning model (ViT or CNN). This includes:

- **Conceptual Basis and Rationale:** Explain the rationale behind using a rotating dynamic plane to represent candlestick data. Clearly articulate how this approach aids in capturing local movement patterns for trend prediction and connects to the theoretical framework of the research.

- **Design and Pseudocode:** Document the complete pseudocode for the Rotating Dynamic Plane Generator. Detail the steps involved in window selection, movement calculation, dynamic frame construction using Principal Component Analysis (PCA), rotation, refocusing, and snapshot rendering.

- **Python Implementation and Optimization:** Describe the Python implementation of the pseudocode, referencing key libraries (NumPy, Matplotlib, PIL). Highlight any optimizations implemented, such as batch processing for time series data. Specify the format of the generated output (image or numerical features) and its compatibility with the chosen deep learning model.

- **Dynamic Frame Rationale and Benefits:** Explain the motivation behind using dynamic candlestick snapshots and their expected benefits compared to traditional static frames. Connect this discussion to the overall project goals.

- **Addressing Functional Requirements and Constraints:** Address the following specific requirements and constraints:
  - **Window Size Parameter:** Explain the generator's window size parameter, its influence on the locality of the dynamic plane, and the justification for the chosen window size in the experiments.
  - **Volume Inclusion:** Discuss the inclusion of volume as a third dimension in movement calculations and its impact on model performance.
  - **Minimum Data Points:** Explain how the implementation addresses the constraint of requiring a minimum of two data points to compute the dynamic plane and any potential implications.
- **Image Rendering:** Clearly explain the generator's image rendering functionality and how these images serve as input for the chosen deep learning model.

- **Integration and Experimentation:**
  - **ViT/CNN Pipeline Integration:** Describe the integration of the generator into the Vision Transformer/CNN training pipeline.
  - **Static vs. Dynamic Frames Experiment:** Document the comparison experiment, presenting and analyzing the results to draw conclusions on the effectiveness of the dynamic approach. This analysis should be a core component of the results and discussion sections.

### B. Codebase Finalization

The finalized codebase should be well-organized, thoroughly documented, and directly reflect the elements discussed in the dissertation. It should include:

- **Rotating Dynamic Plane Generator Module:** Provide a well-documented and modular implementation of the generator, facilitating understanding and potential reuse.
- **Clear Input/Output Documentation:** Document the input (candlestick data: time, price, volume) and output (2D plane representation) formats of the generator, emphasizing compatibility with the chosen deep learning model.
- **Usage Instructions and Examples:** Include clear instructions and examples demonstrating the generator's usage with different parameters and configurations.
- **Experiment Code:** Include the code used for the static vs. dynamic frame comparison to enable independent verification of the reported results. Ensure the code is well-structured and documented for clarity.

By addressing these points, the dissertation and codebase will provide a comprehensive and compelling account of the research, ensuring reproducibility and contributing to a more impactful project outcome.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on documenting the research, finalizing the codebase, and ensuring alignment between the two. Clear and insightful visualizations within the dissertation are crucial for conveying the complexities of the dynamic plane implementation.

### A. Dissertation Content (Continuity)

The dissertation should comprehensively cover the development and application of the dynamic plane visualization, addressing the following key aspects:

- **Dynamic Plane Implementation:** Detail the implementation, including smoothing techniques (e.g., Heikin-Ashi), dataset generation for CNN and ViT training based on this principle, and the animation visualizing the plane's evolution. Address the challenges and solutions related to animation initialization with limited data points (specifically the "Fix Animation Initialization Error").

- **Animation of Plane Evolution:** Emphasize the animation's role in illustrating the dynamic evolution of the plane as new data is introduced. Discuss the ability to simulate longer sequences and its implications on visualization and model training. Include visual examples of the data model at different stages of transformation:

  - **Raw Candlestick Input:** Provide example images showcasing the raw candlestick input _before_ any transformation.
  - **Transformed Candlestick Input:** Provide example images showcasing the candlestick input _after_ refocusing the origin and applying static 3D axes to the principal 2D rotating axes.

- **Addressing Visualization Challenges:** Describe the technical constraints and solutions implemented for visualizing the dynamic plane, particularly with limited data:
  - **Minimum Points for Animation:** Explain the requirement of at least two data points for proper animation function, especially in early frames visualizing dynamically rotated points after PCA/SVD analysis. Detail the handling of single-point frames to prevent errors.
  - **PCA Instability:** Discuss the potential instability of PCA calculations when data points collapse onto a single line, especially with limited data in early frames. Explain how this instability is addressed in the visualization, potentially through alternative visualizations or explanatory text.

### B. Codebase Finalization (Facilitator)

The final codebase should be clear, well-documented, and directly reflect the implementation details described in the dissertation. Specifically, the code should include:

- **Dynamic Plane Module:** A modularized and well-documented implementation of the dynamic plane visualization, including clear explanations of smoothing techniques, dataset generation, and animation implementation.
- **Edge Case Handling:** Robust and clearly commented code handling edge cases in animation initialization (zero or one initial points) and addressing PCA instability with limited data. This includes the delayed rotation and plotting until sufficient data is available, placeholder visuals for single-point frames, and careful formatting of offsets to prevent dimension mismatches.
- **Simulation Capabilities:** Well-documented code for simulating longer sequences. This ensures reproducibility and facilitates further exploration by others. The documentation within the codebase should explicitly link to the relevant sections of the dissertation, further strengthening the connection between the two.

## V. Dissertation and Documentation

This section details the finalization of the project, encompassing both the dissertation and the codebase. It focuses on the technical aspects of visualizing and documenting the Heiken-Ashi data transformations and dynamic plane implementation, ensuring clear communication and reproducibility.

### A. Dissertation Visualizations

Effective data visualization is crucial for conveying the complexities of the Heiken-Ashi transformations and dynamic plane implementation within the dissertation. The following visualizations should be generated and included:

- **Standard Heiken-Ashi Charts:** Generate and include standard Heiken-Ashi candlestick charts derived from OHLC data. These charts will provide a clear visual baseline of the smoothed price action, with green candles representing upward movements (close ≥ open) and red candles representing downward movements (close < open).

- **Dynamically Rotated and Recentered Heiken-Ashi Charts:** Generate visualizations demonstrating the dynamic plane implementation. These charts should depict the Heiken-Ashi data points rotated and recentered based on the midpoints of the open and close values, effectively illustrating the dynamic plane concept and its impact on data representation. Visualizations should showcase the live frame rotation and recentering during the animation process, emphasizing the dynamic nature of the plane.

- **Example Images of Input and Transformed Data:** Include example images showcasing both the raw Heiken-Ashi input and the transformed data after applying the dynamic plane rotation. This visual comparison will clearly illustrate the effects of the transformation.

### B. Codebase Finalization

The finalized codebase should be well-documented and include the following modules to ensure reproducibility and facilitate understanding:

- **Heiken-Ashi Data Processing Module:** This module should encapsulate all functions related to Heiken-Ashi data processing, including the generation of standard Heiken-Ashi candles from OHLC data and the subsequent transformations for dynamic plane visualization.

- **Dynamic Plane Implementation Module:** This module should contain the core implementation of the dynamic plane, including the rotation and recentering logic based on the midpoints of open and close values. It should also incorporate the architectural decision to delay rotation until at least three stable points are established to avoid PCA instability. The implementation of smooth rotation matrices and the smoothing of early plane formation should also reside within this module. Clear documentation explaining the rationale behind these decisions is essential.

- **Visualization Module:** This module should handle the generation and saving of all visualizations, including both standard and dynamically rotated Heiken-Ashi charts. The module should save the generated charts as PNG images (e.g., `/mnt/data/standard_heiken_ashi.png` and `/mnt/data/rotated_dynamic_plane_heiken_ashi.png`) for easy integration into the dissertation. Furthermore, it should include a standalone animation simulator that embodies the dynamic plane visualization, allowing for a step-by-step visualization of the process, including the delayed rotation and smoothing techniques.

By adhering to these guidelines, the dissertation and codebase will effectively communicate the research process, results, and facilitate future research based on this work.

## V. Dissertation and Documentation

This section outlines the tasks required for visualizing and documenting the research findings, including the behavior of different market regimes within the dynamic plane, for inclusion in the dissertation. It also addresses the necessary steps for finalizing the codebase and ensuring its clarity and accessibility for future research.

### A. Market Regime Analysis and Visualization

This subsection details the analysis and visualization of different market regimes within the dynamic plane.

1. **Market Regime Analysis:** Analyze and summarize the behavior of trend (uptrend and downtrend), reversal, and sideways market regimes within the dynamic plane. This summary should be suitable for inclusion in the dissertation and discuss the implications of these behaviors for model learning.

2. **Visualization of Market Regimes:** Create clear visualizations demonstrating the behavior of each market regime (trend, reversal, and sideways) within the dynamic plane. Maintain consistency with existing visualizations for comparability.

3. **Extended Market Regime Analysis:** Simulate and analyze a third market regime, such as a strong linear uptrend or a sharp V-shaped recovery, to complement the existing trend-reversal-recovery and choppy sideways regimes. This will provide a more comprehensive analysis of the model's performance under diverse market conditions.

4. **Comparative Chart Visualization:** For each of the three market regimes, generate side-by-side visualizations comparing standard Heiken-Ashi charts with rotated Dynamic Plane charts. Use a subplot grid to present these comparisons clearly, avoiding figure stacking. This direct comparison will effectively illustrate how each chart type represents the different regimes.

5. **Comprehensive Visualization Panel:** Combine all visualizations into a single, comprehensive panel for easier comparison and analysis of the three market regimes across both chart types. This combined visualization will be a key component of the dissertation.

### B. Additional Simulations and Robustness Analysis

To enhance the robustness of the model and provide richer context for the dissertation analysis, the following simulations will be conducted:

- **Complex Price Pattern Simulation:** A simulation incorporating a rally, drop, and recovery phase will be implemented. This realistic market scenario will provide a deeper understanding of the model's performance under complex market dynamics compared to simplified price patterns.

- **Choppy Market Conditions Simulation:** A simulation of a chaotic, choppy sideways market, generated using the `generate_choppy_candlesticks(n=30)` function, will be implemented to assess the robustness of the dynamic plane under extreme volatility and rapid price fluctuations. Both standard and rotated dynamic Heiken-Ashi charts will be generated from this data:

  - **Standard Heiken-Ashi Chart:** Generated using the `generate_heiken_ashi` and `plot_heiken_ashi_candlestick` functions, and saved to `/mnt/data/standard_heiken_ashi_choppy.png`.
  - **Rotated Dynamic Heiken-Ashi Chart:** Generated by applying the `dynamic_rotate_recenter_heiken` function to the choppy Heiken-Ashi data, visualized using `plot_rotated_heiken`, and saved to `/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`. This will demonstrate the impact of dynamic plane rotation on representing choppy market data.

### C. Dissertation Writing

The dissertation should thoroughly cover the core concepts of the SCoVA project, including the unique architectural choices. Focus on the following areas:

- **Relational Model Design:** Explain how the model learns relationships and structures within the data by recognizing patterns in the dynamic coordinate system generated by Principal Component Analysis (PCA). Contrast this with traditional methods reliant on fixed semantic meanings of price, time, and volume.

- **Geometric Pattern Recognition:** Detail the model's ability to identify geometric shapes and flows in the normalized PCA space and discuss the reduced relevance of traditional technical indicators in this context.

- **Interpretability Projection:** Describe the mechanism for projecting the model's focus back into the original Time-Price-Volume space to enable human interpretation of its decision-making process. This is crucial for conveying the model's insights meaningfully.

- **Window Size and Smoothing:** Clearly document the chosen PCA window size and its rationale, including the implemented smoothing/stability thresholds and their role in mitigating noise-driven rotations. Justify these choices in the context of preventing overfitting and include any related experimental findings.

- **Impact of PCA on Feature Representation:** Analyze the impact of applying PCA to the numerical features (Price, Time, Volume) of the input images. Describe how PCA affects feature representation on the resulting graph and whether these representations vary based on input patterns (e.g., prioritizing Price and Volume versus Time and Volume).

### D. Codebase Finalization

The final codebase should be clear, well-documented, and accessible for future researchers.

- **Code Availability:** Ensure the codebase used for the dissertation experiments is readily available.
- **Code Completion:** Finalize the codebase before dissertation submission, ensuring all components are functional and documented.
- **Detailed Code Modules:** Provide well-documented code modules, particularly for the PCA implementation, dynamic rotation, and interpretability projection mechanism, to facilitate future research.

## V. Dissertation and Documentation

This section details the final stages of the project, focusing on completing the dissertation and finalizing the codebase. It also addresses several key algorithmic refinements related to the dynamic plane and error signal integration.

### A. Dissertation Writing

1. **Structure and Write Dissertation:** This encompasses the complete process of writing the dissertation, from outlining and drafting to revising and finalizing the document. The dissertation must thoroughly document the research, including motivation, methodology, results, conclusions, and a clear explanation of the dynamic plane algorithm, its error signal augmentation, and the rationale behind specific design choices (e.g., the 5-day candlestick window). It should also integrate relevant insights from discussions and thought experiments, including those documented in external sources like the "Thought Experiment Discussion" chat. Accessing this chat content will require investigation into appropriate methods (e.g., API calls, database access), with careful consideration given to permissions and data security.

2. **Refine Dissertation Title:** Craft a concise and informative title that accurately represents the research contribution.

### B. Codebase Finalization

1. **Provide Codebase for Dissertation:** Prepare the complete, finalized codebase for submission alongside the dissertation, either as a separate repository or with clear instructions for accessing the relevant sections within a larger project.

2. **Finalize Codebase:** Ensure the codebase is fully functional, well-documented, and consistent with the methods described in the dissertation. This includes implementing the algorithmic refinements detailed below.

3. **Provide Detailed Code Modules:** Provide comprehensive documentation for each code module, explaining its purpose, functionality, inputs, outputs, and interactions with other components. Pay particular attention to documenting the dynamic plane algorithm, the error signal mechanism, and all associated hyperparameters.

### C. Algorithmic Refinements

The following algorithmic refinements related to the dynamic plane and error signal integration must be implemented in the final codebase and thoroughly discussed within the dissertation:

- **Error Signal Integration:** Detail how the model uses feedback from realized market outcomes to refine its internal market structure representation, drawing an analogy to error signal usage in human motor control.

- **Frame Confidence Correction:** Explain how the model compares predicted and actual market movements to adjust rotational frame assumptions, and how this process allows the model to learn the stability of its dynamic plane projections.

- **Prediction Error Memory:** Describe the implementation of a rolling memory of prediction errors across recent dynamic frames and how it influences the weighting of rotations based on the frequency of misalignments between PCA frame rotations and realized market structure. Thoroughly explore this adaptive dynamic plane stability.

- **Feedback-Driven Frame Smoothing:** Explain how the model slows dynamic plane rotation during periods of high prediction error to enhance smoothing, focusing on the rationale, especially in relation to noisy market conditions.

- **Dual-Frame Estimation:** Explain the concept of maintaining two overlapping local frames ("optimistic" and "stable") and how predictions are dynamically weighted between them based on observed market consistency. Clearly present the benefits of blending "fast update" and "slow correction" pathways.

- **Remove Static Error Value in PCA:** Justify and detail the removal of the static, scalar float value representing prediction error memory within PCA calculations, emphasizing its detrimental impact on performance.

- **Lagging Rotation Deactivation Strategy:** Define a clear, quantifiable strategy for deactivating the lagging rotation mechanism within the dynamic plane algorithm and reverting to the normal state.

## V. Dissertation and Documentation

This section details the final stages of the project, focusing on documenting the research in the dissertation and finalizing the codebase. It incorporates the technical refinements related to error handling, frame correction, and dynamic plane analysis into both the dissertation content and the code documentation.

### A. Dissertation Writing

The dissertation should comprehensively cover the project's methodology, results, and conclusions, emphasizing the novel contributions related to dynamic plane analysis and enhanced error handling. Specific areas to address include:

- **Mitigating Plateau in Dual Records:** Detail the problem of dual records plateauing, leading to lag and static weighting, and explain the implemented solution and its effectiveness in adapting to evolving market dynamics.
- **Frame Coincidence Correction with Rolling Rewiring:** Explain the concept and implementation of frame coincidence correction, using the analogy of wound healing to illustrate the gradual return to a normal state after corrections. Clearly describe the rolling rewiring mechanism and its parameters.
- **Rolling Frame Correction Algorithm:** Thoroughly describe the rolling frame correction algorithm for the dynamic PCA frame, including:
  - **Prediction Error Buffer:** Explain the purpose and implementation of the error buffer, justifying the chosen buffer size.
  - **Error Trend Detector:** Detail the error trend detector based on rolling mean and variance, including the threshold for triggering correction mode, its rationale, derivation, and impact on system responsiveness.
  - **Frame Correction Action:** Describe the specific corrective actions (e.g., rotation adjustments, damping) taken when errors exceed the threshold, and their effects on system behavior.
  - **Healing Phase:** Explain the process and parameters controlling the gradual removal of the correction as errors subside, mirroring the biological healing process.
- **Lightweight Prediction-Error Feedback:** Describe the integration of the lightweight prediction-error feedback mechanism, explaining the chosen implementation strategy (e.g., auxiliary loss functions, frame stability monitoring) and its rationale.
- **Simulating Peripersonal vs. Extrapersonal Gap:** Detail the method developed to simulate the peripersonal vs. extrapersonal gap concept, explaining how it provides insights into model performance under different market conditions.
- **Dynamic Plane Algorithm with Error Signal Integration:** Include a comprehensive sketch or diagram illustrating the workings of the dynamic plane algorithm and the integrated error signal mechanism, clarifying their interplay.

### B. Codebase Finalization

The finalized codebase must be well-documented and readily available for reference. This includes detailed comments within the code and potentially a separate technical document. Ensure the codebase accurately reflects the final implementation described in the dissertation. Key modules to document include:

- **Error Handling and Frame Correction:** Provide clear and comprehensive documentation for the code implementing the mitigating plateau solution, frame coincidence correction, and the rolling frame correction algorithm (including the error buffer, trend detector, correction action, and healing phase).
- **Prediction-Error Feedback Mechanism:** Document the code implementing the chosen lightweight prediction-error feedback mechanism.
- **Peripersonal vs. Extrapersonal Gap Simulation:** Document the code implementing the simulation method.
- **Dynamic Plane Algorithm:** Document the core implementation of the dynamic plane algorithm, including the integration of the error signal mechanism. Provide clear explanations connecting the code implementation with the theoretical background presented in the dissertation.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on documenting the research, finalizing the codebase, and ensuring consistency between the two. This includes crucial clarifications regarding the dynamic plane implementation and error calculations.

### A. Dissertation Writing

The dissertation must clearly articulate the project's methodology, findings, and contributions, with specific attention paid to the dynamic plane implementation and error correction. The following points require detailed explanation:

- **Dynamic Plane Implementation Details:** Clearly state the number of rotational angles and distance vectors used in the dynamic plane implementation and justify this choice.
- **Error Correction Methodology:** Provide a comprehensive explanation of the error correction process within the dynamic 2D plane, including:
  - **Distance Error Calculation:** Define and explain how the magnitude difference between predicted and realized displacement vectors is calculated.
  - **Angle Error Calculation:** Define and explain how the orientation difference between predicted and realized direction vectors is calculated.
  - **Initial Frame Rotation Exclusion:** Explicitly state and justify why the initial frame creation rotation is excluded from error correction.
  - **Illustrative Diagram:** Include a diagram illustrating the two rotation layers (global frame transformation via PCA and local vector misalignment) and visually demonstrating how prediction error relates to these rotations.
- **Refine Dissertation Title:** Ensure the title accurately and concisely reflects the core research topic and findings.

### B. Codebase Finalization

The codebase must be finalized to reflect the clarifications and explanations provided in the dissertation. This includes:

- **Implementation Consistency:** Ensure the codebase implementation aligns with the documented number of rotational angles and distance vectors used in the dynamic plane.
- **Error Calculation Documentation:** Thoroughly document the code modules responsible for calculating distance and angle errors within the dynamic plane, ensuring the logic aligns with the dissertation's explanations.
- **Comprehensive Code Comments:** Add comprehensive comments and documentation throughout the codebase to explain the dynamic plane implementation, error correction process, and the rationale behind design choices. This will enhance readability and facilitate future development or analysis.

By addressing these points, the dissertation and codebase will form a cohesive and comprehensive representation of the project, ensuring clarity, accuracy, and reproducibility.

## V. Dissertation and Documentation

This section details the final project stages, emphasizing the documentation of research and codebase finalization, particularly the novel PCA frame management techniques. Clear communication of these techniques, both theoretically and practically, is crucial for project integrity and reproducibility.

### A. Dissertation Writing

The dissertation must clearly articulate the advantages of the implemented "freeze frame" and "reproject realization" methods for handling PCA frame shifts compared to traditional distance/angular error calculations. Existing methods often assume a static PCA plane, overlooking potential shifts in the data distribution, a limitation addressed by the proposed techniques.

- **Distinguishing from Traditional Approaches:** Emphasize how the "freeze frame" and "reproject realization" methods address the dynamic nature of the PCA plane by fixing the frame of reference at the prediction time. This ensures consistent comparisons even when the underlying data distribution shifts, unlike traditional methods that rely on a static PCA plane.

- **Freeze Frame Method:** Detail the "freeze frame" method, where the PCA rotation matrix 'R' is calculated and then _frozen_ at the time of prediction (t). Both predicted and subsequently realized data points are projected using this fixed matrix 'R', ensuring comparisons within a consistent frame of reference.

- **Reproject Realization Method:** Describe the "reproject realization" method, where the realized movement vector at time 't+1' is reprojected back into the original PCA frame established at time 't using the initial rotation matrix 'R'. This approach maintains consistency despite potential market shifts.

- **Illustrative Examples and Visualization:** Include numerical examples with predicted and realized movement values to enhance understanding. Calculate the angle/distance error within the frozen frame and provide plots visualizing the predicted and realized paths in both the frozen and shifted frames. These visualizations will offer a clear, practical demonstration of the PCA frame management techniques.

### B. Codebase Finalization

The finalized codebase requires meticulous documentation and modularization of the PCA frame management techniques:

- **"Freeze and Correct" Module:** The codebase should feature a well-defined "Freeze and Correct" module encapsulating the logic for both the "freeze frame" and "reproject realization" methods. Thorough documentation with robust pseudocode explaining implementation details is essential for reader comprehension and reproducibility.

- **Comprehensive Code Documentation:** Ensure clear and comprehensive documentation for all code related to PCA calculations, including the generation and application of the rotation matrix 'R' for projections. This documentation is crucial for reproducibility and future development. This includes documentation of the specific data structure used to store the PCA basis (rotation matrix) for each window, highlighting its efficiency in facilitating reprojection.

## V. Dissertation and Documentation

This section details the required documentation and finalization tasks for both the dissertation and the codebase. These components are closely linked, with the codebase documentation directly supporting the clarity and reproducibility of the research presented in the dissertation. Particular emphasis should be placed on a comprehensive explanation of the error computation methodology, supported by illustrative examples and pseudocode within the dissertation.

### A. Dissertation Content and Structure

The dissertation should effectively communicate the research, methodology, results, and conclusions of the SCoVA project using clear and concise language. A refined title accurately reflecting the core contribution and scope of the research is essential. Beyond the standard dissertation structure, the following specific topics require dedicated sections and detailed explanations:

- **Robust PCA Frame Management:** Explain the chosen approach (Freeze Frame or Reproject Realization) for handling shifts in market data distribution. Discuss how the chosen method maintains relational consistency between predicted and realized data points and prevents model hallucinations. Support this discussion with empirical evidence and analysis.

- **Error Calculation Methodology:** Provide a comprehensive explanation of the error computation process, encompassing the following:

  - **Error Calculation with Shifted PCA Planes:** Describe how the error checking mechanism accounts for shifting PCA planes. Justify the focus on deviation errors between PCA1 and PCA2 for both real and predicted values, rather than static plane calculations. Include mathematical formulations and illustrative examples.

  - **Total Error Calculation:** Detail the calculation of the Total Error, combining vector deviation error within the dynamic local frame and frame shift error (change between PCA axes). Clearly present and explain the formula: `Total Error = Vector Deviation Error + Frame Shift Error`. Discuss the rationale for combining these error components and their respective contributions to the overall error. Provide pseudocode illustrating the calculation.

  - **Frame Drift Error Measurement:** Explain the methodology for measuring frame drift error using principal angles between two subspaces. In the 2D case, describe the calculation of the angle between PCA1<sub>t</sub> and PCA1<sub>t+1</sub>, and between PCA2<sub>t</sub> and PCA2<sub>t+1</sub>. Detail the weighted formula: `Frame Error = α * Angle between PCA1 vectors + β * Angle between PCA2 vectors`, including a discussion of the tunable weights α and β.

  - **Weighted Error Components:** Clearly describe the implemented weighted error calculation, including the following formulas and a breakdown of their components:

    - **Vector Error = α₁⋅d<sub>vec</sub> + α₂⋅θ<sub>vec</sub>**
    - **Frame Shift Error = β₁⋅θ<sub>PCA1</sub> + β₂⋅θ<sub>PCA2</sub>**
    - **Total Error = γ₁⋅Vector Error + γ₂⋅Frame Shift Error**

    Explain the roles of the weighting parameters:

    - **α₁, α₂:** Control the balance between distance and angle within the frame.
    - **β₁, β₂:** Control the balance between PCA1 drift and PCA2 drift.
    - **γ₁, γ₂:** Control the overall balance between prediction error and frame instability. Justify the chosen weighting scheme and explain whether distinct weights are assigned or if the existing alpha and beta parameters are sufficient.

  - **Normalization of Errors:** Address the current method of directly summing distance and angular errors. Evaluate this approach, considering the different units and scales of these errors, and justify any necessary normalization for accurate aggregation.

- **Frame Drift Error as Confidence Indicator:** Explore and analyze the potential of using Frame Drift Error as a confidence indicator for the model's decisions (e.g., trading versus holding positions).

- **Simulation of Vector Deviation and PCA Frame Drift:** Integrate a small-scale simulation demonstrating vector deviation and PCA frame drift, visually showcasing their impact on model performance.

### B. Codebase Finalization

The finalized codebase, submitted alongside the dissertation, must be well-documented and modularized to ensure reproducibility and clarity. This includes:

- **Clear Code Correspondence:** Ensure the code corresponding to the simulations, error calculations, and core algorithms is readily available and clearly referenced in the dissertation.

- **Modular Code Structure:** Provide well-documented code modules for:

  - Simulation of vector deviation and PCA frame drift.
  - Frame drift error calculation.
  - Weighted error calculation.

- **Comprehensive README:** Include a comprehensive README file guiding readers through the code structure, functionality, and execution instructions, especially concerning the concepts discussed in the dissertation. In-line comments should also be used liberally throughout the codebase.

## V. Dissertation and Documentation

This section details the finalization of the dissertation and the accompanying codebase, incorporating crucial elements like the error detection and healing system, parameter tuning, and performance-based healing mechanisms.

### A. Dissertation Writing

The dissertation should comprehensively cover the following aspects of the research:

- **Structure and Content:** Organize the dissertation into a coherent narrative, including an introduction, literature review, methodology, results, discussion, and conclusion. Clearly articulate the project's goals, approach, and contributions. Specifically address the motivation and implementation of the error detection and healing system, including the rationale behind the chosen parameters and its impact on overall system performance. Illustrate the system's effectiveness with visualizations of the healing phase applied to market data. Discuss the process of tracking true prediction values and its role in dynamically adjusting the decay rate. Explain the performance-based healing process, emphasizing its reliance on predictive accuracy rather than solely time.

- **Title Refinement:** Ensure the dissertation title accurately and concisely reflects the core focus and findings of the research, including the novel aspects of the error detection and healing system.

### B. Codebase Finalization

The final codebase submission should adhere to the following guidelines:

- **Complete and Functional Code:** Submit a complete and functional codebase as supplementary material, including clear instructions on how to run and reproduce the experiments. Ensure all code is clean, well-documented, and adheres to best practices.

- **Modular Design and Documentation:** Organize the codebase into well-defined modules, particularly for the error computation and healing system. Provide comprehensive documentation for each module, explaining its purpose, inputs, outputs, and dependencies. This documentation should be sufficient for others to understand and potentially extend the work. Specifically, document the implementation for tracking true prediction values, the dynamic decay rate adjustment mechanism, and the performance-based healing process.

- **Parameterization and Visualization:** The codebase should reflect the tuned parameters for the error detection and healing system. Include functionality to generate visualizations demonstrating the healing phase in action, as presented in the dissertation.

- **Error Handling and Robustness:** Ensure the code includes robust error handling mechanisms, particularly within the error detection and healing system, to gracefully handle unexpected situations and maintain system stability.

## V. Dissertation and Documentation

This section details the final stages of the project, encompassing both the dissertation writing and codebase finalization. A clear and consistent narrative between the dissertation and the documented code is crucial for reproducibility and a comprehensive understanding of the research contributions. This section addresses the documentation of data preprocessing for Principal Component Analysis (PCA), data representation choices for model interpretation, and the finalization of the codebase.

### A. Dissertation Writing

The dissertation should meticulously articulate the methodologies employed, including data preprocessing techniques and data representation choices. Specifically:

- **Data Preprocessing for PCA:** This section should detail the rationale and implementation of the preprocessing steps for PCA, emphasizing the importance of normalization for meaningful PCA axes.

  - **Normalization of OHLCV and Timestamp Variables:** Explain the necessity of normalizing Price (P), Time (T), and Volume (V) to a uniform scale before applying PCA to prevent scale-induced distortions.

  - **Z-score Normalization:** Describe the application of z-score normalization within a rolling window of _N_ data points. Each feature (P, T, V) is independently centered and scaled to zero mean and unit variance using the following formula:

    $\\mu_x = \\frac{1}{N}\\sum_i x_i,\\quad \\sigma_x = \\sqrt{\\frac{1}{N}\\sum_i(x_i-\\mu_x)^2}$

    $x_{\\text{scaled}} = \\frac{x - \\mu_x}{\\sigma_x}$
    where _x_ represents each feature (t, p, v). This ensures equal feature contribution to the PCA.

  - **Time Handling:** Justify and explain the chosen approach for handling timestamps, either:

    1. **Relative Time Index:** Detail the use of normalized sequential integers representing relative time within each window.
    2. **Absolute Clock Time:** Describe the calculation of normalized time deltas ($\\Delta t_i = \\frac{\\text{timestamp}_i - \\mu_t}{\\sigma_t}$) and discuss potential issues with large timestamp gaps.

  - **Volume Transformation:** Justify and explain the chosen transformation for volume data, either:

    1. **Log Transformation:** Detail the application of a log transformation (e.g., $v_i' = \\log(1 + v_i)$) and its impact on outliers and standard deviation.
    2. **Robust Scaling:** Explain the use of the median and interquartile range (IQR) for scaling and its robustness against extreme outliers.

  - **PCA Implementation:** Document the implementation using Singular Value Decomposition (SVD) on the scaled data matrix ($X_{scaled}$). Include relevant code snippets, for example, extracting the first two principal components:

    ```python
    u, s, vh = np.linalg.svd(X_scaled, full_matrices=False)
    axes = vh[:2]   # two principal directions in T-P-V space
    ```

- **Data Representation for Model Interpretation:** Clearly articulate the rationale behind using relative returns and fractional elapsed time, analyzing their impact on model performance and result interpretation. This discussion should be integrated within the methodology section.

- **Dissertation Structure and Title:** The dissertation should follow a clear and logical structure, effectively communicating the research process, findings, and conclusions. The title should accurately and concisely reflect the core research focus and employed techniques.

### B. Codebase Finalization

The finalized codebase must be consistent with the dissertation's methodological descriptions, ensuring reproducibility and transparency. Specifically:

- **Documented Code Modules:** Provide well-documented and modular code for all implemented procedures, including:
  - Data preprocessing for PCA (normalization, time and volume transformations, SVD calculation).
  - Data transformations for model interpretation (relative returns, fractional elapsed time).
  - Handling of unseen or outlier values for price and volume.
- **Code Completeness and Functionality:** Ensure the codebase is complete, functional, and thoroughly tested before dissertation submission. Clear comments and documentation are crucial for reviewers to understand the implementation and replicate the results. This includes a clear link between the specific code implementation and the corresponding explanations within the dissertation.

### C. Model Refinement and Healing Logic (from previous checklist section)

- **Dynamic Decay Rate:** The model incorporates a dynamic decay rate for its healing process. This rate is not fixed but adjusts based on the model's prediction accuracy. A higher prediction accuracy results in a faster decay, allowing the model to recover more quickly from disruptions. Conversely, lower accuracy slows the decay, allowing the model more time to adapt to changing conditions.

- **Rolling Prediction Correctness Buffer:** A rolling buffer stores the prediction correctness scores (1 for correct, 0 for incorrect) for the last _N_ timesteps. The mean of this buffer is used to calculate the dynamic decay rate. This implementation provides a real-time measure of prediction performance and allows the decay rate to adapt continuously. The specific calculation of the decay rate based on the buffer's mean should be clearly documented in the code and explained in the dissertation.

This structured approach ensures a coherent and comprehensive final product, connecting the theoretical aspects described in the dissertation with the practical implementation in the codebase.

## V. Dissertation and Documentation

This section details the final stages of the project, focusing on completing the dissertation and finalizing the codebase. It's crucial that the data transformations applied to the model's input are thoroughly documented within the dissertation and accurately reflected in the accompanying codebase.

### A. Dissertation Writing

1. **Structure and Content:** The dissertation must clearly articulate the SCoVA project methodology, including all data preprocessing techniques. The rationale for the chosen normalization procedures, their implementation, and their impact on model performance should be thoroughly explained and justified. Specifically, address the following:

   - **Visualizations and Data Accuracy:** Ensure all visualizations accurately represent the data. Clearly document and justify the chosen method for handling extreme values (e.g., clipping, scaling).

   - **Non-linear Time Representation:** Discuss the limitations of linear time representation and justify the chosen non-linear method. Explain how this method preserves chronological order while enabling meaningful scalar representation. Analyze the impact of this non-linear representation on model performance and result interpretation.

   - **Encoding Time as Fractional Elapsed Time:** Explain the rationale for using fractional elapsed time (between 0 and 1) to represent timestamps. Highlight how this approach addresses the limitations of linear indexing with irregular time spacing.

   - **Transforming Price into Relative Returns:** Justify the use of relative returns (percentage change or log returns) instead of raw price values. Detail the calculation method relative to the first price in the window. Emphasize the benefits of anchoring the price to zero at the beginning of the window and mitigating the impact of extreme price spikes. Justify the choice between percentage change and log returns based on the data characteristics and research objectives.

   - **Consideration of Live Feed for Time Tracking:** Briefly document the decision not to implement a live feed of Last Traded Prices (LTPs) for time tracking, explaining the rationale behind this choice (e.g., potential loss of historical context).

   - **Data Transformation for Robust PCA:** Detail the preprocessing steps for robust Principal Component Analysis (PCA). This includes:

     - **Robust Volume Scaling:** Explain the rationale and implementation of the log transformation and median/IQR scaling applied to volume data to minimize the impact of outliers.
     - **Fractional Elapsed Time:** Describe the transformation of timestamps into fractional elapsed time within each window, specifying the calculation: `(timestamp - minimum timestamp) / (maximum timestamp - minimum timestamp)`. Emphasize the importance of capturing relative timing.
     - **Window-Relative Returns:** Detail the transformation of price data into window-relative returns (percentage or log returns), calculated relative to the first price in the window. Explain the normalization effect of this transformation.
     - **3D Matrix Construction:** Explain how the transformed variables (fractional elapsed time, relative return, and scaled volume) are combined into a 3D matrix for PCA input.

   - **PCA Application:** Clearly explain the application of PCA to the 3D matrix. Discuss the interpretation and significance of the resulting principal components in capturing the relationships between the transformed variables.

2. **Refine Dissertation Title:** Ensure the title accurately reflects the core research focus and includes relevant keywords (e.g., candlestick analysis, CNNs, ViTs, dynamic plane rotation).

### B. Codebase Finalization

1. **Code Submission:** Submit the complete and finalized codebase as supplementary material for the dissertation. The code should be clean, well-commented, and easily understandable.

2. **Code Finalization (Complete before Dissertation Submission):** Ensure the codebase accurately reflects all described methodologies and includes the final data transformation steps. Implement and clearly document the following normalization procedures:

   - **Normalize Price, Volume, and Time Data:** Implement functions to normalize price (P), volume (V), and time (T) data to the range [-1, 1].
     - **Normalize `time_frac`:** Transform using `(2 * time_frac) - 1`.
     - **Normalize Price (Log Return):** Calculate the log return of the price relative to the opening value. Clearly document and justify the chosen method for handling extreme values (clamping or scaling). Options include dividing by the maximum absolute value within the window or using min-max scaling to the [-1, 1] range.
     - **Transform Volume:** Apply a log transformation followed by robust scaling using the median and interquartile range (IQR) to handle outliers. Thoroughly document this process within the code and explain it in the dissertation.

By meticulously documenting these steps in both the dissertation and codebase, the project maintains transparency and ensures reproducibility, which are essential for robust research.

## V. Dissertation and Documentation

This section details the visualization and documentation requirements for the dissertation, focusing on clearly illustrating the data transformation pipeline applied to the candlestick data for the CNN model.

### A. Data Transformation Explanation and Visualization

The dissertation should include a dedicated section explaining the data transformation methodology. This explanation must clearly articulate the steps involved in preparing the data, including the creation of the dynamic plane snapshots used as CNN input. Visualizations are crucial for demonstrating the effect of these transformations.

**1. Data Preprocessing and Normalization:**

The following preprocessing and normalization steps should be described in detail, including their rationale (e.g., mitigating PCA distortion from outliers, ensuring features contribute equally):

- **Log Transformation and Percentile Clipping (Volume):** Volume data is transformed using a logarithmic transformation (`np.log1p(volume)`), followed by clipping extreme outliers at the 5th and 95th percentiles.
- **Percentile Clipping (Price):** Log returns of price are clipped at the 5th and 95th percentiles, maintaining consistency with the volume preprocessing.
- **Min-Max Scaling:** Time_frac, log return of price, and log-transformed volume are min-max scaled to the [-1, 1] range. Crucially, the minimum and maximum values for scaling are calculated across the entire input window (e.g., the 5-day window) for consistency.
- **Principal Component Analysis (PCA):** Following normalization, the time, price, and volume data are combined into a matrix. PCA is applied to this matrix to reduce dimensionality while preserving essential information. The potential redundancy of centering the matrix before PCA, given the min-max scaling to [-1, 1], should be addressed.

**2. Dynamic Plane Transformation:**

The transformation of candlestick and volume data into dynamic plane snapshots should be thoroughly explained. This includes the use of log returns, PCA rotation, and normalization, highlighting the benefits of each step for the model's performance.

**3. Visualization Requirements:**

The following visualizations, each covering a single day with 10-minute intervals, should be included and individually presented within the dissertation:

- **Pre-transformation Candlestick Charts:** Displaying the raw price and volume data before any transformations. These serve as a baseline for comparison.
- **Post-transformation Candlestick Charts:** Showing the price and volume data after applying the dynamic plane transformation and normalization to the [-1, 1] range. This illustrates the final input to the CNN.
- **Transformed Image Sequences:** A sequence of five image pairs, each consisting of the original candlestick chart with volume and the corresponding transformed dynamic plane snapshot. This visually demonstrates the transformation process across multiple instances.

### B. Codebase Finalization

The submitted codebase must include clearly documented modules implementing the entire data transformation pipeline, including the generation of the visualizations described above. The code should be well-commented to ensure reproducibility and understanding. Include the code prompt used for generating the visualizations within the dissertation or accompanying documentation.

## V. Dissertation and Documentation

This section outlines the necessary documentation and visualizations for the dissertation, focusing on candlestick chart generation, dynamic plane transformations, and Principal Component Analysis (PCA). These elements will support the findings and methodology presented.

### A. Visualizations and Analysis

The dissertation should incorporate the following visualizations and analyses:

- **Market Pattern Examples:** Generate candlestick charts with volume data showcasing five distinct market patterns: uptrend with rising volume, downtrend with volume spikes, reversal (down then up), sideways chop, and breakout spike then stabilize. These charts provide visual context for the analyzed market behaviors.

- **Dynamic Plane Transformation:** Clearly describe how time, log return, and log volume are normalized to the range [-1, 1] to ensure consistent scaling and prevent features with larger values from dominating the PCA rotation. Then, describe the application of PCA rotation to the normalized data, explaining its purpose in dimensionality reduction and creating the 2D representation (dynamic plane snapshot). Clarify how this 2D representation captures the essential information from the original three dimensions. Illustrate this transformation process with five pairs of images, each depicting one of the market patterns listed above. Each pair should show the original candlestick chart with volume and the corresponding dynamic plane snapshot. Display images individually for clear comparison. The code used to generate these images should be included in the provided codebase.

- **Breakout Spike and Stabilize Scenarios:** Generate five variations of the "Breakout Spike then Stabilize" scenario, each with a distinct volume profile. Visualize each scenario using both the original candlestick chart with volume (10-minute intervals) and the transformed dynamic plane projection. The dynamic plane projection should use time, log return, and log volume (all normalized to [-1, 1]) and be PCA-rotated to the PC1/PC2 plane.

- **PCA Pattern Analysis:** Compare the five PCA patterns derived from the "Breakout Spike then Stabilize" scenarios. Focus on how variations in volume magnitude and timing influence the trajectory's shape in the PCA plane. Highlight key observations, including:
  - Data point clustering in the early stages (pre-spike) near a specific region.
  - The jump to one extreme of PC2 during the spike, driven by volume.
  - Cluster or trajectory formation during the post-spike stabilization phase.

### B. Codebase Requirements

The final codebase should include well-documented modules related to data transformation, specifically:

- **Dynamic Plane Snapshot Module:** A dedicated, clearly commented module for generating the dynamic plane snapshots, incorporating log returns, normalization, and PCA rotation. Include the code for generating the example images demonstrating the various market patterns, clearly marked for easy reproduction and verification.

- **Volume Representation:** Address whether volume bars are explicitly needed in the transformed dynamic plane snapshot or if the transformed data points sufficiently represent volume information. Document the chosen approach and its rationale.

- **Breakout/Spike/Stabilize PCA Examples:** Include the generation of five distinct PCA patterns for the "Breakout Spike then Stabilize" scenario, varying data volumes to assess visual similarities or differences in the resulting patterns. Clearly document and demonstrate this in the code.

## VI. Model Selection and Data Processing Justification

The dissertation must justify the chosen data processing and model selection strategies, including a thorough discussion of the following:

- **Bivariate Spline Interpolation Analysis:** Analyze bivariate spline interpolation and its suitability, documenting the evaluation process, considering project goals, data characteristics, and performance requirements. Explain the rationale for including or excluding this method, exploring 3D extensions or alternative smoothing approaches for 3D price-time-volume data. Address potential distortions introduced by interpolation on discrete data changes, central to the core approach.

- **Linear Interpolation Evaluation:** Evaluate linear interpolation as a simpler smoothing alternative, comparing it with bivariate spline interpolation and highlighting the advantages and disadvantages of each in preserving crucial data signals.

- **Justification for Excluding Bivariate Spline Interpolation from Feature Extraction:** Justify the decision to avoid bivariate spline interpolation for feature extraction, articulating the rationale (specifically the risk of smoothing sharp features crucial for CNN or PCA capture). Discuss the impact of interpolation on the existing method's reliance on discrete changes.

## V. Dissertation and Documentation

This section details the requirements for documenting the SCoVA project, encompassing both the dissertation and the final codebase. The dissertation should provide a comprehensive overview of the project, including its theoretical underpinnings, implementation details, results, and conclusions. The codebase, submitted alongside the dissertation, should be well-structured, documented, and directly support the findings presented in the written work. This section focuses on ensuring alignment between these two crucial components.

### A. Dissertation Content

The dissertation must clearly articulate Swaha's role, functionality, and development within the context of the Indian equities market. Additionally, it needs to provide a thorough explanation of the dynamic frame methodology, its implementation, and its contribution to the overall system.

**Swaha's Context:**

- **Introduction:** Introduce Swaha as an algorithmic trading bot designed specifically for the Indian equities market. Explain the rationale behind focusing on this market and its specific characteristics.
- **Role and Purpose:** Clearly define Swaha's function as a tool for developing and implementing trading strategies. Describe its capabilities and limitations. Maintain a consistent description of Swaha as an algorithmic trading bot assistant throughout the dissertation.

**Dynamic Frame Methodology:**

- **Construction:** Detail the dynamic 2D plane construction using PCA on normalized Time, Price, and Volume data. Explain the rationale behind choosing PCA, define the interpretation of the principal components (PC1 and PC2) within the market context, and discuss the impact of the chosen PCA window size.
- **Projection and Refocusing:** Describe the projection of market history onto the dynamic 2D plane and the re-centering process with the last data point as the origin (0,0). Justify this approach and its benefits for focusing on recent market trends.
- **Image Generation:** Explain the transformation of the 2D plot into an image, specifying the chosen chart type (Candlesticks or Heiken-Ashi) and its suitability for the predictive model (e.g., Vision Transformer). Include visual examples and discuss their informational value.
- **Predictive Goals and Challenges:** Articulate the model's predictive goals: forecasting a 2D movement vector (Δx', Δy') representing future trajectory and predicting the 'Rally Time'. Explain their translation into actionable trading signals. Discuss the inherent challenges of predicting both magnitude and time.
- **Self-Correction Mechanism:** Detail the self-correcting mechanism's implementation using the Total Error signal (Vector Deviation Error and Frame Shift Error). Explain 'Wound Detection' with its dynamic threshold, the 'Healing Phase', and how dynamism is restored. Include performance metrics demonstrating the mechanism's effectiveness.
- **Model Development and Training:** Thoroughly document the model development process, training procedures, and the achieved results, referencing relevant sections from other parts of the project documentation.
- **System Architecture and Deployment:** Discuss the overall system architecture, including crucial components like the DynamicPlaneGenerator module and data normalization techniques. Address any monitoring requirements and their integration into the deployable system.

### B. Codebase Implementation

The submitted codebase should directly reflect the functionalities and methodologies described in the dissertation. This requires clear, well-documented code with a modular structure that facilitates understanding and reproducibility.

- **Modular Structure:** Organize the code into distinct modules for each aspect of the dynamic frame approach (e.g., PCA calculation, rotation, image generation, error calculation, healing mechanism).
- **Comprehensive Documentation:** Provide thorough in-code documentation, including clear comments explaining the purpose, inputs, outputs, and functionality of each module. Ensure the code's structure and comments align with the dissertation's explanations.
- **Traceability:** Establish clear links between the codebase and the dissertation, allowing readers to easily trace implementation details back to the theoretical framework and experimental results. This ensures transparency and supports the reproducibility of the research.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on documenting the research and finalizing the codebase for submission alongside the dissertation. This documentation should thoroughly address the implementation of the multi-scale temporal model and associated data processing techniques.

### A. Dissertation Writing

The dissertation should comprehensively detail the project's architecture and methodologies, including a clear explanation of the following key elements:

- **Multi-Scale Data Processing:** Describe how market data is processed across various timeframes (intraday, daily, weekly, monthly) and how this multi-scale information provides context for short-term predictions. Justify the incorporation of multiple timeframes and detail the specific data integration methods. This includes addressing the potential data processing overhead and proposing mitigation strategies if necessary.

- **Dynamic Rotating Plane Implementation:** Explain the implementation of the dynamic 2D plane based on the local flow of market data. Detail the use of PCA on normalized Time, Price, and Volume data to determine the plane's orientation.

- **Model Architectures:** Document the exploration and evaluation of two primary model architectures:

  - **Ensemble Model Approach:** Detail the rationale, training process, and prediction combination method for an ensemble approach using specialist models for each timeframe (e.g., 10-minute, daily, weekly).
  - **Multi-Input Transformer Model:** Explain how this model receives input from all timeframes simultaneously, leverages an attention mechanism for dynamic weighting, and compares to the ensemble approach.

- **Weight Assignment Strategies:** Discuss the explored weight assignment strategies, including static and dynamic (learned) weights using an attention mechanism. Justify the chosen strategy based on experimental results and relevant literature. This discussion should also address the architectural design choices made to handle different timescales within the multi-scale temporal model and the rationale behind the chosen weighting schemes. Clearly present and analyze experimentation with various periodicity weights and their respective outcomes. Pay specific attention to how the model accounts for cyclical patterns and the influence of after-market forces on price movements.

- **Data Pipeline Complexity:** Address the complexity of the data pipeline required for integrating data from various timeframes using the Dynamic Rotating Plane method. Document the data preparation steps for each dataset and any implementation challenges encountered.

- **Title Refinement:** Ensure the dissertation title accurately reflects the core contributions and findings of the research, including the multi-scale temporal modeling approach. Highlighting innovative aspects, such as the weighted periodicity and the consideration of after-market forces, will strengthen the title's impact.

### B. Codebase Finalization

The complete and documented codebase should be submitted as a supplement to the dissertation. Ensure the following before submission:

- **Code Completeness and Clarity:** Finalize the codebase, ensuring all implemented features, particularly the multi-scale temporal model, are functional, well-documented, and easily understandable. Use meaningful variable names, provide inline comments to explain complex logic, and maintain consistent formatting for readability.

- **Documented Modules:** Provide well-documented code modules, especially those related to data processing, model architectures (ensemble and multi-input transformer), and the dynamic rotating plane implementation. This facilitates reproducibility and further investigation. The codebase should clearly reflect the implementation details of the multi-scale temporal model, including the mechanisms for incorporating different timeframes and their associated weights.

## V. Dissertation and Documentation

This section details the requirements for documenting the project in the dissertation and finalizing the accompanying codebase. While some checklist items pertain to a potential trading application interface, the underlying principles of clear communication and effective presentation are relevant to both the dissertation and codebase.

### A. Dissertation Writing

The dissertation must comprehensively detail the project's technical implementation, rationale, and results. Key aspects include:

- **Vision Transformer (ViT) Architecture:** Justify the use of a ViT as the predictive engine, explaining how it processes multi-timescale dynamic planes as a sequence of tokens and learns dependencies between them. Include architectural diagrams and detailed explanations of design choices.

- **Self-Correcting Mechanism:** Thoroughly document the performance-driven feedback loop. Define the "Total Error" signal, its components ("Vector Deviation Error" and "Frame Shift Error"), and the "correction mode" functionality. Explain the dampening effect on frame rotation during error spikes and the gradual recovery process linked to prediction accuracy. Include mathematical formulations and algorithmic details.

- **Data Normalization Process:** Detail the normalization of Time (fractional elapsed time), Price (log-returns), and Volume (log-transformed, robustly scaled) to the [-1, +1] range. Justify the chosen techniques and their impact on model performance, including relevant formulas and parameters.

- **Dynamic Rotating Plane Method:** Describe the process of generating datasets for each timeframe using this method. Explain the principal axes and the dynamic re-centering on the latest data point.

- **Non-Hierarchical Attention Model (if implemented):** If used for dynamic querying, detail the model's architecture, functionality, and motivation, emphasizing its computational efficiency and interpretability.

- **Multi-Scale Model Evaluation:** Describe the evaluation process against a baseline intraday model, including attribution analysis to understand reliance on different timeframes and performance analysis during major market events.

### B. Codebase Finalization

The submitted codebase must align with the dissertation's descriptions, ensuring reproducibility and clarity:

- **Clear Documentation:** Thoroughly document the code related to multi-scale data processing, the Dynamic Rotating Plane method, the hierarchical attention model (if applicable), the ViT architecture, the self-correcting mechanism, and the data normalization process.

- **Modular Code for Data Processing:** Implement modular and well-organized code for processing data across different timeframes to facilitate understanding and future modifications.

- **Robustness and Reproducibility:** Ensure the codebase is robust, easily reproducible, and allows others to replicate experiments and validate findings. Structure the code to reflect the ViT architecture, self-correction mechanism, and data normalization process described in the dissertation, with well-documented modules for each component. Provide clear instructions for running and understanding these core aspects.

## V. Dissertation and Documentation

This section outlines the final steps required to complete the SCoVA project, focusing on the dissertation write-up and the finalization of the accompanying codebase to ensure a cohesive and presentable final product that accurately reflects the research conducted.

### A. Dissertation Writing

This subsection details the process of compiling the research findings, methodology, and conclusions into a comprehensive dissertation.

1. **Structure and Content:** Organize the accumulated research materials, experimental results, and analysis into a structured document following standard dissertation guidelines. This includes a compelling introduction, a thorough literature review, a detailed methodology section, comprehensive results presentation and analysis, and meaningful conclusions. Specifically, ensure the following aspects are thoroughly addressed:

   - **Data & Timeframe Configuration:** Detail the selected asset universe (e.g., NIFTY 50, NIFTY 500, Custom Watchlist) and the chosen date range, explaining the rationale behind these choices and how the data was split into training and validation sets.
   - **Dynamic Plane Configuration:** Provide a thorough description of the model's perception system, specifying the candlestick type, the local window size, and the included features (price, time, volume). Justify these choices based on their impact on model performance and interpretability.
   - **Model & Learning Architecture:** Expand this section significantly to cover model selection, hyperparameter tuning, and associated learning processes. Include specific details regarding:
     - _ViT-specific hyperparameters:_ Document the chosen patch size, embedding dimensions, number of transformer layers and attention heads, and the dropout rate, along with explanations for these choices.
     - _Multi-scale context fusion methods:_ Explain how multi-scale context was incorporated, specifying the method used (e.g., attention-based, concatenation, weighted average).
     - _Optimizer selection and parameters:_ Detail the selected optimizer (e.g., Adam, SGD, AdamW) and its parameters, including learning rate, weight decay, and any learning rate schedulers.
     - _Loss function selection:_ Clearly state the loss function used and its relevance to the primary target variable.
   - **Self-Correction System Configuration:** Describe the self-correction mechanism, explaining the wound detection threshold (related to frame instability sensitivity) and the healing trigger (prediction accuracy threshold for healing).

2. **Refine Dissertation Title:** Ensure the title accurately reflects the scope and focus of the research. It should be concise, informative, and engaging.

### B. Codebase Finalization

This subsection covers preparing the codebase for submission alongside the dissertation.

1. **Preparation and Submission:** Prepare the codebase for inclusion with the dissertation, ensuring it is well-organized and readily understandable. Include clear instructions on how to execute the code and reproduce the experimental results. A concise README file or similar documentation is recommended to guide the reader. The codebase should directly reflect the configurations and parameters described in the dissertation.

2. **Code Review and Refinement:** Thoroughly review and finalize the codebase. Ensure all code is functional, well-documented, and free of errors. Remove any unnecessary or experimental code not directly relevant to the core research.

3. **Module Documentation:** Clearly document individual code modules and their functionality to facilitate understanding of the system's components and their interactions. Provide explanations of the algorithms and techniques used within each module.

## V. Dissertation and Documentation

This section details the documentation requirements for the dissertation and the finalization of the accompanying codebase. It outlines how the implemented features and enhancements, driven by the project checklist, should be integrated into the dissertation narrative and reflected in the final code.

### A. Dissertation Writing

The dissertation should provide a comprehensive overview of the project, including the following key aspects:

- **Data Configuration:** Explain the rationale and implementation of the configurable data options, including data splitting (training, validation, testing), lookahead period selection, and the choice of data source (candlestick or Heiken-Ashi data). Analyze how these options influence experimental results.
- **Dynamic Plane Visualization:** Detail the implementation of the dynamic 2D plane representation of candlestick data, including the rotation and re-centering mechanism. Emphasize the importance of the live, side-by-side visualization with the standard Heiken-Ashi chart for understanding model perception. Include this visualization in the dissertation.
- **Smooth Frame Rotations:** Describe the smoothing mechanism implemented to mitigate jittery frame transitions in the dynamic plane visualization, particularly during periods of high market volatility. Discuss the user-adjustable smoothing factor and its effect on the visualization and model performance.
- **Error and Healing Mechanism:** Thoroughly document the enhanced error and healing mechanism, including the adjustable weights for different error components (vector error, frame shift error) and the various healing parameters. Explain the different healing triggers (performance-based, time-based) and their respective impacts.
- **Lookahead Period:** Explain the implementation of the lookahead period for predictions, enabling the model to forecast multiple future candlesticks. Analyze the implications of varying lookahead periods on prediction accuracy and trading strategies.
- **Multi-Scale Context Fusion:** Discuss the implemented multi-scale context fusion methods (attention-based (ViT), concatenation, and weighted average) and their influence on model performance. Explain how users can select between these methods and the rationale behind each choice.
- **Total Error Metric:** Detail the composition and implementation of the total error metric, which combines vector error and frame shift error with adjustable weights. Justify the weighting scheme and explain how this metric provides a comprehensive performance evaluation.
- **Performance-Based and Time-Based Healing Triggers:** Describe the functionality and rationale behind both the performance-based and time-based healing triggers. Explain the activation criteria for each trigger, including thresholds and durations, and their impact on mitigating performance degradation.
- **Data Acquisition:** Describe the process of acquiring the intraday data used for training, validation, and testing. Specify the data sources, intervals, and any challenges encountered during data acquisition. This includes the download of intraday data for all stocks in India and the US across all intervals.

### B. Codebase Finalization

The final codebase should be well-documented, modular, and reproducible. It should include the following implemented features:

- **Data Configuration Controls:** Implement controls for data splitting, lookahead period selection, and data source selection.
- **Dynamic Plane Visualization:** Include the code for generating the dynamic plane visualization and its side-by-side comparison with the Heiken-Ashi chart.
- **Smooth Frame Rotation Implementation:** Include the smoothing mechanism for frame rotations, with a user-adjustable smoothing factor.
- **Error and Healing Mechanism Implementation:** Incorporate the expanded error and healing mechanism, including adjustable weights, parameters, and triggers.
- **Lookahead Period Implementation:** Implement the functionality to define and use a variable lookahead period for predictions.
- **Multi-Scale Context Fusion Implementation:** Include the code for the different multi-scale context fusion methods and the selection mechanism.
- **Enhanced Monitoring Display (Optional):** Consider including the enhanced diagnostic plots and real-time metrics developed for monitoring and evaluation, even if not directly required for dissertation submission. Mention this in the dissertation as a tool used during development.

By adhering to these guidelines, the dissertation and the accompanying codebase will provide a comprehensive and insightful account of the research, ensuring reproducibility and contributing valuable knowledge to the field.

## V. Dissertation and Documentation

This section outlines the requirements for the dissertation and accompanying codebase, ensuring clear documentation and reproducibility of the research.

### A. Dissertation Writing

The dissertation must comprehensively document the research process, findings, and technical implementation. It should address the following key areas:

- **Context Awareness:** Detail the exploration and rationale behind the optimal number of candlesticks per frame and the total number of frames. Explain the impact on model performance and highlight the variable nature of the optimal configuration.
- **Transfer Learning:** Evaluate the effectiveness of transfer learning across different markets (US-US, US-India, and India-India). Compare performance metrics and discuss implications for model generalization.
- **Context-Aware Periodicity:** Explain the implementation and evaluation of weighted predictions based on various periodicities (day, week, month, quarter, and year). Discuss the factors considered (number of frames, candles per frame, and stock category) in determining the optimal configuration for each category, the reasoning behind the weighting scheme, and its impact on predictive accuracy.
- **PCA Analysis:** Present the results of repeating the experimental process using a 2-dimensional dynamic plane derived from Principal Component Analysis (PCA). Discuss the impact of using PCA-transformed data on model performance and interpretability.
- **Hyperparameter Permutation Testing:** Document the "try all permutations" approach for hyperparameter optimization, including the range of tested values, the baseline performance achieved with one training epoch per permutation, and the identified optimal configuration within the specified training and testing date ranges.
- **Library Research:** Justify the choice of library for handling long-term intraday data, comparing its advantages and disadvantages to alternatives and explaining its suitability for the project's data requirements.
- **Model Input Candles:** Detail the implementation of categorized input candles based on market capitalization, sector, and share price bins. Explain the motivation for this categorization and its impact on model performance.
- **Rally Time Calculation:** Describe the methodology for calculating rally times based on the defined categories and how this information is utilized within the trading strategy.
- **Dynamic Capital Allocation:** Elaborate on the research and implementation of the dynamic capital allocation strategy. Explain the chosen strategy, including the starting capital and probability distribution used for trades, justifying the approach with relevant research and analysis.
- **Error Detection and Healing:** Document the research and implementation of dynamic error detection and healing strategies. Explain the mechanisms for detecting and mitigating errors in a non-time-based manner, emphasizing their importance for robustness and reliability.

Finally, ensure the dissertation title accurately reflects the core focus and contributions of the research.

### B. Codebase Finalization

The finalized codebase, submitted alongside the dissertation, should be well-organized, documented, and easily reproducible. Specifically:

- **Complete and Functional Code:** Provide the complete and functional codebase supporting the dissertation's findings and claims.
- **Clean and Commented Code:** Ensure the code is clean, well-commented, and free of unnecessary files or code.
- **Modular Structure with Documentation:** Organize the code into modular and well-documented units, particularly for the following functionalities: library selection, input candle categorization, rally time calculation, dynamic capital allocation, and error detection and healing. This will facilitate understanding and reproducibility.
- **Reproducibility Instructions:** Include clear instructions for running the code.

## V. Dissertation and Documentation

This section outlines the requirements for documenting the project and finalizing the codebase, ensuring alignment between the dissertation and the implemented system.

### A. Dissertation Writing

The dissertation must thoroughly reflect the project's architectural and philosophical foundations. Key areas to address include:

- **Zerodha KiteConnect API Integration:** Detail the API's use for data acquisition (historical and live via websocket), order management, portfolio generation, and tracking. Justify its selection and analyze its impact on system performance and functionality.
- **Integration of Philosophical Principles:** Explore the four paths to liberation (Gyaan, Bhakt, Karam, and Raaj Yoga) and demonstrate their influence on the project's architecture, features, and implementation. Provide an in-depth analysis, moving beyond superficial mentions to showcase how these philosophies shaped the project's direction and outcomes.
- **Ethical Considerations (Dharmic Mandate):** Define and justify the hard-coded ethical rules based on Satya, Shaucha, and Santosha. Provide concrete examples of their implementation within the codebase and their influence on the bot's trading behavior. Explain how these constraints contribute to the system's overall integrity and sustainability.

### B. Codebase Finalization

The submitted codebase must directly correspond to the descriptions and analyses within the dissertation. This requires:

- **Clear Code and Documentation:** Include comprehensive documentation and clear code comments that illustrate the implementation of architectural choices, API usage, and the embodiment of the philosophical and ethical principles discussed in the dissertation.
- **Modular Structure:** Organize the codebase into well-defined, documented modules that align with the dissertation's functional descriptions. This facilitates understanding of the connection between theory and implementation, simplifying review and validation of the dissertation's claims.

## V. Dissertation and Documentation

This section details the finalization of the dissertation and the supporting codebase. Thorough and technically detailed documentation is crucial to minimize ambiguity and ensure the project is easily understood. Philosophical discussions should be avoided; the focus is on providing a comprehensive and self-sufficient technical record.

### A. Dissertation Writing

The dissertation should be structured logically and present a technically detailed account of the project. Every aspect, from the chosen methodologies and technical libraries to specific implementation details, must be thoroughly documented. This includes:

- **In-depth Algorithm Explanations:** Provide detailed explanations of all algorithms used, including mathematical formulations, pseudocode, and justifications for their selection.
- **Comprehensive Model Architecture Descriptions:** Clearly describe the architectures of all models, including the rationale behind design choices, layer configurations, and hyperparameter settings.
- **Detailed Experimental Setup Presentation:** Meticulously explain the experimental setup, detailing data preprocessing steps, training procedures, validation techniques, and performance metrics. Include a thorough analysis of the results, highlighting both successes and limitations.
- **Thorough Discussion of Technical Challenges and Solutions:** Document any encountered technical challenges, describing the troubleshooting process and the implemented solutions in detail.
- **Exhaustive Software and Hardware Specifications:** List all software libraries and versions, along with hardware specifications used in the project. This includes details about the development environment, dependencies, and computational resources.
- **Refined Dissertation Title:** Ensure the dissertation title accurately reflects the technical focus of the work, avoiding vague or overly broad phrasing.

### B. Codebase Finalization

The complete and finalized codebase must accompany the dissertation. It should be thoroughly documented to ensure reproducibility and understanding.

- **Code Organization and Documentation:** Organize the codebase into well-defined modules with comprehensive documentation for each. This includes:
  - **Detailed Functionality Descriptions:** Explain the purpose and functionality of each module, detailing input parameters, output values, and any side effects.
  - **Comprehensive Inline Comments:** Use inline comments to explain the logic within each function and clarify the purpose of individual code segments.
  - **Clear Dependency Documentation:** Document all external libraries and dependencies used by each module, specifying the required versions and installation procedures.
- **Functionality and Error Handling:** Ensure the codebase is fully functional and error-free. All dependencies should be clearly documented to facilitate reproducibility.

By adhering to these guidelines, the dissertation and accompanying codebase will provide a comprehensive and technically detailed account of the project, minimizing ambiguity and the need for further clarification.

## V. Dissertation and Documentation

This section details the finalization of the dissertation and the accompanying codebase, including considerations for cost optimization, federated learning, and clear documentation of the technical implementation.

### A. Dissertation Writing

1. **Structure and Write Dissertation:** The dissertation should articulate the project's methodology, findings, and conclusions. Key aspects include:

   - **Cost Optimization:** Address the challenges and solutions related to image generation and processing costs, detailing the implemented cost-saving measures (e.g., offline processing, caching, alternative architectures) and their impact on performance, cost-effectiveness, and usability within the UI/UX design for model training and retraining.
   - **Federated Learning:** Dedicate a section to the federated learning implementation, explaining the rationale, client-server workflow design, performance considerations, the "client-side heavy lifting" approach (iPad performing image generation and model training, server orchestrating), encountered challenges, and implemented solutions (e.g., efficient data and model update handling).
   - **Model Performance:** Emphasize the model's performance and analysis, ensuring clear presentation of results and insights.

2. **Refine Dissertation Title:** Ensure the title accurately reflects the core research focus, encompassing contributions related to cost optimization, federated learning, and mobile device utilization for computationally intensive tasks in financial modeling.

### B. Codebase Finalization

1. **Provide Codebase for Dissertation:** Prepare the codebase for submission, including clear instructions for running the core components, particularly the federated learning aspects (image generation on the iPad, local model training using TensorFlow.js, and server-side update aggregation). Ensure the code is well-documented and demonstrates the implemented cost-optimization strategies (offline processing, caching, iPad offloading if applicable, and relevant UI elements).

2. **Finalize Codebase:** Before submission, finalize the codebase to reflect the final implementation of all optimizations, addressing memory constraints, data access considerations (especially for iPad offloading), and the specific implementation of federated learning or hybrid approaches. Ensure the "client-side heavy lifting" architecture is clearly represented and the federated learning framework (using TensorFlow.js) is robust and understandable.

3. **Provide Detailed Code Modules:** Provide well-documented code modules illustrating the core system components, particularly those related to:

   - **Image Generation and Processing:** Explain how these modules address cost optimization challenges and maintain a user-friendly training and retraining experience via the app UI. Include examples of image caching, offline processing workflows, and iPad-server communication (if applicable).
   - **Federated Learning:** Document the iPad's image generation process, on-device training with TensorFlow.js, and the server's update management and aggregation mechanisms.
   - **Interactive Results Dashboard, Live Trading Engine, and Karma Ledger:** Provide clear explanations of these key components, ensuring their functionality is readily understandable.
   - **Frontend and Backend Components:** Document the chosen technologies (Flutter, Google Cloud Firestore, Google Cloud Storage, Firebase Hosting, Google Cloud Run/Functions) and their integration within the system. Include instructions for replicating the development and deployment environment using IDX.google.

## V. Dissertation and Documentation

This section outlines the necessary documentation and dissertation writing process, focusing on the technical challenges and architectural decisions related to using Progressive Web Apps (PWAs) for computationally intensive tasks, specifically training Vision Transformer models on resource-constrained devices like iPads. It addresses the feasibility of this approach, potential alternatives, and the final implementation details.

### A. Dissertation Writing

The dissertation should thoroughly address the following technical challenges and their impact on the chosen architecture:

- **PWA Suitability for GPU-Intensive Tasks:** Investigate the feasibility and potential drawbacks of running computationally intensive GPU tasks, such as Vision Transformer training, within a PWA. This investigation should include an analysis of potential issues like browser crashes, performance limitations, and the feasibility of long-term, uninterrupted operation, especially on iPads. Clearly document the research findings and justify the resulting architectural decisions.

- **PWA Stability Under GPU Load:** Assess and document the likelihood of browser crashes when a PWA is subjected to heavy GPU loads during model training. Consider factors influencing crash probability, including iPad limitations, browser limitations, and resource management strategies.

- **Resource Limitations for PWA Training:** Analyze and document the limitations encountered when running resource-intensive Vision Transformer training within a PWA on an iPad. Focus on the feasibility of long-term, uninterrupted operation and its potential impact on the training process.

- **Technical Deep Dive: GPU-Intensive Training in PWAs:** Address the core technical challenge of running GPU-intensive Vision Transformer training within a PWA on an iPad. Research and document up-to-date information on TensorFlow.js performance and WebGPU capabilities in the context of large model training within a browser. This research should inform the final architectural decisions and be clearly presented.

- **Alternative Training Platforms:** If PWAs prove unsuitable for reliably running intensive GPU tasks like Vision Transformer training, explore and document alternative solutions. This could include native application development or cloud-based training solutions. Clearly justify the chosen alternative and explain its implementation.

### B. Codebase Finalization

The finalized codebase and its documentation should clearly reflect the chosen platform for training Vision Transformer models, aligning with the research findings presented in the dissertation. Provide detailed and well-commented code modules corresponding to the final implementation, ensuring consistency and traceability between the dissertation's theoretical analysis and the practical implementation. If an alternative platform is chosen, adapt the code accordingly and thoroughly document the changes. Include clear instructions for running the code, replicating experiments, and understanding the different modules. This documentation should cover:

- **Server-Side Orchestration (if applicable):** Document the backend implementation, including data storage and serving, management of the global master model, and coordination of the distributed training process. This should align with the API endpoint implementation and federated averaging logic (if used).
- **Client-Side Processing:** Detail the frontend implementation, including data fetching, image generation, local model training, and model updates. Clearly explain the use of relevant technologies (e.g., TensorFlow.js, Canvas API, Web Workers).
- **UI Adaptations (if applicable):** Document any UI changes made to reflect client-side processing, including new status indicators and resource monitoring.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on documenting the research in the dissertation and finalizing the codebase. This includes clearly articulating the project's motivation, methodology, results, and conclusions within the dissertation, and ensuring the codebase is well-structured, documented, and consistent with the dissertation's content.

### A. Dissertation Writing

1. **Structure and Content:** The dissertation should present a logical flow, guiding the reader through the research process. It must clearly articulate the project's goals, methodology, results, and conclusions. The rationale behind the chosen hybrid architecture, balancing server-side heavy lifting (initial model training and large-scale multi-permutation campaigns on Google Cloud Run with GPU acceleration) and client-side fine-tuning (delta training on recent data, interactive backtesting, and live inference) on the iPad, needs to be thoroughly explained. The dissertation should also address the evaluation of hybrid training solutions and the mitigation of risks associated with client-side training, such as browser crashes and memory constraints, through strategies like batch processing. The chosen execution target (Cloud GPU, On-Device Fine-tuning, or On-Device Backtesting) and its impact on performance and stability should be discussed. Finally, the rationale for the frontend technology decision (PWA or native iOS Swift app) and its compatibility with the hybrid architecture must be justified. Any challenges encountered during development, analogous to those potentially faced with PWAs, and their corresponding solutions should be documented. The justification for client-side processing needs, even if ultimately performed server-side, should be clearly explained.

2. **Title Refinement:** The dissertation title should accurately and concisely reflect the core contribution of the research, including the hybrid architecture and the chosen frontend technology.

### B. Codebase Finalization

1. **Codebase Submission:** A clear and concise code prompt showcasing key elements of the project should be included in the dissertation or appendix, enabling readers to understand the core functionality and implementation. The complete, well-documented, and thoroughly tested codebase should clearly demonstrate the implementation of the hybrid architecture. This includes server-side training scripts for Google Cloud Run with GPU acceleration and the client-side code for model fine-tuning, interactive backtesting, and live inference on the iPad, using WebSockets and the DynamicPlaneGenerator. The code should also demonstrate how weight updates (deltas) are sent back to the server. Well-documented modules should highlight components related to the hybrid architecture, including server-side training, client-side fine-tuning, and client-server communication. The "Campaign Runner" UI code, including the "Execution Target" dropdown menu, should also be clearly documented. Any transition to a native iOS Swift app and associated codebase changes should be clearly documented and explained.

2. **Finalization:** The codebase should be finalized before completing the dissertation to ensure consistency between the implementation and the written documentation. It should be thoroughly reviewed for efficiency, clarity, and correctness.

3. **Module Documentation:** The codebase should be well-organized and modular, with each module thoroughly documented, explaining its purpose, inputs, outputs, and any relevant technical details. Functional requirements, like client-side image generation (even if server-side), should guide the organization and documentation of relevant code modules. Addressing technical constraints should be documented within the relevant modules to provide context and explain design decisions.

## V. Dissertation and Documentation

This section details the finalization of the project documentation and codebase, focusing on the backend infrastructure's role and description, ensuring alignment between the dissertation and the implemented system.

### A. Dissertation Writing

The dissertation should clearly articulate the backend's minimalist role within the overall system architecture. This minimalist approach, where the backend primarily serves data and manages API connections, should be justified within the dissertation's architectural discussion. This section should also connect the backend's function to the client-side's responsibilities, emphasizing the distribution of workload.

### B. Codebase Finalization

The Python backend codebase must be finalized and thoroughly documented before dissertation submission. The documentation should clearly outline the backend's responsibilities as a minimal orchestrator. Key aspects to document include:

- **Data Serving:** Detail the backend's function as the central repository for raw numerical data. Explain the data format, request mechanisms used by the iOS app, and any preprocessing performed by the backend before serving the data.
- **API Management (Zerodha Kite Connect):** Describe the backend's handling of secure connection and authentication with the Zerodha Kite Connect API. Document the authentication flow, API key management, how the application interacts with the Zerodha API (making calls, receiving data, error handling), and the data retrieved.
- **Minimal Orchestrator Role:** Clearly document the backend's limited role, focusing on its core functions: serving raw data, potential storage and serving of model versions (if applicable), and authenticating with external APIs. Use the "traffic controller" analogy to illustrate the relationship between the backend and the client (iOS app). Emphasize that the client (iOS app) handles on-device processing, including image generation, model training, backtesting, and live inference.
- **Backend Technology:** Explicitly state the use of Python for the backend implementation.

Well-documented code modules demonstrating these functionalities are crucial for clarity and reproducibility. The provided documentation should enable another developer to understand and interact with the backend code effectively.

## VI. Dissertation and Documentation (Client-Side Focus)

This section outlines the finalization of the dissertation and client-side codebase, emphasizing the distribution of tasks between the backend and the client application.

### A. Dissertation Writing

- **Dissertation Structure and Content:** The dissertation should clearly articulate the project's goals, methodology, results, and conclusions. Critically, it should detail the system architecture, including the distinct roles of the backend and client applications, highlighting the client's focus on computationally intensive tasks.
- **Dissertation Title Refinement:** The title should accurately reflect the research focus and key technologies (backend, client application, Core ML).

### B. Codebase Finalization

- **Codebase Submission:** The submitted codebase should clearly delineate the functionalities implemented on both the backend (Python) and client-side (iOS), reflecting the described division of labor:
  - **Backend:** Manages master models, orchestrates updates from the client, and manages experiment templates and results.
  - **Client (iOS App):** Handles computationally intensive tasks: image generation, model training/fine-tuning, backtesting, and live inference.
- **Codebase Finalization:** The final codebase must be well-documented and consistent with the dissertation's functional descriptions. The code structure should clearly demonstrate the division of labor between the backend and client.
- **Detailed Code Modules:** Include detailed documentation for each module, explaining its contribution to the overall architecture and interaction with other components on both the backend and client-side. Clearly describe how the client-side performs computationally intensive tasks.

## V. Dissertation and Documentation

This section outlines the final steps for dissertation completion and codebase preparation. It consolidates guidance from various checklist items regarding frontend development, backend integration, and core research documentation.

### A. Dissertation Writing

The dissertation should provide a comprehensive overview of the project, encompassing the following key aspects:

1. **Frontend Technology Justification:** Clearly justify the chosen frontend technology (e.g., Flutter, Swift). If Flutter was selected, detail the evaluation process, including its integration with the Python backend, access to native device permissions and resources, and overall performance, especially regarding client-side processing.

2. **Backend Technology Confirmation:** Confirm and document the use of Python as the backend technology, reinforcing the project's architectural decisions.

3. **Client-Side Processing Rationale:** Explain the rationale for client-side processing, emphasizing the benefits of leveraging native app permissions and resources.

4. **Flutter/Python Integration (If applicable):** If Flutter is the chosen frontend, thoroughly document the integration process with the Python backend, including data transfer mechanisms, authentication procedures, and API design choices.

5. **Flutter vs. Alternative Evaluation (If applicable):** If Flutter replaced another technology (e.g., Swift), provide a detailed evaluation, focusing on Flutter's performance in machine learning (including TensorFlow Lite integration), graphic capabilities (especially regarding dynamic plane generation), and support for on-device training.

6. **Model Fine-tuning:** Describe the final model fine-tuning process undertaken before frontend integration to ensure optimal real-world performance.

7. **Image Processing in Deployment:** Clarify that, in deployment, image processing is limited to daily predictions and occasional model re-tuning triggered by elevated error rates. This differs from the full image processing pipeline used in research and should be explained to highlight the focus on practical application and resource optimization.

### B. Codebase Finalization

The finalized codebase should be well-structured, documented, and aligned with the dissertation's descriptions. Key considerations include:

1. **Codebase Alignment with Dissertation:** Ensure the codebase accurately reflects the implementation details described in the dissertation to facilitate reproducibility and validation.

2. **Documented Code Modules:** Provide comprehensive documentation for all code modules, particularly those related to frontend/backend integration, client-side processing, and specific technologies like TensorFlow Lite and dynamic plane generation.

3. **Removal of Experimental Setup:** Remove any experimental code used during development to streamline the codebase for deployment and focus on core functionality.

4. **Flutter UI Implementation (If applicable):** If Flutter is used, include the complete Flutter project, with well-documented UI components (e.g., Experiment Designer, Live Dashboard). Explain the chosen design approach (Material Design, Cupertino, or custom).

5. **Flutter's ML and Graphics Integration (If applicable):** Clearly demonstrate the integration of ML models (e.g., using `tflite_flutter`), and document any custom shaders or graphics implementations for dynamic plane visualization.

By addressing these points, the dissertation and codebase will provide a coherent and reproducible representation of the project, enabling readers to understand the rationale behind key decisions and potentially build upon the work.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on documenting the research and finalizing the codebase for submission. A well-structured and documented codebase is crucial for reproducibility and a comprehensive understanding of the project's contributions. The dissertation should provide a complete narrative of the project, including the rationale for architectural choices, implementation details, and performance considerations.

### A. Dissertation Writing

The dissertation will serve as the primary documentation of the entire project, encompassing the background research, methodology, results, and analysis. Given the hybrid approach using Core ML for iOS and TensorFlow Lite for other platforms, the dissertation should address the following key aspects:

- **Rationale for Hybrid Approach:** Justify the decision to use Core ML on iOS and TensorFlow Lite on other platforms, emphasizing the performance benefits of Core ML on iOS devices and the cross-platform compatibility offered by TensorFlow Lite.
- **Platform Channel Implementation:** Detail the implementation of platform channels, explaining how they bridge the Dart code with the native Swift (iOS) and Kotlin (Android) code for machine learning operations. This should include a clear explanation of the data flow between Dart and native code, specifically how image data is passed to the native layer and how predictions are returned.
- **Model Management Strategy:** Clearly explain the chosen model management strategy, detailing how models are loaded, updated, and managed across different platforms. This should be reflected in the codebase structure.
- **Shared Dart Code and Platform-Specific Implementations:** Describe the structure and organization of the shared Dart code and the platform-specific native implementations (Swift and Kotlin). This discussion should provide context for understanding the overall architecture and code organization.

### B. Codebase Finalization

The codebase accompanying the dissertation needs to be well-structured, documented, and easy to understand. The following points highlight key aspects of the codebase finalization process:

- **Clear Code Structure and Comments:** The codebase should be organized logically with clear comments explaining the functionality of different modules and components. Unnecessary code should be removed, and all dependencies should be clearly documented.
- **Platform Channel Implementation (Code):** The code should clearly demonstrate the implementation of platform channels, including the data exchange between Dart and native layers.
- **Core ML Handler Documentation:** The Swift `CoreMLHandler.swift` class within the iOS folder should be thoroughly documented, explaining its role in handling image data, running the Core ML model, and returning predictions. The inclusion and location of the `.mlmodel` file in the Xcode project should be clearly indicated.
- **TensorFlow Lite Integration and Documentation:** The codebase should include the TensorFlow Lite implementation for other platforms and its integration within the overall architecture. This implementation should be documented with the same level of detail as the Core ML implementation.
- **Data Management Module:** The implementation for local data storage using SQLite or Hive should be well-documented, including details on data access and manipulation.
- **ML Inference Module:** The TensorFlow Lite inference module should be clearly documented, including model loading, input preprocessing, inference execution, and output postprocessing.
- **Graphics Module:** The graphics module responsible for dynamic plane generation using `ml_linalg` and Flutter's `CustomPainter` should be well-documented, addressing performance considerations and any encountered limitations.
- **Cross-Platform Compatibility:** Document any specific implementations for ensuring cross-platform compatibility, particularly between TensorFlow Lite and Core ML. Address any known bugs or limitations in the chosen frameworks and any implemented workarounds.
- **Reproducibility Instructions:** Provide a clear "code prompt" or instructions on how to run key components of the codebase, enabling others to easily reproduce the results presented in the dissertation.

Providing a clean, well-documented codebase and a comprehensive dissertation will significantly enhance the value and impact of the research, facilitating understanding, reproducibility, and potential future extensions.

## V. Dissertation and Documentation

This section outlines the requirements for documenting the project in the dissertation and finalizing the accompanying codebase. Given the focus on cross-platform deployment and on-device training, the documentation should clearly articulate the architectural choices and their implications.

### A. Dissertation Writing

The dissertation must comprehensively describe the project, including the motivation, methodology, results, and conclusions. Specifically, address the following points related to model training and deployment:

- **Training Framework Selection:** Discuss the evaluation process for choosing the training framework and the rationale behind using Core ML (for Apple devices) versus a cross-platform solution. Explain the trade-offs considered, such as model performance, development effort, and long-term maintenance, highlighting the advantages and disadvantages of each approach.

- **Universal Source Model (USM):** Detail the implementation of the USM in a framework-agnostic format (e.g., PyTorch or TensorFlow). Explain the rationale behind this choice and how it serves as the single source of truth for the model's weights and architecture. Emphasize the benefits of maintaining a single source for model management and deployment across multiple platforms.

- **Conversion and Deployment Process:** Document the steps involved in converting the USM to platform-specific formats (`.mlmodel` for iOS, `.tflite` for Android, and potentially TensorFlow.js for web). Explain the tools and techniques used for conversion and describe the deployment process to each target platform.

- **On-Device Training:** Provide a thorough explanation of the on-device training/fine-tuning implementation using platform-specific models (e.g., `.mlmodel` with Core ML on iOS). Discuss the benefits and challenges of on-device training, such as privacy preservation, personalization, and resource constraints.

- **Synchronization Mechanism:** Clearly describe the mechanism for extracting updated weights (or deltas) from the client-side model after on-device training and transmitting them back to the Python backend. Explain the data transfer methods employed and any considerations for security and efficiency.

- **Model Management and Portability:** Clearly articulate the chosen strategy for managing the different machine learning model formats (`.mlmodel` for iOS and `.tflite` for Android). This includes detailing how the models are stored, versioned, and accessed within the respective mobile environments. Explain the rationale behind selecting these specific formats and any trade-offs considered, such as file size, performance, and platform compatibility. Discuss the portability of the trained models, specifically addressing the challenges of using Core ML models on other platforms and the chosen solution for cross-platform compatibility (e.g., model conversion, cross-platform framework). Justify the selected solution and discuss its implications for the project's broader applicability and future research.

Finally, ensure the dissertation title accurately reflects the scope and focus of the project, encompassing the key aspects of cross-platform deployment and on-device training.

### B. Codebase Finalization

The finalized codebase accompanying the dissertation should reflect the chosen model management and portability strategy. It must be well-documented, modular, and easily understandable for potential extension by others. Specifically:

- **Complete and Functional Codebase:** Submit a complete and fully functional codebase as a supplement to the dissertation.

- **Clear Implementation of Model Handling:** The provided code (`TFLiteHandler.kt` for Android) and the shared Dart code responsible for data management and business logic should clearly demonstrate how the different model formats are handled. The Dart code should effectively utilize the `MethodChannel` to call the platform-specific methods (`runPrediction`) for model execution on both iOS and Android.

- **Documentation of Model Conversion or Cross-Platform Framework:** If model conversion or a cross-platform framework was used to address portability, this process must be thoroughly documented within the codebase. Provide clear instructions and explanations for replicating the process.

- **Detailed Code Modules:** Include clear documentation for all code modules related to model conversion, deployment, on-device training, and synchronization. This documentation should facilitate understanding and potential replication of the work.

By addressing these considerations in both the dissertation and the codebase, the project's findings can be more easily understood, reproduced, and potentially extended by other researchers.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on the dissertation and finalizing the codebase. While the provided checklist information may not directly pertain to these tasks, the principles of thoroughness, clarity, and robust documentation emphasized in security considerations are equally applicable here.

### A. Dissertation Writing (Continuity)

1. **Structure and Write Dissertation:** Organize the research, findings, and analysis into a coherent and well-structured document. This includes outlining the chapters, writing the content, incorporating figures and tables, and ensuring proper referencing. A clear and logical structure, akin to a well-designed system architecture, ensures the dissertation is robust and easy to follow.

2. **Refine Dissertation Title:** Craft a concise and informative title that accurately reflects the research conducted and the core contributions of the dissertation. Like a strong password, a well-crafted title should be memorable and clearly convey the essence of the research.

### B. Codebase Finalization (Facilitator)

1. **Provide Codebase for Dissertation:** Prepare and submit the relevant code used in the research. This might involve creating a specific branch or tag in the repository and providing clear instructions on how to access and run the code. Include concise instructions or examples (code prompts) to reproduce key findings.

2. **Finalize Codebase:** Before submitting the dissertation, ensure the codebase is complete, functional, and free of errors. This includes thorough documentation, removing unnecessary files or debugging code, and ensuring the code runs smoothly and reproducibly. This parallels the importance of a secure and stable system in the security checklists.

3. **Provide Detailed Code Modules:** Provide well-documented code modules that are easy to understand and navigate. Include clear comments, function descriptions, and appropriate module-level documentation explaining the purpose and functionality of each part of the code. This detailed documentation, like clear security specifications, allows others to understand and potentially build upon the work.

## V. Dissertation and Documentation

This section details the finalization of the dissertation and codebase, emphasizing the alignment between the project's implementation, documentation, and the research presented in the dissertation. It covers both the structure and content of the dissertation and the organization and documentation of the codebase.

### A. Dissertation Writing

The dissertation should adhere to standard academic structure, encompassing an introduction, literature review, methodology, results, discussion, and conclusion. Clear and concise writing is paramount, effectively communicating the research question, hypotheses, experimental design, findings, and implications. The dissertation should explicitly state the project goals and how they were achieved, providing ample evidence and justification for the choices made. Each section should be meticulously documented, mirroring the structured approach of detailed technical specifications. The dissertation title should be concise, informative, and accurately reflect the research conducted, allowing readers to quickly grasp the scope and focus of the work. Any resource constraints and their impact on the project, such as budget limitations and strategies employed to minimize server costs, should be transparently addressed. For example, if on-device training and short training runs were utilized to reduce costs, these strategies should be explained and their effectiveness analyzed.

### B. Codebase Finalization

The final codebase should be organized, well-documented, and easily understandable, enabling others to comprehend and potentially reproduce the experiments. Clear explanations of data structures, algorithms, and dependencies are crucial, mirroring the clarity expected in technical documentation. The codebase should be modular, with well-documented modules corresponding to different project components. This modularity, combined with comprehensive documentation, allows readers to understand the functionality of each part of the system. The documentation should also highlight any design decisions and implementation choices influenced by resource constraints, such as optimized code sections for minimized training sessions or reduced data ranges used during testing. This promotes transparency and reproducibility while demonstrating practical resource-constrained development strategies. Finally, the provided codebase must be complete and free of errors or inconsistencies before submission with the dissertation.

## V. Dissertation and Documentation

This section details the finalization of the dissertation and the accompanying codebase. These components are intrinsically linked: the codebase provides verifiable evidence for the methodologies and claims presented in the dissertation. Therefore, a coordinated approach to their completion is essential. While not explicitly listed in the checklist, the development, testing, and documentation processes significantly inform both the dissertation content and the final state of the codebase.

### A. Dissertation Content

The dissertation should clearly articulate the research question, methodology, results, and conclusions, grounded in a thorough literature review and culminating in a discussion of future work. Furthermore, it must comprehensively document the following technical aspects:

- **Multi-Stage Testing Strategy:** A detailed explanation of the testing methodology is crucial for demonstrating a robust and cost-effective approach. This includes describing the three stages: 1) unit and integration tests using mock data; 2) end-to-end dry-run simulations; and 3) on-device smoke tests with a dummy model. The rationale behind this staged approach, emphasizing its efficiency in managing cloud resource consumption, should be explicitly addressed.

- **Mock Data Generation:** The process for generating mock data, including the chosen formats (CSV or JSON) and the specific inclusion of edge cases (price spikes, flat periods, and data gaps), must be thoroughly documented. The justification for these edge cases and their relevance to the research should be clearly articulated.

- **Dummy Model Implementation:** The dummy neural network used for on-device smoke tests should be described, including its architecture and purpose. The dissertation should explain why the dummy model undergoes one epoch of training during these tests, justifying the necessity of this seemingly redundant step. Specifically, it should clarify that this single epoch validates the training and update mechanism, not the inference capabilities, of the on-device training pipeline.

- **Dry-Run Functionality:** The inclusion of the "Dry Run" mode in the Experiment Designer UI should be mentioned as part of the overall experimental setup and workflow.

### B. Codebase Finalization

The finalized codebase, submitted as a supplement to the dissertation, must be complete, functional, and well-documented to ensure reproducibility and facilitate future development. This includes:

- **Code Cleanup and Commenting:** Remove any unnecessary code, add comprehensive comments, and ensure all dependencies are documented. Given the complexity of on-device training and the "smoke test" implementation, thorough documentation is particularly critical.

- **Detailed Code Modules:** Provide well-documented modules for core functionalities, especially the `DynamicPlaneGenerator` and the on-device training process. These modules should clearly explain their purpose, inputs, outputs, and any relevant implementation details. The documentation should explicitly address the following:

  - **Smoke Test Clarification:** Emphasize that the single-epoch training with the dummy model is designed to validate the training and update mechanism (forward pass, loss calculation, backpropagation, and weight updates), _not_ for inference. Explain how this provides a low-cost verification of the pipeline's functionality prior to full-scale training.

  - **Data Pipeline Verification:** Thoroughly document the connection between the `DynamicPlaneGenerator` and the Core ML training session, demonstrating how transformed image tensors are correctly fed into the training process.

  - **Model Update Explanation:** Clearly document how the one-epoch training contributes to model improvement and how Core ML updates the universal model. This is crucial for understanding the chosen model management process and the rationale behind on-device training, particularly its implications for performance and resource utilization.

By addressing these aspects, the dissertation and codebase will effectively communicate the research process, ensuring reproducibility and facilitating future extensions of the SCoVA project. This combined approach bridges the gap between the project's practical implementation and its academic presentation.

## V. Dissertation and Documentation

This section outlines the requirements for documenting the project in the dissertation and finalizing the associated codebase. It details the structure and content expected in the dissertation and provides guidelines for preparing the codebase for submission. Key areas include a comprehensive description of the Cognitive Threat Analysis Module (CTAM) architecture, integration, and impact, as well as clear instructions for code organization and documentation.

### A. Dissertation Writing

The dissertation should provide a complete and coherent narrative of the research project, encompassing methodology, results, conclusions, and potential future research directions. Specific areas to address include:

- **Project Overview:** Provide a comprehensive overview of the project, including its motivation, objectives, and key contributions. This should also incorporate discussions on potential future research areas, such as real-time event detection and computer vision applications on various financial instruments.

- **Defining "Shocker" Events:** Clearly define and quantify the characteristics of a "shocker event" within time series data. Include a discussion of the chosen metrics for identifying these events (e.g., volatility spikes, volume anomalies, rapid price changes) and justify their selection.

- **CTAM Architecture and Integration:** Detail the architecture of the CTAM and its integration into the overall framework. Describe the lightweight CNN models designed for visual "shocker event" pattern analysis, explaining the balance between accuracy and computational efficiency required for real-time anomaly detection. Describe how the Dynamic Plane Generator interacts with the CTAM's "Threat Level" score and any modifications made to the smoothing function or learning rate based on this interaction. Address the rationale and implementation details of using multiple visual data sources, including how real-time snapshots of primary equity charts, corresponding futures charts, and visualized options chain data (heatmaps) are ingested and preprocessed. Detail the specialized CNN architectures and training methodologies employed for detecting anomalies in equities (gaps and volume spikes) and derivatives (options chain heatmaps). Finally, provide a clear explanation of the fusion mode and how the "Threat Level" assessment is derived.

- **Shocker Event Implementation and Impact:** Describe the implementation of "shocker events" in the model and analyze their impact on shifting the model from a reactive to a proactive approach. Explain how the model responds to unexpected occurrences in financial data.

- **Title Refinement:** Ensure the dissertation title accurately reflects the core focus of the research, potentially highlighting the use of computer vision and the prediction of market movements.

### B. Codebase Finalization

The finalized codebase should be well-documented, organized, and easily reproducible. This includes:

- **Code Availability:** Ensure the complete and functional codebase is readily available alongside the dissertation, either directly or through a repository link, allowing for verification and reproducibility of results.

- **Code Cleanliness:** Remove all debugging or testing artifacts, unnecessary files, and dependencies. The code should be clean, commented, and adhere to best practices for readability and maintainability.

- **Modular Documentation:** Provide clear and comprehensive documentation for each code module, explaining its purpose, operation, and any necessary instructions for execution or usage. Pay particular attention to core functionalities like data processing, model training, and backtesting. Specifically, document the modules related to "shocker event" detection using computer vision, including those processing equity charts and any designed for futures, options, and derivatives data. Clearly identify and document the CTAM module, including the lightweight CNN models. Ensure the code clearly demonstrates the integration of the CTAM with the Dynamic Plane Generator and how the "Threat Level" score influences its behavior.

- **Testing Protocol:** Implement a robust testing protocol, including Unit & Integration Testing, End-to-End Pipeline Simulation, and an On-Device "Smoke Test" to ensure code stability and identify potential issues.

- **Technical Documentation:** Produce a comprehensive technical document detailing the entire project lifecycle. This document should serve as a resource for future reference, maintenance, and potential extensions. It should cover all aspects of the project, including the three-stage testing strategy.

## V. Dissertation and Documentation

This section details the requirements for the dissertation and the finalization of the codebase, emphasizing the clear articulation of the trading agent's architecture, objectives, and the innovative dual-system approach.

### A. Dissertation Content and Structure

The dissertation must provide a comprehensive account of the project lifecycle, from data acquisition and preprocessing to model development, evaluation, and inherent limitations. The core trading logic and the dynamic interaction between predictive models should be central to the narrative.

Specifically, the dissertation should address the following:

- **Dual-System Architecture:** Thoroughly explain and justify the dual-system architecture comprising the "Flow Engine" (for normal market conditions) and the "Shockwave Prediction Model" (SPM) designed to capitalize on volatile market events ("shockwaves" such as flash crashes, earnings gaps, or news spikes).
- **Dynamic Weighting Mechanism ("Seesaw"):** Detail the implementation and rationale behind the dynamic weighting mechanism that balances the predictions from the Flow Engine and the SPM. Explain how the Systemic Threat Level (STL) influences this weighting and the logic for shifting reliance between models based on market volatility.
- **Systemic Threat Level (STL):** Define and justify the STL score calculation and describe its integration into other core system components. This includes its use in modulating proactive Pratyahara (withdrawal), informing the Dynamic Plane (adjusting smoothing factor), and its role as a context token in final predictions. Also detail the aggregation method (e.g., simple sum or weighted average) used to combine outputs from specialized threat detectors.
- **Model Limitations and Mitigation Strategies:** Discuss the limitations of the current error-correction model, including its oversimplification of market dynamics using the "wound" and "healing" analogy and its inherent mean reversion bias. Explain how this bias, stemming from the model's sensitivity to standard deviations, can lead to overlooking significant "shock" events and potentially smoothing out crucial market fluctuations. Propose solutions and further research directions to mitigate this smoothing effect, including a more nuanced framework that captures the complexities of market behavior. Provide a clear technical explanation of how the error-correction mechanism creates negative feedback, effectively establishing a "zone of comfort" where the model performs well under normal conditions but struggles with sharp, volatile movements.
- **Model Retraining Strategy:** Articulate the model retraining strategy, emphasizing the distinction between "flow" and "shock" conditions. Explain how the retraining process prevents desensitization to data peaks and valleys and avoids performance plateauing. Justify the decision not to rely solely on errors for continuous retraining.
- **Profit Maximization Objective:** Clearly establish profit maximization as the primary objective of the trading bot, emphasizing that mere market participation is insufficient.

**Dissertation Title:** The dissertation title should clearly reflect the project's core innovation—the adaptive strategy employing a dual-model approach and a dynamic weighting mechanism driven by the STL.

### B. Codebase Finalization

The finalized, well-documented codebase must be submitted alongside the dissertation, clearly demonstrating the implementation of key components:

- **Complete and Refined Code:** The codebase should reflect the dual-system architecture (Flow Engine and SPM) and be thoroughly commented.
- **SPM Implementation:** The code should clearly demonstrate the SPM's implementation for predicting short-term directional movements and magnitude following shockwave events.
- **"Seesaw" Mechanism:** The code should illustrate the dynamic weighting ("seesaw") mechanism, showing how predictions from the Flow Engine and SPM are blended based on the STL.
- **Modular Code with Documentation:** Provide well-documented modules for the SPM, STL calculation, and the dynamic weighting mechanism. This will facilitate review and understanding of the core components of the adaptive trading strategy.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on compiling the research and findings into a cohesive dissertation and finalizing the accompanying codebase. The project's architectural overhaul, based on the four pillars of Continuity, Enforcement, Facilitation, and Specialization, should be clearly reflected in both the dissertation's narrative and the codebase's structure.

### A. Dissertation Writing (Continuity)

The Continuity role leads the dissertation writing process, ensuring a consistent narrative that reflects the project's evolution and incorporates feedback. The dissertation should clearly articulate the roles of each component within the four pillars framework.

1. **Structure and Write Dissertation:** The dissertation should follow a logical structure, mirroring the architectural principles established. The atomization of components, as described in the architectural checklist, should be evident in the detailed explanation of each module's functionality. The dissertation should explain how the system's design promotes continuity in data processing, enforces necessary constraints, facilitates inter-component communication, and leverages specialized modules.

2. **Refine Dissertation Title:** The title should concisely convey the research focus, reflecting the final architecture and incorporating key terms related to the four pillars where appropriate.

### B. Codebase Finalization (Facilitator)

The Facilitator role prepares the finalized codebase, ensuring it mirrors the architectural decisions and is easily understandable and reproducible. The clear delineation of components into well-defined functions, informed by the four pillars, is crucial for this understandability.

1. **Prepare Codebase for Dissertation:** The provided code should be clean, well-commented, and organized according to the four pillars, allowing readers to easily map the code to the dissertation's descriptions.

2. **Finalize Codebase:** Ensure the codebase is complete, functional, and consistent with the final architecture presented in the dissertation. All components should be thoroughly documented, including their role within the four pillars framework.

3. **Provide Detailed Code Modules:** Each module should have clear documentation explaining its purpose, inputs, outputs, and relationship to the overall system architecture. This documentation should emphasize how each module embodies one of the four pillars and how it interacts with other modules within the system. The deep refactoring and component atomization from earlier stages will significantly aid in producing this documentation.

The dissertation's methodology section should also address the following supporting services, which contribute to the research's reliability and reproducibility:

- **Pre-flight Validation Service:** This service ensures all necessary preconditions are met before major campaigns (e.g., training, backtesting) by verifying data integrity, environment health, and template schema validity. This demonstrates a commitment to robust experimental design.

- **Post-flight Analytics Service:** This service analyzes results after each campaign, providing high-level insights and updating the system's "memory." This includes generating campaign summaries, updating the model registry, and identifying archetypal patterns. Referencing this service and the model registry further strengthens the dissertation's discussion of results.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on the dissertation and codebase finalization. While the architectural details of the codebase might not be explicitly discussed in the dissertation, they are crucial for ensuring the research's reproducibility and robustness, indirectly contributing to the dissertation's overall quality and integrity.

### A. Dissertation Writing

The dissertation should clearly communicate the research process, findings, and conclusions. While not explicitly addressed in the provided architectural details, the code's structure and documentation should support the dissertation's narrative by enabling clear explanations and reproducible results.

- **Structure and Content:** The dissertation should be structured logically, with each chapter building upon the previous ones. The code's organization can inform this structure by reflecting the underlying methodological approach.
- **Title Refinement:** The dissertation title should accurately and concisely reflect the core focus of the research.

### B. Codebase Finalization

Finalizing the codebase involves ensuring its stability, clarity, and reproducibility, ultimately supporting the claims made in the dissertation. The architectural principles described below are essential for this process.

- **Code Integration with Dissertation:** Relevant code snippets and explanations should be integrated within the dissertation to illustrate the implemented methods. A consistent function naming convention enhances the clarity and understandability of these code examples.
- **Codebase Refinement:** The codebase should be thoroughly reviewed and refined before the dissertation is finalized. Adhering to architectural principles like clear communication pathways and controlled data access ensures the code is well-organized, functions reliably, and is easy to understand. Continuously refactoring the code improves its architecture, resulting in a more robust and maintainable final product.
- **Modular Documentation:** Providing well-documented code modules is crucial for research reproducibility. Clear documentation, combined with a consistent function naming convention, allows others to understand, replicate, and verify the work. Well-defined architectural boundaries between components further aid in understanding the role and interaction of each module within the larger system.

The architectural principles underpinning the codebase, while not explicitly part of the dissertation writing process, are fundamental for ensuring the research is robust, reproducible, and clearly communicated. These principles should guide the finalization of the codebase and its presentation within the dissertation.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on documenting the research in the dissertation and finalizing the associated codebase. While the preceding checklist items detailed specific service implementations (NormalizationWindow, ComputePrincipalComponents, ProjectToPlane, TrainOneEpoch), this section connects those modular design principles to the broader context of the dissertation and final code submission.

The modularity demonstrated by the specialist services contributes significantly to the clarity and reproducibility of the research. Each service encapsulates a specific functionality, minimizing dependencies and promoting maintainability. This structure should be clearly reflected in both the dissertation and the code documentation.

The dissertation should explain how these individual modules contribute to the overall system. For example, the data preprocessing pipeline can be described as a sequence of calls to NormalizeWindow and ComputePrincipalComponents, highlighting the clear separation of concerns. Similarly, the model training process can be explained in terms of iterative calls to TrainOneEpoch, emphasizing the decoupled and reusable nature of this module.

The codebase accompanying the dissertation should include comprehensive documentation for each module, including clear descriptions of their inputs, outputs, and functionalities. This documentation will enable readers to understand the individual components of the system and how they interact. Furthermore, referencing the modular design within the dissertation narrative reinforces the connection between theory and implementation.

Beyond the specific services described earlier, the broader architectural principles of Continuity, Enforcement, Facilitation, and Specialization, underpinned by the base classes (ContinuityService, EnforcementService, FacilitationService, and SpecialistService), should be reflected in both the code structure and the dissertation's explanation of the system architecture. This consistent framework strengthens the link between the theoretical underpinnings and the practical implementation, enhancing the overall coherence and credibility of the research. Finally, the dissertation should include a discussion of post-implementation analysis, reflecting on the project's outcomes, limitations, and potential future work. This reflective component adds further depth and value to the research presented.

## V. Dissertation and Documentation

This section outlines the final stages of the project, focusing on documenting the work in the dissertation and finalizing the codebase for submission.

### A. Dissertation Writing (Continuity)

The dissertation should clearly articulate the project's goals, methodology, results, and architectural decisions. Specifically, it should address the following:

- **Project Overview:** Provide a structured overview encompassing the architectural philosophy, functional system design, frontend and on-device computations (if applicable), and backend architecture. This overview should include:
  - **Architectural Philosophy:** Detail the underlying principles guiding the system's design, including the rationale behind the Dynamic Rotating Plane representation of market data (Time, Price, and Volume).
  - **Functional System Design:** Outline the overall system functionality, including the Dual-Engine Perception system (Flow Engine and Shockwave Prediction Model), the weighted "seesaw" mechanism based on the Systemic Threat Level (STL), and the resulting prediction generation.
  - **Frontend & On-Device Computation (if applicable):** Describe any frontend components and on-device computations.
  - **Backend Architecture:** Describe the backend architecture, highlighting the microservice decomposition based on Continuity, Enforcement, Facilitation, and Specialization.
- **Microservice Architecture:** Explain the rationale for choosing a microservice architecture and detail the specific decomposition strategy employed. Discuss the technical challenges encountered (e.g., latency, network overhead) and the mitigation strategies used (e.g., asynchronous communication, service discovery, load balancing).
- **Results and Analysis:** Present the project's results and analyze the effectiveness of the chosen architecture. Include performance testing methodologies and results, highlighting efforts to monitor latency and address bottlenecks.
- **Title Refinement:** Ensure the dissertation title accurately reflects the core focus of the research and the implemented architecture. Incorporate relevant keywords related to microservices and the specific problem domain.

### B. Codebase Finalization (Facilitator)

The submitted codebase should be a clean, well-documented, and functional representation of the project. Ensure the following:

- **Code Structure and Modularity:** The code structure should clearly reflect the modularity inherent in the microservice architecture.
- **Code Documentation:** Thoroughly document each microservice module, explaining its purpose, functionality, and interaction with other services. Include details on performance testing and optimization strategies.
- **Completeness and Functionality:** Address any remaining technical tasks before submission, such as mitigating network bottlenecks and implementing asynchronous communication (e.g., using Google Cloud Pub/Sub).

By addressing these points, the dissertation and codebase will provide a comprehensive and coherent representation of the project, facilitating understanding, reproducibility, and future development.

## V. Dissertation and Documentation

This section details the final stages of the SCoVA project, encompassing the dissertation write-up and codebase finalization. Thorough documentation is crucial for communicating the project's findings, ensuring reproducibility, and facilitating future development.

### A. Dissertation Writing (Continuity)

The dissertation should comprehensively cover all aspects of the SCoVA project, adhering to academic standards (introduction, literature review, methodology, results, discussion, and conclusion). It should clearly articulate the project's goals, methods, findings, and contributions. A pre-writing analysis of the project, from both coding and algorithmic perspectives, is crucial given its complexity, ensuring a comprehensive and robust dissertation. Specific areas to address include:

- **Project Inception and Data Acquisition:** Detail the project's origins, motivations, and the process of acquiring and preparing the candlestick chart data.
- **Model Development and Training:** Describe the chosen model architectures (CNN, ViT), their rationale, and the training process. This includes the dynamic plane implementation and any specific modifications made to the architectures.
- **Evaluation and Results:** Present the performance metrics obtained through backtesting and stress testing, highlighting the strengths and limitations of the implemented strategies.
- **Client-Side Heavy Lifting Architecture:** Articulate the rationale behind this design choice and its implications for performance, cost, and usability. Discuss the advantages of offloading computationally intensive tasks (image generation, model training) to the client device. Explain the selected native app frontend (Swift/Core ML or Flutter/hybrid native ML) and its benefits for accessing hardware resources and achieving performance gains. Detail the interplay between the client-side operations and the lightweight backend orchestrator (Python, Cloud Run/Functions with Firebase) responsible for data serving, model management, and backtesting orchestration. Comprehensively describe the central "universal source model" residing on the backend and its update mechanism based on client-side training feedback.
- **Future Work:** Discuss potential enhancements, including:

  - **Advanced Error Signal:** Implementing a total error signal composed of Vector Deviation Error and Frame Shift Error.
  - **Performance-Based Healing:** Tying the system's healing process to the model's prediction accuracy.
  - **Multi-Scale Periodicity:** Integrating context from multiple timeframes (intraday, daily, weekly).
  - **"Rally Time" Prediction:** Predicting the expected time (in candles) for a predicted movement.

- **Dissertation Title:** Refine the title to concisely and accurately reflect the core research focus: developing and evaluating a stock market prediction agent using computer vision techniques applied to candlestick chart data.

### B. Codebase Finalization (Facilitator)

The finalized codebase, submitted alongside the dissertation, should be clean, well-documented, and fully functional. Include clear instructions for running the code and reproducing the reported results. Provide well-documented modules for core components (data preprocessing, model training, backtesting, dynamic plane implementation). Documentation should also address potential future architectural improvements, such as implementing distributed tracing using OpenTelemetry, highlighting its potential for performance monitoring and bottleneck identification.

## V. Dissertation and Documentation

This section details the finalization of the dissertation and the accompanying codebase, focusing on documenting the key system components and the integration of the narrative generation service. This documentation is crucial for transparency, reproducibility, and demonstrating a comprehensive understanding of the trading system's design and functionality.

### A. Dissertation Content

The dissertation should include dedicated sections detailing the following system components and their role within the overall trading strategy:

- **Backtesting Engine:** Describe the chosen backtesting framework (e.g., Backtrader, Zipline), the incorporated market factors (market impact, latency, slippage), and the rationale behind these choices. This documentation ensures the reproducibility and validity of the presented results.

- **Portfolio Construction Algorithm:** Explain the selected portfolio construction algorithm (e.g., Mean-Variance Optimization, Risk Parity), its implementation, parameter tuning, and its function in translating individual stock predictions into a portfolio.

- **Portfolio Risk Management:** Detail the implemented risk management rules (maximum drawdown limits, concentration limits, volatility targeting) and their enforcement within the system. This demonstrates a comprehensive approach to trading system design.

- **Adaptive Seesaw Blending:** Explain the design and implementation of the Meta-Model used for dynamically blending prediction engines ("Flow" and "Shock"). Include details on input features (STL, market volatility), the blending mechanism, and its contribution to system adaptability.

- **Explainable Model Predictions (Narrative Generation):** This section is crucial for transparency and should thoroughly document the narrative generation service. Specifically, address the following:

  - **Data Input:** Clearly define the data points used as input for the LLM, encompassing both technical attributions (e.g., feature importance from SHAP or LIME) and system state information (e.g., STL, seesaw weights, CorrectionFactor, risk exposure, available capital).
  - **Narrative Structure and Content:** Describe the structure and content of the generated narratives. Provide examples to illustrate how the narratives explain trade decisions in a human-readable format. Emphasize how these narratives enhance the interpretability of the trading system's actions.

### B. Codebase Submission

The submitted codebase should be well-documented, clean, functional, and directly correlate with the experiments and findings presented in the dissertation. Include a clear "code prompt" or guide to facilitate understanding and reproduction of the results. Specifically, ensure the following:

- **Narrative Generation Service (`Narrative_Generation_Service`):** Include and thoroughly document this module. The documentation should cover its dependencies (feature store, LIME, SHAP, LLM, etc.), functionality, and how it contributes to generating trade narratives. Include an example of the generated narrative within the code documentation. Clearly articulate how this service addresses the "black box" nature of model predictions.

- **Feature Store Integration:** Document the integration with the feature store, highlighting its role in providing versioned input features and system state information, specifically for the narrative generation service and reporting/compliance purposes.

- **Complete and Functional Code:** Ensure the codebase includes all implemented components and is fully functional.

This combined approach of detailed dissertation documentation and a well-structured, documented codebase will significantly strengthen the research by demonstrating a deep understanding of the complexities involved in developing a robust, adaptable, and transparent trading system.

## V. Dissertation and Documentation

This section outlines the requirements for the dissertation and codebase, emphasizing the integration of risk mitigation strategies through paper trading. This includes architectural changes for paper trading, UI/UX updates, and data model considerations. The dissertation must also thoroughly document the technical implementation details, including real-time data integration, realistic trade simulation, and order book feature engineering.

### A. Dissertation Writing

The dissertation should clearly articulate the project's methodology, results, and conclusions. Ensure the title accurately reflects the research scope, including the incorporation of risk mitigation techniques. Crucially, the dissertation must cover the following aspects:

- **Narrative Generation:** Describe the structure of the generated narratives, outlining sections such as Market Perception, Flow Engine Analysis, Shockwave Analysis, Final Prediction, and Execution Decision. Define each section's purpose and information content. Explain the technical integration of the Large Language Model (LLM, e.g., Gemini API), including prompt construction, data input formatting, and the prompt template used. Detail how the narratives are integrated into a Karma Ledger for transparency and accountability.

- **Risk Mitigation and Paper Trading:** Dedicate a section to risk mitigation strategies, highlighting the implementation and results of paper trading. Discuss its benefits as a bridge between backtesting and live trading.

- **Real-time Data Integration and Simulation:** Explain the integration of live tick data from Zerodha via a WebSocket feed within the `Paper_Brokerage_Simulator`. Detail how the simulator achieves realistic order fills, including partial fills, based on live bid/ask prices and volume. Emphasize the importance of live WebSocket data for simulating realistic market conditions, including network latency and bid-ask spreads. Document the chosen data model for this live tick data.

- **Order Book Feature Engineering:** Describe the `DeriveOrderBookFeatures` service, which processes raw market depth data and generates derived features: Order Book Imbalance (OBI), Weighted Average Price (WAP), and Bid-Ask Spread. Explain how these features are used as input to the `DynamicPlaneGenerator`. Thoroughly document all current usages of market depth data within the project.

- **Realistic Fill Simulation:** Elaborate on the `Paper_Brokerage_Simulator`'s function and its ability to simulate realistic order fills, including partial fills. Provide concrete examples and explain the logic behind these simulations.

### B. Codebase Finalization

The provided codebase must be fully functional, well-documented, and reflect the final implementation discussed in the dissertation. The paper trading functionality should be thoroughly tested and validated. The documentation should clearly correspond to the dissertation's descriptions.

- **Narrative Generation Module:** Include a well-documented module for the narrative generation service, aligning with the dissertation's description.

- **Paper Trading Implementation:** Include all components related to paper trading: the `Paper_Brokerage_Simulator`, modifications to the `Live_Execution_Enforcer`, and UI changes to the Live Trading Dashboard. Clearly indicate how these elements interact to provide a realistic paper trading experience. The documentation should explain how the `Paper_Brokerage_Simulator` mimics the Zerodha Kite Connect API, how the 'paper_portfolio' collection in Firestore is utilized, and how users can switch between LIVE and PAPER trading modes within the UI.

- **`Paper_Brokerage_Simulator`:** The code should clearly demonstrate the logic for handling live tick data from Zerodha, simulating realistic order fills (including partial fills), and using WebSocket data for real-time simulations.

- **`DeriveOrderBookFeatures` Specialist:** The code should clearly implement the calculation of OBI, WAP, and Bid-Ask Spread. It should also show how raw market depth data is processed and how these derived features are passed to the `DynamicPlaneGenerator`.

## V. Dissertation and Documentation

This section details the necessary documentation and code finalization steps for the dissertation, encompassing the integration of market depth data analysis, visualization enhancements, and architectural considerations.

### A. Dissertation Writing

The dissertation should address the following aspects:

1. **Market Depth Analysis and Data Limitations:** Clearly articulate the challenges posed by Zerodha's market depth data precision (limited to increments of ₹0.05 between -₹0.25 and +₹0.25). Justify the decision to focus on order quantity and count at the five fixed bid/ask levels rather than spread-based calculations, explaining how this approach extracts meaningful insights from the available data. Detail the implementation and results of the `CalculateOrderBookImbalance` and `GenerateDepthQuantityHeatmap` services, including their integration with the dynamic plane and CNN model, respectively.

2. **Order Book Volatility and System Resilience:** Discuss the implementation of the "Order Book Volatility" specialist and its integration with the Self-Correction & Healing Controller. Analyze how dynamic adjustments to the `CorrectionFactor` based on order book volatility improve system resilience during market instability, providing rationale, implementation details, and empirical results.

3. **Cognitive Threat Analysis and Market Shock Response:** Describe the "Top-of-Book Pressure Gradient" metric and its role within the Cognitive Threat Analysis Module (CTAM). Explain how rapid changes in bid-ask quantity ratios assess systemic threat levels and trigger the Shockwave Prediction Model, analyzing how this enhances market shock response.

4. **Intraday Limit Order Behavior and Exchange Interactions:** Address the nuances of intraday limit order execution on exchanges, explaining how the system handles limit orders placed "at or better" than the specified price. Discuss the potential impact on trading strategies and any adjustments made to reconcile the system's behavior with real-world exchange practices, supported by empirical data and backtesting results.

5. **Refined Dissertation Title:** Ensure the dissertation title reflects the incorporation of market depth analysis and potentially highlights the novel approach taken to overcome data limitations.

### B. Codebase Finalization

The final codebase should include the following, accompanied by comprehensive documentation:

1. **Market Depth Analysis Modules:** Include the `CalculateOrderBookImbalance` service (computing and normalizing Order Book Imbalance) and the `GenerateDepthQuantityHeatmap` service. Documentation should detail input parameters, calculation processes (including OBI normalization and heatmap generation), output format, and the integration of OBI with the `DynamicPlaneGenerator` and the heatmap with the `MarketDepthAnomalyDetector` CNN.

2. **Paper Brokerage Simulator with Price Improvement:** Update the `Paper_Brokerage_Simulator` to simulate price improvement for limit orders accurately, documenting the logic for filling limit buy/sell orders at favorable live bid/ask prices and its impact on simulated performance.

3. **Enhanced Karma Ledger and Dharma Adherence Score:** Extend the `Karma Ledger` to include "Slippage" and "Price Improvement" metrics for each trade, documenting their calculation. Refine the `Dharma Adherence Score` to incorporate these new metrics for a more comprehensive measure of execution quality, clearly explaining the updated calculation methodology and how it distinguishes between alpha-derived profits and those from favorable execution.

## V. Dissertation and Documentation

This section details the requirements for the dissertation, codebase documentation, and the integration of the anxiety model within the larger SCoVA (Snapshot Computer Vision Algorithm) project.

### A. Dissertation Content and Structure

The dissertation should comprehensively document the entire research process, from motivation and literature review to methodology, results, analysis, and conclusions. It should specifically address the following:

1. **SCoVA Project Overview:** Detail the overall project goals, architecture, and the rationale behind the chosen approach.

2. **Novel Feature Integration:** Explain the incorporation and impact of novel features, including the "Price Improvement Rate," "Book Resilience Score," and the "Anxiety Model." Analyze their contribution to model performance and overall system effectiveness.

3. **Component Deep Dive:** Discuss the architectural choices and implementation details of key components, including:

   - The "CalculatePriceImprovementRate" specialist service.
   - The enhanced "ComputeOrderBookState" specialist.
   - The "Execution Quality Feedback Loop" within the Self-Correction & Healing Controller and its influence on the Dynamic Plane.

4. **Anxiety Model Integration:** Provide a comprehensive description of the anxiety model, including its architecture, training process utilizing backtest data and features like Order-to-Quantity Ratio, Rate of Change of Order Book Imbalance, Level 1 Dominance, and Book "Flicker" Rate, and its functional requirements within the SCoVA system. Explain how the "Anxiety Level" output modulates the Error Detector and Weight Shifter.

5. **Title Refinement:** Ensure the dissertation title accurately and concisely reflects the core contributions of the research, incorporating relevant keywords such as "Vision Transformer," "Dynamic Plane," "Price Improvement," "Order Book Resilience," and "Anxiety Model" where appropriate.

### B. Codebase Finalization and Documentation

The codebase should be well-structured, thoroughly documented, and readily reproducible. Adhere to the following guidelines:

1. **Comprehensive Documentation:** Document all code, particularly sections related to the "CalculatePriceImprovementRate" service, the "ComputeOrderBookState" specialist, the "Execution Quality Feedback Loop," and the "Anxiety Model." Include clear explanations of implementation choices and rationale.

2. **Modular Structure:** Organize the codebase into well-defined modules with clear documentation for each, including input/output specifications, internal logic, and their role within the overall architecture.

3. **Finalization and Testing:** Before submission, finalize the codebase, ensuring all functionalities are working as expected. Conduct comprehensive testing, particularly for features related to price improvement, order book resilience, and the anxiety model.

4. **Code Availability:** Prepare the codebase for submission as supplementary material for the dissertation, potentially including a demonstration or usage example.

## V. Dissertation and Documentation

This section details the finalization of the SCoVA (Snapshot Computer Vision Algorithm) project, encompassing both the dissertation and the supporting codebase. Emphasis is placed on clear, comprehensive documentation of the algorithm's architecture, functionality, and implementation.

### A. Dissertation Writing

The dissertation should provide a comprehensive and coherent narrative of the SCoVA project. This includes clearly articulating the project's goals, methodology, results, and conclusions, adhering to required academic standards and incorporating feedback received. The core focus should be on snapshot computer vision, with SCoVA’s use of discrete, dynamically generated visual snapshots of market data clearly contrasted with traditional continuous time-series approaches. This distinction is fundamental to understanding SCoVA’s operational principles and potential advantages.

Key aspects to be thoroughly addressed within the dissertation include:

- **Project Context and Scope:** Clearly define the project's focus on snapshot computer vision and the official designation of the algorithm as "SCoVA" (Snapshot Computer Vision Algorithm). This establishes the context for all subsequent discussions.

- **SCoVA Architecture and Functionality:** Detail the architecture of SCoVA and provide a comprehensive explanation of its functional specifications, including its precise operation and interaction within the overall trading system. Specifically, document the use of computer vision as the primary modality of perception for SCoVA, clarifying the technical approach employed.

- **Asymmetric Design and Implementation:** The dissertation must thoroughly explain the rationale and implementation of the following asymmetric functionalities:

  - **Prediction Models:** Detail the separate prediction models for bull and bear markets (Bull_Flow_Engine and Bear_Flow_Engine), including their training processes using respective historical data (uptrends for Bull_Flow_Engine and downtrends for Bear_Flow_Engine). Explain the role of the high-level regime-detection model in selecting the appropriate prediction engine.
  - **Risk Management:** Document the asymmetric risk management within the Portfolio_Risk_Manager, including the different maximum drawdown limits for short vs. long positions. Justify this asymmetry based on the potential long bias in equity markets.
  - **Self-Correction:** Explain the asymmetric self-correction mechanism of the HealingController, focusing on its differential response to unexpected gains and losses. Connect this implementation to the principles of prospect theory.

- **Graph-Based Perceptual Model:** Detail the implementation of the graph-based perceptual model, representing different timeframes (Intraday, Daily, Weekly, etc.) as nodes, with edges representing learned influence between them. Explain the use of a Graph Neural Network (GNN) as the fusion mechanism and how dynamic influence learning allows the GNN to adjust the influence of different timeframes based on market conditions, potentially inverting the hierarchy based on factors like trend strength and volatility.

- **Dissertation Title:** The title should accurately and concisely reflect the research's core focus, clearly indicating the subject matter and methodology employed. Exploration of incorporating terminology such as "Non-Hierarchical Asymmetric," potentially resulting in a title like "Anhad’s Non-Hierarchical Asymmetric Snapshot Computer Vision Algorithm" (ANHASCoVA), should be considered separately and not impede the primary writing process.

### B. Codebase Finalization

The final codebase, submitted alongside the dissertation, should be a clear, well-documented, and modularized implementation of SCoVA, aligning with the documented architecture and functional specifications described in the dissertation. This includes:

- **Complete and Functional Code:** Ensure the codebase is complete, functional, and free of errors before submission. All dependencies should be clearly documented to facilitate reproducibility.

- **Detailed Code Documentation:** Each module should be thoroughly explained, outlining its purpose, inputs, outputs, and dependencies. The level of detail provided in the dissertation for components like the "Anxiety Model" should serve as a guide for documenting individual code modules. This includes clear descriptions of features, their calculation, and integration within the larger SCoVA algorithm.

This dual focus on a comprehensive dissertation and a well-documented codebase will ensure clarity and facilitate a thorough understanding of the SCoVA algorithm and its implementation.

## V. Dissertation and Documentation

This section details the final project stages, encompassing both dissertation writing and codebase finalization. These two components should be closely aligned to ensure clarity, reproducibility, and a comprehensive presentation of the research.

### A. Dissertation Writing

The dissertation should clearly articulate the novel approach of Asymmetric Feature Engineering for Market Dynamics. This involves detailing the implementation and impact of several key enhancements:

- **Asymmetric Feature Engineering:** Explain the core concept that market rises and falls exhibit distinct behaviors due to the persistent nature of state economics, making a complete market collapse unlikely. Describe how this inherent asymmetry is reflected in the engineered features. Specifically discuss the integration of the `AsymmetricFeatureEngine` service and its output as a context token for the Vision Transformer.

- **Asymmetric Loss Function:** Detail the design and rationale for a custom loss function that prioritizes minimizing large losses by penalizing underestimation more significantly than overestimation. Include the specific penalty factors used and their impact on model performance and risk aversion.

- **Risk-Averse Loss Function:** Expand on the broader concept of risk aversion within model training. Explain how the custom loss function contributes to the model's cautious behavior.

- **State-Dependent Attention Mechanism:** Describe how the attention mechanism dynamically adapts based on market conditions. Specify the market state indicators used (e.g., volatility, threat level) and how they influence the attention weights.

- **Upside and Downside Volatility Features:** Describe the calculation of separate upside and downside volatility measures (standard deviation of positive and negative returns, respectively) and justify their inclusion as input features. Analyze their impact on model performance.

The dissertation should also follow standard academic structure and include a refined, descriptive title.

### B. Codebase Finalization

The final codebase should be complete, functional, well-documented, and submitted alongside the dissertation. It must clearly demonstrate the implementation of the Asymmetric Feature Engineering concepts, particularly through the `AsymmetricFeatureEngine` service. This service should calculate a vector of asymmetric features from a window of raw data and provide this vector as a context token to the Vision Transformer. Ensure the code includes clear documentation for all modules and functions.

The `AsymmetricFeatureEngine` service should include the following functionalities:

- **Price & Volatility Asymmetry Features:** Implement Upside and Downside Volatility (using semi-deviation), Volatility Skewness, and Volatility Kurtosis to capture nuances in price movements.

- **Volume & Participation Asymmetry Features:** Calculate the Accumulation/Distribution Ratio (volume flow on up vs. down days) and Order-to-Quantity Asymmetry (comparing bid-side and ask-side order-to-quantity ratios) to provide insights into market conviction and participation.

- **Correlation Asymmetry Feature:** Implement the Price-Volume Correlation State feature by calculating the correlation between log-returns and log-volume separately for positive and negative return candles to distinguish between market fear and greed.

The code should clearly demonstrate the implementation of these features within the `AsymmetricFeatureEngine` service and its integration with the overall model architecture. Detailed documentation is crucial for reproducibility and understanding.

## V. Dissertation and Documentation

This section details the project's finalization, encompassing both comprehensive documentation and codebase refinement. The implemented "Dual-Token Context Injection" approach, a core architectural enhancement, should be thoroughly documented within the dissertation and reflected in the final codebase.

### A. Dissertation Writing

The dissertation must articulate the design and implementation of the Dual-Token Context Injection approach within the Vision Transformer (ViT) model. This includes:

- **Rationale for Dual-Token Injection:** Justify the use of both a discrete regime identifier (Regime ID) and a continuous asymmetric feature vector as separate input tokens. Explain the trade-off between explainability (Regime ID) and granularity (feature vector) and how this dual approach leverages the strengths of both.

- **Asymmetric Feature Engineering:** Detail the functionality of the `AsymmetricFeatureEngine`, outlining the specific features calculated (e.g., upside/downside volatility, skewness) and their mathematical definitions. Explain how this engine derives the raw asymmetric feature vector.

- **Asymmetric Regime Identification:** Describe the process of generating the Regime ID. Explain the logic within the `IdentifyAsymmetricRegime` component, including the criteria used for regime classification (e.g., unsupervised clustering using a Gaussian Mixture Model or Self-Organizing Map trained on historical data) and its real-time application.

- **Vision Transformer Integration:** Explain the modifications made to the ViT architecture to accommodate the Regime ID and asymmetric feature vector as input tokens. Detail how the self-attention mechanism within the transformer leverages these contextual tokens in conjunction with the dynamic plane image data.

- **Terrain-Based Input and Performance Evaluation:** Discuss the representation of environmental factors (humidity, temperature, AQI) using a terrain-based categorization approach. Justify this approach and address the trade-off between model simplicity and potential loss of granularity. Document experiments comparing the model's performance using terrain-based input versus individual environmental metrics. Quantify the impact on prediction accuracy and other relevant performance metrics.

### B. Codebase Finalization

The final codebase should clearly reflect the Dual-Token Context Injection implementation and include comprehensive documentation for the following modules:

- **`AsymmetricFeatureEngine` Module:** Document the input requirements, output format (the raw asymmetric feature vector), and the specific calculations performed within this module.

- **`IdentifyAsymmetricRegime` Module:** Document the input (asymmetric feature vector), output (Regime ID), and the underlying logic for regime classification. Include details on the chosen unsupervised clustering model, training process, and parameterization.

- **Modified Vision Transformer Module:** Document the modifications made to the ViT architecture to integrate the two context tokens. Provide clear explanations of the input modifications, internal processing of these tokens, and the interpretation of the model's output. Include the code for data preprocessing, model training, and model saving/loading.

This combined approach of detailed documentation and a well-structured codebase ensures the reproducibility and clarity of the research findings.
Dissertation Writing

This section details the structure and content requirements for the dissertation, ensuring clear communication of the project's methodology, findings, and contributions. A continuous and verbose record of the thought process throughout the project lifecycle will significantly aid in crafting a comprehensive and insightful dissertation. This documentation should be readily accessible and provide a clear path from initial ideas to final implementation choices, allowing for a clear articulation of the motivations, challenges, and solutions developed. Connecting the code repository to the specific experiments described in the dissertation will further strengthen the narrative and provide concrete evidence.

**Data and Preprocessing:**

- **Data Acquisition:** Explain the acquisition of OHLC and index data, specifying Yahoo Finance as the source for OHLC data and Nasdaq for index data, covering the period from 2016 to 2023. Clarify that close prices are adjusted for stock splits but not dividends. Describe the equal weighting of index data for OMXS All-Share, OMXS Large Cap, OMXS Mid Cap, OMXS Small Cap, and First North All-Share (from Nasdaq), and S&P 500 (from Yahoo Finance).

- **Return Label Calculation:** Detail the formula used: `Return(t+h) = (Close(t+h) - Open(t+1)) / Open(t+1)`, where `t` is the last date of the chart image and `h` is the holding period (a hyperparameter ranging from 1 to 5). Define each component and justify the use of `Open(t+1)` as the baseline. Explain how the holding period (`h`) is encoded in the filenames of generated graphs (e.g., `YYYY-MM-DD__YYYY-MM-DD__3.25__h=3__ABB.ST.png`) or stored in a separate CSV file, justifying the chosen approach.

- **Training and Validation:** Describe the training and validation process, including the mean squared error loss function. Justify the use of the last 30% of the training data (July 1, 2020, to December 31, 2021) as a validation set, employing out-of-sample, last-block validation. Document the implementation of early stopping (halting training if validation loss doesn't improve for two consecutive epochs) and the selection of the best-performing model based on the lowest validation loss.

**Model and Trading Logic:**

- **Regime Modeling:** Thoroughly explain the regime identification process, including the specific implementation details and the chosen number of regimes. Include comprehensive regime explanations. Discuss the "Dual Token Approach" (using both a feature vector and a regime classifier value), explaining how the feature vector's influence depends on its explanatory power beyond the regime classifier. Address how the implementation avoids bottlenecks if the feature vector doesn't add value and explain the benefits of this flexible approach, including the potential removal of the feature vector.

- **Trade Execution Logic:** Detail the trade execution logic, emphasizing its use of actual prices from t+1 to t+5 for evaluating model performance. Highlight that this future price data is unavailable to the model during prediction, preventing look-ahead bias. Explain how the logic assesses model predictions, trade profitability, and the appropriateness of long/short decisions based on these actual future prices. Describe how returns are calculated during the trading/backtesting phase (within `trade.py`), explicitly stating that this calculation uses actual future prices from Yahoo Finance, mirroring the training return calculation logic.

- **`Narrative_Generation_Service` Integration:** Document the functionality of the `Narrative_Generation_Service` and its incorporation of the Regime ID and raw asymmetric feature vector into the trade narrative explanation. Explain how this provides context and detail for the reasoning behind each trade, enhancing understanding of the overall system.

**Dissertation Structure and Refinement:**

The dissertation should follow a standard structure including introduction, literature review, methodology, results, discussion, and conclusion, clearly articulating the research question, approach, findings, and their implications. Refine the dissertation title to concisely and accurately reflect the research scope and contributions. This may involve several iterations and feedback from advisors.

## Dissertation Content and Structure

This section outlines the structure and content of the dissertation, consolidating the technical details, visualizations, and findings from the SCoVA project. The dissertation should provide a comprehensive overview of the research, clearly articulating the project's objectives, methodology, results, and conclusions.

**Data and Model Inputs:**

- **Data Acquisition and Preprocessing:** Detail the data acquisition process, specifying the use of Yahoo Finance as the primary source for OHLCV (Open, High, Low, Close, Volume) data.
- **Model Input:** Clearly describe the model's input, a 5-day candlestick chart. Detail the format of this input data, ensuring accurate representation of OHLC prices within the 5-day window. Explain the handling of edge cases for the last _n_ data points in each candlestick graph and specify the size _n_ of the data points used.
- **CNN Input and Output:** Explain how the Convolutional Neural Network (CNN) transforms the visual input of the candlestick chart into numerical outputs, including predicted entry, exit, and return values. Clearly define the meaning of these numerical values. Explicitly state the exclusion of dates, real prices, and ticker symbols from the CNN's input, emphasizing the model's reliance solely on scaled visual bar representations within the candlestick charts.

**Model Development, Training, and Evaluation:**

- **Model Architecture and Training:** Describe the CNN architecture and training process, including the rationale for using candlestick chart images paired with 5-day future return labels. Explain the objective of minimizing prediction error between predicted and actual returns. Document explorations into combining CNNs with LSTMs or Transformers to improve temporal understanding and the potential incorporation of fundamental and macro data for broader market context.
- **Model Evaluation Metric:** Define the chosen model evaluation metric (e.g., MSE or RMSE) and justify its selection. Explain how the metric reflects the model's accuracy in forecasting returns from the next _n_ data points.
- **Dataset Preparation:** Discuss the process of preparing a large-scale dataset of candlestick chart images and corresponding return labels to demonstrate model robustness and generalization capabilities.

**Backtesting, Validation, and Robustness:**

- **Backtesting Methodology:** Thoroughly explain the backtesting methodology, specifically the implementation of rolling walk-forward validation.
- **Robustness Testing:** Document stress tests conducted under various market conditions (volatility regimes, sectors, liquidity conditions). Analyze the results, highlighting the model's robustness and any identified weaknesses.
- **Historical Prediction Error Profiling (HPEP):** Detail the implementation of HPEP as an optimizable hyperparameter during backtesting. Explain its calculation, role in optimization, and the explored value ranges.

**Trading Strategies and Model Interpretability:**

- **Alternative Trading Strategies:** Analyze alternative trading strategies based on maximum prediction accuracy percentage compared to decile ranking. Describe the methodology for each, present comparative performance results, and discuss their implications.
- **Model Interpretability:** Discuss the model's decision-making process using feature attribution tools like Grad-CAM or SHAP. Include visualizations to illustrate the CNN's focus on specific candlestick chart aspects, enhancing explainability and transparency.

**Visualizations:**

- Integrate visualizations created during the project, such as candlestick charts generated using `matplotlib`. These should effectively communicate model input data and visually represent the 5-day window. The transition from `mplfinance` to `matplotlib` can be briefly mentioned in the implementation details section.

**Return Label Calculation:**

- Explicitly describe the methodology for calculating 5-day future returns using the formula `((Close(t+5) - Open(t+1)) / Open(t+1)) * 100`, clarifying how the target output for the CNN model was derived.

## Dissertation Writing and Refinement

This section details the final stages of documenting the project, focusing on clear and concise writing to effectively communicate the research contributions. The dissertation should encompass not only the technical implementation but also a robust theoretical framework and thorough analysis of the results.

### Code Implementation and Model Description

Document the code implementation meticulously, including specific details about the Historical Prediction Error Profiling (HPEP) module. Provide details on its integration within the `test_model.py` script for post-training analysis and the `trade.py` script for real-time trade filtering. A working prototype with dummy data can be beneficial for reader comprehension. Explain the process of constructing the HPEP map:

1. Group validation set predictions into bins based on predicted return ranges (e.g., -5% to -3%, -3% to -1%, etc.).
2. Calculate the accuracy within each bin. Consider including additional metrics such as average error magnitude or Sharpe ratio for a more comprehensive analysis.
3. Describe how the HPEP map is stored and accessed for later use.

Explain how the HPEP map is used during backtesting to filter trades:

1. Look up the historical accuracy associated with the predicted return bin.
2. Execute trades only if the accuracy exceeds a predefined threshold. Discuss the optimization of this threshold as a hyperparameter.

### Justification of Design Choices

Justify key design choices, referencing relevant literature where applicable:

- **5-Day Candlestick Window:** Thoroughly investigate and document the rationale behind using a 5-day candlestick window for predictions, referencing existing literature and research (e.g., Jiang et al., 2023). Articulate the benefits and limitations of this choice. Clearly distinguish between the 5-day _input_ window, justified by existing literature, and the 5-day _output_ window (prediction horizon), chosen based on practical considerations such as trading frequency and portfolio turnover.

- **Input Window Justification:** Reiterate the justification for the 5-day input window, explicitly referencing supporting literature (Jiang et al., 2023). Emphasize the empirical effectiveness demonstrated in prior research.

### Soft Labeling Exploration and Implementation

The dissertation should explore the potential of soft labels for the Convolutional Neural Network (CNN). Analyze the limitations of the current hard label approach on validation accuracy and detail how soft labels, representing a probability distribution, could enhance the model's ability to capture nuanced market behavior.

Document the implementation of soft labeling:

1. **Discretizing the Return Space:** Describe the process of dividing the continuous range of possible returns into discrete bins (e.g., 20 bins from -5% to +5% in 0.5% increments). Justify the chosen bin size and range.

2. **Modifying the CNN Output Layer:** Detail the architecture of the new output layer, consisting of a fully connected layer followed by a softmax activation function. Explain its integration with the existing CNN architecture.

3. **Converting Labels to Soft Labels:** Describe the application of a Gaussian kernel to transform hard labels into probability distributions. Discuss the parameters of the Gaussian kernel (e.g., standard deviation) and their impact.

4. **Changing the Loss Function:** Explain the change in loss function (e.g., KL Divergence or Cross-Entropy) necessitated by the use of probability distributions. Justify the chosen loss function.

Thoroughly analyze the impact of these modifications on trading strategy performance. Discuss the motivation for using soft labels, provide implementation details, and present a comprehensive performance evaluation. Include a refined and informative dissertation title that accurately reflects the scope and contributions of the research.
Trading Costs and Performance Analysis

A critical aspect of this dissertation is the analysis and evaluation of trading costs and their impact on alpha generation. Specifically, the impact of high trading costs needs to be thoroughly addressed. The dissertation should analyze and explain how costs like commissions, slippage, and borrowing fees (for short selling) eroded portfolio returns. Several factors contributed to these high costs:

- **Frequent Rebalancing:** The trading strategy's frequent portfolio adjustments (every 5 days) likely resulted in substantial transaction costs, significantly impacting profitability. The dissertation should quantify this impact and explore potential mitigation strategies, such as increasing the rebalancing period.
- **Lack of Transaction Cost Modeling:** The backtesting framework likely underestimated real-world transaction costs, leading to inflated performance expectations. The dissertation should acknowledge this limitation and propose methods for incorporating more realistic friction into future backtests.
- **Uniform Weighting:** The strategy's uniform weighting within deciles might have amplified the impact of trading costs, particularly for less liquid assets. The dissertation should analyze this and explore alternative weighting schemes, such as market-cap or volatility weighting. Potential backtests of these alternative schemes should be included.
- **Short-Selling Constraints:** Restrictions on short selling, especially in the small-cap segment, likely hindered performance. The dissertation should analyze the implications of these constraints and potentially compare the performance of strategies with and without short-selling capabilities.

Dissertation Structure and Content: Performance, Refinement, and Enhancements

This section outlines key aspects of the dissertation, focusing on performance analysis, refinement of the trading strategy, and the implementation of key enhancements.

**Performance Analysis:**

A key finding is the +8.89% annual alpha achieved after transaction costs when applying the CNN model to the First North All-Share (small-cap) index using the OMXS All-Share model. This represents a significant +37.57% annual outperformance compared to the benchmark return of -27.88%. However, a more nuanced analysis across all portfolios reveals that while six of eight achieved positive alpha _before_ costs, most showed negative or marginally positive alpha _after_ costs. This discrepancy highlights trading inefficiencies despite a valid predictive signal, stemming from high turnover, uniform weighting, and the inclusion of low-confidence predictions due to the lack of smart trade filtering.

**Trading Strategy Refinement and Enhancements:**

The dissertation should address the following refinements and enhancements to the trading strategy:

- **Detailed Performance Breakdown:** Include a deeper analysis of alpha broken down by long vs. short positions and a table of per-index Sharpe ratios before and after transaction costs for a comprehensive performance overview.
- **Addressing Trading Inefficiencies:** Discuss the current strategy's shortcomings (high turnover, uniform weighting, lack of smart trade filtering) and propose modifications to mitigate these, such as increasing the rebalancing period and incorporating alternative weighting schemes.
- **Short-Selling Constraints:** Further analyze the impact of short-selling constraints, particularly in the small-cap segment, acknowledging the likely infeasibility of some short positions due to limited borrow availability. This is particularly relevant given the identified profitability of the small-cap segment.
- **Handling Unsuccessful Trades:** Clarify how unsuccessful trades (where outcomes contradicted predictions) are identified and handled in the absence of a stop-loss mechanism in the referenced paper.
- **Stop-Loss Mechanism:** Detail the implementation and impact of a stop-loss mechanism to mitigate losses. This includes discussing the rationale, providing backtesting results with various stop-loss levels, and describing the prototype module developed for testing and refinement. An example implementation could trigger an exit if a predicted +3.0% return results in a -2.5% drop within two days.
- **Prediction Confidence Thresholding:** Explain the implementation of a filter that executes trades selectively based on prediction confidence, either using the magnitude of predicted returns or Historical Prediction Error Profiling (HPEP). This addresses the previous limitation of treating all trades equally. Include backtesting results from a prototype dynamic trade filtering layer.
- **Risk-Based Weighting:** Discuss the development and performance of a risk-based weighting scheme. This scheme should incorporate factors like prediction confidence, inverse historical volatility, and the signal-to-noise ratio of predicted versus actual returns during validation, allowing for a more sophisticated approach to portfolio management.

By addressing these enhancements and refinements, the dissertation will demonstrate advancements beyond the original model's limitations. The final dissertation should be carefully structured and titled to effectively communicate these findings.
This section focuses on structuring and writing the dissertation, ensuring a cohesive narrative that integrates the research, development, and findings related to predicting "rally time" in algorithmic trading. The following key areas should be clearly articulated:

**Dataset and Labeling Methodology:** Thoroughly document the process of generating the labeled dataset for rally-time prediction. This includes specifying the data sources, detailing the methodology for labeling rally time (scanning forward from time _t_ to _t+N_ to find the smallest _k_ where `Close[t+k] ≥ Close[t] + predicted_return`), and explaining how cases where the target price isn't reached within _N_ days are handled (_k = N+1_). Justify the rationale behind this approach and discuss any preprocessing steps undertaken.

**Model Architecture and Justification:** Provide a meticulous description of the multi-head neural network architecture. Detail the CNN architecture, including the rationale for using EfficientNet features and the structure of the two heads: one for return regression and the other for rally-time prediction (predicting the number of candlesticks). Justify the choice of a weighted sum of MSE losses for each head. Explore alternative approaches, specifically survival analysis models (e.g., DeepSurv, DeepHit, Weibull Time-To-Event models), highlighting their advantages in handling censored data and expressing uncertainty over time.

**Trade Decision Logic and Integration of Rally Time:** Explain how the predicted rally time is integrated into the trade decision logic. Describe how this information influences dynamic capital allocation, position sizing based on the predicted time premium, and the implementation of early exit strategies if the rally fails to materialize within the anticipated timeframe. Clearly articulate the enhancements made to the trading logic to leverage rally-time predictions.

**Backtesting, Performance Evaluation, and Risk Management:** Describe the backtesting methodology and present a comprehensive performance evaluation, including metrics like cumulative return, Sharpe ratio, alpha vs. a benchmark, and accuracy/F1-score. Analyze the impact of transaction costs and short-selling constraints, particularly on small-cap stock performance. Explain how unsuccessful trades are handled within the backtesting framework and describe the implementation and impact of risk management techniques, such as stop-loss orders and risk-based weighting. Specifically detail the implementation of volatility-aware exit thresholds based on the Average True Range (ATR), explaining how ATR is calculated and how the exit tolerance is scaled.

**Algorithmic Exit Strategies and Learning from Unsuccessful Trades:** Discuss the research conducted on algorithmic exit strategies beyond a simple hardcoded stop-loss. Evaluate their potential effectiveness and justify the chosen strategy. Explain the mechanisms developed to learn from losing trades and how these learnings are integrated back into the trading model, including any adjustments to model parameters, decision rules, or incorporation of new data points. Evaluate the effectiveness of this learning mechanism.

**Prediction Targets and Rationale:** Explain the rationale behind choosing the specific prediction targets (return magnitude and rally time) and their relationship to developing a trading strategy.

**Refinement of Dissertation Title:** Finally, ensure the dissertation title accurately and concisely reflects the core contributions and findings of this research.

### A. Dissertation Writing (Continuity)

This section outlines the structure and content of the dissertation, ensuring a cohesive narrative that integrates the project's key findings and architectural decisions. The focus is on clearly articulating the model's learning process, particularly its handling of errors and leveraging sequential data.

**Learning from Mistakes:** A dedicated section will explain the strategies implemented to enhance the model's ability to learn from its errors. This includes:

- **Sample Re-weighting During Retraining:** Cost-sensitive learning will be detailed, explaining how higher loss weights are assigned to misclassified high-loss trades during retraining. This approach prioritizes avoiding costly mistakes, and the rationale behind this strategy will be thoroughly discussed.

- **Bootstrapping of Hard Cases:** The creation of a "hard sample bootcamp" dataset, composed of high-loss samples identified after each training epoch, will be described. This specialized training cycle focuses on correcting the model's weaknesses, enhancing its performance on challenging examples.

- **Meta-Model for Trade Review:** The development and implementation of a meta-learner model will be explored. This model predicts the likelihood of a trade failing before execution by analyzing the initial prediction, features of the chart/trade setup, and the actual outcome. Its role in risk management and improving overall trading performance will be emphasized.

**Architecture and Sequential Data Handling:** The dissertation will clearly explain the model's architecture, emphasizing its handling of sequential image data and incorporation of temporal dependencies.

- **Reward Logic Enhancement:** The implemented reward logic will be detailed, highlighting its consideration of preceding images in the sequence. This nuanced approach, where rewards are determined by sequences rather than individual images, improves the model's ability to learn temporal dependencies.

- **Model Input Adjustment for Sequences:** The adaptation of the model's input layer to handle image sequences will be described. The technical details of this modification will be explained, emphasizing its importance for incorporating temporal dependencies and enabling the model to analyze market trends over time. This will be directly linked to the reward logic enhancement, demonstrating how the architecture supports the learning process.

**Vision Transformer Implementation and Evaluation:** The dissertation will include a detailed discussion of the Vision Transformer (ViT) implementation for sequential candlestick data, covering two explored options: (1) encoding images with EfficientNet and feeding the resulting vectors into the transformer, and (2) using ViT-style patch embedding with positional encoding. The results of the comparison between image subtraction and feature subtraction for delta feature generation within the ViT architecture will be analyzed, justifying the chosen approach. A comprehensive benchmark comparison, contrasting "static picture vs. picture-pair vs. sequence" approaches, will provide empirical evidence supporting the chosen method for incorporating temporal context.

**Dataset Generation and Hybrid Architectures:** The design and implementation of a 2-Image Paired Input Dataset Generator, used to create paired candlestick images for training, will be documented. Furthermore, the implementation and analysis of a CNN + LSTM Hybrid Architecture will be presented, including the rationale for this approach and its performance compared to other architectures.

**Data Model and Feature Engineering:** The dissertation will explain the chosen data model, emphasizing the use of sequential candlestick windows as input for the ViT model. The incorporation of delta features, calculated via image or feature subtraction from sequential windows, will be detailed, along with an analysis of their impact on model performance. The process of creating the dataset of image sequences, using a sliding window approach (t-2, t-1, t), will be thoroughly documented.

Finally, the dissertation title will be refined to accurately and concisely reflect the core contributions of the research.

## Dissertation Writing and Structure

This section details the structure and content of the dissertation, focusing on the Vision Transformer (ViT) model's implementation and the novel approach of predicting candlestick images rather than directly predicting returns. It also emphasizes connecting the experimental work and codebase to the written narrative for a coherent and well-supported final product.

### ViT Architecture, Training, and Implementation

The dissertation should comprehensively address the implementation and experimentation with the ViT model for processing candlestick image sequences.

- **Input Image Sequence Length (N):** The dissertation should justify the chosen input sequence length, starting with a discussion of any limitations on the number of input images the ViT training layer can handle. Clarify whether the use of three images in previous examples was a constraint or a flexible parameter. Document experiments with N=3, 4, and 5, including the rationale for these values, thoroughly analyzing the results and performance comparisons to determine the optimal input sequence length.

- **Positional Embeddings:** Explain the importance of positional embeddings for the ViT model to understand the sequential order of candlestick images. Clearly describe the implementation method, whether developed from scratch or using a pre-trained backbone with positional extension.

- **Handling Variable-Length Input Sequences:** Detail the implementation of masking and padding for dynamic-length input sequences. Specify the maximum sequence length, the padding mechanism using blank chart images, and the mask vector's role in guiding the transformer to ignore padded segments. Explain the chosen method for training the ViT with variable-length sequences, whether masking or dynamic positional encodings, highlighting how this adaptability benefits real-world market data processing.

### Predicting Candlestick Images: A Novel Approach

The dissertation should clearly articulate the shift from predicting numerical return values to predicting future candlestick images, framing it as visual sequence forecasting (image-to-image prediction) with a financial interpretation layer. Describe the process of extracting returns from the predicted images. This section should thoroughly explore the conceptual and theoretical underpinnings of this novel approach.

- **Conceptual Validity:** Compare how human traders visually interpret candlestick patterns with how the model processes and interprets these visual sequences. This comparison forms a central discussion point.

- **Theoretical Advantages:** Analyze and document the potential advantages of image-based prediction, including:

  - **Representation Richness:** Explain how image representation captures complex relationships between OHLCV values more effectively than traditional numerical representations.
  - **Uncertainty Modeling:** Explore the potential for modeling inherent market uncertainty.
  - **Causal Reasoning:** Analyze how predicting visual patterns might offer insights into market-driving factors.
  - **Training Supervision:** Discuss implications for training using image-based targets.
  - **Interpretability:** Address the interpretability of results, potentially using techniques like Grad-CAM or SHAP applied to images.
  - **Generative Flexibility:** Explore the potential for generating synthetic market scenarios for stress testing and model validation.

- **Evaluating the Approach:** Dedicate a section to evaluating the effectiveness of this novel approach. Compare its performance against traditional return prediction methods, critically assessing whether predicting candlestick patterns improves accuracy. Acknowledge and discuss the increased coding and maintenance complexity.

### Data Generation and Backtesting

- **Label Generation and Image Sequence Generation:** Clearly explain the generation of labels (return value, rally time, signal class) corresponding to time steps t-1 and t. Describe the functionality of the custom image sequence generator tool and how it implements the sliding window approach.

- **Backtesting Framework:** Detail the design and implementation of the backtesting framework, emphasizing its role in simulating real market conditions. Explain how the framework handles trading logic, order execution, and portfolio management to assess the trading strategy's viability.

### Reusable Components and Finalization

Beyond architectural choices, document the reusable components developed (data loaders, masking logic, ViT wrappers) to enhance research reproducibility and contribute to future work. Clearly explain and demonstrate their usage. The dissertation should also clearly state its final structure, refined title, and chosen architecture and training methodology, justifying these choices based on experimental findings.

## Dissertation Writing and Refinement

This section details the structure and content of the dissertation, emphasizing clarity, coherence, and a thorough analysis of the image-based prediction model.

The dissertation should comprehensively cover the following aspects:

- **Model Architecture and Rationale:** Provide a detailed explanation of the chosen architecture (CNN decoder or transformer-based generator like ViT) for the image-to-image candlestick forecaster, clearly justifying the selection. (Refer to "Prototype Design for Image-to-Image Candlestick Forecaster")
- **Return Extraction Methodology:** Describe the method for extracting open/high/low/close values and derived returns from the predicted candlestick images. This explanation should cover techniques like analyzing pixel locations or rendering the image data into a usable format. (Refer to "Return Extraction from Predicted Images")
- **Comparative Analysis:** Thoroughly compare the proposed model with existing scalar regression and probabilistic return models. The comparison should encompass output type, supervisory signal, link to trading strategies, richness of learned structure, interpretability, risk of ambiguity, data requirements, and modeling complexity. (Refer to "Comparison with Existing Models")
- **Risk and Drawback Assessment:** Dedicate a section to assessing the inherent risks and drawbacks of image-based prediction. This includes, but is not limited to, challenges in direct evaluation, potential for compounding errors, lack of direct reward supervision, and the ambiguity of financial interpretations. (Refer to "Risk and Drawback Assessment")
- **Experimental Design and Validation:** Provide a comprehensive description of the experimental design, including details on training (ViT or U-Net architecture), post-processing for return extraction, baseline model comparisons, and performance metrics (RMSE, SSIM/LPIPS, backtested profit). (Refer to "Experimental Design for Validation")

**Evaluation and Trading Implications:**

Beyond technical details, the dissertation should address the model's evaluation, purpose, and implications for trading:

- **Evaluation Metrics:** Emphasize that evaluation should not solely rely on financial returns. Visual accuracy in generating plausible candlestick sequences, reflecting real-life patterns, is a crucial aspect and should be a significant component of the evaluation. Justify the chosen metrics and their inclusion.
- **Primary Objective:** Clearly state that accurate prediction of visual candlestick patterns is the primary goal, with maximizing financial returns as a secondary objective. Explain the rationale for this prioritization.
- **Actionable Trading Signals:** Discuss the model's capacity to inform actionable trading decisions. The model should not only identify patterns but also determine the necessity of a trade based on the predicted pattern. Explore the decision-making process and demonstrate how the model provides clear trading signals.
- **Relationship between Visual Accuracy and Financial Performance:** Investigate the correlation between the model's visual predictive accuracy and its actual financial performance. Analyze whether improved visual accuracy translates to increased profitability.
- **Addressing Ambiguity in Financial Interpretation:** Acknowledge and address the potential ambiguity in interpreting realistic candlestick predictions financially. Clarify how these visual predictions translate into concrete trading decisions and subsequent financial outcomes.

**Realism vs. Trading Value:**

The dissertation must distinguish between generating realistic-looking charts and their actual trading value. While the model may produce visually compelling outputs, the dissertation must demonstrate their practical relevance by showing whether they translate into actionable trading opportunities with identifiable entry and exit points. Include a robust experimental protocol comparing the image-based prediction model against traditional scalar return prediction models under various market conditions.

**Dissertation Title:**

The dissertation title should be concise, impactful, and accurately reflect the core research focus: predicting market movements based on visual patterns in candlestick chart images. It should highlight novel aspects like the use of CNNs/ViTs or the dynamic plane implementation. If relevant, the title could also hint at key findings related to causal grounding or the effectiveness of the dual-module framework. For example, if causal grounding is significant, the title could emphasize the model's ability to explain market behavior. The current working title, "Automating Technical Analysis in Intraday Trading by using image snapshots of price action and making predictions using computer vision: a case study of Indian equity markets," requires significant refinement to achieve this. Retain the core concepts of "snapshot" and "computer vision" while making it more concise and impactful.

## Dissertation Writing

This section details the requirements for structuring, writing, and supporting the dissertation. It emphasizes creating a cohesive narrative that effectively communicates the research contributions, particularly the novel dynamic 2D plane representation and its application to financial modeling. The dissertation title should accurately and concisely reflect these core concepts. A finalized and well-documented codebase, including relevant prompts, should be prepared for inclusion as supplementary material.

### Structure and Content

The dissertation must adhere to standard academic formatting guidelines, with clear chapters, abstracts, and proper citations. The structure should include a well-defined introduction, literature review, methodology, results, discussion, and conclusion. Each chapter should have clear headings, subheadings, and appropriate use of figures and tables. Chapter abstracts should summarize key findings, and all sources must be properly cited.

Beyond the standard structure, the dissertation should specifically address the following key areas related to the dynamic 2D plane representation:

- **Introduction and Literature Review:** Provide comprehensive literature framing for the methodological innovations, positioning the research within the existing body of knowledge. This includes reviewing relevant literature on coordinate transformations, manifolds, moving frames, and related concepts. Justify the transition from a fixed 3D Cartesian frame to the dynamic 2D plane, highlighting its potential advantages for financial modeling.

- **Methodology:** Detail the mathematical formulation of the dynamic 2D plane concept, using standard notation and equations. Explain the mechanics of the 2D plane, including its rotational axes and dynamic origin. Formalize the concept as a moving frame on a 1-dimensional differentiable manifold. Describe how curves (e.g., parabolas) are represented within this dynamic chart through axis rotations. Analyze the information balance and degrees of freedom, demonstrating how the 2D representation captures the necessary information despite having fewer explicit dimensions.

- **Technical Research and Discussion:** Explore the theoretical underpinnings of the dynamic 2D plane, drawing parallels to concepts like Frenet frames, tangent planes, parallel transport, affine connections, the SE(3) group, and local inertial frames in general relativity. Discuss the implications of axis rotation on projections within the 2D plane and how this relates to the overall model. Analyze the implications of attaching an orthonormal frame at each point on the manifold and rotating it along the data's trajectory. Explain how this model captures the evolving nature of financial time series data. Discuss the potential practical applications and limitations of this representation in various fields, including robotics, computer graphics, navigation, and data compression, while focusing on its relevance to financial modeling. Compare and contrast the performance of the primary model with a benchmark model utilizing image prediction as a latent feature. This comparison will help contextualize the performance and strengthen the arguments for the utility of image-based prediction in tasks like risk management and opportunity identification.

- **Conclusion:** Summarize the key findings and contributions of the dissertation. Reiterate the significance of the dynamic 2D plane representation and its potential impact on financial modeling. Suggest future research directions.

### Supporting Materials

The following materials are crucial for supporting the dissertation:

- **Codebase:** A finalized and well-documented codebase corresponding to the project is required. Ensure its completeness and relevance to the dissertation's content by including necessary prompts and documentation. This codebase can be included as supplementary material or referenced within the dissertation.

- **Detailed Code Modules:** Well-documented code modules are essential for technical validation. The documentation should enable readers to understand the implementation details and reproduce the results. This can be integrated within the codebase or provided as a separate appendix.

## Dissertation Refinements Based on Project Checklist

This section details how insights from the project checklist can strengthen the dissertation narrative, focusing on dimensionality reduction, moving frames, and analogous explorations within the SCoVA project's financial modeling context.

### Dimensionality Reduction and its Theoretical Foundation

The dissertation must clearly explain the mapping of the three-body problem (or equivalent project concept) to the reduced dimensions of the 2D dynamic frame. This explanation should detail the separate handling of orientation and scaling, and thoroughly discuss the challenges of chaos and complexity inherent in the original problem and their representation within the simplified 2D framework. Connections to any newly discovered periodic orbits (or similar project-relevant discoveries) should be explored. This builds upon the concept of "mapping to reduced dimensions."

Furthermore, the dissertation needs to establish the mathematical and conceptual validity of the chosen dimensionality reduction technique. A thorough exploration of relevant concepts, including coordinate charts, the Frenet-Serret frame, stereographic projections, shape space, moving frames, and the trade-off between information completeness and the introduction of additional orientation parameters, is crucial. This addresses the theoretical underpinnings of abstracting 3D space into a 2D representation. Specifically, the implications of path dependence, singularities, and computational overhead associated with these transformations should be thoroughly examined.

### Moving Frame Implementation Details

If a moving frame (such as the Frenet-Serret frame) is employed, the dissertation must describe its implementation in detail. This includes the representation of 3D curves in a 2D chart using (u,v) coordinates, the parameters defining the chart's position and rotation in 3D space, and any simplifying assumptions (e.g., fixing the center of mass) and their implications.

### Applying Analogous Concepts to the SCoVA Project

While originating from a different domain, several concepts from the checklist can inspire analogous explorations within the financial modeling context of the SCoVA project, enriching the analysis and discussion within the dissertation:

- **Abstraction Trade-offs:** The dissertation should discuss the advantages and disadvantages of the SCoVA model's abstractions (e.g., 5-day candlestick windowing, dynamic plane implementation) within specific financial market use cases. Consider different market regimes (bull/bear/sideways) and asset classes. Explore scenarios where specific model components might become redundant and the impact of including or excluding specific data points (e.g., trading volume, moving averages).

- **Model Robustness and Edge Cases:** Explore potential vulnerabilities or edge cases in the SCoVA model, focusing on scenarios where predictions might become unstable or unreliable, such as during periods of extreme market volatility (analogous to "chaos").

- **Model Generalizability and Consistency:** Demonstrate the SCoVA model's ability to navigate diverse market conditions and generate consistent results, highlighting its robustness and generalizability.

- **Computational Cost Analysis:** Analyze the computational cost of the SCoVA model, comparing the expenses of different components, particularly the dynamic plane implementation and the use of CNNs, LSTMs, or ViTs. Consider memory usage and processing time, justifying the chosen approach based on its efficiency and scalability.

- **Relating Model Behavior to Human Trading:** Discuss how the SCoVA model's decision-making process relates to human trading behavior. Consider whether the model captures aspects of human intuition or follows a purely data-driven approach.

By incorporating these analogous explorations and framing them within the context of financial modeling, the dissertation can provide a more comprehensive analysis of the SCoVA model's strengths and weaknesses, enhancing its overall contribution to the field. Future work could involve formulating an explicit rotation law for the dynamic plane, investigating relevant curvature invariants, and exploring generalizations to arbitrary surfaces and applications to the three-body problem using techniques like pairwise relative coordinates, barycentric frames, or shape space representations. This future work should consider recent advances, including machine learning approximations and newly discovered periodic solutions. Finally, the dissertation title should be refined to accurately reflect these core contributions and attract readership.

## A. Dissertation Writing (Continuity)

This section details the visualization requirements for the dissertation, focusing on conveying the relationship between 3D helical motion and its 2D representation within a moving frame of reference. All visualizations must be generated using `matplotlib` and `numpy`; the use of `seaborn` is prohibited.

**Visualization Requirements:**

- **3D Helix:** A standalone figure depicting a 3D helix with a radius of 1.0, a pitch of 0.5, and 4 turns. This figure should be titled "3D Helix in Laboratory Coordinates" and include labeled X, Y, and Z axes.

- **2D Unfolded Representation:** A separate figure illustrating the 2D unfolded representation of a straight line trace on the 3D helix. This visualization should clearly demonstrate how the straight line in 3D corresponds to a straight line in the 2D moving frame representation. Enhancements to improve clarity, such as redesigned visuals, tooltips, annotations, or supplementary materials, are strongly encouraged.

- **Dynamic Projection Integration (Trading Model Context):** The dissertation must clearly articulate how the dynamic 2D projection integrates with the trading model. Specifically, it should describe how the introduction of new candlestick data influences the re-centering and rotation of the feature space before generating the next prediction.

- **Rotation Mechanism:** A detailed explanation of the mathematical transformation used for "rotation" is required. The dissertation should specify and mathematically describe the chosen method (e.g., affine transform, PCA rotation, learned rotation). If PCA rotation is used, its justification and its impact on prioritizing relative local movement should be discussed.

- **Dynamic Projection Goal:** The dissertation must explicitly state the objective of the dynamic projection. Is it to achieve prediction invariance to prior trend direction or to prioritize relative local movement over absolute position?

**Future Considerations (Not Required for Initial Submission):**

- **Orientation Evolution:** Visualizing the evolution of orientation over time using rotation matrices or quaternion frames could be a valuable addition to future iterations of the work. This visualization should also adhere to the single chart per figure constraint.

**Dissertation Structure and Title:**

The dissertation writing process will be iterative, with the structure and content evolving alongside the research findings. The dissertation title should be continuously refined to accurately reflect the core focus and contributions of the work.

Movement over absolute position is emphasized through the use of Principal Component Analysis (PCA). PCA allows the model to focus on changes in position by identifying the principal axes of variation in the data. These axes represent the directions of greatest variance in the price movements, effectively capturing the relative changes in price over time rather than their absolute coordinates.

If dynamic PCA is integrated into the Vision Transformer or CNN pipeline, its design and implementation must be thoroughly documented. This includes a clear description of the process of dynamically rotating and re-centering candlestick image sequences using PCA at each step. This dynamic approach allows the model to operate within a locally optimized frame of reference based on the latest movement, further emphasizing relative (local) price behavior.

The dissertation must clearly articulate several key technical aspects:

- **Rotational Axis Paradigm:** The implementation of the rotational axis paradigm requires detailed explanation, particularly concerning the integration and behavior of the rotating axes at each refocus point.

- **Dynamic PCA Risks and Challenges:** A critical analysis of potential risks and challenges associated with dynamic PCA is necessary. This should include discussion of overfitting to noise within small windows, computational overhead, integration complexity, loss of absolute reference frame, potential instability and jitter, model dependency on the transformation, the complexity of explaining the dynamic PCA approach, edge cases, and ensuring fair comparison to baseline models.

- **Baseline and Comparison Strategy:** A clear baseline and comparison strategy is crucial for quantifying the benefits of dynamic PCA refocusing. Comparisons should include models trained without PCA, with static PCA, and with simpler transformations, as well as alternative data-driven focus methods. An ablation study comparing performance with PCA without re-centering and vice-versa should also be included.

- **Computational Cost of PCA:** The computational burden of adding PCA to the image processing pipeline must be addressed. Any optimizations or alternative dimensionality reduction techniques employed to mitigate performance issues should be explored and justified.

- **Image Input Justification:** The requirement for image-based input must be explicitly stated and justified. The rationale for choosing a specific image type (e.g., candlestick images, Heiken Ashi charts) should be clearly articulated.

Dynamic Snapshot Generation, a key component of the system, must be thoroughly described. This process generates images after the rotation and re-centering of the coordinate system, redrawing the candlesticks relative to the new X' and Y' axes, where time is no longer strictly horizontal. These dynamically generated images serve as input to the chosen model (CNN or Vision Transformer).

The following technical challenges and their solutions should be addressed:

- **Rotation Artifacts:** Explain the methods used to minimize distortions introduced by rotation, such as anti-aliasing or other interpolation techniques.

- **Volatility Jump Handling:** Detail how the system handles abrupt price movements and mitigates their effect on frame rotations, potentially through smoothing or limiting rotation angle changes.

- **Consistent Axis Scaling:** Explain how consistent axis scaling (units per % move) is maintained across all generated frames and its importance for consistent model training.

A clear and concise pseudocode representation of the dynamic rotating snapshot generator should be included to aid understanding and reproducibility.

Furthermore, the dissertation should incorporate:

- **Conceptual Diagram:** A visual representation of the dynamic rotation operation on a sequence of price movements, illustrating the two rotational axes and how they capture price and time changes.

- **Simplified 2D Representation Rationale:** Justify the reduction of the three-dimensional system (time, price, volume) to a dynamic two-dimensional plane with two rotational axes, emphasizing the improved computational efficiency while preserving the essence of price movement.

- **Dynamic Plane Redrawing:** Detail how the 2D plane and its axes are redrawn and reoriented with each price or time change.

- **Dynamic Origin Refocusing:** Clearly describe the dynamic origin refocusing, explaining how the origin shifts to the new data point after each price and/or time change. A clear example, such as the movement of a "unit dot," should be used for illustration.

- **Rotating Snapshot Generator Module:** Design and document a `RotatingSnapshotGenerator` module encapsulating the logic for generating the dynamic 2D plane representations.

## Dissertation Writing

This section details the structure and content of the dissertation, focusing on the Rotating Dynamic Plane Generator and its integration within the Vision Transformer (ViT) pipeline for stock market prediction. The dissertation will articulate the project's goals, methodology, results, and conclusions, incorporating visual aids and data representations effectively.

### Dynamic Plane Implementation

The dissertation will provide a comprehensive explanation of the dynamic plane implementation, enabling readers to reproduce and understand its functionality. Key aspects to address include:

- **Re-evaluation of Angle Theta Calculation:** The dissertation will critique the initial calculation of angle theta based on simple price difference and justify the revised approach within the 2D rotational plane. The new method, considering the dynamic origin and rotational axes, will be thoroughly explained, highlighting its advantages and addressing the limitations of the previous method.

- **Dynamic Redrawing of the Coordinate System:** The dissertation will elucidate the core concept of dynamically redrawing the coordinate system at each time step. It will detail how the axes of the 2D system are derived from the principal components of local price movements, emphasizing the blending of time, price, and potentially volume into a new local frame. This dynamic approach will be contrasted with simpler methods (e.g., trend-based rotation, PCA on windowed price variance), highlighting its advantages in capturing market dynamics.

- **Local Frame Definition using PCA:** The application of Principal Component Analysis (PCA) to define the local frame of reference will be meticulously described. This includes the process of defining movement vectors in time-price-volume space within a local window of previous candles and using the top two principal components to form the dynamic plane's axes. A mathematical description of this process will be provided.

- **Pseudocode and Conceptual Diagram:** For clarity, the dissertation will include pseudocode for the dynamic plane generator algorithm, covering data input, local window definition, movement vector calculation, PCA application, coordinate system rotation, and 2D plane reconstruction. A conceptual diagram will illustrate the dynamic 2D plane's evolution and shifts with market movements, enhancing understanding of the dynamic coordinate system.

### Rotating Dynamic Plane Generator

The dissertation will provide a detailed explanation and justification of the novel Rotating Dynamic Plane Generator. Key aspects include:

- **Input for Vision Transformer/CNN:** The dissertation will clearly articulate how the 2D plane generated serves as input to the ViT or CNN. It will specify the input format (image or numerical features) and justify this choice.

- **Pseudocode, Implementation, and Optimization:** The dissertation will include complete pseudocode for the Rotating Dynamic Plane Generator, detailing candlestick data handling (time, price, volume), the rotation process, and visualization integration. The Python implementation using libraries like NumPy, Matplotlib, and PIL will be documented, including any deviations from the initial pseudocode. Furthermore, the optimization process for efficient batch processing of entire time series will be described, emphasizing the chosen methods and their impact on performance. The dissertation will explicitly detail the generator's functionalities: window selection, movement calculation, dynamic frame construction (potentially using PCA), rotation and refocusing, and snapshot rendering.

### Experimental Design and Results

The dissertation will thoroughly document the experimental design and results obtained using the Rotating Dynamic Plane Generator and ViT. Key aspects to address include:

- **Dynamic Candlestick Snapshots for ViT:** The rationale behind using dynamic candlestick snapshots as input for the ViT will be explained, emphasizing their design choices and effectiveness in capturing relevant market information.

- **Integration with ViT Pipeline:** The integration of the Rotating Dynamic Plane Generator within the ViT training pipeline will be described in detail, justifying its inclusion and explaining the technical implementation.

- **Static vs. Dynamic Frames Comparison:** A comparative analysis of model performance using static candlestick frames versus dynamic rotating frames will form a core component of the dissertation. The experimental setup, results, and analysis will be meticulously documented.

- **Addressing Functional Requirements:** The dissertation will discuss how the functional requirements of the generator were addressed, specifically: the determination and impact of the window size parameter, and the inclusion and influence of volume as a third dimension in movement calculations.

By thoroughly documenting these aspects, the dissertation aims to effectively communicate the research process, design decisions, and experimental findings, ultimately contributing to the reproducibility of the research and providing valuable insights for future work.
Visualizations are crucial for effectively communicating the functionality and benefits of the dynamic plane generator. The dissertation should include:

- **Example Candlestick Charts:** Several examples of raw candlestick charts, as input to the model, should be presented to establish a baseline for comparison.
- **Transformed Candlestick Charts:** Examples of candlestick data after dynamic plane transformation should be shown to demonstrate the effect of origin refocusing and principal axis rotation. Comparing pre- and post-transformation images will highlight the core contribution of the dynamic plane approach.
- **Dynamic Plane Evolution Animation:** An animation visualizing the step-by-step evolution of the dynamic plane as new data points are introduced is essential. This animation should clearly depict how the plane redraws and its axes shift in response to incoming data, illustrating the dynamic nature of the approach.

The dissertation's methodology section should mention the technical constraint of the dynamic plane generator requiring at least two data points for computation. Additionally, the following points should be incorporated into the dissertation:

- **Smoothing Techniques:** The impact of smoothing techniques, such as Heikin-Ashi, on the visualization and subsequent model performance should be explored and discussed.
- **Animation of Plane Evolution:** The animation of the plane’s evolution is a powerful visual aid in explaining the dynamic adaptation of the model to incoming data, showing how the plane's orientation shifts with new information. Any issues encountered with animation initialization, particularly with limited initial data, and their solutions should be documented.
- **Impact of Data Model Enhancements:** The dissertation should explain how simulating longer sequences and using batch generation during training impact model performance, particularly with CNNs and ViTs. Discuss the challenges and benefits of extended data sequences and how batch generation facilitates efficient training. Highlighting the ability to handle longer sequences emphasizes the model's capacity to capture longer-term dependencies in financial time series data.
- **Architectural Decision for Minimum Rotation Points:** Justify the decision to initiate plane rotation only after three stable points, explaining how this choice addresses PCA instability and ensures robust calculations.
- **Rotation Matrix Smoothing:** Detail the techniques implemented for smoothing or stabilizing rotation matrices. Explain the purpose of this smoothing—preventing dimension blowups and ensuring smooth transitions in the visualization—and describe the specific methods and their effectiveness.
- **Standalone Animation Simulator:** Document the development and functionality of the standalone animation simulator, including its features (delayed rotation, smoothing of early plane formation, and step-by-step visualization) and its role in understanding and presenting the model's behavior.
- **Dynamic Point Movement and Live Frame Rotation:** Clearly articulate how the dynamic movement of points and live frame rotation are visualized, emphasizing their importance in showcasing the real-time rotation and re-centering and their contribution to interpreting the model's outputs and analysis.
  This section details the structure and content of the dissertation, focusing on integrating experimental findings, particularly those related to complex price patterns and chaotic market conditions.

**Simulations and Visualizations**

The dissertation must thoroughly document the following simulations and visualizations:

- **Complex Price Simulations:** Detail the implementation and results of realistic price simulations, including rallies, drops, and recovery phases, demonstrating the model's robustness under varied market behavior.
- **Chaotic/Choppy Market Simulations:** Analyze the model's performance under chaotic and choppy sideways market conditions, focusing on how the dynamic plane handles extreme volatility and rapid price fluctuations. Highlight any insights into the model's limitations or strengths under such circumstances.

These simulations rely on the following data generation and visualization techniques, which should be adequately documented:

- **Choppy Candlestick Data Generation:** Explain the `generate_choppy_candlesticks(n=30)` function, including its parameters, rationale, and method for generating random price fluctuations.
- **Standard Heiken-Ashi Charting:** Document the creation of the standard Heiken-Ashi chart using the `generate_heiken_ashi` and `plot_heiken_ashi_candlestick` functions. Specify the chart's saved location ('/mnt/data/standard_heiken_ashi_choppy.png').
- **Rotated Dynamic Heiken-Ashi Charting:** Detail the rotation and re-centering of Heiken-Ashi data using the `dynamic_rotate_recenter_heiken` and `plot_rotated_heiken` functions. Specify the saved chart's location ('/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png').

**Market Regime Analysis and Visualization**

The dissertation should include a comprehensive analysis and visualization of market regimes within the dynamic plane:

- **Market Regime Analysis:** Analyze how trend, reversal, and sideways market regimes are represented within the dynamic plane, exploring their distinct characteristics and summarizing the findings concisely for inclusion in relevant chapters.
- **Market Regime Visualization:** Create a clear visualization of these three market regimes within the dynamic plane, maintaining consistency with existing examples.
- **Comparative Chart Visualization:** Provide a side-by-side visual comparison of standard Heiken-Ashi and rotated Dynamic Plane charts across three distinct market regimes: Trend-Reversal-Recovery, Choppy Sideways, and a third regime (e.g., a strong linear uptrend or a sharp V-shaped recovery). Highlight the differences in information conveyed by each chart type for each regime. Note that due to previous matplotlib issues, charts will need manual replotting onto a subplot grid.
- **Third Regime Simulation:** Simulate a third market regime, distinct from Trend-Reversal-Recovery and Choppy Sideways (e.g., a strong linear uptrend or a sharp V-shaped recovery), for use in the comparative visualization.
- **Combined Visualization Panel:** Combine the visualizations of all three regimes into a single, cohesive panel for easier comparison and enhanced presentation.

**Model Architecture and Behavior**

The dissertation should address the following aspects of the model's architecture and behavior:

- **Relational Model Design:** Emphasize the model's ability to learn relationships within data, focusing on pattern identification in the dynamic coordinate system generated by PCA, rather than absolute feature values.
- **Geometric Pattern Recognition:** Explain the model's recognition of geometric shapes and flows within the normalized PCA space. Contrast this with traditional technical indicators and their limitations in this context.
- **Interpretability Projection:** Detail the mechanism for projecting the model's focus back into the original Time-Price-Volume space to provide understandable insights.
- **Window Size and Smoothing:** Discuss the control of the PCA window size and the implementation of smoothing/stability thresholds. Explain how these mitigate noise-driven rotations and the potential for overfitting. Justify the chosen parameters.
- **PCA's Impact on Feature Representation:** Analyze PCA's impact on the numerical features (Price, Time, Volume) of the input images, describing how PCA affects feature representation and how these representations vary with different input patterns. Explain the rationale for using PCA and its effect on model performance.

**Overall Dissertation Structure**

The dissertation should cohesively document the entire research process, including background, methodology, experimental setup, results, discussion, and conclusions. Clear and concise writing is crucial for effectively communicating these complex technical concepts. The dissertation's title should accurately reflect these core elements.

### A. Dissertation Writing (Continuity)

This section focuses on integrating the core algorithmic enhancements and analytical findings into the dissertation narrative. While the checklist doesn't explicitly address dissertation structure, the technical details it contains are crucial for a comprehensive and rigorous presentation of the research. These details should be woven into the relevant sections of the dissertation, such as Methodology, Results, and Discussion.

Specifically, the dissertation should thoroughly document and analyze the following:

- **Error Signal Integration within the Dynamic Rotating Plane Algorithm:** Detail the implementation and impact of the error signal mechanism. This mechanism, analogous to human movement correction based on sensory feedback, allows the model to adjust its perception of market structure based on the difference between predicted and actual market movements. Include a visual representation (e.g., a schematic) to clarify how the error signal influences dynamic plane transformations and subsequent predictions. Discuss how this mechanism contributes to improved prediction accuracy and market responsiveness.

- **Frame Confidence Correction Based on Prediction Comparison:** Explain how comparing predicted and actual market movements informs corrections to the rotational frame assumptions. This process allows the model to learn the reliability of its dynamic plane projections and adapt future rotations based on identified prediction errors. Emphasize how this adaptive approach enhances the model's robustness.

- **Prediction Error Memory for Adaptive Rotation Weighting:** Describe the prediction error memory mechanism, which maintains a rolling history of prediction errors. Explain how detecting consistent misalignments between PCA frame rotations and realized market structure allows the model to adjust the influence of rotation, making dynamic plane stability adaptive to its success rate. Quantify the impact of this adaptive weighting on overall model performance.

- **Feedback-Driven Frame Smoothing for Noise Mitigation:** Elucidate the feedback-driven frame smoothing technique, which slows down the dynamic plane's rotation during periods of high prediction error. This effectively increases smoothing and makes dynamic frames more conservative during market volatility or noise. Analyze the trade-off between responsiveness and stability introduced by this technique.

- **Dual-Frame Estimation for Balanced Prediction:** Document the implementation of dual-frame estimation, which maintains "optimistic" and "stable" local frames. Explain how predictions are dynamically weighted between these frames based on observed market consistency, balancing rapid updates and cautious corrections. Provide empirical evidence demonstrating the effectiveness of this balanced approach.

- **Mitigating Plateau in Dual Records:** ( _This point was incomplete in the original. Expand on the nature of the "plateau" and the strategies employed to mitigate it. Provide context and connect it to the overall algorithmic framework._) For example, describe the specific challenges encountered when both the optimistic and stable frames reach a plateau in predictive performance and how the model adapts to overcome this stagnation. Discuss any implemented mechanisms for detecting and escaping such plateaus, such as introducing exploratory perturbations or adjusting frame weighting parameters.

By thoroughly integrating these algorithmic enhancements and their associated analyses, the dissertation will effectively communicate the model's ability to learn from its predictions and adapt to changing market conditions, strengthening the overall contribution and novelty of the research.
This section focuses on structuring, writing, and refining the dissertation. While the previous checklist excerpt highlighted important technical details for inclusion, this section addresses the overall organization and presentation of the dissertation itself. The technical concepts from the checklist should be integrated into the relevant sections outlined below, ensuring a cohesive and comprehensive presentation of the research.

The dissertation should be structured logically, building a clear narrative from introduction to conclusion. The following structure is recommended:

- **Introduction:** Provide the necessary background and context for the research. Clearly state the research problem, specifically addressing the challenge of dual records leading to performance plateaus in trading agents. Define the issue of long-term records lagging and settling on static weights despite changing market conditions. Briefly introduce the proposed solution and outline the dissertation's structure.

- **Literature Review:** Thoroughly review existing literature related to trading agents, performance plateaus, dynamic adaptation, and relevant machine learning techniques. Position the research within the existing body of knowledge, highlighting the novelty and contributions of the proposed approach.

- **Methodology:** Provide a detailed explanation of the developed trading agent. This section should incorporate the technical details from the checklist, including:

  - **Frame Coincidence Correction with Rolling Rewiring:** Explain the rationale, implementation, and impact of this mechanism, drawing parallels to wound healing. Detail the rolling frame correction algorithm applied to the dynamic PCA frame.
  - **Prediction Error Buffer:** Specify the buffer size and justification. Define the error calculation method.
  - **Error Trend Detector:** Detail the chosen method, including its consideration of price, time, and volume, and justify its selection over simpler approaches like rolling means and variance. Explain the threshold selection for triggering the correction mode.
  - **Frame Correction and Healing Phase:** Describe the corrective actions (rotations, damping) and the gradual return to the normal operational state as errors subside.
  - **Robust Code Implementation:** Describe the transition from pseudocode to functional code, addressing any challenges encountered.
  - **Enhanced Error Trend Detector and Dynamic 2D Frame Analysis:** Explain the use of the dynamic 2D frame, detailing how deviations from expected relational movement are evaluated, incorporating both distance and angular error.
  - **Deviation Vector Monitoring and Calculation:** Define and explain the calculation of the deviation vector within the dynamic 2D plane, linking it to the model's predictions and outcomes.
  - **Angular Error Tracking and Calculation:** Provide the formula for calculating angular error (arccosine of the dot product of the vectors divided by the product of their magnitudes) with illustrative examples. Relate this to model performance and accuracy.
  - **Composite Error Score:** Detail the development and application of the composite error score.
  - Data acquisition and preprocessing steps.
  - Model architecture and training process.
  - Evaluation metrics used to assess the agent's performance.

- **Results:** Present the findings of the experiments, using visualizations, tables, and statistical analysis to demonstrate the effectiveness of the proposed approach. Compare the performance of the developed agent with relevant benchmarks and alternative methods.

- **Discussion:** Interpret the results and analyze their implications. Discuss the strengths and limitations of the proposed approach. Connect the findings back to the research problem and the initial hypotheses.

- **Conclusion:** Summarize the key contributions of the research, emphasizing the novel aspects of the developed trading agent and its ability to address the problem of performance plateaus. Suggest potential future research directions.

- **Dissertation Title Refinement:** Ensure the final title is concise, informative, and accurately reflects the core contributions and findings of the research. It should highlight the key aspects of the developed approach, such as the dynamic frame correction and error trend detection mechanisms.

## Dissertation Writing: Integrating Technical Details and Ensuring Clarity

This section focuses on structuring and writing the dissertation, ensuring clear communication of the technical details and theoretical underpinnings of the SCoVA project. A strong emphasis should be placed on clearly explaining the dynamic plane implementation, error calculations, and potential advancements. Several key aspects need to be integrated into the dissertation for completeness and clarity:

**Dynamic Plane Implementation and Error Analysis:**

- **Rotational Angles and Distance Vectors:** The dissertation must clearly explain the implementation details of the dynamic plane, including the precise number and purpose of rotational angles and distance vectors used. Justify the chosen number and resolve any discrepancies between the implementation and the dissertation's description. A thorough review, potentially including confirmation with collaborators, is recommended.

- **Error Correction Mechanism:** Provide a comprehensive explanation of the error correction mechanism within the dynamic 2D plane. Define and calculate the two error values:

  - _Distance Error:_ The magnitude difference between the predicted and realized displacement vectors.
  - _Angle Error:_ The orientation difference between the predicted and realized direction vectors.
    Explicitly state and justify the decision to exclude the initial frame creation rotation from error correction.

- **Composite Error Score and Weighting:** Thoroughly explain the composite error score, which combines distance and angular error using tunable weighting factors (alpha and beta). Discuss its impact on model performance and the rationale for the chosen weighting factors.

- **PCA Plane Consistency:** Address how the consistency of the 2D plane derived from PCA is maintained between predicted and actual data. Explain how potential differences in price and volume, which might affect the PCA axes, are handled. Detail any necessary transformations or alignments to ensure comparability.

- **Freezing the Dynamic Frame:** Clearly explain and justify the decision to freeze the dynamic PCA frame at time 't' for predictions over a short future horizon (e.g., 1-3 candles). Discuss the assumption of a stable market structure during this short interval and how it avoids recomputing PCA at 't+1' for error calculation.

- **Reprojecting Realized Movement:** Describe the process of reprojecting the realized movement at 't+1' back into the PCA frame established at 't'. Explain the use of the original PCA basis (rotation matrix) for this reprojection, enabling comparison within the initial prediction frame.

**Visualizations and Pseudocode:**

- **Rotations and Prediction Error:** Include a diagram illustrating the two layers of rotation: global frame transformation (PCA) and local vector misalignment. Visually connect these rotations to the concept of prediction error, clarifying the relationship between rotations, predictions, and the resulting error calculations.

- **Frame Alignment Simulation:** Include a visual simulation demonstrating the alignment of predictions and realized movements within the dynamic PCA frame. This visual should clearly illustrate the "freeze and compare" technique.

- **Freeze Frame and Projection Visualizations:** Provide clear visual examples demonstrating the "freeze frame" and "projection" functionalities. Highlight the differences between these methods and traditional distance/angular error calculations based on a static PCA plane.

- **Freeze & Compare Pseudocode:** Include pseudocode detailing the "Freeze & Compare in Dynamic PCA Frame" technique.

- **PCA Basis Data Structure:** Describe the lightweight data structure used to store the PCA basis (rotation matrix).

**Potential Enhancements and Future Directions:**

- **Enhanced Error Trend Detector:** Document the implementation of the enhanced Error Trend Detector based on the provided pseudocode. Explain the pseudocode, integration of the composite error calculation, connection to rolling window analysis, and its advantages over simpler methods.

- **Dynamic Error Correction:** Discuss the potential for expanding the enhanced Error Trend Detector into a dynamic rolling error correction module. Explore theoretical advantages and outline potential implementation strategies.

- **Visualizing Vectorial Misalignments:** Consider a visual simulation demonstrating the accumulation of small vectorial misalignments compared to simple price error. This can enhance understanding of the complexities of modeling price movements and the importance of angular drift.

- **New Loss Functions:** Discuss the potential for new loss functions penalizing both scalar and angular drift. Include drafted loss functions and analyze their theoretical benefits for model training, even if not fully implemented and tested.

By thoroughly addressing these points and incorporating clear visualizations and explanations, the dissertation will effectively communicate the technical complexities and contributions of the SCoVA project. This comprehensive approach will strengthen the overall quality and impact of the research. Remember to refine the dissertation title to accurately reflect these core contributions.

## Dissertation Writing

This section details the structure and content requirements for the dissertation, focusing on clearly articulating the novel "Freeze Frame" and "Reproject Realization" methods for PCA frame management and their associated error calculations. These methods address the challenge of shifting PCA planes, a phenomenon not accounted for in traditional distance/angular error calculations that assume a constant PCA plane.

**PCA Frame Management and Error Calculation:**

The dissertation must thoroughly explain and justify the chosen PCA frame management method (either Freeze Frame or Reproject Realization). The "Freeze Frame" method freezes the rotation matrix (R) at the prediction time, providing a consistent frame of reference. "Reproject Realization," alternatively, applies the original rotation matrix (R) to the realized movement vector, achieving similar consistency. This discussion should emphasize the importance of maintaining relational consistency between predicted and realized data points and preventing model hallucinations due to minor market fluctuations. The impact of shifting PCA planes on error calculation needs detailed explanation, clarifying the shift from calculating error between vectors on a static plane to calculating deviation errors between PCA1 and PCA2 for both real and predicted values.

**Error Metrics and Their Interpretation:**

A comprehensive explanation of the total error calculation is crucial. The total error encompasses both the vector deviation error within the dynamic local frame and the frame shift error (change between PCA axes), represented by: Total Error = Vector Deviation Error + Frame Shift Error. Each component requires clear definition and justification. The method for measuring frame drift error, quantifying the difference between two sets of basis vectors (frames) using principal angles, should also be detailed. In the two-dimensional context of this project, this involves calculating the angle between PCA1 at time _t_ and PCA1 at time _t+1_, and similarly for PCA2. The formula for frame error, Frame Error = α _ Angle between PCA1 vectors + β _ Angle between PCA2 vectors, where α and β are tunable weights, should be included and explained, along with the rationale for the chosen weights.

The dissertation should explore the potential of Frame Drift Error as a confidence indicator for trading decisions (buy/sell/hold). Furthermore, it needs to analyze the weighting of different error components, addressing the rationale for assigning distinct weights to vector deviation error and frame shift error, and the efficacy of the existing alpha and beta parameters within the frame error calculation. The justification for the chosen weighting scheme must be provided. Given the differing units and scales of distance and angular errors, the dissertation must evaluate the current method of direct addition and justify whether normalization or another adjustment is necessary for accurate error aggregation.

**Illustrative Examples and Pseudocode:**

Numerical examples showcasing predicted vs. realized movement values and associated angle/distance error calculations within the frozen frame are necessary. Visualizations, such as plots illustrating prediction and reality paths in both frozen and shifted frames, will enhance understanding. A clear, concise, small-scale simulation demonstrating vector deviation and PCA frame drift should be included. Pseudocode for the "Freeze and Correct" module and the total error calculation, incorporating both vector error and frame shift error, should be provided (likely in an appendix) for clarity and reproducibility.

**Weighted Error Calculation Formula:**

The dissertation should provide a detailed explanation of the weighted error calculation, including the formula:

Vector Error = α₁⋅d<sub>vec</sub> + α₂⋅θ<sub>vec</sub>
Frame Shift Error = β₁⋅θ<sub>PCA1</sub> + β₂⋅θ<sub>PCA2</sub>
Total Error = γ₁⋅Vector Error + γ₂⋅Frame Shift Error

Where:

- d<sub>vec</sub> represents the vector distance error.
- θ<sub>vec</sub> represents the vector angle error.
- θ<sub>PCA1</sub> represents the angle between PCA1 vectors at different times.
- θ<sub>PCA2</sub> represents the angle between PCA2 vectors at different times.
- α₁, α₂ control the trade-off between distance and angle within the frame.
- β₁, β₂ control the trade-off between PCA1 drift and PCA2 drift.
- γ₁, γ₂ control the overall trade-off between prediction error and frame instability.

Each variable should be clearly defined, and the selection of these specific parameters, including the assigned weights and their impact on model performance, must be justified.

Finally, the dissertation title should accurately reflect the core contributions of this work.
The following technical points derived from the error analysis and correction mechanisms of the SCoVA project should be meticulously documented in the dissertation:

- **Normalization of Angle Units:** Clearly explain and justify the normalization process for angle units (e.g., mapping degrees to the range [0, 1]) within the context of the dynamic plane implementation. This normalization simplifies the integration of angular errors with other error components.

- **Pseudocode for Error Computation:** Include formal pseudocode detailing the multi-weight error computation. This unambiguous representation allows readers to understand and potentially reproduce the error calculation process.

- **Default Weights (α, β, γ):** Justify the chosen default values for the α, β, and γ weights used in the error computation. Explain the rationale behind these values, referencing any relevant theory (e.g., from trading or physics) or empirical evidence supporting their selection.

- **Numerical Example:** Provide a concise numerical example demonstrating the calculation of the total error from its individual components. This concrete example will clarify the error computation methodology for the reader.

- **Error Trend Detection and Healing Phase Design:** Detail the design of the Error Trend Detector and its associated Healing Phase. Describe the mechanism for monitoring a rolling window of errors, the criteria for triggering the Healing Phase, the method for applying a correction factor to the PCA frame construction, and the decay mechanism for this correction factor. Emphasize the self-regulating nature of the system and its ability to adapt to dynamic market conditions. Specifically address the transition between correction mode and the healing phase, particularly how error spikes are handled during the healing phase.

Furthermore, the dissertation must integrate the performance-based healing mechanism and its associated data tracking. The following aspects should be clearly articulated:

- **Prediction Correctness Tracking:** Describe the system implemented to track prediction accuracy, explaining the scoring mechanism (+1 for correct predictions, 0 for incorrect predictions, regardless of frame-related issues). This scoring is fundamental to evaluating model performance and driving dynamic decay rate adjustment.

- **Rolling Prediction Correctness Buffer:** Detail the implementation and purpose of a rolling buffer storing the prediction correctness scores (1 or 0) for the last N timesteps. Clearly define N and explain how the buffer is used to calculate the mean prediction correctness, a critical component of the performance-based healing process.

- **True Prediction Value Tracking:** Explain how the algorithm's predicted values are tracked and logged. This data is essential for dynamically adjusting the decay rate and providing empirical evidence for the effectiveness of the performance-based healing approach. Clarify how these values are used in subsequent calculations.

- **Dynamic Decay Rate Adjustment:** Describe the mechanism for dynamically adjusting the decay rate based on the prediction accuracy stored in the rolling buffer. Explain the relationship between consistent correct predictions and the reduction of lag introduced to compensate for previous disruptions. Clearly articulate how this adjustment contributes to the model's overall performance and stability.

- **Performance-Based Healing Implementation:** Elaborate on the performance-based healing concept, emphasizing its reliance on predictive accuracy rather than a simple time-based approach. Explain how the correction factor is dynamically adjusted in proportion to the true predictive recovery using the rolling prediction correctness buffer. Clearly state the absence of arbitrary exponential decay and highlight the dynamic, data-driven nature of this healing mechanism. Provide a comprehensive explanation of how this process enhances the model's resilience and adaptability.

Finally, ensure the dissertation's title accurately reflects the scope and contributions of the research, including the core concepts of error detection, correction mechanisms, and the performance-based healing system. Integrate these technical details into the broader narrative of the dissertation, connecting them to the overall research goals and findings.
The following sections detail the necessary data transformations, modeling techniques, and writing considerations for the dissertation. This includes preparing time, price, and volume data for Principal Component Analysis (PCA), addressing the Healing-by-Correctness system, and ensuring clear and consistent data representation throughout the document.

**Healing-by-Correctness System**

The Healing-by-Correctness system's adaptive correction mechanism, driven by prediction accuracy, should be thoroughly explained and justified. A high rolling prediction correctness (e.g., ≥ 80% over the last N steps) proportionally reduces the correction factor, while deteriorating correctness maintains or increases it.

The dissertation must include a formal definition and explanation of the `dynamic_decay_rate(mean_correctness)` function, which calculates the decay rate of the correction factor. The suggested implementation, `Decay Rate = 1 - (mean_correctness - healing_threshold)`, requires detailed justification, emphasizing how it accelerates decay as correctness surpasses the healing threshold, with rates varying based on performance.

Formal, modular, and well-commented pseudocode for the complete Healing-by-Correctness system, incorporating prediction correctness tracking and dynamic decay rate calculations, is essential for clarity and reproducibility.

A simulated toy example demonstrating the "wound → correction → true healing" flow should be presented. This example should illustrate the system's response to drops in predictive accuracy (the "wound"), the application of corrections, and the eventual achievement of "true healing" through regained accuracy, solidifying the reader's understanding of the system's dynamics.

Finally, the dissertation should discuss the proposed initial healing thresholds (e.g., 75-80% directional correctness), justifying their selection within a relevant trading context and analyzing the potential implications of different threshold values on overall system behavior. This system's connection to the broader research context and its contribution to the dissertation's overall objectives must be clearly articulated.

**Data Preprocessing for Principal Component Analysis (PCA)**

Thorough documentation of the preprocessing steps for price (P), time (T), and volume (V) data before applying PCA is crucial for ensuring the validity and interpretability of results.

- **Normalization:** The rationale for normalizing P, T, and V to a uniform scale before PCA should be explained, emphasizing how this prevents scale-induced bias in the principal components.

- **Z-score Normalization:** Detail the z-score normalization process applied independently to each feature within a rolling window of N data points. Explicitly state the formula used for scaling: centering each feature by subtracting its mean ($μ_t$, $μ_p$, $μ_v$) and dividing by its standard deviation ($σ_t$, $σ_p$, $σ_v$). This ensures equal feature contribution to PCA, regardless of original units.

- **Time Handling:** Discuss the specific considerations for handling timestamps, detailing both suggested approaches: 1) using a relative time index within each window and normalizing it, and 2) calculating normalized time deltas. Explain the benefits and drawbacks of each, particularly the impact of large timestamp gaps when using absolute clock time.

- **Volume Transformation:** Address the heavy-tailed distribution of volume data and justify the chosen transformation. Clearly describe the application of either a log transformation (e.g., $v_i' = log(1 + v_i)$) or robust scaling using the median and interquartile range (IQR), explaining how these mitigate the influence of outliers on PCA.

- **PCA Implementation:** Provide a clear and concise description of the PCA implementation, including the core code snippet for singular value decomposition (SVD) and principal component extraction. Justify the number of principal components used (e.g., the first two). These details should be sufficient for reproducibility.

**Data Representation and Handling**

The dissertation must clearly articulate the chosen methods for normalizing and representing price, volume, and time data. The rationale for encoding time as fractional elapsed time (between 0 and 1 within the window) and transforming price into relative returns (percentage change or log returns, relative to the first price in the window) should be thoroughly explained and justified. This includes highlighting the benefits of anchoring the price and mitigating the impact of extreme price spikes.

The handling of outliers and extreme values for price and volume, including a detailed description of implemented scaling adjustments, must be addressed. This demonstrates an understanding of potential data skewness issues and the chosen mitigations.

Finally, the chosen method for time tracking should be analyzed. Any alternatives considered to a live feed of Last Traded Prices (LTPs) should be discussed alongside their advantages and disadvantages, particularly regarding maintaining chronological context and a robust event record. The implications of the chosen approach for the overall project and results should be thoroughly explored. This ensures transparency and provides context for the chosen methodology. Meticulous documentation of these steps enhances the credibility and impact of the research findings. The dissertation title should accurately reflect the core contributions and scope of the research.
Data Transformation and Preparation for Principal Component Analysis (PCA)

This section details the data transformations required to prepare price and volume data for Principal Component Analysis (PCA). These transformations ensure consistent scaling, emphasize temporal dynamics, and mitigate the impact of outliers, enabling PCA to effectively capture the underlying relationships between time, price returns, and volume.

**Data Normalization and Scaling:**

- **Fractional Elapsed Time:** Time within each window is represented as fractional elapsed time (e.g., the first data point is 0.0, the last is 1.0, and intermediate points are linearly interpolated between these minimum and maximum timestamps). This captures the relative timing of data points and accounts for potential irregularities in data arrival rates. This transformation is crucial for PCA as it emphasizes the temporal dynamics of price and volume changes. To maintain consistency with other normalized features, this fractional time is further transformed to the range of -1 to +1 using the formula (2 \* `time_frac`) - 1.

- **Window-Relative Returns:** Absolute prices are transformed into window-relative returns, expressed as either percentage change or log returns, calculated relative to the first price in the window. This anchors the price series at zero at the beginning of each window, mitigating the influence of absolute price magnitudes on the PCA and allowing for more meaningful comparisons across different stocks and time periods. Focusing on returns captures the inherent volatility and trends within the window. These log returns are then scaled to the -1 to +1 range. The chosen scaling method (e.g., dividing by the maximum absolute value within the window or min-max scaling) will be detailed in the dissertation.

- **Robust Volume Scaling:** Volume data is scaled using a two-step process to address potential spikes and outliers:
  1. **Log Transformation:** A logarithmic transformation compresses the range of volume values and reduces the impact of extreme outliers.
  2. **Median and IQR Scaling:** The log-transformed volume data is normalized using the median and interquartile range (IQR). This robust scaling method is less sensitive to extreme values than mean and standard deviation, ensuring the scaling is not unduly influenced by volume spikes. This scaled volume is also mapped to the -1 to +1 range.

**Principal Component Analysis (PCA):**

A 3-dimensional matrix is constructed where each row represents a data point within a window, and the columns correspond to fractional elapsed time, window-relative return, and scaled volume. PCA is then performed on this matrix to identify the principal components that explain the most variance in the data, capturing the underlying relationships between these three variables. The dissertation will include a detailed explanation and presentation of the PCA results, incorporating visualizations and interpretations of the principal components and their significance in understanding market dynamics.

**Dissertation Writing - Visualizing Data Transformations:**

To enhance the reader's understanding, the dissertation will include visualizations of the data transformations, specifically focusing on candlestick data:

- **Pre-transformation Candlestick Images:** Five examples of candlestick charts will depict the raw OHLCV (Open, High, Low, Close, Volume) data over a single day, using 10-minute intervals for each candlestick. This provides a baseline representation of market activity before any transformations are applied.

- **Post-transformation Normalized Images:** Corresponding images will visualize the data after applying the transformations described above (log returns, PCA rotation, and normalization to the -1 to +1 range). Presenting these directly alongside the pre-transformed counterparts provides a clear visual comparison of the transformation's impact.

## Visualizing and Documenting Dynamic Plane Transformations

This section details the creation of visualizations and their incorporation into the dissertation to effectively communicate the transformation of candlestick and volume data into dynamic plane snapshots for CNN input.

### Visualization Requirements

The dissertation should include the following visualizations:

1. **Sequential Transformed Image Pairs:** Five sequences of image pairs, each showing a standard candlestick chart with volume alongside its corresponding dynamic plane snapshot (after applying log returns, PCA rotation, and normalization). Each sequence should span a five-day window to illustrate the transformation's effect on market dynamics over time.

2. **Representative Market Pattern Examples:** Five individual image pairs illustrating distinct market patterns: an uptrend with rising volume, a downtrend with volume spikes, a reversal (down then up), sideways chop, and a breakout spike followed by stabilization. Each pair should consist of the original candlestick chart with volume and its transformed counterpart.

3. **Breakout Spike Stabilize Variations:** Five variations of the "Breakout Spike then Stabilize" scenario, each with a unique volume profile. For each variation, include both the standard 10-minute interval candlestick chart with volume and the corresponding dynamic plane projection (time, log-return, and log-volume scaled to [-1, 1] and rotated using PCA onto the PC1/PC2 axes).

4. **PCA Pattern Analysis:** Five distinct PCA patterns generated from the "Breakout Spike then Stabilize" scenario, varying the data volumes to demonstrate volume's influence on the visualizations.

### Dissertation Content

The dissertation should clearly articulate the following:

- **Transformation Method:** A detailed explanation of the transformation process, including the application of log returns, normalization of time, log return, and log volume to the range [-1, 1], and PCA rotation to create the 2D dynamic plane snapshots.

- **Visualization Justification:** A rationale for presenting individual image pairs rather than combined images, emphasizing clarity and focus on specific changes. A discussion on the inclusion or exclusion of explicit volume bars in the transformed dynamic plane snapshot, considering whether the transformed data sufficiently captures volume information.

- **PCA Pattern Analysis:** A comparison of the five PCA patterns, focusing on the effects of volume magnitude and timing on the trajectory shape. Include observations on: (1) the early cluster (pre-spike) mapping, (2) the spike's jump to a PC2 extreme (likely volume-driven), and (3) post-spike stabilization cluster/trajectory formation.

- **Code and Explanation:** Include and explain the code used to generate the visualizations.

- **Interpolation Method Justification:** A clear justification for _not_ using bivariate spline interpolation for feature extraction (due to potential distortion of crucial sharp features and spikes) and the rationale for selecting the chosen interpolation method (e.g., linear interpolation, 3D interpolation), referencing the specific checklist items if applicable (e.g., `f0ea2c98-9ef6-4195-967d-40627ef3cc70`, `eccd679f-34e9-4be6-a5c8-e5f393af04a9`, `c6b51b3a-4361-45ff-9a9a-abba2eca1cab`).

By meticulously incorporating these visualizations and explanations, the dissertation will effectively communicate the core principles of the dynamic plane transformation and its impact on the model's performance. A precise and informative title reflecting the chosen methodologies and research contributions should be finalized.

## Dissertation Writing

This section focuses on structuring, writing, and refining the dissertation to effectively communicate the research, development, and findings of the project. It outlines key aspects derived from the project checklist, ensuring comprehensive documentation of the system's functionality, model architecture, and experimental results. The dissertation should maintain a coherent narrative, connecting the code implementation and experimental findings to the final written document. The title should accurately and concisely reflect the core contributions and findings.

**Connecting Implementation and Findings to the Dissertation:**

The dissertation must clearly explain the system's functionality, including a detailed description of implemented modules like the `DynamicPlaneGenerator`. The rationale behind data transformations, such as the normalization of Time, Price, and Volume data, should be thoroughly justified. The training process and analysis of the models should be comprehensively documented, providing a clear understanding of the development process and the final system. The dissertation should demonstrate how the project fulfills the functional requirement of a deployable algorithmic trading bot.

**Key Model Components and Architectural Choices:**

The following key components of the model architecture require detailed explanation within the dissertation:

- **Dynamic Frame Construction:** This section should thoroughly explain the dynamic plane creation process using Principal Component Analysis (PCA) on normalized Time, Price, and Volume data. The explanation should cover how PCA extracts the two dominant axes of correlated movement (PC1 and PC2) to form the dynamic 2D plane. Include mathematical justification and visual aids for clarity.

- **Rotation and Refocusing:** Describe how recent market action history is projected onto the newly created 2D plane and the re-centering process, where the last data point becomes the new origin (0,0). Provide clear visual representations and mathematical explanations.

- **Image Generation:** Detail the process of rendering the transformed 2D plot into an image, specifying whether Candlestick or Heiken-Ashi charts are used as input for the predictive model (e.g., a Vision Transformer). Include sample images and a discussion of design choices.

- **Predictive Goals:** Clearly articulate the model's predictive aims, including predicting a 2D movement vector (Δx', Δy') within the dynamic plane, representing future trajectory, and "Rally Time," predicting the time required for the predicted movement. Explain how these predictions are derived and their practical interpretations.

- **Self-Correction Mechanism:** Provide a comprehensive explanation of the self-correcting mechanism, including the performance-driven feedback loop using a Total Error signal composed of Vector Deviation Error (Distance and Angular Error) and Frame Shift Error. Explain the "Wound Detection" based on a dynamic threshold and the "Healing Phase" where dynamism is gradually restored based on improved predictive accuracy. Discuss algorithms, parameters, and potential limitations.

- **Multi-Scale Temporal Modeling:** Articulate the rationale and implementation of incorporating daily, weekly, monthly, quarterly, and yearly contexts into the model. Discuss the importance of capturing cyclical patterns and accounting for influences like after-market forces. Detail the architectural design choices for integrating different time scales, including the weighting scheme and handling of cyclical patterns. Present and analyze experimental results demonstrating the impact of different weighting schemes on predictive accuracy. Discuss the model's ability to consider after-market forces and their influence, even if not directly reflected in intraday data.

- **Model Architecture Comparison:** Provide a detailed comparison of the explored model architectures, including:
  - **Ensemble Model Approach:** Explain the rationale and implementation of the ensemble approach, where specialist models are trained for different timeframes. Detail how individual model predictions are combined for the final output.
  - **Multi-Input Transformer Model:** Discuss the investigation into a multi-input transformer model as a potential alternative, outlining its architecture and rationale. Include a comparison of the performance and limitations of each approach.

This comprehensive documentation will ensure that the dissertation effectively communicates the complexities of the project and its contributions to the field.

## Dissertation Content and Structure

This section outlines key model architecture, data handling, evaluation strategies, and implementation details that should be thoroughly documented in the dissertation. A clear and comprehensive presentation of these aspects will allow readers to understand the intricacies of the project and appreciate the chosen solutions.

**Model Development and Implementation:**

- **Multi-Scale Data Processing:** Detail the implementation of multi-scale data processing, including the generation of intraday, daily, weekly, and monthly datasets. Explain the rationale for using multiple timeframes and describe the Dynamic Rotating Plane method used to generate context images. Clearly explain how the datasets for each timeframe are prepared and integrated.
- **Dynamic Rotating Plane Implementation:** Provide a detailed explanation of the Dynamic Rotating Plane method, including the normalization of Time, Price, and Volume data within a [-1, +1] range. Justify the chosen transformations and range. Describe the application of PCA to determine principal axes and the re-centering of the plane on the latest data point. Articulate the advantages of this dynamic market data representation.
- **Vision Transformer (ViT):** Detail the utilization of a Vision Transformer (ViT) as the core predictive engine. Explain how the ViT processes dynamic planes from each timescale as a sequence of tokens, emphasizing its ability to learn inter-timescale dependencies. Discuss the ViT architecture, its implementation specifics, and the rationale for its selection.
- **Non-Hierarchical Attention Model (Optional):** If employed, thoroughly document the architecture and motivation of any hierarchical attention mechanism used for dynamically querying longer timeframes based on intraday context. Highlight the aims of computational efficiency and interpretability in the design choices.
- **Weight Assignment Strategies:** Elaborate on the different weight assignment strategies considered for combining multi-timescale information, such as static weights and dynamic, learned weights using an attention mechanism. Analyze the impact of these strategies on predictive performance.
- **Self-Correcting Mechanism:** Document the performance-driven feedback loop and self-correcting mechanism. Define and explain the "Total Error" signal, combining "Vector Deviation Error" and "Frame Shift Error." Describe the "correction mode," which dampens frame rotation in response to error spikes, and the gradual "healing" process.

**Data Handling and Processing:**

- **Data Pipeline Complexity:** Describe the challenges and solutions implemented to manage the data pipeline complexity arising from integrating multi-timescale data using the Dynamic Rotating Plane method.
- **Data Processing Overhead:** Analyze and discuss the potential data processing overhead from integrating multiple datasets. Present mitigation strategies.

**Model Evaluation and Analysis:**

- **Performance Evaluation:** Dedicate a section to evaluating the multi-scale model. Compare its performance against a baseline intraday model.
- **Attribution Analysis:** Use attribution analysis (e.g., Grad-CAM, SHAP) to explain the model's reliance on different timeframes.
- **Robustness Analysis:** Analyze and discuss the model's performance during significant market events to demonstrate robustness.

**Application and Implementation (Optional):**

- **Technology Stack:** Briefly document the technologies used (e.g., Python backend, PWA frontend) within the methodology or implementation chapters.
- **Application Functionality:** Describe the application's functions (e.g., training, backtesting, live trading, error monitoring) and their contribution to the research goals.
- **UI/UX Design (Optional):** If relevant, briefly discuss UI/UX elements to illustrate practical application and user interaction (e.g., dashboard components).

**Dissertation Structure and Title:**

Ensure the dissertation title accurately and concisely reflects the scope and core findings. Incorporate relevant keywords, such as the specific model (e.g., ViT), the market analyzed, and the overall objective (e.g., stock prediction, automated trading). Structure the dissertation logically and clearly to guide the reader through the research process. The principles of clarity emphasized in the UI/UX design should also be applied to the dissertation writing, ensuring clear explanations and visual distinctions when presenting results.

### A. Dissertation Writing (Continuity)

This section focuses on structuring and writing the dissertation based on the project's findings. While the provided checklists may focus on UI/UX elements or implementation details, these aspects inform the content and structure of the dissertation. The dissertation should clearly articulate the connections between the project's implementation and the research contributions. Therefore, ensure the following aspects are thoroughly addressed:

- **Data and Timeframe Configuration:** Describe how the system allows users to configure the asset universe (e.g., NIFTY 50, NIFTY 500, custom watchlists) and the date range for experiments, including the automatic splitting into training and validation sets. This ensures reproducibility and clarifies the scope of the research.

- **Dynamic Plane Configuration:** Detail the model's perception system, including configurable options like candlestick type, local window size, and the inclusion of features such as price, time, and volume. Explain how this configuration impacts the model's input representation.

- **Model and Learning Architecture:** Provide a comprehensive description of the chosen model architecture. This includes specific details about model selection, hyperparameter tuning, optimizer settings, and loss function selection. For example, if using a ViT, document the patch size, embedding dimensions, transformer layers, attention heads, and dropout rate. Similarly, detail the multi-scale context fusion methods, optimizer choices (e.g., Adam, SGD, AdamW) and their parameters (learning rate, weight decay, scheduler), and the rationale behind the selected loss function.

- **Self-Correction System:** Explain the self-correction mechanism, including how users can adjust the wound detection threshold (frame instability sensitivity) and the healing trigger (prediction accuracy threshold). This demonstrates the system's robustness and adaptability.

- **Visualizations (Optional):** Consider including visualizations that compare the standard Heiken-Ashi chart with the dynamically rotated and re-centered 2D plane image fed into the model. This can effectively illustrate the system's unique approach to data representation.

- **Connecting Implementation Details to Research:** Throughout the dissertation, connect the implementation details (e.g., UI/UX choices, specific algorithms used) to the broader research questions and contributions. Explain the rationale behind design choices and their impact on the results.

By addressing these points, the dissertation will provide a comprehensive overview of the system's design, functionality, and its contribution to the research field. This will enhance the clarity, completeness, and overall impact of the work.

### A. Dissertation Writing (Continuity)

This section details the ongoing process of structuring and writing the dissertation. It emphasizes the importance of iteratively refining the document throughout the project lifecycle and ensuring alignment with institutional requirements. Specifically, this involves two key activities:

1. **Structuring and Writing:** Organize the dissertation into standard chapters (e.g., Introduction, Literature Review, Methodology, Results, Discussion, Conclusion), maintaining a clear and logical flow. Focus on articulating the research question, providing background context, describing the chosen methodology, presenting findings with supporting evidence (charts, tables, code snippets as appropriate), and discussing the implications of the research. Regularly update and revise the content to ensure coherence and quality. Integrate the research and implementation details outlined in previous checklist sections, including library selection, model input categorization, dynamic capital allocation, and error detection/healing strategies. Further, incorporate discussions of context awareness, transfer learning experiments, context-aware periodicity implementation, PCA analysis, and hyperparameter permutation testing. Finally, if applicable, connect the project's technical aspects to the philosophical underpinnings of the four paths to liberation (Gyaan Yoga, Bhakti Yoga, Karam Yoga, and Raja Yoga) and the Dharmic mandate, exploring how the project reflects these principles.

2. **Refining the Title:** Craft a concise and informative title that accurately reflects the evolving scope and focus of the research. This is an iterative process; revisit and revise the title as the project progresses to ensure its continued relevance and clarity. The title should effectively communicate the core research contribution and attract the target audience.

### A. Dissertation Writing (Continuity)

This section addresses the development of the dissertation documenting the SCoVA project. While the provided checklist chunk primarily focuses on UI/UX elements and module renaming, these elements inform the dissertation's structure and content. The emphasis on clear documentation within the checklist underscores the importance of a well-structured and comprehensive dissertation.

The detailed screen specifications and project blueprint requirements highlighted in the checklist translate directly to the need for meticulous documentation of the project's technical implementation and user interface design within the dissertation. This documentation should not be merely descriptive but analytical, explaining the "why" behind design choices and connecting them to the overall project goals. The focus on clarity and conciseness in the checklist reinforces the need for a clear and accessible writing style in the dissertation.

The project's modular structure, as reflected in the checklist's emphasis on module-specific documentation, should guide the dissertation's organization. Each module, with its defined functionality and revised naming conventions, represents a discrete unit of analysis within the dissertation. For example, the "Gyaan Shala (The House of Wisdom)" module, focused on data management and experiment design, should be thoroughly explained within the dissertation, highlighting its role in ensuring data integrity and informing strategic decision-making.

Finally, the dissertation should connect the project's technical details to its philosophical grounding in the four paths of Yoga. This connection should be woven throughout the narrative, demonstrating how these principles inform the project's architecture, functionality, and ethical considerations. The dissertation title should accurately reflect this integrated approach, encompassing both the technical and philosophical dimensions of the project.

## Dissertation Writing (Continuity)

This section details the process of structuring and writing the dissertation, ensuring clear communication of the project's methodology, findings, and contributions. The dissertation must be technically detailed and comprehensive, avoiding philosophical discussions. It should provide thorough explanations of the project's technical aspects, including specific features, libraries, and methods used. This thoroughness aims to minimize ambiguity and create a self-sufficient document. The title should accurately reflect the technical nature of the work and be continuously refined to ensure clarity and precision as the project evolves.

The dissertation should specifically address the following architectural and implementation details:

- **Backend Stack:** Justify the choice of Python 3.11+, containerized microservices on Google Cloud Run, and Google Cloud Tasks for orchestration. Compare and contrast this approach with alternatives, highlighting the advantages and disadvantages of the selected technologies.

- **Frontend Stack:** Detail the decision to use a Progressive Web App (PWA) built with Flutter. Explain the benefits of a PWA architecture and the chosen technologies within this context.

- **Database and Storage:** Explain the rationale behind using Google Cloud Firestore for metadata, configuration, and real-time state, and Google Cloud Storage for historical market data in Parquet format and trained model artifacts. Discuss the advantages of these storage solutions given the project's specific requirements.

- **IDE & Deployment:** Describe the use of VS Code with Gemini Code Assist Agent for development and the deployment process to Firebase Hosting (for the PWA) and Google Cloud Run/Functions. Justify the selection of these tools and platforms.

- **Core Components:** Provide meticulous documentation of the "interactive results dashboard," the "live trading engine," and the "Karma Ledger," including technical specifications, algorithms, and design choices.

Furthermore, the dissertation should address any cost optimization strategies and their impact on the project. For example, if offline processing, shifting processing to the iPad, or optimized image generation techniques were employed, the dissertation should include:

- **Justification for Optimization:** Explain the rationale behind these strategies, emphasizing the cost reduction achieved and how it enabled project feasibility or enhanced specific aspects. Clearly articulate the addressed cost challenges (e.g., the cost of generating and processing large image datasets).

- **Implementation Details:** Describe the technical implementation of the chosen architecture, including data transfer mechanisms, UI interaction with offloaded processes, and any challenges encountered (e.g., memory constraints and data access on client devices). If techniques like federated learning or hybrid approaches were explored, these should be thoroughly documented.

- **Performance and Cost Analysis:** Analyze the impact of these optimizations on the project's overall performance and cost. Quantify improvements in speed, efficiency, and cost savings whenever possible. Provide a comparative analysis of the costs associated with the original approach versus the optimized solution.

By comprehensively documenting these aspects, the dissertation will provide a robust and complete overview of the project's technical decisions, rationale, and impact on the research outcomes. This approach ensures a technically sound and self-explanatory final document.

### A. Dissertation Writing (Continuity)

This section focuses on structuring, writing, and refining the dissertation, incorporating the technical details and decisions made throughout the project. While the provided checklist chunk primarily addresses implementation, these details directly inform the dissertation's content. The dissertation must clearly describe and justify these technical choices. Therefore, the following points should be addressed:

- **Client-Side Model Training:** Discuss the rationale and implementation of client-side training using Core ML and hardware acceleration (ANE and GPU). Compare and contrast this approach with server-side training, considering factors like privacy, latency, resource utilization, and the benefits of leveraging on-device processing. Provide specific details on Core ML usage and hardware acceleration techniques.

- **Framework Selection (Core ML and Metal):** Justify the selection of Apple's Core ML for machine learning and Metal for GPU computation. Explain the process of converting models from PyTorch or TensorFlow to the .mlmodel format. Detail how Metal facilitates image generation and rendering within the DynamicPlaneGenerator component. Include performance comparisons or benchmarks, if available.

- **Local Data Storage (Core Data or Realm):** Describe the data management strategy and rationale for choosing Core Data or Realm to store OHLCV data. Justify the choice based on factors like performance, offline availability, and data management capabilities. Explain how this storage strategy supports model training and other application operations.

- **DynamicPlaneGenerator Implementation:** Provide a detailed explanation of the DynamicPlaneGenerator's implementation as a native Swift module. Articulate how it leverages Metal for PCA, rotations, and rendering of numerical data to generate images or tensors for Core ML. Clearly connect the DynamicPlaneGenerator's functionality to the overall model architecture and its role in data preprocessing and feature engineering.

- **Hybrid Architecture and UI/UX Changes:** Describe the shift towards a hybrid architecture, including the rationale and implications for the overall system design. Document the UI/UX changes made to accommodate client-side processing and the integration of the native Swift module. This includes details on status indicators, resource monitoring, and any adjustments to the Campaign Runner and Experiment Designer UI.

- **Frontend Technology Shift (if applicable):** If the project involves a transition in frontend technologies (e.g., from web-based to native or hybrid), clearly document this shift, explaining the reasons behind the change and its impact on the project's architecture and user interface.

Furthermore, remember to:

1. **Structure and Write Dissertation:** Continuously structure and write the dissertation, ensuring adherence to the required format and style guidelines. This includes outlining chapters, drafting content, and iteratively refining the writing throughout the project lifecycle.

2. **Refine Dissertation Title:** Continuously review and refine the dissertation title to accurately and concisely reflect the project's scope and core contributions. Seek feedback from advisors to ensure the title is informative and engaging.
   This section focuses on structuring and writing the dissertation to ensure clear communication of the project's architectural and functional aspects, including the rationale for key technology choices.

The dissertation should articulate the chosen architecture, featuring a minimal Python backend acting as an orchestrator. This backend's core responsibilities include:

- **Data Serving:** Managing the primary database and serving raw numerical data to the iOS app for client-side processing and model training. The dissertation should illustrate and justify this data flow.
- **Authentication and API Management:** Securely connecting and authenticating with the Zerodha Kite Connect API. The dissertation should detail the interaction and security measures employed.
- **Model Versioning and Aggregation:** Storing and serving different model versions and aggregating updates from the app. This supports reproducibility and facilitates model improvement.
- **API Key Management:** Securely storing and managing API keys, particularly for services like Zerodha Kite Connect. This should be addressed within the broader security and backend functionality discussions.

The dissertation must emphasize the client-side processing performed by the iOS app, including on-device image generation, model training, backtesting, and live inference. The client-backend interaction should be clearly explained, potentially using analogies (e.g., the backend as a "traffic controller") to illustrate the distributed nature of the system.

Furthermore, the dissertation should explicitly confirm and justify the use of Python for the backend, considering factors such as existing infrastructure, library support (especially for machine learning tasks), and development efficiency. The rationale for choosing Flutter (or potentially migrating from Swift to Flutter) for the frontend should also be thoroughly explored, addressing aspects like integration with the Python backend, access to native device features, performance, suitability for high-performance ML with TensorFlow Lite, and custom GPU graphics capabilities. This justification should include a discussion of the feasibility study conducted and compare Flutter against other potential frontend technologies.

Finally, the dissertation should address future deployment considerations, including:

- **Constrained Image Processing:** Limiting image processing on other frontends (web and Android) to daily predictions and re-tuning only when higher error rates necessitate model adjustment. The rationale and implications of this strategy should be discussed.
- **Web Frontend Support:** Document the planned web frontend, potentially including screenshots or diagrams of its user interface and functionality.
- **Android Frontend Support:** Document the planned Android frontend, similarly showcasing its design and features.

A compelling and informative title is crucial. The dissertation title should be refined to concisely and accurately reflect the project's core components and contributions.

### A. Dissertation Writing (Continuity)

This section focuses on integrating the technical implementation details, highlighted in the checklist, into a coherent and well-structured dissertation. While the checklist itself doesn't dictate the dissertation's structure, it provides crucial content points that should be addressed within the relevant chapters.

The dissertation should clearly articulate the project's technical decisions and their rationale. This includes:

- **Backend Technology:** Justify the choice of Python for backend development, emphasizing its suitability for the project's needs and any advantages it offers. This discussion should be integrated into the methodology or implementation chapters.

- **Client-Side Processing:** Discuss the advantages and disadvantages of performing heavy processing on the client device, compared to server-side processing. Analyze the impact on architecture, performance, and user experience. This analysis should be included in the relevant sections discussing the application's design and performance evaluation.

- **Flutter/Python Integration:** Detail the chosen method for integrating Flutter and Python, including data transfer mechanisms, authentication procedures, and API design. This explanation is crucial for the implementation and architecture chapters.

- **Model Management:** Describe the strategy for managing machine learning models, including formats (`.mlmodel`, `.tflite`), versioning, storage, and updates. This discussion contributes to the methodology and implementation chapters, demonstrating a systematic approach to model management.

- **Model Portability:** Address the challenges and solutions for cross-platform model compatibility, particularly regarding Core ML models on Android. Analyze potential approaches like direct transfer or conversion techniques, justifying the chosen solution. This analysis belongs in the discussion or future work sections of the dissertation.

- **Framework Selection:** Explain the rationale behind choosing specific training frameworks (e.g., Core ML vs. cross-platform frameworks), considering performance, development effort, and maintenance. This justification should be part of the methodology chapter.

- **Model Architecture:** Detail the "Universal Source Model" architecture, implementation, and rationale. Describe the conversion and deployment processes for different platforms (iOS, Android, web), including platform-specific adaptations. This is essential content for the implementation and results chapters.

- **On-Device Training:** Explain the on-device training capabilities and their contribution to the project's objectives. Document the synchronization mechanism for updated weights or deltas back to the backend, including the chosen method, implementation, and rationale. Also, address potential synchronization challenges. This discussion should be incorporated into the implementation and evaluation sections.

These technical details should be seamlessly woven into the dissertation narrative, ensuring a cohesive and comprehensive presentation of the research. The dissertation's structure (introduction, literature review, methodology, results, discussion, and conclusion) should guide the placement of this information, creating a logical flow and supporting the overall research narrative. The dissertation title should concisely and accurately reflect this work, encompassing both the technical implementation and the core research contribution.

### A. Dissertation Writing (Continuity)

This section details the process of structuring, writing, and refining the dissertation to ensure consistent progress and a cohesive final product.

1. **Structure and Write Dissertation:** Organize your research, findings, analysis, and conclusions into a coherent and well-structured document following established academic guidelines. This typically includes chapters for:

   - Introduction: Provide context, background, and the research question.
   - Literature Review: Synthesize existing research relevant to your topic.
   - Methodology: Detail your research methods and data collection process.
   - Results: Present your findings clearly and objectively.
   - Discussion: Analyze and interpret your results, connecting them to the literature and research question.
   - Conclusion: Summarize your findings and their implications, along with potential future research directions.
   - References: Accurately cite all sources using the appropriate style guide.

Maintain momentum and ensure timely completion through regular writing sessions and consistent updates. Prioritize clear and concise language, logical flow of arguments, and proper referencing throughout.

2. **Refine Dissertation Title:** A strong title accurately and concisely reflects the core research question and findings. It should be engaging, informative, and capture the essence of your study. Refining the title is an iterative process and may involve feedback from advisors and peers. Aim for clarity, conciseness, and impact in the final version.

### A. Dissertation Writing (Continuity)

This section outlines the process of structuring, writing, and refining the dissertation based on the SCoVA project implementation. Maintaining continuity throughout the writing process is crucial for ensuring a consistent flow of information and a cohesive narrative. This involves iterative writing and refinement, aligning the dissertation content with the evolving project outcomes.

1. **Structure and Write the Dissertation:** Organize the dissertation into logical chapters, following established academic guidelines. This structure should clearly present the project's contributions and typically includes:

   - **Introduction:** Introduce the research topic, problem statement, and the proposed SCoVA solution. Clearly state the research question and hypotheses.
   - **Literature Review:** Provide a comprehensive overview of existing research relevant to stock market prediction, computer vision applications in finance, and related areas. Justify the novelty and significance of the SCoVA project.
   - **Methodology:** Detail the data acquisition process, preprocessing steps, model architecture, training and validation techniques, and evaluation metrics. Provide clear explanations and justifications for each methodological choice. Reference specific sections of the codebase where applicable.
   - **Results:** Present the SCoVA model's performance using appropriate metrics. Visualize the results using charts and tables, analyzing the strengths and weaknesses of the model's predictions.
   - **Discussion:** Interpret the results in the context of the research question and hypotheses. Discuss the implications of the findings, limitations of the study, and potential future research directions.
   - **Conclusion:** Summarize the key findings and contributions of the dissertation. Restate the research question and provide a concise answer based on the results.
   - **References:** Include a comprehensive list of all cited sources, adhering to a consistent citation style.
   - **Appendices (if needed):** Include supplementary materials, such as code snippets, detailed experimental results, or mathematical derivations.

2. **Refine the Dissertation Title:** The title should be concise, informative, and accurately reflect the core focus of the dissertation. It should be engaging while clearly conveying the research topic. Revisit and revise the title throughout the writing process to ensure it remains aligned with the evolving content and contributions. Consider incorporating relevant keywords for improved discoverability. Seek feedback from advisors and peers on the title's clarity and effectiveness.

Continuously refine and expand these sections as the SCoVA project progresses and new findings emerge. This iterative process ensures the dissertation accurately reflects the completed work and acquired knowledge.

## Dissertation Writing and Technical Documentation

This section details the process of writing the dissertation and highlights the importance of comprehensive technical documentation. While the dissertation provides a high-level overview of the research, the technical document serves as a detailed companion, ensuring clarity and reproducibility.

The dissertation should encompass the standard structure: introduction, literature review, methodology, results, discussion, and conclusion. Each section contributes to the overall research narrative. The dissertation title should be concise, informative, and accurately reflect the research scope. Refinement of the title is an iterative process.

Given the project's complexity, the dissertation must thoroughly address the following key aspects of the SCoVA project:

- **Multi-Source Visual Inputs:** Detail the rationale and implementation of using multiple visual data sources, including primary equity charts, corresponding futures charts, and visualized options chain data (heatmaps) for anomaly detection within the CTAM (Cybernetic Threat Assessment Module). This includes discussing data acquisition, preprocessing, and the justification for visual data representations.

- **Specialized Anomaly Detection Models:** Describe the architecture and training of the specialized CNNs for threat detection. Explain how these models are tailored to identify anomalies in equities (gaps and volume spikes) and derivatives (options chain heatmap analysis). Justify the choice of CNNs and provide implementation specifics.

- **Fusion and Threat Level Assessment:** Provide comprehensive documentation of the process for combining outputs from specialized detectors into a Systemic Threat Level (STL) score. Explain the chosen fusion model or weighted-average function and its rationale. Discuss how the STL score contributes to understanding systemic risk.

- **Integration with Core Systems:** Elaborate on how the STL score from the CTAM influences core system behaviors, including proactive Pratyahara (withdrawal), adjustments to the Dynamic Plane's smoothing factor, and its use as a context token for final predictions. Analyze the impact of these integrations on system performance and stability.

- **Model Retraining Strategy:** Provide a detailed explanation of the model retraining strategy, emphasizing how it accounts for both flow and shock as distinct entities to prevent desensitization to data peaks and valleys. Discuss the rationale for this approach and its implications for long-term model performance.

The technical document, supporting the dissertation, should include the three-stage testing strategy:

1. **Unit & Integration Testing with Local Mocks:** Detail the process, tools, frameworks, and rationale.
2. **End-to-End Pipeline Simulation in "Dry Run" Mode:** Explain the steps and data used in the simulation.
3. **On-Device "Smoke Test" with a Dummy Model:** Explain the procedure and its purpose for early identification of integration issues.

This dual documentation approach ensures thorough record-keeping, facilitates future understanding and reproducibility, and provides a detailed reference for the methods and processes used in the research.

## Dissertation Writing (Continuity)

This section outlines the process of structuring and writing the dissertation, ensuring a coherent narrative that reflects the complete lifecycle of the SCoVA project. The dissertation should not only present the project's findings but also provide a comprehensive understanding of its underlying principles, methodology, and implications, with a particular focus on profit maximization as the primary objective of the developed algorithmic trading bot.

The initial model's limitations and the subsequent development of a more robust architecture are crucial aspects to address. Specifically, the dissertation should cover:

- **Limitations of the Initial "Wound and Healing" Framework:** The initial model, based on an error-correction mechanism analogous to "wound and healing," exhibited inherent limitations. This framework, while providing a foundation for understanding negative feedback and mean reversion, proved overly simplistic in capturing market complexities. The dissertation should explain how this mechanism creates a "zone of comfort" where the model performs well under normal market conditions but struggles during periods of high volatility due to its inherent mean reversion bias. It should also discuss how this bias impacts the model's ability to react to and capitalize on significant "shock" events by smoothing out these deviations from the perceived equilibrium.

- **Evolution to a Dual-System Architecture:** To overcome the initial model's limitations, a dual-system architecture was developed, incorporating:
  - **Flow Engine:** Designed to exploit predictable patterns during normal market behavior.
  - **Threat Engine (CTAM):** Designed to detect and respond to outlier "shock" events, acting as a contextual override to shift the model's action mode during extreme market conditions.

The dissertation should thoroughly explore the benefits and complexities of this dual-system approach, highlighting the interplay between the Flow and Threat Engines.

- **Integration of the Shockwave Prediction Model (SPM):** The dissertation must detail the integration of the SPM, its function in predicting short-term directional movements and magnitudes during and after "shocker events," and its contribution to profit maximization. This includes explaining the dynamic weighting mechanism ("seesaw") that balances the SPM's influence with the Flow Engine based on market volatility, allowing the system to capitalize on opportunities presented by volatile market conditions (Opportunistic Threat Response). A clear explanation of the system's response to various systemic threat levels and how the "seesaw" mechanism optimizes performance under different market conditions is essential.

Finally, the dissertation title should be iteratively refined throughout the writing process to accurately and concisely capture the novel contributions and core focus of the SCoVA project, reflecting its evolution from a basic error-correction model to a sophisticated dual-system architecture incorporating shockwave prediction.

### A. Dissertation Writing (Continuity)

This section details the ongoing process of structuring, writing, and refining the dissertation. While the final draft will be completed towards the end of the project, actively developing the dissertation content throughout the research lifecycle is crucial for coherence and manageable workload. Integrating findings and insights as they emerge ensures a more thoughtful analysis and prevents a last-minute rush. The "Continuity" principle emphasizes this iterative approach, advocating for consistent progress and integration with the evolving project architecture.

The key activities for maintaining continuity in dissertation writing include:

1. **Structure and Write Iteratively:** Develop a clear structure for the dissertation, outlining chapters, sections, and subsections that align with the project's progression and established academic conventions. Begin writing early and integrate findings, insights, and architectural decisions as they emerge. This iterative approach ensures the dissertation reflects the evolving nature of the research.

2. **Refine Dissertation Title:** The dissertation title should accurately reflect the research scope and key contributions. Regularly revisit and refine the title as the project progresses and a clearer understanding of the research outcomes emerges. This ensures the title remains relevant, concise, engaging, and effectively communicates the dissertation's core focus.

3. **Document Architectural Decisions:** Thoroughly document the evolving project architecture, including the rationale behind key decisions, within the dissertation. This includes descriptions of component deconstruction, atomization, and reassignment based on the four pillars (Continuity, Enforcement, Facilitation, and Specialization). Clearly explain the communication protocols between components, especially those governing interactions between Specialist components through Facilitators. This documentation provides crucial context for understanding the implementation and ensures reproducibility. Specific examples, such as the atomization of modules like the Dynamic Plane Generator and Multi-Scale Vision Transformer, should be included to illustrate the application of these principles. Highlighting the mapping of component functions to the four pillars within the dissertation strengthens the connection between the theoretical framework and the practical implementation.
   This section focuses on structuring and writing the dissertation. While the previous checklist section discussed software architecture principles, those concepts can be applied analogously to dissertation writing. A structured approach, version control, a centralized information hub, and methodological consistency are crucial for both software development and academic writing.

Think of your dissertation as a complex system with foundational elements, similar to the software services described earlier. The core chapters (introduction, literature review, methodology, results, discussion, and conclusion) are like "base classes," each serving a specific purpose. Just as software services inherit core functionalities from base classes, your dissertation chapters should maintain a consistent flow of logic and argumentation, building upon foundational concepts.

Outlining key arguments and structuring each chapter is akin to drafting base class specifications and pseudocode. Defining clear objectives for each section and outlining the logical flow ensures a cohesive narrative. Establishing the core theoretical framework and methodology mirrors defining base classes in software, providing a consistent foundation for your analysis and conclusions.

Building upon previously established concepts and arguments throughout the dissertation is analogous to implementing class inheritance in software. Each chapter should contribute to the overall narrative by expanding upon or refining earlier ideas, creating a logical progression and a strong, cohesive argument. This structured approach, mirroring software development principles, will result in a well-organized and impactful dissertation.

**A. Dissertation Writing (Continuity)**

1. **Structure and Write Dissertation:** Begin structuring and writing the dissertation. This ongoing process involves outlining chapters, drafting content, and iteratively refining the writing. Focus on clear, concise language, a logical flow of arguments, and supporting evidence for all claims. A typical dissertation structure includes:

   - **Introduction:** Provides context, states the research problem and hypothesis, and outlines research questions.
   - **Literature Review:** Summarizes existing research, identifies gaps addressed by the current work, and justifies design choices.
   - **Methodology:** Details data acquisition, model architecture, training procedures, and evaluation metrics. Any specific implementation details, like a dynamic rotating plane or a dual-engine perception model, should be thoroughly explained here.
   - **Results:** Presents the research findings clearly and concisely, using appropriate visuals (tables, figures, etc.).
   - **Discussion:** Interprets the results, discusses their implications, and connects them back to the research questions and hypothesis. Limitations of the research should also be addressed.
   - **Conclusion:** Summarizes the key findings and contributions of the research, and suggests potential future work.

2. **Refine Dissertation Title:** Continuously review and refine the dissertation title to ensure it accurately and concisely reflects the scope and core findings of the research. Seek feedback from advisors and peers.

## A. Dissertation Writing (Continuity)

This section details the process of structuring, writing, and refining the dissertation for the SCoVA project. This involves organizing research findings, developing arguments, and presenting a cohesive narrative based on the project's goals, methodology, results, and conclusions. This is an iterative process requiring continuous refinement and revision. Key tasks include:

1. **Structure and Write Dissertation:** This encompasses the entire writing process, from outlining the structure to drafting individual chapters and sections. The dissertation must thoroughly document and analyze the crucial aspects of the project highlighted below.

2. **Refine Dissertation Title:** Crafting a clear and concise title that accurately reflects the dissertation's core focus is crucial. This involves iterative refinement to ensure it effectively communicates the research's scope and contribution. The title should highlight key innovations such as the dynamic rotating plane concept or the dual-engine perception model.

The following components, derived from the project's development and implementation, must be addressed within the dissertation:

- **Robust Backtesting Engine:** Detail the development and implementation of the backtesting engine, emphasizing the incorporation of real-world market conditions such as market impact, latency, slippage, order queues, commissions, and fees. Justify the choice of any specific backtesting libraries (e.g., Backtrader, Zipline) and explain their utilization within the research.

- **Portfolio Construction and Risk Management:** Explain the portfolio construction and risk management algorithms. Describe the chosen algorithms (e.g., Mean-Variance Optimization, Risk Parity, Hierarchical Risk Parity for construction, and drawdown limits, concentration limits, and volatility targeting for risk management) and analyze their performance and impact on the overall trading strategy. Explore the interplay between these two components and their contribution to the agent's effectiveness.

- **Adaptive Seesaw Blending (Meta-Model):** Provide a detailed explanation of the meta-model's architecture, inputs (Systemic Threat Level, market volatility), and adaptive weighting mechanism. Analyze the effectiveness of this dynamic blending approach and its contribution to improved adaptability in varying market scenarios.

- **Explainability of Model Predictions:** Discuss the methods employed to explain the model's predictions, particularly if another AI model was used for this purpose. Describe the contextual information used for explanation, such as market conditions and the Systemic Threat Level (STL). Clearly explain the Statistical Prediction Model (SPM) and its interaction with the STL-based weighting.

- **Advanced Error Signal Implementation and Analysis:** Detail the implementation of the Total Error signal, encompassing both Vector Deviation Error and Frame Shift Error. Explain how this composite error metric provides a comprehensive view of model health and contributes to system robustness and reliability. Discuss its use for performance monitoring and system stability.

- **Performance-Based Healing Mechanism:** Describe the dynamic healing process tied to the model's prediction accuracy. Contrast this with traditional timer-based healing, emphasizing the benefits of a performance-driven approach for system resilience and adaptability. Analyze its effectiveness and impact on overall system performance.

- **Multi-Scale Periodicity Integration:** Explain the methodology for incorporating multi-timeframe context (intraday, daily, weekly) into the model. Discuss how this fusion of information improves the system's understanding of cyclical patterns and enhances its predictive capabilities. Analyze the impact of multi-scale data on model performance.

- **"Rally Time" Prediction and Interpretation:** Detail the implementation and functionality of the "Rally Time" prediction feature. Explain how this prediction is generated and its practical implications for trading strategies. Analyze its accuracy and reliability.

- **System Architecture and Distributed Tracing:** Provide a comprehensive overview of the system architecture and explain the implementation of distributed tracing using OpenTelemetry. Demonstrate how this tracing mechanism enables workflow visualization, performance bottleneck identification, and system optimization. Include relevant diagrams and visualizations within the dissertation.

- **Experimental Results and Discussion:** Present the findings of any experiments, including backtesting performance metrics such as Jensen's Alpha and the Sharpe Ratio. Use feature attribution tools to provide insights into model behavior. Address the impact of high trading costs and short-selling constraints. Connect the code implementation to the experimental results and discuss their implications. Finally, summarize the key findings and contributions of the research, and suggest potential avenues for future work.
  Model predictions, individual model statuses, available capital, and risk exposure should be thoroughly described. The rationale for presenting explanations as human-readable narratives, including timestamps, should be justified, and their effectiveness evaluated. This explainability is crucial for understanding and trusting the agent's decision-making.

The dissertation title should accurately reflect the core focus and contributions of the research, incorporating relevant keywords. The following sections detail how specific project elements should be incorporated into the dissertation.

### A. Dissertation Content: Explainability and Narrative Generation

The dissertation should include a detailed description of the `Narrative_Generation_Service`. This service generates human-readable explanations for each trade executed, providing transparency and addressing potential "black box" concerns. Triggered after a trade's completion, it leverages information from the Feature Store, attribution methods (LIME and SHAP), attention maps, and potentially an LLM to create comprehensive narratives. These narratives are crucial for reporting, auditing, and demonstrating the rationale behind each trade.

Specifically, the dissertation should cover:

- **Feature Store Integration:** Explain how the `Narrative_Generation_Service` retrieves versioned input features and system state information from the Feature Store to generate accurate and consistent trade explanations. This connection ensures explanations are grounded in the specific data used for each trade.
- **Attribution Method Implementation:** Detail the implementation of both model-agnostic (LIME and SHAP) and model-specific (attention maps) attribution methods within the `Narrative_Generation_Service`. Describe how these methods identify influential features contributing to each trade decision and how this information is incorporated into the generated narratives. This explanation contributes to a deeper understanding of the model's behavior and decision-making process. Furthermore, explore and incorporate "explanation AI" to enhance the system's explainability and provide deeper insights into the transformer model's predictions.

### B. Dissertation Content: Real-time Data Integration and Feature Engineering

The dissertation should address the following aspects of real-time data handling and feature engineering:

- **Real-time Data Integration:** Discuss integrating live tick data from Zerodha using a WebSocket feed within the `Paper_Brokerage_Simulator`. Emphasize the importance of real-time data for accurately simulating market conditions, including network latency and realistic bid-ask spreads. Explain how the simulator handles partial fills and uses live tick data to simulate realistic order fills, considering live bid/ask prices and volume. This highlights the system's robustness and the attempt to bridge the gap between historical backtesting and live trading.
- **Feature Engineering from Market Depth Data:** Detail the `DeriveOrderBookFeatures` service implementation. Describe how it processes raw market depth data from live tick updates to generate derived features such as Order Book Imbalance (OBI), Weighted Average Price (WAP), and Bid-Ask Spread. Explain how these features are used as input to the `DynamicPlaneGenerator` and justify their inclusion in the model. Include a comprehensive review and documentation of all market depth data usages within the system.
- **Architecture and Design Choices:** Discuss the architectural decision to use a WebSocket for live tick data and justify choosing Zerodha as the data provider. Elaborate on the design of the `Paper_Brokerage_Simulator` and the `DeriveOrderBookFeatures` service. Clearly explain the interaction between these components and other system parts. This strengthens the overall argument and showcases the project's technical depth.

### C. Dissertation Content: Addressing Data Source Limitations

The dissertation must address the limitations encountered with Zerodha's market depth data, specifically its fixed price increments and the subsequent shift from spread-based analysis.

The initial approach of calculating spreads proved unsuitable due to the fixed ₹0.05 increments in Zerodha's market depth data (between -₹0.25 and +₹0.25). This constraint should be clearly explained, along with the rationale for discarding spread calculations, focusing on the low information content and potential uselessness of spread values derived from such data. The dissertation should then detail the adopted alternative approach.
This section focuses on incorporating the technical enhancements related to order book dynamics and price improvement, driven by the limitations of the Zerodha API, into the dissertation. These enhancements, developed to extract meaningful insights from limited market depth data, form a significant contribution of this project and should be thoroughly documented.

Specifically, the dissertation should prominently feature the following:

- **Order Book Imbalance (OBI) Calculation and Integration:** Detail the implementation of the `CalculateOrderBookImbalance` service and explain how the calculated OBI, a normalized value ranging from -1.0 to +1.0 representing selling and buying pressure, is integrated as a fourth dimension (alongside Time, Price, and Volume) within the `DynamicPlaneGenerator`. This discussion should highlight how this approach addresses the limitations of the available data.

- **Depth Quantity Heatmap Visualization and Anomaly Detection:** Describe the `GenerateDepthQuantityHeatmap` service, which visualizes order book quantity changes over time. Explain how this heatmap, with 10 rows (5 bid and 5 ask levels) and N time steps (columns), uses color intensity to represent quantity. Crucially, emphasize its role as input for the `MarketDepthAnomalyDetector` CNN, designed to detect visual patterns indicative of market shocks.

- **Price Improvement Rate Integration:** Detail the integration of the `CalculatePriceImprovementRate` service and its output—the Price Improvement Rate—as a context token within the Vision Transformer model. Analyze its impact on predicting short-term trend reversals and overall model performance.

- **Enhanced Order Book State Representation:** Describe the enhanced `ComputeOrderBookState` service, including the newly incorporated "Book Resilience Score." Explain how this score, reflecting the quantity ratios at different order book levels, contributes to more informed trading decisions and how this enhanced feature vector, combined with flow engine patterns, impacts the Vision Transformer's predictive accuracy.

- **Execution Quality Feedback Loop:** Explain the new feedback loop within the Self-Correction & Healing Controller. Detail the mechanism monitoring rolling average execution quality, including slippage and price improvement, and the dynamic adjustment of the Correction Factor based on this quality. Analyze its effect on `DynamicPlane` perception and its contribution to robust and adaptive trading strategies.

By thoroughly documenting these enhancements and their impact, the dissertation will demonstrate a deeper understanding of market dynamics and the effectiveness of the project's approach, especially given the data limitations. It will also showcase the research and development efforts undertaken to improve the model's predictive power and overall performance, including its profitability considering the incorporated price improvements. Finally, the dissertation title should be refined to accurately reflect these contributions and the project's scope.

### Dissertation Content and Structure

This section outlines the essential content and structural considerations for the dissertation documenting the SCoVA (Snapshot Computer Vision Algorithm) project. Clear and concise language, along with sufficient detail, is crucial for a comprehensive understanding of the project.

**Core SCoVA Algorithm:**

- **Definition and Purpose:** Clearly define SCoVA, detailing its purpose within the broader trading system.
- **Architecture and Implementation:** Provide a comprehensive explanation of the underlying architecture and implementation details. This includes specifying the chosen model architecture (CNN, ViT, or hybrid) and explaining how it processes candlestick chart input data.
- **Snapshot Methodology:** Emphasize that SCoVA uses discrete, dynamically generated visual snapshots of market data, rather than continuous time series. Clearly articulate the rationale and implications of this approach.

**Anxiety Model Integration:**

- **Model Description:** Dedicate a section to explaining the Anxiety Model, including its purpose, functionality, and integration within SCoVA.
- **Feature Engineering:** Describe the features used by the Anxiety Model (Order-to-Quantity Ratio, Rate of Change of Order Book Imbalance, Level 1 Dominance, and Book "Flicker" Rate). Explain how these features are calculated from high-frequency market depth data and justify their selection.
- **Operational Modes:** Explain the "flow" and "shock" trading modes within the Anxiety Model, and how the model switches between them based on its real-time assessment of market conditions using the calculated features.
- **Integration with SCoVA:** Detail how the Anxiety Level, derived from the Anxiety Model, serves as input to both the Error Detector and Weight Shifter components of SCoVA.
- **Post-Hoc Analysis:** Explain how post-hoc analysis, correlating algorithm actions with historical order book data, is used to refine and improve the Anxiety Model’s predictive capabilities.

**Asymmetric System Components:**

- **Prediction Models:** Detail the development and rationale behind separate prediction models for bull and bear markets (Bull_Flow_Engine and Bear_Flow_Engine). Explain their training process using respective historical data (uptrends for Bull_Flow_Engine, downtrends for Bear_Flow_Engine) and the regime-detection model used for real-time engine selection.
- **Risk Management:** Describe the asymmetric risk management implemented within the Portfolio_Risk_Manager, specifically the distinct maximum drawdown limits for short and long positions. Justify this approach in relation to potential long bias in equity markets.
- **Self-Correction:** Explain the asymmetric self-correction mechanism within the HealingController, articulating how the system responds differently to varying market conditions.

**Advanced Considerations (Optional):**

- **Non-Hierarchical Asymmetric Design:** Explore the potential implications of incorporating a "Non-Hierarchical Asymmetric" design. Analyze how these concepts relate to the current algorithm and consider potential title adjustments (e.g., "Anhad’s Non-Hierarchical Asymmetric Snapshot Computer Vision Algorithm" or ANHASCoVA).
- **Graph-Based Perceptual Model:** Discuss the potential of developing a graph-based perceptual model, where timeframes (Intraday, Daily, Weekly, etc.) are represented as nodes, and edges represent learned influence. This model could replace the existing multi-scale context model, utilizing a Graph Neural Network (GNN) for dynamic timeframe influence learning.

**Dissertation Title:**

Refine the dissertation title to accurately reflect the core focus and contributions of the research, specifically highlighting the SCoVA algorithm and its key features.

By thoroughly documenting these aspects, the dissertation will effectively communicate the design, functionality, and contribution of the SCoVA project. Maintain a cohesive narrative throughout, connecting the individual components to the overall goals and findings of the research.

## Dissertation Writing and Key Architectural Decisions

This section outlines key model enhancements and architectural decisions that must be thoroughly documented and justified within the dissertation. These decisions are central to the research contribution and should be presented with a clear theoretical foundation and a discussion of their implications on model performance. The asymmetry of market dynamics—specifically, the distinct characteristics of market rises and falls—should be a central theme throughout the dissertation. This asymmetry stems from the observation that market rises are a natural consequence of market existence, while falls represent pullbacks from excessive rises. A complete market collapse is unlikely given the persistent nature of state economics, further highlighting the importance of modeling this asymmetry.

The development and integration of the `AsymmetricFeatureEngine` service is a core contribution of this research. This specialized component computes a rich vector of features describing asymmetries in recent price action and volume, taking a window of raw data as input and outputting a vector of calculated features. These features are then used as a context token for the Vision Transformer. The dissertation should detail the rationale behind exploring asymmetric feature engineering and provide a thorough analysis of the `AsymmetricFeatureEngine` service and its impact on model performance.

**Key Model Enhancements and Architectural Decisions:**

- **Risk-Averse and Asymmetric Loss Functions:** The dissertation should detail the design and implementation of custom risk-averse and asymmetric loss functions. The risk-averse function's heavier penalization of underestimated losses should be explained and justified, connecting it to the overall research goals. Furthermore, the rationale behind the asymmetric loss function, which applies different penalty factors based on the relationship between predicted and actual returns, should be thoroughly explored. The specific penalty values used and their rationale should be documented.

- **Asymmetric Feature Engineering:** The dissertation should thoroughly document the rationale and implementation of calculating separate volatility measures for positive and negative returns (Upside and Downside Volatility). It should also explain the justification for including these as features and describe their expected and observed impact on model performance. This discussion should encompass the following features implemented within the `AsymmetricFeatureEngine`:

  - **Price & Volatility Asymmetry:** Discuss the implementation and results of Upside vs. Downside Volatility (using semi-deviation), Volatility Skewness, and Volatility Kurtosis, explaining how these features capture the distinct characteristics of upside vs. downside price movements.
  - **Volume & Participation Asymmetry:** Elaborate on the implementation and effectiveness of the Accumulation/Distribution Ratio (measuring volume flow on up vs. down days) and Order-to-Quantity Asymmetry (comparing bid-side and ask-side order-to-quantity ratios). Highlight how these features capture differences in buying vs. selling conviction and participation.
  - **Correlation Asymmetry:** Describe the implementation and results of the Price-Volume Correlation State feature, which calculates the correlation between log-returns and log-volume separately for positive and negative return candles. Analyze how this feature helps distinguish between market fear and greed.

- **State-Dependent Attention Mechanism:** The rationale for implementing a state-dependent attention mechanism within the model architecture should be clearly articulated, emphasizing its ability to dynamically adapt to changing market conditions. The specific implementation details, including how market state is represented and integrated into the attention process, should be thoroughly explained.

- **Regime Identification as Context:** The integration of the Regime ID from the Asymmetric Regime Detection model into the Vision Transformer as a context token represents a significant architectural choice. The dissertation must explain this integration, describing how the Regime ID provides a concise representation of market conditions. The rationale for replacing the raw vector of asymmetric features with the Regime ID should be justified, highlighting the benefits of this approach.

- **Cross-Timeframe Pattern Recognition:** The ability of the graph-based model to recognize non-hierarchical patterns across different timeframes should be clearly articulated and illustrated with concrete examples (e.g., the combination of a specific consolidation pattern on a daily chart with a specific volume spike on an intraday chart). Emphasize the model's ability to learn these relationships without being constrained by a rigid top-down structure. Provide supporting evidence from experimental results.

- **Prospect Theory and Loss Aversion:** The model's behavior with respect to large unexpected losses versus large unexpected gains should be connected to the principles of prospect theory. A thorough analysis of the impact of this behavior on overall system performance should be provided.

Finally, the dissertation title should be refined to accurately and concisely reflect these core contributions.
Integrating market asymmetry information into the Vision Transformer (ViT) architecture is a central focus of this dissertation. The chosen approach uses a computed vector as input, balancing simplicity with potential for future enhancements. This section details the rationale and implementation of this approach.

- **Rationale for Vector Input:** The decision to use a computed vector as input for the ViT stems from a desire for simplicity and future adaptability. This approach allows for efficient integration of asymmetry information without significantly increasing model complexity. Alternative methods, such as directly incorporating raw asymmetry data, were considered but deemed less suitable due to potential performance impacts and increased implementation complexity (see Chat2.json messages #124 and #125).

- **Integration with the Asymmetric Feature Engine:** The `AsymmetricFeatureEngine` generates the feature vector, which the `Workflow_Broker` retrieves and passes to the `Model_Inference_Service`. This vector serves as a context token, presented to the ViT alongside the Dynamic Plane image tensor. The ViT architecture has been adapted to accept and process this additional context token (Chat2.json message #124).

- **Impact on Self-Attention:** The inclusion of the asymmetric feature vector as a context token influences the ViT's self-attention mechanism. The dissertation will analyze how effectively the model learns relationships between visual patterns in the candlestick images and the provided context, examining whether the context token enhances the model's understanding of market dynamics.

- **Future Enhancements and Technical Constraints:** While the current implementation uses a simplified vector input, potential future enhancements for understanding market asymmetry and volatility have been explored (Chat2.json message #125). However, prioritizing maintainability and long-term adaptability, the simpler approach was chosen to avoid over-engineering and potential complications during future pipeline renovations. This aligns with the project's emphasis on practical, maintainable solutions over complex, potentially brittle implementations (Chat2.json message #125).

### A. Dissertation Writing (Continuity)

This section outlines the structure and content requirements for the dissertation, ensuring clear and comprehensive documentation of the research conducted. While the previous section detailed specific architectural choices, this section focuses on the overall writing process.

The dissertation should adhere to established academic guidelines, encompassing the following key components:

1. **Structure:** The dissertation should follow a standard structure, including an Introduction, Literature Review, Methodology, Results, Discussion, and Conclusion. Each section should logically flow from the preceding one, creating a cohesive and compelling narrative. Specific emphasis should be placed on documenting the implemented enhancements, such as asymmetric regime detection and terrain-based input for the ViT, including their rationale, implementation details, and performance impact. Any trade-offs considered, such as the potential loss of granularity associated with terrain categorization, should be thoroughly addressed.

2. **Content:** The writing should clearly articulate the research question, methodology, findings, and implications. All sources must be properly cited, adhering to the required style guide. The content should maintain technical accuracy while remaining accessible to the intended audience. Furthermore, the dissertation should clearly explain the dual-token approach for context injection, including the rationale for using both a regime ID and a raw asymmetric feature vector, and how this approach benefits the ViT. The discussion should also cover how potential bottlenecks arising from feature vector computation are addressed, and how both the regime ID and feature vector contribute to the final narrative generation.

3. **Title:** The dissertation title should accurately and concisely reflect the core research focus and contributions. It should be informative, engaging, and clearly communicate the subject matter and scope, for example, highlighting the novel application of ViT with terrain-based inputs for financial forecasting if relevant.

### B. Codebase Finalization (Facilitator)

- 1. P

### B. Codebase Finalization

This section details the finalization of the codebase for inclusion with the dissertation. The goal is to ensure the code is complete, well-documented, easily understandable, and reproducible. This includes clear connections to the dissertation's experiments, comprehensive documentation, and instructions for accessing and executing the code.

1. **Connecting Code to Dissertation:** The codebase will explicitly reflect the experiments detailed in the dissertation. Clear links between the code and the corresponding experimental setups, results, and discussions will be established, enabling readers to understand the implementation and reproduce the findings.

2. **Code Organization and Documentation:** The code will be organized into well-documented modules with clear explanations of their purpose, inputs, outputs, and dependencies. This modular structure will enhance readability and facilitate review and analysis. Comprehensive documentation will be provided for all functions, classes, and data structures. Specific attention will be given to documenting:

   - Data acquisition and preprocessing, including the `Return(t+h) = (Close(t+h) - Open(t+1)) / Open(t+1)` calculation.
   - The `generate_graphs.py` function, especially its handling of the `holding_days` parameter, filename conventions, and the use of a separate CSV file for holding period metadata.
   - Model training and validation procedures, including the mean squared error loss function, the validation dataset split (July 1, 2020, to December 31, 2021), out-of-sample last-block validation, and the early stopping criterion.
   - Trading and backtesting logic within `trade.py`, ensuring accurate return calculations using future prices (t+1 to t+5) from Yahoo Finance while avoiding look-ahead bias.
   - The 5-day windowing approach for candlestick chart image generation and return label calculation.
   - Verification of the input data format to the CNN.

3. **Code Availability and Execution Instructions:** The complete and final codebase will be submitted alongside the dissertation. Clear instructions (a "code prompt" or guide) will be provided for accessing and executing the code, enabling readers to reproduce the models and results. This will likely involve a link to a repository or a packaged archive. The instructions will also include guidance on reproducing specific models and analyses if requested.

## B. Codebase Finalization

This phase focuses on preparing the codebase for inclusion with the dissertation, ensuring reproducibility, transparency, and ease of understanding. This involves providing the complete, functional, and well-documented code, along with clear instructions for execution and reproduction of the results.

1. **Provide Codebase and Execution Instructions:** The complete codebase, including all scripts for data acquisition (using Yahoo Finance), preprocessing, model implementation (CNN, and any incorporated LSTMs or Transformers), training, backtesting (including rolling walk-forward validation), and performance evaluation, should be provided. Clear, step-by-step instructions (a "code prompt") should accompany the code, detailing how to set up the necessary environment, run the code, and reproduce the results presented in the dissertation. This prompt should also specify any required dependencies or system configurations.

2. **Finalize and Clean Codebase:** Before submission, the codebase must be thoroughly cleaned, commented, and organized. This includes:

   - Removing any unused or experimental code snippets.
   - Clarifying variable and function names for improved readability.
   - Providing comprehensive in-code documentation explaining the purpose and functionality of each component.
   - Ensuring consistency between the code implementation and the descriptions within the dissertation.
   - Clearly documenting any data exclusions (dates, prices, tickers) to prevent misunderstandings regarding information leakage during CNN training.

3. **Modularize and Document Code Structure:** The code should be organized into well-defined modules with clear documentation for each. Each module should have:

   - A clear description of its purpose and functionality (e.g., data preprocessing, CNN model training, LSTM/Transformer integration, backtesting).
   - Clearly defined inputs and outputs.
   - Documentation of any dependencies.

   Specifically, the following modules and their documentation are crucial:

   - **Candlestick Chart Generation:** Code for creating candlestick charts from 5-day OHLCV data, including candles, volume bars, and a moving average, which serve as input to the CNN. This should also address edge cases for the last data points of each chart.
   - **Return Label Calculation:** Code implementing the calculation of 5-day future returns using the formula `((Close(t+5) - Open(t+1)) / Open(t+1)) * 100`, which serve as the target output for the CNN.
   - **Model Evaluation Metric:** Clear definition and implementation of the chosen evaluation metric (e.g., MSE, RMSE) for assessing CNN performance.
   - **Model Training Data Scaling:** Code for preparing and processing the training dataset, including detailed documentation of data scaling and handling techniques.
   - **CNN Numerical Output Interpretation:** Comprehensive documentation explaining the CNN's output and its interpretation in terms of predicted market movements.

By adhering to these guidelines, the provided codebase will be readily understandable, reproducible, and suitable for inclusion in the dissertation, enhancing the transparency and rigor of the research.

### B. Codebase Finalization

This section details the finalization of the codebase for inclusion with the dissertation, ensuring clarity, completeness, and reproducibility. This involves incorporating all implemented features, cleaning up the code, and providing comprehensive documentation.

**Key Codebase Deliverables:**

1. **Complete and Functional Codebase:** The complete codebase should be prepared for submission, either as a packaged version or with clear instructions for accessing the repository. This includes all modules related to HPEP, confidence-based filtering, ranking, and soft labeling.

2. **Clean and Commented Code:** All code should be finalized, cleaned, and thoroughly commented. This includes removing any experimental code snippets, debugging statements, or temporary files not directly relevant to the final implementation. Consistent styling and adherence to best practices should be followed. A README file outlining the project structure, dependencies, and instructions for running the code is essential.

**Specific Implementations and Documentation:**

- **HPEP and Confidence-Based Trading:**

  - The HPEP module, `test_model.py` (for post-training confidence profile generation), and `trade.py` (for trade filtering during backtesting) should be fully documented. This documentation should explain the logic behind the HPEP module, its integration points, the process of building the post-training confidence profile (including binning predictions, calculating accuracy metrics, and storing the HPEP map), and how `trade.py` utilizes the HPEP map for filtering trades. A working prototype with dummy data for simplified understanding and testing is recommended.

- **5-Candlestick Window Justification:** Clear documentation justifying the choice of a 5-candlestick input window, referencing relevant literature (Jiang et al., 2023) and the original research, should be included. The lack of similar justification for the 5-day _output_ (prediction) window should be acknowledged and addressed in the dissertation's discussion, potentially suggesting future research directions.

- **Separate CNNs for Holding Periods:** The code should implement and document the training of separate CNNs for each holding period (1 to 5 days). The training process should track and log key metrics, including validation loss, Sharpe Ratio, Alpha, and Mean Squared Error (MSE). The final codebase should include these separate models and their corresponding training scripts.

- **Soft Labeling and Probabilistic Trading:** The implementation of soft labeling should be thoroughly documented, covering the following aspects:
  - **Discretized Return Space:** Clearly demonstrate how the range of returns is discretized into bins (e.g., -5% to +5% in 0.5% increments).
  - **Modified Output Layer:** Document the CNN's modified output layer (fully connected layer with softmax activation) and its rationale.
  - **Soft Label Conversion:** Document the conversion of hard labels to soft labels using a Gaussian kernel, including the kernel's parameters.
  - **Loss Function and Uncertainty-Aware Architectures:** Clearly document the chosen loss function (Categorical CrossEntropy or KL-Divergence) and any decisions regarding uncertainty-aware architectures.
  - **Probabilistic Trading Strategy:** Explain the implementation of the probabilistic trading strategy, including confidence thresholds and trade decision logic.
  - **Backtesting with Soft Labels:** Document the backtesting procedure, including specific parameters and data used. Ensure results are readily reproducible.

This comprehensive documentation, coupled with a clean and organized codebase, will significantly enhance the reproducibility of the research and contribute to the overall strength of the dissertation.

### B. Codebase Finalization

This phase focuses on preparing the codebase for inclusion with the dissertation. This involves ensuring the code is well-organized, documented, and easily understandable for review. The final submission should include a clean, documented, and reproducible codebase, along with clear explanations of implemented strategies, parameter tuning, and the rationale behind design choices. Furthermore, the documentation should acknowledge limitations and potential avenues for future research.

**Addressing Existing Implementation and Limitations:**

The current codebase reflects a trading strategy with inherent limitations, which must be clearly documented:

- **Trading Strategy Inefficiencies:** The strategy exhibits inefficiencies related to high turnover (trades every 5 days), uniform asset weighting, and a lack of smart trade filtering (inclusion of low-confidence predictions). These aspects, and their impact on the realized alpha (particularly after transaction costs), should be thoroughly documented.
- **Impact of Short-Selling Constraints:** The code does not currently account for short-selling constraints. This limitation, and its potential impact on performance, especially within the profitable small-cap segment, must be explicitly documented.
- **Unsuccessful Trade Handling:** The documentation should clarify how unsuccessful trades (where outcomes contradict predictions) are handled, specifically noting the absence of a stop-loss mechanism and its potential impact on mitigating losses.

**Enhancements and Prototypes (Included for Context and Future Research):**

While not fully implemented in the current trading strategy, the following enhancements and prototypes are included to provide context, demonstrate a thorough understanding of the limitations, and suggest avenues for future research:

- **Potential Alpha Enhancements:** The documentation should discuss potential alpha enhancement strategies, including:
  - Analysis of alpha breakdown by long vs. short legs.
  - A table of per-index Sharpe ratios before and after costs.
  - Discussion of how portfolio construction logic (addressing turnover, weighting, and trade filtering) could be modified.
- **Prototype Stop-Loss Module:** A prototype demonstrating the implementation and backtesting of a stop-loss mechanism, exploring various stop-loss levels and their impact on performance.
- **Prototype Dynamic Trade Filtering Layer:** A prototype implementing dynamic trade filtering based on predicted return volatility or HPEP data. The documentation should detail backtesting results using historical or simulated data.

**Core Functionality and Implemented Strategies:**

The following core functionalities and risk management/portfolio management strategies are implemented and documented within the codebase:

- **Clear Prediction Output:** The chosen method for encoding trading signals (BUY, SELL, HOLD) is clearly implemented and documented, including details on one-hot encoding, label smoothing, and modifications to the loss function and evaluation metrics.
- **Backtesting Framework Integration:** A functional backtesting framework allows for evaluating trading strategies based on model predictions, calculating key performance metrics like cumulative return, Sharpe ratio, alpha vs. benchmark, and accuracy/F1-score.
- **Distinct Prediction Modules:** Separate, well-documented modules exist for predicting trend direction and reward magnitude.
- **Stop-Loss Mechanism:** A robust stop-loss mechanism automatically exits trades when they move against the predicted direction by a defined percentage (e.g., triggering a stop-loss at -2.5% within two days for a predicted return of +3.0%).
- **Prediction Confidence Thresholding:** A filtering mechanism executes trades only when specific confidence criteria are met, such as prediction magnitude exceeding a threshold or historical accuracy of the prediction bin being above a certain level, potentially using HPEP.
- **Risk-Based Weighting:** A system weights positions based on risk and return factors, including prediction confidence, inverse historical volatility, and the signal-to-noise ratio of predicted vs. actual returns. This optimized allocation prioritizes trades based on individual risk profiles.

## B. Codebase Finalization

This phase focuses on preparing the codebase for inclusion with the dissertation, ensuring its completeness, clarity, and reproducibility. This includes incorporating key model enhancements discussed elsewhere in the checklist, specifically related to exit strategies, learning from mistakes, and architectural improvements. A clear "code prompt" or instructions should be provided to guide readers through the codebase and reproduce the results presented in the dissertation.

The following steps outline the finalization process:

1. **Prepare Codebase for Submission:** Create a specific branch or tag in the version control system corresponding to the dissertation's finalized experiments. Include all code related to the experiments and analyses, ensuring it is complete, functional, error-free, and consistently formatted. Clearly specify all dependencies and required libraries. Remove any unnecessary experimental or debugging code.

2. **Provide Clear Execution Instructions:** Accompany the code submission with a "code prompt" or detailed instructions explaining how to execute the code and reproduce the core findings presented in the dissertation.

3. **Document Code Modules and Enhancements:** Organize the code into well-defined, documented modules. Each module should include a description of its purpose, inputs, outputs, and important implementation details. Critically, ensure comprehensive documentation for the following enhancements:

   **Exit Logic Enhancements:**

   - **Volatility-Aware Exit Thresholds:** Dynamic exit thresholds based on the Average True Range (ATR), calculated during trading simulation (`trade.py`) and scaling the exit tolerance (e.g., 1.5x ATR).

   - **Time-Based Confidence Decay:** Exit trades if the predicted reward doesn't materialize within the expected rally timeframe.

   - **Prediction Divergence Exit Logic:** Exit trades if retrained model predictions diverge significantly (e.g., >3-4%) from previous predictions for the same asset.

   - **Portfolio Contextual Exit Logic:** Monitor peer trades and exit underperforming trades within their prediction group.

   **Learning From Mistakes:**

   - **Error Map During Validation:** Track predicted and actual returns, logging and analyzing "bad" trades (e.g., predicted long positions with negative returns) to identify patterns for filtering or re-weighting similar samples in future training.

   **Architectural Enhancements:**

   - **Sample Re-weighting During Retraining:** Assign higher loss weights to misclassified, high-loss trades during retraining.

   - **Bootstrapping of Hard Cases:** Implement functionality for bootstrapping hard cases (details of this implementation should be included in the documentation).

Integrating these enhancements and providing clear documentation will ensure the reproducibility of the research and facilitate future development based on these concepts. While the overall project requires predicting "rally time," and this is not fully addressed in the current codebase, the inclusion of these enhancements lays the groundwork for its future implementation. The codebase documentation should clearly indicate where this functionality would be integrated when implemented.

## B. Codebase Finalization

This phase focuses on preparing the codebase for inclusion with the dissertation. This entails not only integrating the research outcomes but also ensuring the code is well-structured, documented, and easily understandable for reproducibility and review. This involves:

**1. Integrating Key Model Architectures and Data Processing Techniques:**

- **Vision Transformer (ViT) Implementation:** The finalized codebase must include the chosen ViT implementation for processing candlestick chart image sequences. Clear documentation should justify the selected approach (EfficientNet encoding or ViT-style patch embedding) and explain its implementation details.
- **Paired Image Dataset Generator:** The code for generating paired candlestick images (Picture A and Picture B) should be included and documented to facilitate training models that learn from temporal transitions.
- **CNN-LSTM Hybrid Architecture:** The finalized CNN-LSTM hybrid model code should clearly articulate the interaction between the CNN and LSTM components within the documentation.
- **Delta Feature Generation:** The chosen method for generating delta features (image subtraction or feature subtraction) should be clearly implemented and documented, along with a justification for the selection.
- **Benchmark Comparison Experiment:** The code for the benchmark comparison (static picture, picture-pair, and sequence) should be included, allowing for a clear performance comparison and justifying the final architecture choice.

**2. Incorporating Research Outcomes and Ensuring Code Clarity:**

- **Data Model and Dataset Generation:** The code must include the implementation details for generating the dataset of sequential candlestick windows. This includes:
  - Clear documentation of the mechanism for transforming time-series data into image sequences.
  - Implementation and documentation of the chosen delta feature calculation and incorporation method.
  - Code and documentation for dataset construction using the sliding window approach, generating sequences (t-2, t-1, t) with corresponding labels (return value, rally time, signal class).
- **Model Training and Evaluation:** Code related to model training and evaluation should be included, along with documentation of the chosen hyperparameters, loss functions, and evaluation metrics.
- **Prediction Output and Code Modularity:** The code should define the prediction output format clearly and provide documented functions or modules for processing these outputs.
- **Code Structure and Documentation:** The overall codebase should be well-structured, with consistent formatting, clear comments, and removed debugging code. Clear instructions on how to access and run the codebase should be provided.

This finalized codebase, with comprehensive documentation, will demonstrate a thorough understanding of the implemented models and techniques, ensuring reproducibility and facilitating future development.

## B. Codebase Finalization

This phase prepares the codebase for inclusion in the dissertation, ensuring reproducibility and demonstrating the technical implementation of the research. This includes addressing specific aspects of the Vision Transformer (ViT) implementation and providing a well-documented, accessible, and functional codebase.

**ViT Implementation Details:**

1. **Input Image Count (N) and Experimentation:** The code supports varying the number of input candlestick images (N) for the ViT (e.g., 3, 4, and 5). The process for configuring N and its impact on performance is documented, including any limitations encountered regarding the maximum permissible value of N.

2. **Positional Embeddings:** The ViT implementation includes positional embeddings to handle the sequential nature of the candlestick image inputs. The chosen positional encoding method is clearly documented.

3. **Masking and Padding:** The code implements masking and padding to accommodate variable-length input sequences. Shorter sequences are padded with blank chart images up to a defined maximum sequence length. The masking mechanism, which guides the ViT's attention, is thoroughly documented.

4. **Variable-Length Sequence Training:** The code supports training the ViT with variable-length sequences. The chosen method for handling variable length input (masking or dynamic positional encoding) and its rationale are documented.

**Codebase Preparation and Documentation:**

1. **Code Accessibility:** The codebase is readily accessible, likely through a version-controlled repository (e.g., GitHub, GitLab) with a specific branch or tag. Clear instructions are provided for accessing and running the code, including dependency management and environment setup.

2. **Code Completeness and Functionality:** The codebase is complete, functional, and free of errors. All dependencies are documented and included. Experimental code not directly relevant to the dissertation has been removed or clearly marked.

3. **Detailed Documentation:** The code is thoroughly documented with clear explanations for each module's purpose, functionality, parameters, and dependencies. This includes:

   - **Data Pipeline Documentation:** The data pipeline for the ViT, which accepts N candlestick images as input, is thoroughly documented.
   - **Memory Management Strategy:** The chosen memory management strategy (e.g., A, B, or C) and its rationale are documented, including the evaluation of its impact on flexibility, performance, training efficiency, and predictive power using metrics like Sharpe ratio, directional accuracy, MSE, and rally-time prediction accuracy.
   - **Key Module Examples:** Examples demonstrating the usage of key modules, such as those related to return extraction from predicted images and the image-to-image candlestick forecaster prototype, are provided.
   - **Reusable Components:** Reusable components, including data loaders, masking logic, and ViT wrappers, are documented to facilitate future work with financial time series data.

This finalized, well-documented, and readily accessible codebase ensures the reproducibility of the research and provides a clear understanding of the model architecture, training process, and data pipeline.
Codebase Finalization

This phase prepares the codebase for inclusion with the dissertation, ensuring its clarity, reproducibility, and alignment with the research presented. While not directly impacting model training or evaluation, this crucial step promotes transparency and facilitates future research.

The finalized codebase must include the following:

1. **Complete and Functional Code:** The entire codebase, ready for execution, should be provided. Clear instructions (a "code prompt") explaining how to run the code and reproduce the results presented in the dissertation are essential.

2. **Stable and Documented Dependencies:** Before dissertation submission, the codebase must be stable and complete. All dependencies should be clearly documented and easily installable. Remove any experimental or unused code, or clearly mark it as such.

3. **Well-structured and Documented Modules:** The code should be organized into well-documented modular components. Each module's documentation should explain its functionality, inputs, outputs, and role within the overall system. This documentation should specifically address:

   - **Model Evaluation Metric:** Implementation of an evaluation metric that assesses both financial returns and the visual plausibility of generated candlestick sequences. Explain how the metric balances financial performance with the accuracy of visual predictions.
   - **Model Training Goal:** Clear reflection of the primary goal of accurately predicting visual candlestick sequences within the training process and loss functions.
   - **Trading Decision Logic:** Explicit implementation and documentation of the decision-making process for _whether_ to execute a trade based on identified patterns.
   - **Handling Ambiguity in Trading Decisions:** Explanation of how the model translates visually accurate predictions into clear trading decisions and addresses potential ambiguities where visually realistic predictions may not lead to profitable trades.

This finalized and well-documented codebase, accompanied by clear execution instructions, ensures the research's transparency and reproducibility, contributing to its overall impact and facilitating a deeper understanding of the relationship between visual pattern prediction accuracy and financial performance. This documentation should also address how the code achieves causal grounding of predictions, mitigates training and evaluation fragility associated with image-based prediction, and distinguishes visual realism from actual trading value, ensuring the model generates actionable trading opportunities rather than just visually appealing charts.
Codebase Finalization (Facilitator)

This section outlines the final steps for preparing the codebase for inclusion with the dissertation. This involves providing well-documented, functional code that clearly demonstrates the implemented concepts, aligns with the dissertation narrative, and is easily understood and potentially reproducible.

The facilitator's role is to ensure the code reflects the core concepts explored in the dissertation, particularly the dynamic 2D plane and coordinate transformations, while maintaining clarity and conciseness. While the theoretical underpinnings informing the code's development are detailed in the dissertation itself, the code documentation should provide sufficient context for understanding its practical implementation.

The finalization process involves the following key steps:

1. **Code Delivery:** Provide the complete and functional codebase, accompanied by clear instructions for access, navigation, and execution. This deliverable should focus on the core implementation of the dynamic 2D plane concept, including relevant modules related to coordinate transformations, rotation matrices, and handling parabolas within the dynamic chart.

2. **Code Finalization:** Ensure the codebase is complete, tested, error-free, and in its final polished state _before_ dissertation submission. This includes removing any unnecessary files and ensuring consistent styling. The code's implementation of the dynamic 2D plane should align with the theoretical concepts investigated in the dissertation, such as moving frames on 1-manifolds and rotating orthonormal frames along paths.

3. **Comprehensive Code Documentation:** Provide detailed documentation within the code itself (e.g., docstrings, comments) to explain the functionality of different modules and functions. This documentation should cover the mathematical underpinnings, referencing relevant concepts, including:
   - Information balance and degrees of freedom within the dynamic 2D representation.
   - Clear explanations of how the two coordinates (u,v) and the three Euler angles (or rotation matrix) are used.
   - Connections to established mathematical and physical frameworks, such as parallel transport, affine connections, Frenet-Serret frames, the SE(3) group, and local inertial frames in general relativity. Illustrative code examples are encouraged.
   - Practical applications and limitations of the implemented methods (e.g., robotics, computer graphics, navigation, data compression), including limitations like path dependence, singularities, and computational overhead. The code structure should reflect these limitations, potentially through comments or dedicated modules addressing edge cases.
   - A concise algebraic summary using appropriate mathematical notation and equations to provide a rigorous and formal representation of the core concepts.
   - Potential future research directions, such as formulating an explicit rotation law, investigating curvature invariants, and generalizing the concept to surfaces.
   - Exploration of the application of the dynamic 2D frame concept to the three-body problem, including challenges, potential solutions (pairwise relative coordinates, barycentric frames, shape space representations), and recent advances like machine learning approximations and new periodic solutions.

Addressing these aspects ensures the codebase effectively complements the dissertation, providing a practical demonstration of the research and facilitating further exploration and understanding by others.
Codebase Finalization

This section outlines the final steps for preparing the codebase for inclusion with the dissertation. This involves refining the code's structure, documentation, and visualizations.

**Pre-Finalization Code Review:**

Before final submission, ensure the following aspects have been addressed:

- **Architectural Analysis:** Analyze the benefits and limitations of the chosen abstractions, considering relevant use cases. Investigate scenarios where certain aspects, like translational and rotational symmetries, might be redundant and document the impact of tracking orientation data.
- **Singularity and Chaos Investigation (if applicable):** If relevant, investigate and document the potential for singularities and chaotic behavior within the algorithms.
- **3D Traversal Feasibility (if applicable):** If the project involves 3D traversal, rigorously verify and document that the implemented approach allows complete traversal, at least in the forward direction of time.
- **Resource Cost Analysis:** Analyze and document the trade-offs between memory usage and computational cost of the chosen data representations and algorithms. Compare different approaches (e.g., storing 2D coordinates plus orientation versus 3D positions) and justify the selected approach.
- **Human Traversal Analogy (if applicable):** If relevant, explore and document analogies between the implemented approach and human traversal behavior.

**Finalization Steps:**

1. **Code Preparation and Submission:** Prepare the finalized codebase for submission alongside the dissertation (e.g., as supplementary material or via a publicly accessible repository). Include clear instructions on how to access and utilize the code.

2. **Code Refinement:** Ensure the code is well-commented, clean, and functions as expected. Remove any debugging code or temporary files not necessary for understanding and running the core project.

3. **Module Documentation:** Provide comprehensive documentation for each module, clearly explaining its purpose, inputs, outputs, and functionality.

4. **Visualization Enhancements:**

   - Create visually informative charts and images representing the underlying data and model outputs using `matplotlib` (avoiding `seaborn`). Each plot should be a self-contained figure (no subplots).
   - Improve the visualization of the planar trace, potentially including better visual representation, tooltips, annotations, or a separate tutorial.
   - Include a 3D helix visualization in its own figure with appropriate axis labels (X, Y, and Z) and the title "3D Helix in Laboratory Coordinates."
   - Visualize the relationship between the 3D helix and its 2D unfolded representation, demonstrating that a straight line trace on the 3D helix appears as a straight line in 2D when aligned with the moving frame of reference. Separate plots or an animation may be necessary. If using animation, consider performance implications.

5. **Data Storage and Logging:** Implement a robust and efficient data storage mechanism for planar coordinates (u, v) and orientation data. Consider using an arc length counter for planar coordinates and quaternions or rotation vectors for orientation, allowing reconstruction of the spiral path. Ensure compatibility with visualization and rendering functionalities.

By addressing these considerations, the codebase will be well-prepared for inclusion with the dissertation.

### B. Codebase Finalization

This section details the finalization of the codebase for inclusion with the dissertation. The code must be thoroughly documented, organized, and readily understandable for review and potential reproduction of the results. This includes addressing the specific technical implementations and considerations discussed in the previous sections.

**Dynamic Projection and PCA Implementation:**

- The code must clearly implement and document the dynamic 2D projection system, including its interaction with the trading model. This includes:
  - How candlestick data influences the re-centering and rotation of the feature space.
  - The specific mathematical transformation used for "rotation" (e.g., affine transform, PCA rotation, or a learned rotation).
  - The overall goal of the dynamic projection (e.g., prediction invariance to previous trend direction or focusing on relative local movement).
- If PCA is used, its implementation must be seamlessly integrated into the model pipeline (Vision Transformer or CNN) and clearly documented. This includes:
  - How PCA dynamically rotates and re-centers candlestick image sequences.
  - How this dynamic PCA optimizes the model's focus on relative (local) price behavior.

**Code Structure and Documentation:**

- The codebase must be organized into well-defined, documented modules for readability and ease of understanding. This modular structure should clarify the application of the dynamic projection system within the larger trading model architecture.
- Comprehensive documentation within the code should explain the following aspects of the dynamic PCA implementation:
  - Specific implementation steps for dynamic PCA, including feature extraction, the PCA calculation process, and how the transformed data is used by the model.
  - The data preprocessing pipeline, including window selection, feature preparation, PCA computation, principal direction identification, feature space rotation, data re-centering, and model input preparation.
  - The explored options for applying PCA within the network pipeline (e.g., preprocessing on raw data, on learned image embeddings, as a fixed layer, or as a data augmentation layer), including the rationale for the chosen approach.
  - The technical challenges and solutions related to backpropagation through dynamic transforms, including handling the non-differentiability of PCA, gradient flow, batch vs. sequential processing, online vs. offline training, impact on model gradients, and any differentiable approximations used.

**Submission and Reproducibility:**

- The finalized codebase should be free of any extraneous experimental or debugging code not essential for reproducing the reported results. All code related to the experiments must be functional and thoroughly commented.
- Clear documentation of each module's purpose, functionality, inputs, and outputs is crucial for reproducibility and understanding.

Codebase Finalization

This phase focuses on preparing the codebase for inclusion in the dissertation. This involves ensuring the `RotatingSnapshotGenerator` module is complete, well-documented, and easily understandable. This module encapsulates the dynamic plane implementation and should be designed for easy integration into the dissertation's experiments.

**Module Functionality and Documentation:**

- **Dynamic Snapshot Generation:** The `RotatingSnapshotGenerator` should clearly delineate the code for generating dynamic snapshots. Documentation should explain how the rotated window of 5-10 candlesticks is redrawn as an image, with candlesticks plotted relative to the new rotated axes (X' and Y'). It should be noted that time is no longer strictly horizontal in these rotated snapshots. This documentation must clearly explain how these images are prepared for input to the CNN or Vision Transformer.

- **Rotation and Translation Implementation:** The module should implement the core logic of the dynamic plane. This includes:

  - **Local Movement Vector Calculation:** Calculate the local movement vector (v) as the difference between the current price (P_current) and the previous price (P_previous): `v = P_current - P_previous`. Clearly define P_current and P_previous in terms of price and time (X,Y) coordinates within the code and documentation.
  - **Rotation Matrix Calculation:** Calculate the rotation angle (θ) using: `θ = arctan(v_y / v_x)`. Construct the rotation matrix R(θ): `R(θ) = [[cos(θ), sin(θ)], [-sin(θ), cos(θ)]]`. Apply this rotation to the candlestick window data.
  - **Dynamic Origin Shift:** Translate the rotated coordinates so the current price point is located at the origin (0,0). This ensures a consistent reference point for analysis.

- **Rotation Artifacts Handling:** Document the chosen interpolation techniques (e.g., anti-aliasing) used to minimize distortion from rotation within the relevant code sections.

- **Volatility Jump Handling:** Document how the code handles sudden, large price movements (volatility jumps). Include any smoothing or limiting techniques applied to rotation angle deltas.

- **Consistent Axis Scaling:** Document how consistent axis scaling (units per % move) is maintained across all generated frames. Explain the rationale behind the chosen scaling method and its impact on model training.

- **Pseudocode Pipeline:** Include a clear and well-commented pseudocode pipeline describing the `RotatingSnapshotGenerator` logic.

- **Conceptual Diagram:** Integrate a visual diagram illustrating the dynamic 2D plane's evolution and shifts into the code documentation.

**Codebase Delivery and Finalization:**

- **Provide Codebase:** Deliver the complete and functional `RotatingSnapshotGenerator` code (e.g., via a code prompt or repository link).

- **Finalize Codebase:** Before dissertation submission, ensure all components are functioning as expected, dependencies are clearly documented, and the code is well-commented.

This revised structure consolidates the repeated and fragmented information into a single, coherent description of the codebase finalization process, focusing on the key deliverable: the `RotatingSnapshotGenerator` module. It clarifies the required functionality, documentation, and delivery aspects, eliminating redundancy and improving overall readability.

## Codebase Finalization for Dissertation

This phase focuses on preparing the codebase for inclusion in the dissertation, ensuring its completeness, clarity, reproducibility, and robust functionality. This involves refining the implementation of the rotating dynamic plane generator, integrating it into the model training pipeline, and providing comprehensive documentation.

**Key Codebase Elements and Deliverables:**

1. **Rotating Dynamic Plane Generator Implementation:**

   - **Functionality:** The code should implement the rotating dynamic plane generator as described, including window selection, movement calculation using candlestick data (time, price, and optionally volume), dynamic frame construction using PCA, rotation, refocusing of the origin to the current data point, and snapshot rendering.
   - **Documentation:** Thorough documentation should explain each module's purpose and functionality. This includes details on:
     - How the plane is rotated and how trading data is incorporated.
     - The implementation of each functionality listed above.
     - How the generated 2D plane is formatted for input to the ViT or CNN.
     - The optimization techniques used for batch processing of time series data.
     - Handling of edge cases, specifically the behavior when fewer than two data points are provided. This could involve throwing an error or implementing a default behavior.
   - **Parameterization:** The implementation should include a parameter for controlling the window size, clearly demonstrating its effect on the dynamic plane generation and subsequent model training.
   - **Optional Volume Integration:** The code should offer the option to include volume data as a third dimension in the movement calculations, clearly documented and easily configurable.

2. **Model Integration and Experimentation:**

   - **ViT Training Pipeline Integration:** The code should demonstrably integrate the dynamic plane generator as a preprocessing step for the ViT model, with clear documentation and modularity.
   - **Static vs. Dynamic Frame Comparison:** The codebase should include the implementation of experiments comparing static and dynamic candlestick frames. This includes the training procedures, data handling, and evaluation metrics used for both approaches, along with clear documentation of the experimental setup and results.

3. **Data and Visualization:**

   - **Example Images:** Include example images showcasing the data model used for the CNN/ViT models:
     - Raw candlestick data _before_ transformation.
     - Candlestick data _after_ dynamic plane transformations (refocusing and alignment to principal axes).
   - **Enhanced Dynamic Plane Visualization:**
     - **Robust Initialization:** Address initialization errors in the dynamic plane animation, ensuring it handles scenarios with zero or one data points gracefully.
     - **Extended Sequence Simulation:** Implement the ability to simulate and visualize longer sequences of data.
     - **Smoothing Techniques:** Explore and integrate smoothing techniques (e.g., Heikin-Ashi) to improve visual clarity. _(Optional, but recommended)_
     - _(Optional)_ An animation visualizing the step-by-step evolution of the dynamic plane as new data points are added is highly encouraged to enhance understanding.

4. **Final Deliverables:**
   - **Complete Codebase:** Deliver the complete and functional codebase, potentially packaged with necessary dependencies and instructions for easy access and execution.
   - **Finalized and Polished Code:** Ensure the codebase is in its final, polished state before dissertation submission, addressing any outstanding bugs, cleaning up unnecessary code, and ensuring consistency in style and documentation.

By addressing these points, the finalized codebase will provide a comprehensive, transparent, and reproducible representation of the work, significantly strengthening the dissertation and facilitating future research.

## B. Codebase Finalization

This phase focuses on preparing a robust and well-documented codebase for inclusion with the dissertation. This encompasses delivering functional code, a standalone animation simulator showcasing dynamic plane rotation, a clear data processing pipeline, and illustrative visualizations. A complete and understandable codebase strengthens the dissertation's findings by ensuring reproducibility and facilitating future research.

The final codebase must include the following:

1. **Complete and Functional Code:** All scripts, modules, and dependencies required to run the models and generate the visualizations must be included. The code should be clean, well-commented, and free of debugging artifacts. All functionalities, including data processing, model training, and visualization, should be thoroughly tested.

2. **Standalone Animation Simulator:** A key component is a standalone simulator visualizing the dynamic plane rotation. This simulator should:

   - **Handle Edge Cases:** Gracefully handle scenarios with limited initial data points. Specifically, it should display a placeholder or an empty canvas when fewer than two data points are available, delaying rotation and plotting until sufficient data is present. Correctly format offsets to prevent dimension mismatches, regardless of the number of data points.
   - **Smooth and Stable Rotation:** Implement robust handling of PCA instability that can arise with limited data. The chosen smoothing technique for rotation matrices, implemented to prevent dimension blowups and ensure smooth transitions, should be clearly documented. The simulator should incorporate delayed rotation (starting after three or more stable points) and smoothing of the initial plane formation. Include clear usage instructions.
   - **Dynamic Visualization:** Clearly depict the dynamic movement of points and the live rotation and recentering of the frame within the visualization. Document the implementation details of these dynamic aspects.

3. **Data Processing Pipeline:** Include the complete data preprocessing steps and the batch generator for candlestick images, accompanied by evidence from small-scale tests validating its operation. This ensures reproducibility of the results.

4. **Data Visualization:** Include functionality to generate example images of both the raw Heiken Ashi input and the transformed Heiken Ashi input. These visualizations, although static, provide valuable snapshots of the data transformation process for inclusion in the dissertation.

5. **Comprehensive Documentation:** Provide detailed documentation for each module, explaining its purpose, functionality, and implementation details. Specifically, document the following:

   - **Rationale for Minimum Points for Rotation:** Explain the architectural decision to initiate plane rotation only after three stable points are available, emphasizing the avoidance of PCA instability.
   - **Rotation Matrix Smoothing Technique:** Detail the chosen smoothing technique and its role in ensuring smooth transitions and preventing errors.
   - **Data Preparation and Transformation:** Describe the data preprocessing steps, the batch generator, and the transformation applied to the Heiken Ashi input.

By addressing these requirements, the finalized codebase will be a valuable, understandable, and reproducible component of the dissertation.

## Codebase Finalization

This section details the finalization of the codebase for inclusion with the dissertation. The goal is to ensure clarity, reproducibility, and a comprehensive understanding of the research process. The finalized codebase should be submitted alongside the dissertation, ideally as a well-documented, readily accessible archive (e.g., a public repository).

The following key elements must be included:

**1. Core Heiken-Ashi Implementation:**

- **Heiken-Ashi Candle Generation:** Well-documented code for generating Heiken-Ashi candles from standard OHLC data. Documentation should clearly explain inputs, outputs, and internal logic.
- **Heiken-Ashi Plotting Function:** Documented function for plotting standard Heiken-Ashi candlesticks (green for up moves, red for down moves). Documentation should describe chart parameters, including the color scheme.
- **Dynamic Rotation and Recentering:** Code and documentation for dynamically rotating and recentering Heiken-Ashi data. Explain the midpoint calculation used for rotation and the rotation/recentering process.
- **Rotated Heiken-Ashi Plotting:** Documented function for plotting the rotated Heiken-Ashi data on a 2D plane. Clearly describe the visualization process and data representation.
- **Chart Saving Functionality:** Code for saving charts as PNG images (e.g., `/mnt/data/standard_heiken_ashi.png` and `/mnt/data/rotated_dynamic_plane_heiken_ashi.png`). Ensure these generated images are included in the codebase submission.

**2. Choppy Market Simulation:**

- **`generate_choppy_candlesticks(n=30)` Function:** This function creates synthetic candlestick data (open, high, low, close, and optionally time) representing a choppy, sideways market. This data tests the robustness of the dynamic plane implementation.
- **Choppy Market Visualization:** Generate and save standard (`/mnt/data/standard_heiken_ashi_choppy.png`) and rotated (`/mnt/data/rotated_dynamic_plane_heiken_ashi_choppy.png`) Heiken-Ashi charts from the choppy data using the previously defined functions. These visualizations demonstrate the impact of the dynamic plane transformation under challenging market conditions.

**3. Market Regime Visualization and Analysis:**

- **Market Regime Visualization:** Code to visualize the behavior of different market regimes (trend, reversal, sideways) within the dynamic plane.
- **Regime Analysis and Summarization:** Functionality to analyze and summarize the behavior of different market regimes within the dynamic plane, providing insights suitable for inclusion in the dissertation.
- **Comparative Visualization:** Code to generate a combined visualization showcasing standard and rotated dynamic plane Heiken-Ashi charts for three distinct market regimes: Trend-Reversal-Recovery, Choppy Sideways, and a third simulated regime (e.g., a strong linear uptrend or sharp V-shaped recovery).

**4. Market Regime Simulation:**

- **Regime Simulation Code:** Functions to simulate the three market regimes used in the visualizations, including clear documentation for reproducibility.

**5. Code Structure and Documentation:**

- **Code Availability:** The complete, functional codebase should be readily accessible (e.g., via a public repository link).
- **Modularity and Documentation:** Code should be modular and well-documented for easy understanding and review. Each of the functionalities described above should be clearly identifiable and explained. A clear prompt or guide should direct users to these functionalities within the codebase.

## B. Codebase Finalization

This phase focuses on preparing the codebase for inclusion in the dissertation. This involves providing a clean, well-documented, and functional codebase, along with addressing specific algorithmic refinements crucial for the project's integrity.

**Key Algorithmic Refinements:**

Two critical algorithmic improvements are essential for finalizing the codebase:

1. **Removal of Static Error Value in PCA Calculations:** A static error value (ID: 770e342b-1b75-438e-9384-53fb25c23d1e), representing the prediction error memory, has negatively impacted results and must be removed from the PCA calculations. The codebase should be updated to eliminate this scalar float and any dependencies on it.

2. **Lagging Rotation Deactivation Strategy:** The current lagging rotation mechanism introduces a lag. A method (ID: 235b86a2-22db-4dfc-94f4-8bb599126d28) for determining when to reduce this lag and return the system to its standard operational state must be implemented and documented within the relevant code modules.

**Further Enhancements Integrated into Finalization:**

Beyond the specific checklist items, two additional enhancements are crucial for a robust and reliable system and are therefore included in this finalization phase:

1. **Frame Coincidence Correction with Rolling Rewiring:** Implement a rolling correction mechanism, inspired by biological wound healing, for the dynamic PCA frame. This mechanism will gradually return the system to its normal state after correcting for dual records causing plateaus.

2. **Rolling Frame Correction (Implementation details to be provided):** [Provide specific implementation details for the rolling frame correction, including the chosen method and relevant parameters. This information was missing in the original text and needs to be added for completeness.]

**Deliverables:**

The key deliverable for this phase is the finalized, well-documented codebase, ready for inclusion in the dissertation. This includes:

- **Complete and Functional Codebase:** The codebase used for the dissertation experiments, complete with clear instructions (a "prompt") for execution and reproduction of results.
- **Polished and Stable Code:** A final version of the code, free of debugging code, unnecessary comments, and inconsistencies, with all dependencies clearly documented and easily installable.
- **Well-Documented Code Modules:** Modules with comprehensive documentation, including in-code comments explaining the purpose and functionality of each component, facilitating understanding and potential reuse by others.

While other tasks like sketching an error-signal augmented dynamic plane algorithm (ID: 34ba4c2b-f171-4939-9e09-81a360d6246b), implementing lightweight prediction-error feedback (ID: 84dad65e-4a0b-45c3-823e-50cc7d8855ce), and simulating a peripersonal vs. extrapersonal gap (ID: fbc85891-a39e-4195-8dbe-1831710e3244) are important, they fall outside the scope of _codebase finalization_ and are presumably addressed elsewhere in the project documentation.

## B. Codebase Finalization

This section details the finalization of the codebase for inclusion with the dissertation, incorporating key enhancements related to error detection, dynamic plane analysis, and rolling frame correction. This preparation ensures the code is robust, understandable, and readily reproducible by anyone reviewing the dissertation.

1. **Code Integration and Refinement:** The following enhancements will be integrated into the codebase:

   - **Robust Rolling Frame Correction:** Implement a robust rolling frame correction algorithm based on the wound healing analogy. This includes:
     - A prediction error buffer storing recent prediction errors (difference between predicted and realized movement within the rotated frame).
     - An error trend detector calculating the rolling mean and variance of the buffered errors. This detector triggers a correction mode when errors exceed a predefined threshold (e.g., 1-2x the rolling standard deviation). The threshold definition and implementation will be clearly documented.
     - A frame correction action applying small rotation adjustments or damping to the dynamic PCA frame upon entering correction mode. The implementation details will be documented.
     - A healing phase gradually removing the correction as errors subside, allowing the system to return to its normal operating state. The logic of this phase will be clearly explained.
   - **Mitigating Plateau in Dual Records:** The codebase will demonstrate how the rolling rewiring and frame correction algorithms address the plateau issue arising from dual records. Documentation will explain how these changes prevent lags and ensure adaptability to changing market conditions.
   - **Enhanced Error Trend Detector:** Develop a sophisticated error trend detector incorporating price, time, and volume influences and their dynamic interactions. This enhanced detector will consider deviations from expected relational movement within the dynamic 2D frame, including both distance and angular discrepancies.
   - **Deviation Vector Monitoring and Angular Error Tracking:** Implement mechanisms to monitor deviation vectors and track angular error (using the arccosine of the dot product of predicted and realized movement vectors divided by the product of their magnitudes) within the dynamic 2D plane.
   - **Pseudocode Implementation:** Convert all existing pseudocode, including that related to the above enhancements, into robust, production-ready code.

2. **Documentation and Modularization:** Comprehensive documentation will ensure code clarity and facilitate understanding:

   - **Detailed Code Modules:** Provide well-documented code modules, including comments and docstrings, explaining the functionality of each component, particularly the error analysis, dynamic plane implementation, and rolling frame correction. The mathematical and logical underpinnings of the code will be clearly explained.

3. **Code Preparation for Dissertation:** The finalized codebase will be prepared for inclusion in the dissertation:

   - **Codebase Submission:** Create a readily accessible archive (e.g., compressed archive or private repository link) containing the complete, finalized codebase. Clear instructions for accessing and navigating the code will be provided, adhering to dissertation submission guidelines.
   - **Final Code Verification:** Thoroughly test all code to ensure functionality and reproducibility before submission. This includes verifying seamless integration of all enhancements and consistent, expected results.

By addressing these points, the finalized codebase will be well-structured, documented, and readily understandable, contributing significantly to the overall quality and comprehensiveness of the dissertation.

## B. Codebase Finalization

This section details the finalization of the codebase for inclusion with the dissertation. This involves ensuring the code is well-structured, documented, easily understandable, and reproducible by reviewers and future researchers.

**Code Delivery and Execution:**

1. **Codebase Submission:** The complete codebase will be provided with the dissertation.
2. **Execution Instructions:** Clear instructions (a "code prompt") will accompany the code, detailing how to run the code and reproduce the experiments presented in the dissertation.

**Code Structure and Documentation:**

1. **Modularization:** The code is organized into well-defined modules with descriptive names and comprehensive documentation. Each module's documentation explains its purpose, inputs, outputs, and key implementation details.
2. **Dynamic Plane Implementation Details:** The dynamic plane implementation is thoroughly documented, including:
   - **Rotational Angles and Distance Vectors:** The number of rotational angles and distance vectors used in the dynamic plane implementation are explicitly defined and justified.
   - **Error Calculations:** Clear and documented code calculates the Distance Error (magnitude difference) and the Angle Error (orientation difference). The code clearly demonstrates that the initial frame creation rotation is _not_ included in the error calculation.
   - **Diagram of Rotations and Prediction Error:** A diagram illustrates the two rotation layers (global frame transformation via PCA and local vector misalignment) and how prediction error relates to these rotations.
3. **Freeze and Compare Technique:** The "Freeze & Compare in Dynamic PCA Frame" technique is implemented and documented. This technique freezes the dynamic PCA frame computed at time 't' and uses it for predicting and evaluating movements over a short future horizon (e.g., 1-3 candlesticks). The realized movement at 't+1' is reprojected back into the PCA frame from time 't' for consistent comparison. Pseudocode and a visual simulation are included to clarify the implementation and validate the alignment process. This approach simplifies the error calculation and assumes market structure remains relatively stable over these short intervals, an assumption particularly relevant outside of high-frequency trading.
4. **PCA Basis Data Structure:** A lightweight and efficient data structure stores each window's PCA basis (rotation matrix) for reprojection and code organization.
5. **Visualizations:** Visualizations distinguish the "freeze frame" and "projection" steps from the distance/angular error calculations, clarifying the novel aspects of the dynamic plane approach. A visual simulation demonstrates the alignment of predicted and realized movements within the dynamic frame.
6. **PCA Plane Consistency:** The code and documentation address how the 2D plane derived from PCA maintains consistency between predicted and realized movement matrices, explaining any necessary transformations or alignment steps.
7. **Lightweight Frame Adjustments:** The code includes a mechanism for applying small, corrective adjustments to the dynamic frame when accumulated angular errors exceed a defined threshold, counteracting drift and maintaining alignment over time.

**Finalization Steps:**

Before submission, the codebase will be finalized to ensure it is complete, functional, and error-free. Any remaining bugs or incomplete implementations will be addressed. All temporary or debugging code will be removed. This finalization will occur before dissertation submission to ensure consistency between the code and the write-up.

## Codebase Finalization and Documentation

This section details the finalization of the codebase and the generation of accompanying documentation for inclusion with the dissertation. The focus is on ensuring the accurate calculation, analysis, and interpretation of errors related to the dynamic PCA plane implementation, specifically addressing vector deviation and frame drift.

### Core Modules and Documentation

The following modules require thorough documentation and refinement:

- **PCA Frame Management:** The chosen PCA frame management method ("Freeze Frame" or "Reproject Realization") must be fully implemented and documented. This documentation should clearly explain the chosen method, contrast it with traditional distance and angular error calculations (which assume a static PCA plane), and demonstrate how it addresses shifting PCA frames and improves comparison accuracy. A numerical example with visualizations (e.g., plots of predicted and realized movement paths in both frozen and shifted frames) should be included to illustrate the practical application and impact of the chosen method. Furthermore, pseudocode outlining the "Freeze and Correct" module's logic should be provided for clarity.

- **Dynamic Error Calculation:** The code must incorporate a robust error checking mechanism that accounts for shifting PCA planes. This involves calculating deviation errors between PCA1 and PCA2 for both real and predicted values. The following error calculations must be implemented and documented:

  - **Vector Deviation Error:** Calculated within the dynamic local frame.
  - **Frame Shift Error:** Calculated as the difference between two sets of basis vectors (frames) using principal angles between subspaces. In the 2D case, this translates to calculating the angle between PCA1<sub>t</sub> and PCA1<sub>t+1</sub>, and between PCA2<sub>t</sub> and PCA2<sub>t+1</sub>. The Frame Error is calculated as: Frame Error = α _ Angle between PCA1 vectors + β _ Angle between PCA2 vectors, where α and β are tunable weights whose impact must be explained.
  - **Total Error:** Calculated as the sum of the Vector Deviation Error and the Frame Shift Error: Total Error = Vector Deviation Error + Frame Shift Error.

  Pseudocode outlining the Total Error calculation, incorporating both the Vector Error and Frame Shift Error, should be included for verification and clarity.

- **Weighted Error Calculation:** The final error calculation should implement the following weighted formula, allowing for configurable weighting parameters:

  - Vector Error = α₁ ⋅ d<sub>vec</sub> + α₂ ⋅ θ<sub>vec</sub>
  - Frame Shift Error = β₁ ⋅ θ<sub>PCA1</sub> + β₂ ⋅ θ<sub>PCA2</sub>
  - Total Error = γ₁ ⋅ Vector Error + γ₂ ⋅ Frame Shift Error

  Where:

  - α₁, α₂ control the trade-off between distance (d<sub>vec</sub>) and angle (θ<sub>vec</sub>) inside the frame.
  - β₁, β₂ control the trade-off between PCA1 drift (θ<sub>PCA1</sub>) and PCA2 drift (θ<sub>PCA2</sub>).
  - γ₁, γ₂ control the overall trade-off between prediction error and frame instability.

  The documentation must explain the meaning and impact of each parameter (α, β, γ).

### Additional Analyses and Refinements

Beyond the core modules, the following analyses and refinements should be incorporated:

- **Simulation:** A small-scale simulation demonstrating the step-by-step calculation of vector deviation and PCA frame drift should be implemented and included as an illustrative example.

- **Frame Drift as Confidence Indicator:** Explore and implement functionality to utilize Frame Drift Error as a confidence indicator for trade decisions (e.g., deciding whether to execute or hold a trade based on the error magnitude).

- **Error Component Weighting Analysis:** Investigate and document different weighting schemes for error components, including experimenting with separate weights for vector deviation and frame shift errors, comparing these schemes to the approach using alpha and beta parameters within the frame error calculation, and justifying the chosen approach.

- **Error Normalization:** Implement and document a strategy for handling the combination of distance and angular errors, addressing their different units and scales. This may involve normalization or other adjustments to ensure accurate aggregation.

### Codebase Submission

Finally, a clean, understandable, and reproducible version of the finalized codebase should be prepared for inclusion with the dissertation. This includes removing debugging code, ensuring consistent formatting, adding comprehensive comments, and providing clear instructions on how to access and execute the code (e.g., via a compressed archive or a link to a version control repository).

### Codebase Finalization

This section details the finalization of the codebase for inclusion with the dissertation. This involves ensuring completeness, clarity, reproducibility, and thorough documentation. The following key areas must be addressed:

**I. Core Dynamic Plane and Error Correction**

The core modules related to dynamic plane implementation and error correction must be complete, functional, and clearly documented:

1. **Rolling Error Buffer:** Implement a rolling buffer to store the total error over a recent window of _N_ steps (e.g., 5-10). Clearly document the chosen buffer size (_N_) and its justification.

2. **Rolling Statistics:** Calculate and maintain rolling statistics (mean and variance) based on the errors stored in the rolling buffer. Ensure the implementation details for these calculations are clear and accessible.

3. **Wound Phase Detection:** Implement the logic for detecting the "Wound Phase," triggered when the mean error exceeds _k_ times the rolling standard deviation (e.g., _k_=2). The value of _k_ should be configurable and its rationale documented.

4. **Correction Factor Application:** During "Correction Mode," apply a correction factor to either the PCA rotation or the smoothing operations. Thoroughly explain the implementation of this correction, its impact, and its relationship to the error.

5. **Healing Phase Detection:** Implement the logic for transitioning to the "Healing Phase," initiated when the mean error falls below a threshold based on the rolling standard deviation (e.g., 1-1.5 times). Document the chosen threshold and its justification.

**II. Error Detection and Healing System Module**

A dedicated, well-structured, and documented module for the Error Detection and Healing System must be created, incorporating the following:

1. **Module Structure:** Clearly separate the different system components within the module for improved understanding and maintainability.

2. **Correction Factor Decay:** Implement a decay mechanism (exponential with λ=0.95 or linear) for the correction factor during the Healing Phase. Document the chosen mechanism and its parameters.

3. **Dynamic Correction Re-entry:** Implement logic for dynamically re-entering Correction Mode if error spikes are detected during the Healing Phase. Clearly define the re-entry conditions and actions.

4. **Parameter Tuning:** Provide recommended initial tuning values for thresholds and decay rates, considering various market regimes. Document these values and their rationale.

5. **Healing Phase Visualization:** Create a visualization demonstrating the triggering and behavior of healing phases over a sequence of market data to illustrate its function and impact.

**III. Prediction Tracking and Dynamic Performance Adjustment**

Implement and document features for tracking prediction performance and dynamically adjusting the system:

1. **Prediction Tracking and Evaluation:** Implement a mechanism to record and store predicted outcomes (direction and magnitude) and evaluate their correctness (+1 for correct, 0 otherwise), independently of visualization issues.

2. **Rolling Prediction Correctness Buffer:** Implement a rolling buffer to store the last _N_ prediction correctness scores (1s and 0s).

3. **Dynamic Decay Rate Adjustment:** Implement a mechanism to adjust the decay rate inversely proportional to recent prediction accuracy, based on the rolling correctness buffer. This adjustment should be performance-driven and not rely on an arbitrary exponential decay function.

**IV. Codebase Preparation for Submission**

Prepare the codebase for submission alongside the dissertation:

1. **Code Organization:** Organize the code into a dedicated branch or tag in the repository.

2. **Dependencies:** Document all necessary dependencies and provide clear instructions for running the code.

3. **Finalization:** Address any outstanding bugs, complete relevant unfinished features, and ensure comprehensive code documentation.

This comprehensive finalization process ensures the codebase's reproducibility, clarity, and completeness, facilitating a thorough understanding of the implemented system.
Codebase Finalization

Prior to dissertation submission, the codebase will be finalized and prepared for inclusion as supporting material. This includes delivering the code, ensuring its stability and completeness, and providing thorough documentation of key modules, especially those concerning data preprocessing, principal component analysis (PCA), and the core healing logic. The code will be delivered via [Method of delivery, e.g., shared repository link, compressed archive]. A clear prompt explaining how to access and navigate the codebase will be provided.

**Documentation:**

Comprehensive documentation within the codebase will facilitate understanding and reproducibility. Key areas of documentation include:

- **Data Preprocessing for PCA:** The normalization process applied to price (P), time (T), and volume (V) before PCA will be documented. Specifically, z-score normalization within each rolling window of N data points will be detailed, including the formulas used for centering and scaling each feature to zero mean and unit variance:

  Compute
  $\mu_t = \frac{1}{N}\sum_i t_i,\quad \sigma_t = \sqrt{\frac{1}{N}\sum_i(t_i-\mu_t)^2}$
  (and similarly $\mu_p,\sigma_p$ ; $\mu_v,\sigma_v$ ).

  Build
  $X_{\text{scaled}}[,i,.,.] = \Bigl(\tfrac{t_i-\mu_t}{\sigma_t}, \tfrac{p_i-\mu_p}{\sigma_p}, \tfrac{v_i-\mu_v}{\sigma_v}\Bigr).$

  Time will be represented as fractional elapsed time within each window, encoded as a value between 0 and 1. This approach addresses potential issues with non-linear time intervals and provides a consistent scale. The rationale for choosing this over alternative time representations, such as relative time indices or absolute clock time, will be explained.

  Volume will be transformed prior to z-score normalization to address its heavy-tailed distribution. The chosen transformation (either log transformation: $v_i' = \log(1 + v_i)$, or robust scaling using the median and interquartile range) will be specified.

  Price values will be transformed into relative returns (either percentage change or log returns), calculated relative to the first price in each window. This normalization provides a consistent scale for price movement and mitigates the impact of extreme values.

- **PCA Implementation:** The PCA implementation will be documented, including the use of singular value decomposition (SVD) and the selection of the first two principal components:

  `u, s, vh = np.linalg.svd(X_scaled, full_matrices=False)
axes = vh[:2]   # two principal directions in T-P-V space`

- **Healing Logic:** The updated healing logic, incorporating a dynamic correction factor based on mean prediction correctness, will be thoroughly documented. This includes:

  - A formal, modular pseudocode representation of the system.
  - The `dynamic_decay_rate(mean_correctness)` function, which calculates the decay rate of the correction factor using the formula: `Decay Rate = 1 - (mean_correctness - healing_threshold)`.
  - The rationale behind the chosen healing thresholds (initially set between 75-80% directional correctness).
  - A simulated toy example demonstrating the healing process, from initial "wound" to correction and true healing as predictive accuracy is regained.

This detailed documentation, combined with a clear code structure and the provided access instructions, will ensure the codebase effectively supports the research presented in the dissertation.
Data Preprocessing and Code Finalization

This section details the necessary data preprocessing steps and the finalization of the codebase for inclusion in the dissertation. All code modules implementing these steps must be clearly documented and easily understandable.

**A. Data Preprocessing**

The following data transformations are crucial for preparing the data for Principal Component Analysis (PCA) and must be implemented within the provided code:

1. **Time Transformation:** Raw timestamps are converted to fractional elapsed time within each window. This is calculated by subtracting the minimum timestamp from each timestamp and dividing by the total time elapsed within the window (the difference between the maximum and minimum timestamps). This process captures data arrival rates and its implementation should be clearly documented.

2. **Price Transformation:** Price data is transformed into window-relative returns, using either percentage or log returns. The first price within the window serves as the reference point. This normalization facilitates comparison of price movements across different stocks or time periods and reduces the influence of absolute price magnitude on the PCA. The chosen return type (percentage or log) and its rationale must be documented.

3. **Volume Transformation:** A log transformation, `np.log1p(volume)`, is applied to the volume data to mitigate the impact of extreme values. This is followed by robust scaling using the median and interquartile range (IQR) to normalize the data and minimize the effect of outliers.

4. **3-Dimensional Matrix Construction:** A 3-dimensional matrix is constructed for PCA. Columns represent fractional elapsed time, relative return (or log return), and the scaled volume data. The code and documentation should clearly explain the construction and purpose of this matrix as input for PCA.

5. **Principal Component Analysis (PCA):** PCA is applied to the constructed 3-dimensional matrix. The implementation and resulting principal components should be well-documented, explaining how they capture the correlations between the three input features.

**B. Codebase Finalization**

This stage focuses on preparing the codebase for inclusion with the dissertation. The code should be well-documented, modular, and focused on the core data transformations. Key aspects of finalization include:

1. **Normalization to [-1, +1]:** Following the initial transformations, time (`time_frac`), log-transformed volume, and log return of price are further normalized to the range of -1 to +1 within each respective window using min-max scaling. This prepares the data for use in downstream analysis, such as PCA.

2. **Outlier Clipping:** Extreme outliers for both the log-transformed volume and the log return of price are clipped at the 5th and 95th percentiles before min-max scaling.

3. **Visualization:** The code should include functionality to generate candlestick chart images both before and after the transformations. These visualizations should be based on single-day charts with 10-minute intervals. Five distinct examples of price and volume action, showcasing the effect of the transformations, should be included. Each example should consist of a pair of candlestick charts (pre- and post-transformation) suitable for direct comparison and presentation.

4. **Code Delivery and Documentation:** The finalized codebase should be delivered via a readily accessible method (e.g., a shared repository link or compressed archive). Clear documentation within the code, using comments and docstrings, is expected for each module performing these transformations. This ensures reproducibility and clarity of the dissertation's technical contributions.
   Codebase Finalization

This section outlines the final steps for preparing the codebase for inclusion with the dissertation. This involves ensuring the code is organized, documented, and easily understandable, accurately reflecting the final methodology and analyses presented in the dissertation.

The finalized codebase should include:

- **Complete and Functional Code:** The codebase should be complete, organized, and free of unnecessary files or debug code. It must run without errors and produce all results and visualizations discussed in the dissertation. This includes the code for data transformation (log returns, PCA rotation, and normalization of time, log return, and log volume data), candlestick chart generation, and dynamic plane snapshots.

- **Documented Modules for Data Processing and Visualization:** Well-documented modules are crucial for clarity and reproducibility. Modules should include clear explanations of functionality, inputs, outputs, and any mathematical transformations applied. This is particularly important for modules handling data normalization (scaling to [-1, 1]), PCA rotation, candlestick chart generation (including various market patterns like uptrends, downtrends, reversals, sideways chops, and breakout spikes), and dynamic plane snapshot generation. Documentation should explain how these transformations contribute to the final visualizations and model input.

- **Clear Implementation of the Dynamic Plane Projection:** The code implementing the dynamic plane transformation, including PCA rotation and data scaling, must be well-structured and commented. The implementation should be easily reproducible, especially for the "Breakout Spike then Stabilize" scenario with its volume variations. The code and ideally the data generating the PCA patterns for this scenario should be included for review.

- **Documentation of Algorithmic Choices:** The codebase should clearly reflect and document all final algorithmic choices and parameter selections. The rationale behind these choices, including any rejected approaches (e.g., if bivariate spline interpolation was considered but not used), should be clearly documented within the relevant modules.

- **Execution Instructions:** Clear instructions (e.g., a "code prompt" or README file) should be provided to guide users on executing specific parts of the codebase relevant to the visualizations and analyses presented in the dissertation.

The goal is to provide a comprehensive, understandable, and reproducible codebase that fully supports and clarifies the findings presented in the dissertation.
Codebase Finalization

This section details the finalization of the codebase for inclusion with the dissertation. This involves providing a complete, well-documented, and easily understandable version of the code that enables others to reproduce the research findings. While the core research focuses on dynamic frame construction and prediction, the finalized codebase should encompass all supporting elements, including data preprocessing, model training, backtesting, and the self-correction mechanism. Furthermore, addressing outstanding architectural considerations, like multi-scale temporal modeling, is crucial before finalization.

The following steps outline the finalization process:

1. **Codebase Preparation and Packaging:** The complete and functional codebase must be prepared for submission. This includes all scripts related to data acquisition and preprocessing, dynamic frame generation, model training and prediction, backtesting, evaluation, and the self-correction mechanism. Clear instructions on how to access and run the code (e.g., via a compressed archive or repository link) should be provided.

2. **Code Refinement and Documentation:** The codebase should be thoroughly reviewed and refined. This includes:

   - Removing any debugging code, unused variables, or temporary files.
   - Ensuring consistent formatting and adherence to coding style guidelines.
   - Adding comprehensive comments to explain the purpose and functionality of each code section.
   - Documenting the dynamic frame generation process, including PCA calculation, projection, re-centering, and image rendering.
   - Clearly delineating the implementation of the self-correction mechanism, encompassing Vector Deviation Error, Frame Shift Error, "Wound Detection," and the "Healing Phase" logic.

3. **Modular Code Structure:** The code should be organized into logical modules to enhance readability and maintainability. Key modules should include:

   - **Data Acquisition and Preprocessing:** Handles OHLCV data acquisition, adjustments, and normalization of Time, Price, and Volume data.
   - **Dynamic Frame Generation:** Implements the core logic for creating the dynamic 2D plane, including PCA, projection, re-centering, and image generation using candlestick or Heiken-Ashi representations.
   - **Model Training and Prediction:** Defines the model architecture (e.g., Vision Transformer), training process, and prediction generation (2D movement vector and Rally Time).
   - **Backtesting and Evaluation:** Implements the backtesting logic, including performance metrics calculation.
   - **Self-Correction Mechanism:** Implements the feedback loop with its components, including Vector Deviation Error, Frame Shift Error, "Wound Detection," and the "Healing Phase."

4. **Multi-Scale Temporal Model Integration:** The model should be extended to incorporate daily, weekly, monthly, quarterly, and yearly contexts. This requires:

   - Designing and implementing an architecture that integrates these different timescales, potentially using a weighted sum approach.
   - Determining and documenting the specific design for multi-scale integration, including weight assignment to each timeframe and handling cyclical patterns.
   - Implementing and testing the code changes related to this multi-scale integration.

Each module should be well-documented, explaining its purpose, inputs, outputs, and key algorithms or logic used. This structured and documented approach will facilitate understanding and potential extension of the codebase for future research.
This section details the finalization of the codebase for dissertation submission, ensuring reproducibility, clarity, and completeness. This process focuses on refining the code structure, documentation, and supporting materials to align with the core research contributions.

The finalized codebase must reflect the following key elements:

- **Multi-Scale Data Processing and Preparation:** Modularized code for processing and preparing datasets across multiple timeframes (intraday, daily, weekly, monthly) using the Dynamic Rotating Plane method. This includes clear documentation and organization for easy access to different timeframe datasets.

- **Non-Hierarchical Attention Model (Optional):** If implemented, a well-defined, documented module for the Non-Hierarchical Attention Model, showcasing its interaction with the multi-scale data. If omitted, a clear explanation and potential future integration points should be documented.

- **Dynamic Rotating Plane Implementation:** A dedicated, well-documented module implementing the 2D plane construction from normalized Time, Price, and Volume data using PCA, including the dynamic re-centering on the latest data point.

- **Model Evaluation and Analysis Implementation:** Code structured to facilitate comparison between the multi-scale model and a baseline intraday model. This includes functions for performance metric calculation and integration with attribution analysis tools (e.g., Grad-CAM, SHAP), along with easy selection of specific market events for analysis.

Furthermore, the following aspects, crucial for reproducibility and clarity, will be addressed during finalization:

- **Data Normalization:** Clear documentation and modularization of the code for normalizing Time (fractional elapsed time), Price (log-returns), and Volume (log-transformed and robustly scaled) within a consistent range (e.g., [-1, +1]).

- **Vision Transformer (ViT) Implementation:** Well-structured and thoroughly documented ViT model code, including explanations of architecture, hyperparameters, input processing, and how dynamic planes from each timescale are processed as a sequence of tokens.

- **Self-Correcting Mechanism Implementation:** Clear and well-documented code for the self-correcting mechanism, detailing the "Total Error" calculation (including "Vector Deviation Error" and "Frame Shift Error"), the "correction mode" implementation, and the gradual healing process linked to prediction accuracy.

The deliverables for this stage include:

1. **Codebase for Dissertation:** The finalized, documented codebase, ideally version-controlled, with clear instructions (a "code prompt") for execution and interaction.

2. **Detailed Code Modules:** Comprehensive documentation for key modules, particularly those related to data normalization, the ViT model, and the self-correcting mechanism.

3. **Finalized Codebase:** A complete and functional codebase, submitted _before_ the dissertation, ensuring consistency between the two.

### B. Codebase Finalization (Facilitator)

This section outlines the process of preparing the codebase for submission alongside the dissertation. A clean, well-documented, and easily reproducible codebase is crucial for demonstrating the technical rigor of the research and enabling others to verify and build upon the work.

1. **Provide Codebase for Dissertation:** This involves packaging the relevant code files, including any necessary scripts or configuration files, and providing clear instructions on how to access and execute the code. This might involve creating a dedicated repository (e.g., on GitHub, GitLab) or providing a compressed archive. A concise "code prompt" within the dissertation should guide the reader on how to reproduce the experiments and results. This prompt should be tailored to the expected technical expertise of the dissertation committee, balancing detail with conciseness.

2. **Finalize Codebase:** Before submitting the dissertation, ensure the codebase is complete, functional, and free of any debugging artifacts, unnecessary files, or experimental code not directly relevant to the presented research. Thoroughly test the code to confirm it runs without errors and produces the reported results. Address any remaining bugs or inconsistencies and ensure the code adheres to best practices for readability and maintainability, including consistent formatting, meaningful variable names, and clear comments.

3. **Provide Detailed Code Modules:** Organize the codebase into well-defined, logical modules with comprehensive documentation. Each module should have a descriptive docstring explaining its purpose, inputs, outputs, and any relevant implementation details. Use comments within the code to clarify complex logic or non-obvious operations. Consider using a documentation generator (e.g., Sphinx, JSDoc) to create structured and navigable code documentation, further enhancing readability and understanding for the dissertation examiners and future users. Key algorithms and data processing steps should be clearly documented and potentially cross-referenced with the dissertation text.
   Codebase Finalization

This section details the final steps to prepare the codebase for inclusion with the dissertation. This involves ensuring the code is well-documented, organized, easily understandable, and reproducible. The following key functionalities and their corresponding code modules require thorough documentation:

1. **Data & Timeframe Configuration:** Document the module responsible for selecting the asset universe (NIFTY 50, NIFTY 500, Custom Watchlist) and date range, including the automatic training/validation split. Clearly explain how the data is loaded, processed, and partitioned.

2. **Dynamic Plane Configuration:** Thoroughly document the model's perception system implementation, detailing how candlestick types, the local window size, and feature inclusion (price, time, volume) are handled.

3. **Model & Learning Architecture:** Provide comprehensive documentation for model selection, hyperparameter tuning, optimizer settings, and loss function selection. Include specific details regarding ViT hyperparameters (patch size, embedding dimensions), multi-scale context fusion, optimizer choices (Adam, SGD, AdamW), and the selected loss functions.

4. **Self-Correction System:** Document the self-correction system implementation, including clear explanations of the wound detection threshold, healing trigger parameters, and their adjustment mechanisms.

5. **Live Visualization:** Document the code responsible for visualizing the standard Heiken-Ashi chart and the dynamic 2D plane image. Explain how the visualization is generated, updated, displayed side-by-side, and synchronized with incoming candlestick data.

In addition to the core functionalities above, the following refinements, derived from recent development tasks, should be incorporated and documented before finalization:

- **Smooth Frame Rotations:** Implement and document a smoothing mechanism (e.g., using an exponential moving average) for frame rotations within the Dynamic Plane Configuration to mitigate jitter during volatile market conditions. Include a user-adjustable smoothing factor.

- **Lookahead Period for Predictions:** Implement and document the ability to specify a 'lookahead period' for predictions, allowing the model to forecast a defined number of future candlesticks (e.g., 1, 3, 5, 10).

Furthermore, the following architectural decisions and implementation details require finalization and clear documentation:

- **Intraday Data Library:** Document the chosen library for managing long-term intraday data, including installation and setup instructions. Ensure consistent usage throughout the codebase.

- **Categorical Model Input:** Document the implementation of model inputs based on market capitalization, sectors, and share price bins. Clearly explain the categorization logic, binning methods, and any associated preprocessing steps.

- **Categorical Rally Time Calculation:** Document the rally time calculation methodology based on market cap, sector, and share price. Provide a clear rationale for using these categories.

- **Dynamic Capital Allocation:** Document the dynamic capital allocation strategy, including the chosen algorithm, parameters (e.g., starting capital, probability distribution of trades), and implementation details. Ensure the code is robust and error-free.

Finally, providing clear instructions (a "code prompt") accompanying the codebase is crucial for facilitating easy setup, execution, and understanding of the project for anyone reviewing the dissertation. This should include details on environment setup, dependencies, and running the code. The code itself should contain clear and concise comments explaining complex logic and non-obvious implementation details. Thorough testing and the removal of any unnecessary or experimental code not directly relevant to the research are essential before final submission.
Codebase Finalization

This section details the finalization of the codebase for inclusion with the dissertation. The process focuses on ensuring the code is complete, functional, well-documented, and easily reproducible by the dissertation committee. While additional features like UI/UX enhancements and a robust experiment management system are valuable for future development, they are outside the scope of this dissertation-focused finalization.

The finalization process encompasses the following key aspects:

**1. Core Functionality and Reproducibility:**

- **Complete Code Submission:** The complete codebase, including all scripts, notebooks, and supporting files, will be provided in a readily accessible format (e.g., a compressed archive).
- **Clear Execution Instructions (Code Prompt):** A comprehensive "code prompt" will accompany the codebase, detailing the steps required to execute the code and reproduce the experiments presented in the dissertation. This will include clear instructions on setting up any necessary dependencies.
- **Integrated Functionalities:** The following core functionalities, central to the dissertation's findings, will be fully integrated and documented within the codebase:
  - **Context Awareness:** Dynamic determination of optimal candlestick count per frame and total frame count, with clear documentation and demonstration of its adaptability.
  - **Transfer Learning:** Code and documentation for evaluating transfer learning effectiveness across different markets (US-US, US-India, and India-India), including execution instructions.
  - **Context-Aware Periodicity:** Implementation of weighted predictions based on various periodicities (daily, weekly, monthly, quarterly, and yearly), incorporating frame count, candles per frame, and stock categories. Clear documentation will explain the weighting logic and category-specific optimizations.
  - **PCA Analysis:** Reproducible implementation of the complete PCA process, including the dynamic plane implementation using 2 PCA axes, with clear usage instructions.
  - **Hyperparameter Permutation Testing:** Functionality to perform a 'try all permutations' test for hyperparameters within specified date ranges for a single epoch, with clear execution instructions.

**2. Code Quality and Maintainability:**

- **Code Cleanup:** Removal of any unnecessary or experimental code, ensuring only essential code remains.
- **Readability and Consistency:** Consistent formatting, meaningful variable names, and comments to explain complex logic will enhance code readability.
- **Functionality Verification:** Thorough testing of the entire codebase to ensure it functions as expected and produces the results presented in the dissertation.

**3. Comprehensive Documentation:**

- **Modularization:** The codebase will be organized into well-defined modules to improve understandability and potential reuse.
- **Docstrings:** Docstrings within functions and classes will explain their purpose, parameters, and return values.
- **README File:** A comprehensive README file will provide an overview of the project structure, dependencies, and execution instructions.
- **Inline Comments:** Inline comments will be used to clarify specific code sections where necessary.

This rigorous finalization process will deliver a clean, well-documented, and readily usable codebase that effectively supports the research and findings presented in the dissertation.
Codebase Finalization

This section outlines the final steps for preparing the codebase for inclusion with the dissertation. This involves ensuring the code is organized, well-documented, and easily understandable for review.

1. **Codebase Delivery:** Deliver the complete and functional codebase, including any necessary scripts or instructions for execution ("code prompts"). The delivery method (e.g., repository link, archived file) will be determined based on dissertation submission requirements. If generated using AI tools, include the prompts used for reproducibility.

2. **Codebase Refinement:** Before final submission, ensure the codebase is polished and ready for review:

   - **Comprehensive Documentation:** Thoroughly document the codebase using comments, docstrings, and a README file. Document each module's purpose, functionality, dependencies, inputs, and outputs. Clearly explain any architectural decisions, especially those related to the project's philosophical framework (if applicable). If a specific philosophical framework (e.g., Bhagavad Gita, Dharmic principles) guided the architecture, ensure the documentation explicitly maps code modules to the relevant principles and justifies the chosen implementation. Highlight key code sections related to ethical considerations (e.g., data integrity, risk management).
   - **Code Cleanup:** Remove unnecessary code, debug statements, and temporary files. Ensure consistent code style and formatting.
   - **Version Control:** Use a version control system (like Git) for tracking changes and ensuring reproducibility.

3. **Modular Structure:** Organize the code into well-defined, documented modules. Each module's documentation should clearly explain its purpose, implementation details, and integration within the larger project architecture. This modular approach enhances readability and maintainability. If relevant, explain how the code integrates with external APIs (e.g., Zerodha KiteConnect) and document the integration thoroughly.

## B. Codebase Finalization

This section outlines the final steps for preparing the codebase for inclusion with the dissertation. The codebase should be complete, functional, and easy to understand for anyone reviewing the dissertation.

1. **Provide Codebase and Execution Instructions:** The complete and functional codebase should be submitted alongside the dissertation, ideally as a compressed archive or a link to a version-controlled repository (e.g., Git). Clear and concise instructions should be included, explaining how to access, set up, and execute the code. This should include details on:

   - **Environment Setup:** Specify required software, libraries, and dependencies, including versions.
   - **Execution Steps:** Provide step-by-step instructions on how to run the code, including any necessary commands or scripts.
   - **Key Functionalities:** Highlight how to access and use key features or modules relevant to the dissertation.

2. **Finalize Codebase:** Before dissertation submission, ensure the codebase is in its final, polished state. This includes:

   - **Removing extraneous code:** Delete any unnecessary or experimental code, commented-out sections, or unused files.
   - **Consistent formatting:** Apply a consistent coding style and formatting throughout the project.
   - **Comprehensive comments:** Add clear and concise comments to explain complex logic, algorithms, and design choices. Focus on explaining the "why" behind the code, not just the "what."
   - **Thorough testing:** Conduct comprehensive testing to verify all functionalities work as expected and produce the results presented in the dissertation. Address any identified bugs or issues.

3. **Provide Detailed Code Documentation:** Structure the codebase into well-defined modules and provide comprehensive documentation for each. This facilitates understanding and allows readers to navigate the code more easily. Documentation should include:
   - **Module-level docstrings:** Provide a high-level overview of each module's purpose, functionality, and how it fits into the overall system.
   - **Function/Method-level docstrings:** Document each function's purpose, inputs, outputs, and any exceptions it may raise.
   - **Inline comments:** Use inline comments sparingly to clarify specific lines of code where necessary. Focus on explaining non-obvious logic or tricky implementations.

Following these steps will ensure the codebase is well-organized, documented, and easy to understand, effectively supporting the research presented in the dissertation. This comprehensive approach minimizes the need for additional clarification and facilitates a smooth review process.

## B. Codebase Finalization

This section details the finalization of the codebase for inclusion with the dissertation. The codebase reflects the distributed nature of the training process, emphasizing the client-side (iPad) focus within the federated learning framework. Clear documentation and access instructions will be provided to facilitate understanding and reproducibility.

The finalized codebase will demonstrate the following key aspects:

1. **Federated Learning Implementation:** The code showcases the complete federated learning workflow:

   - **Client-Side (iPad):** Image generation, model training using TensorFlow.js, efficient caching, and transmission of model updates using Web Workers to maintain iPad interactivity. This implementation leverages the iPad's hardware capabilities for computationally intensive tasks.
   - **Server-Side:** Orchestration, coordination, initial model distribution, aggregation of client updates, and data management.

2. **Efficient Data and Model Handling:** The code implements optimized methods for data transfer and model updates between the iPad and server, including handling small update packages.

3. **Modular Code Structure and Documentation:** The codebase is organized into well-documented modules, clearly explaining the functionality of each component and its role within the overall architecture. This documentation will aid in understanding, reproducibility, and connect the codebase to the experiments conducted. Specific repository/model aspects can be highlighted upon request.

4. **Code Access and Execution:** Clear instructions (e.g., repository access, packaged archive) will be provided for accessing and running the code, ensuring ease of navigation and understanding for reviewers. The codebase will be finalized and free of debugging/testing artifacts before dissertation submission.

### B. Codebase Finalization

This section details the final steps for preparing the codebase for inclusion in the dissertation. The codebase should reflect the final chosen architecture and clearly demonstrate the core functionalities discussed.

1. **Codebase Access Instructions:** Provide clear and concise instructions for accessing the codebase. This might include a link to a repository, specific instructions for downloading and compiling the code, or a description of how to access a hosted solution. Any dependencies or specific setup requirements should be explicitly stated. If alternative architectures were explored, the instructions should clearly indicate how to access and utilize the chosen solution.

2. **Codebase Completeness and Functionality:** The codebase included with the dissertation must be complete, functional, and polished. This includes removing any unnecessary or experimental code, ensuring all required files are included, and verifying its functionality according to the provided access instructions. Any deviations from the initially proposed architecture (e.g., a shift from a PWA to a native application) should be clearly explained, and the codebase should reflect this final architectural decision.

3. **Documented Code Modules:** Clear and well-documented code modules are essential for understanding the core functionalities of the system. Each module should have sufficient comments explaining its purpose, inputs, outputs, and any important implementation details. This documentation should highlight key aspects of the implementation, including:

   - **Server-Side Components (if applicable):** Document the server's role, API endpoints, and any relevant logic, such as federated averaging.
   - **Client-Side Processing:** Detail the client's functionality, including data fetching, image generation, model training, and communication with the server (if applicable). Note any specific libraries or frameworks used (e.g., TensorFlow.js, Canvas API).
   - **Platform-Specific Considerations:** Address any architectural choices and implementation details specific to the chosen platform (e.g., PWA, native iOS application, cloud-based solution). This includes discussing how the code addresses platform-specific challenges, such as resource management, GPU utilization, and background processing.
   - **UI Adaptations (if applicable):** Document any UI changes made to support the implemented functionalities, including new status indicators or interactive elements. Clearly link this documentation to the described functionality in the dissertation.

This documentation will enable reviewers to understand the implementation and assess the feasibility and effectiveness of the chosen approach. It also facilitates potential future work based on the provided codebase.
Codebase Finalization

This section details the finalization of the codebase for inclusion in the dissertation. It outlines the preparation of the Python backend, the iOS application (including the `DynamicPlaneGenerator` module), and the core model, emphasizing the division of responsibilities between client and server. Porting the application to other platforms (e.g., PWA, Android) is out of scope for the dissertation.

**iOS Application:**

The iOS application codebase should be finalized as follows:

- **Functionality:** The code must demonstrate core functionalities, including:
  - Client-side model training using Core ML (both from scratch and fine-tuning).
  - Efficient data handling using Core Data or Realm for OHLCV data.
  - The Metal-accelerated `DynamicPlaneGenerator` module.
- **Documentation:** Clear and comprehensive documentation is essential. The `DynamicPlaneGenerator`, Core ML training process, and data storage mechanism must be thoroughly documented, explaining their purpose, inputs, outputs, and internal logic. This documentation should allow for reproducibility and highlight the architectural decisions behind using Core ML and Metal.
- **Delivery:** A clean, well-commented version of the Swift codebase, along with clear instructions for running the code and reproducing the results presented in the dissertation, must be provided.

**Python Backend:**

The Python backend serves a critical role in data management, API interaction, and model versioning:

- **Responsibilities:**
  - Serving numerical data to the iOS application.
  - Securely managing authentication and API connections (e.g., Zerodha's Kite Connect API).
  - Storing and serving different versions of trained models and aggregating model updates from the app.
- **Documentation:** The delivered Python codebase must be well-documented, clearly explaining its functionality and architecture. The documentation should emphasize the shift in architecture, highlighting the backend's reduced scope and the client's increased responsibility for computation.
- **Delivery:** The finalized Python code, along with clear instructions for its use, will be included.

**Core Model:**

Before final submission, the following steps are crucial for the core model:

- **Fine-tuning:** Perform a final round of fine-tuning to ensure optimal performance before integration.
- **Cleanup:** Remove any experimental code or configurations to create a clean and efficient final version.
- **Streamlining:** The image processing component should be optimized for daily predictions and re-tuning triggered by increased error rates. Future frontend support for other platforms is a separate consideration.

**Overall Codebase Delivery:**

The delivered codebase should be modular, well-documented, and easy to understand. A clear prompt or instructions should accompany the code, specifying the required components and explaining how to reproduce the key experiments presented in the dissertation. The codebase must be complete and functional before dissertation submission.

### B. Codebase Finalization

This section outlines the process for preparing the codebase for inclusion with the dissertation. It addresses the finalization of the core research code and key architectural considerations, particularly regarding frontend and backend technologies, to ensure the final implementation aligns with the project's goals and is accurately reflected in the dissertation. Application development (Android/PWA) is handled separately.

1. **Address Architectural Considerations:** Before finalizing the code, the following points should be addressed:

   - **Frontend Technology:** The potential shift from Swift to Flutter requires thorough investigation. Key considerations include seamless integration with the Python backend (data transfer, authentication, API design), performance in computationally intensive tasks (TensorFlow Lite integration, custom GPU graphics handling, on-device training support), and overall suitability as a Swift replacement.
   - **Backend Technology:** Reaffirm the choice of Python as the backend technology, ensuring its compatibility with the chosen frontend technology.
   - **Client-Side Processing:** Confirm that client-side processing remains the primary approach, leveraging native app permissions and resources. Evaluate the performance implications of any frontend technology changes.

2. **Prepare Codebase for Dissertation:** Create a readily accessible version of the finalized codebase (e.g., dedicated branch or tag) with clear instructions for access and execution. Ensure alignment between the provided code and any frontend implementation (especially regarding ML integration and graphics rendering). Include all necessary dependencies and documentation.

3. **Finalize Codebase:** Ensure the codebase is complete, functional, thoroughly tested, and well-documented. Remove any unnecessary experimental code or clearly mark it as such. Address any outstanding bugs, particularly those related to TensorFlow Lite integration in Flutter, gathering logs and attempting reproduction. Investigate cross-platform compatibility between TensorFlow Lite and Core ML, exploring conditional compilation or runtime checks for Apple device optimization.

4. **Provide Detailed Code Modules:** Organize the code into well-defined, documented modules. Key modules include:
   _ **ML Core (TensorFlow Lite Integration):** Integrates TensorFlow Lite for high-performance ML inference within the Flutter app using `tflite_flutter` for iOS and Android compatibility. The trained model will be converted to `.tflite` format.
   _ **Image Generation:** [ *Add description of the image generation module here.* ]
   - **Data Management:** [ *Add description of the data management module here.\* ]
     Each module's documentation should explain its purpose, functionality, and dependencies. If Flutter is involved, clearly delineate and document Flutter modules, including integrations with TensorFlow Lite or custom shaders for graphics.

Image Generation and Data Management

The Dart-based `DynamicPlaneGenerator` module handles image generation. It normalizes data, performs Principal Component Analysis (PCA), and manages rotation logic using libraries like `ml_linalg`. Rendering leverages Flutter's `CustomPainter` API. Data management is handled by a separate module that uses local storage solutions like SQLite or Hive within the Flutter application to efficiently store and retrieve raw market data. This modular design, coupled with thorough documentation, enhances project understanding and reproducibility.

Codebase Finalization

This section details the preparation of the codebase for dissertation submission, focusing on readability, reproducibility, and future cross-platform compatibility. While immediate mobile deployment is not required, the architecture should accommodate it. This includes a clear blueprint for platform-specific optimizations using Core ML for iOS and TensorFlow Lite for other platforms. Communication between the shared Dart code and native implementations (Swift for iOS, Kotlin for Android) will be handled using Platform Channels.

Key Tasks and Considerations:

- **Model Management:** A clear strategy is essential for managing machine learning models, specifying formats (`.mlmodel` for iOS, `.tflite` for Android), storage, access, and update mechanisms.
- **Model Portability:** The portability of Core ML models to other platforms will be investigated. This involves understanding the `.mlmodel` format and dependencies. Solutions for cross-platform compatibility, such as converting to `.tflite` or ONNX, or using a cross-platform training framework, will be explored and documented.
- **Mobile Platform Integration:** Clear and functional code will integrate the model into both Android and iOS:
  - **Android:** A Kotlin class (`TFLiteHandler.kt`) will manage image data, execute the TensorFlow Lite model, and return predictions. The `.tflite` model will be included in the Android assets.
  - **iOS:** Corresponding iOS integration will be completed and documented using Swift.
  - **Cross-Platform UI and Logic:** The user interface, `DynamicPlaneGenerator`, data management, and core business logic will reside in the shared Dart codebase. Decision logic in Dart will use a `MethodChannel` to call the platform-specific `runPrediction` method for iOS or Android.
- **Codebase Delivery:** The codebase will be prepared for submission (e.g., compressed archive or version control repository) with clear build and execution instructions.

Final Codebase Preparation

This final stage prepares the codebase for inclusion with the dissertation, ensuring a usable and understandable implementation.

1. **Codebase Delivery:** A functional, well-documented codebase, along with a "code prompt" (clear execution instructions including setup, libraries, and example usage), will be provided for review and reproduction.
2. **Codebase Finalization:** Before submission, the codebase will be thoroughly checked for completeness, functionality, and errors. Debugging code, unused variables, and temporary files will be removed, and the code will be cleaned and commented for readability.
3. **Modularization and Documentation:** The code will be organized into well-documented modules, each with clear explanations of purpose, inputs, outputs, and implementation details. A README file will provide a high-level project overview, organization details, and execution instructions.

This process ensures a well-organized, documented, and easily accessible codebase, supporting the dissertation's findings and facilitating review and reproduction. Any discrepancies between initial checklist items and the final codebase implementation have been resolved for consistency and clarity.

### B. Codebase Finalization (Facilitator)

This section outlines the final steps for preparing the codebase for submission alongside the dissertation. This involves providing the necessary code components and ensuring they are well-documented for review. While broader project security considerations (like secure recovery and restoration protocols) are important, they are outside the scope of this section, which focuses specifically on preparing the _research code_ for inclusion with the dissertation.

1. **Provide Codebase for Dissertation:** Deliver the relevant codebase for inclusion with the dissertation. Clearly communicate the delivery method (e.g., compressed archive, link to a repository). Ensure the code is well-organized and easily accessible for reviewers. Include clear instructions on how to access and utilize the codebase (e.g., repository access details, download instructions).

2. **Finalize Codebase:** Ensure the codebase is complete, functional, and polished before dissertation submission. This includes removing unnecessary files (e.g., temporary files, test data), cleaning up comments, and ensuring consistent formatting. The goal is to provide a clean, understandable, and easily navigable representation of your work.

3. **Provide Detailed Code Modules:** Provide well-documented code modules to enhance understanding and reproducibility. This includes:
   - Clear in-code comments explaining the purpose and functionality of each module and function.
   - A comprehensive README file explaining how to run the code, dependencies, and any specific instructions.
   - A requirements file (e.g., `requirements.txt` for Python) listing all necessary libraries and dependencies.
   - Clear examples of how to use the code, including sample input data and expected output.

### B. Codebase Finalization (Facilitator)

This section outlines the process for finalizing the codebase for inclusion with the dissertation. This involves preparing the code for submission and ensuring its clarity, completeness, and reproducibility.

1. **Prepare the Codebase:** Before submission, finalize the codebase by removing any debugging code, temporary files, or unused dependencies. Ensure consistent formatting and comprehensive documentation throughout. This preparation guarantees a clean and understandable codebase for reviewers.

2. **Provide Submission Instructions (Code Prompt):** Prepare clear and concise instructions (a "code prompt") explaining how to access, navigate, and execute the provided code. This might include details on setting up the necessary environment, running specific scripts, or reproducing key results. The instructions should enable reviewers to easily understand and interact with the codebase. Consider providing a minimal reproducible example if appropriate.

3. **Deliver Modularized and Documented Code:** Organize the code into well-defined and documented modules. Each module should have clear documentation explaining its purpose, functionality, and how it integrates into the overall project. This modular approach facilitates understanding and allows reviewers to focus on specific aspects of the codebase relevant to the dissertation. Include a comprehensive README file outlining the project structure, dependencies, and any specific instructions for building or running the code.

### B. Codebase Finalization (Facilitator)

This section outlines the process of preparing the codebase for inclusion with the dissertation. Addressing security considerations is crucial before finalization to ensure the integrity and protect sensitive information.

**Security Considerations:**

Before finalizing the codebase, implement the following security measures:

- **Sensitive Information Handling:** Never hardcode sensitive data (API keys, passwords, etc.) directly into the codebase. Use environment variables, configuration files, or secure credential management systems.
- **Input Validation:** Implement robust input validation and sanitization to prevent vulnerabilities like injection attacks. This is crucial for any code involving user interaction, even for testing purposes.
- **Access Control:** If the codebase involves remote access or execution, implement proper authentication and authorization mechanisms to prevent unauthorized access.
- **Risk Assessment:** Conduct a thorough security risk assessment of the codebase. Consider potential vulnerabilities like unauthorized access, data breaches, and code injection, and implement appropriate mitigation strategies.

**Finalization Steps:**

Once the security considerations are addressed, complete the following finalization steps:

1. **Prepare for Submission:** Prepare the codebase for submission alongside the dissertation. This includes:

   - Removing unnecessary files and sensitive data.
   - Creating a clear README file with instructions for running the code and listing dependencies.
   - Organizing the codebase for easy navigation and understanding. Consider creating an archive (e.g., zip file) for submission.

2. **Finalize Code:** Ensure all code revisions are complete and all functionalities are working as expected before submitting the dissertation. Thoroughly test the code and address any remaining bugs or issues.

3. **Document Code Modules:** Thoroughly document the code, explaining the purpose, inputs, outputs, and any important design choices for each module. Include clear and concise comments within the code and consider creating separate documentation files for complex modules or functionalities.

## B. Codebase Finalization

This section outlines the final steps for preparing the codebase for inclusion with the dissertation. Due to resource constraints, particularly regarding Google Cloud credits (limited to ₹25,000), the development process prioritized efficiency. To manage computational load and expenses, 1-epoch training sessions were conducted on iOS devices using limited data ranges during testing and debugging. This approach enabled thorough testing and refinement while staying within budget. The finalized, optimized codebase will be provided with comprehensive documentation for each module, illustrating this cost-effective development strategy. Clear instructions (the "code prompt") will facilitate access to the core components of the project's technical implementation.

The finalization process entails the following:

1. **Provide Codebase for Dissertation:** The codebase will be submitted according to the dissertation guidelines. This may involve packaging the code, creating a specific branch or tag in version control, and providing clear instructions on how to execute the code.

2. **Finalize Codebase:** Before submission, the codebase will be thoroughly cleaned and prepared. This includes removing unnecessary files, ensuring all necessary components are present, and addressing any remaining bugs or minor issues. Final testing procedures, including those outlined below, will be conducted to guarantee the stability and reliability of the submitted code.

3. **Provide Detailed Code Modules:** The codebase will be organized into well-defined, documented modules. Each module will include comprehensive documentation, including docstrings for functions and classes, explanations of complex algorithms, and potentially a separate README file outlining the overall code structure and dependencies. This documentation aims to facilitate understanding and potential reproducibility of the research.

The following testing measures will ensure the robustness and functionality of the codebase:

- **Mock Data Creation:** Synthetic data, including edge cases such as spikes, flat periods, and gaps in CSV or JSON format, will be generated to rigorously test various code components without relying on the full dataset.

- **Dummy Model for Smoke Tests:** A simplified "dummy" neural network will be created for rapid smoke testing. This smaller, faster model allows for efficient verification of core functionalities without the computational overhead of the full model.

## B. Codebase Finalization

This section outlines the finalization of the codebase for inclusion in the dissertation. This process ensures reproducibility and demonstrates a clear link between the research and its implementation. While not directly related to core model functionality, this preparation is crucial. Note: The experimental "shocker events" and Cognitive Threat Analysis Module (CTAM) components, described below, are _not_ included in the final code submission for the dissertation. These represent potential future research directions and are documented for completeness.

**Deliverables for Dissertation:**

1. **Codebase:** The primary codebase, excluding the experimental CTAM components, will be provided as part of the dissertation submission. This allows for review and potential reproduction of the core model and results. Clear instructions will be included to guide reviewers on accessing and executing the code. This may involve a compressed archive or access to a specific repository branch/tag.

2. **Documentation:** The provided codebase will be well-documented, with clear explanations of individual modules and their functionality. The documentation should detail the purpose and role of each module within the system, including:
   - Data loading and preprocessing within the training pipeline.
   - Training loop execution (forward pass, loss calculation, backpropagation) and its compatibility with Core ML.
   - Model weight extraction after training.
   - The connection between the `DynamicPlaneGenerator` and the Core ML training session on the iPad, specifically how image tensors are provided to the training process.
   - The model update mechanism and its relationship to the single-epoch training used for the smoke test, which verifies core functionalities without full-scale training. Further investigation into how Core ML updates the universal model is required for complete documentation. The necessity of on-device training, given the availability of a pre-trained dummy model, will be carefully evaluated considering performance and resource costs.
   - Clarification of the single-epoch training's purpose as a smoke test rather than for inference.

The experimental CTAM-related code will be documented separately to indicate its experimental nature and avoid confusion with the core dissertation work.

**Pre-Submission Finalization:**

Before submission, the core codebase will be finalized and cleaned. This includes:

- Removing any extraneous experimental code (e.g., "shocker events", CTAM) not directly related to the core research presented in the dissertation, ensuring clarity and focus. The experimental code will be kept separate.
- Ensuring the codebase is complete, functional, and free of debugging or development artifacts.
- Documenting all necessary dependencies.

This finalization ensures the code accurately reflects the work presented in the dissertation.

**Experimental "Shocker Events" and CTAM Components (Not Included in Dissertation Codebase):**

The following components represent future research directions related to "shocker events" and the CTAM. While not included…

## B. Codebase Finalization

This section details the finalization of the codebase for dissertation submission, focusing on its alignment with the core model architecture and proposed enhancements. While the current codebase primarily reflects the existing error-correction mechanism, the dissertation discusses a proposed dual-system architecture as a future development direction. The provided code modules are well-documented to facilitate understanding and potential future modifications.

### Current Model and Limitations

The implemented model, based on an error-correction mechanism (analogous to "wound" and "healing"), exhibits a bias towards mean reversion. This effectively acts as negative feedback, punishing deviations from the mean and creating a "zone of comfort" where performance is strong under normal market conditions. However, this focus on mean reversion results in a smoothing effect that struggles to capture sharp, volatile movements or "shock" events. The simplified framework lacks the nuance to effectively handle complex market behavior, especially during extreme conditions, and is overly sensitive to standard deviations, potentially missing significant trading opportunities.

### Proposed Dual-System Architecture

To address these limitations, a more sophisticated dual-system architecture is proposed, comprising two distinct engines:

- **Flow Engine:** Handles normal market behavior within the established "zone of comfort" where the error-correction mechanism is effective.
- **Threat Engine (CTAM - Cognitive Threat Analysis Module):** Designed to detect and react to outlier "shock" events, acting as a contextual override to the Flow Engine. Upon detecting extreme fluctuations, the Threat Engine signals a shift in action mode, enabling the model to adapt to volatile conditions and potentially capture high-alpha opportunities.

This dual-system approach combines the stability of the existing exploitation-focused Flow Engine with the dynamic responsiveness of an exploration-focused Threat Engine to better handle market extremes.

### Codebase Contents and Documentation

The codebase demonstrates the following components, each with comprehensive in-code documentation:

- **Multi-Source Visual Inputs:** Handles various visual inputs, including primary equity charts, corresponding futures charts, and visualized options chain data (heatmaps), as required by the CTAM. The data ingestion and preprocessing pipeline are clearly documented.
- **Specialized Anomaly Detection Models:** Includes specialized CNNs trained for anomaly detection, with separate modules for equity anomalies (gaps, volume spikes) and derivatives analysis (options chain heatmaps). The training process and architecture are thoroughly documented.
- **Fusion and "Threat Level" Assessment:** Implements the fusion model, generating the Systemic Threat Level (STL) score by combining outputs from specialized detectors. Documentation clarifies how individual detector outputs are combined and the rationale behind the chosen fusion method.
- **Integration with Core Systems (Conceptual):** Includes a conceptual outline and stub functions demonstrating how the STL score would interact with core system components like Pratyahara, Dynamic Plane adjustments, and prediction input.
- **Model Retraining Strategy:** Provides documentation and example configurations illustrating the retraining strategy, emphasizing the consideration of both flow and shock to mitigate performance plateaus. Retraining frequency and criteria are clearly outlined.

### Dissertation Focus and Code Submission

For dissertation inclusion, the codebase should specifically highlight the following aspects related to dynamic prediction weighting and the Shockwave Prediction Model (SPM):

1. **Targeted Code Request:** A clear request should be made for the specific code required, focusing on modules related to Adaptive Strategy Weighting, Opportunistic Threat Response, the SPM, the seesaw mechanism, and profit maximization strategies.
2. **Finalized and Functional Code:** Ensure the codebase, particularly the SPM and seesaw mechanism (dynamic prediction weighting), is complete, well-documented, and functional _before_ dissertation submission. Any experimental code should be clearly marked and its inclusion justified.
3. **Documented Modules:** Provide detailed documentation for relevant modules, especially the SPM and seesaw mechanism.

This finalized codebase, while primarily reflecting the current model, provides a strong foundation for future research and development toward the proposed dual-system architecture. The comprehensive documentation enables reproducibility of the reported results and facilitates further exploration of the outlined enhancements.

## B. Codebase Finalization (Facilitator)

This section outlines the final steps for preparing the codebase for inclusion with the dissertation. This ensures the research is reproducible and provides a clear understanding of the implemented system. A key element of this process is modularity, enabling clear separation of functionalities and facilitating future extension or adaptation. A well-defined API or clear execution instructions will serve as the entry point for interacting with these modules.

1. **Provide Codebase for Dissertation:** The finalized and documented codebase will be submitted along with the dissertation. A concise README file or equivalent documentation will provide clear instructions for navigating, executing, and reproducing the experiments. This should include details on system requirements, dependencies, installation procedures, and example usage scenarios. This "code prompt" ensures that others can easily understand and interact with the code.

2. **Finalize Codebase:** Before submission, the codebase will undergo a final review and cleanup process. This includes removing any temporary or experimental code not directly relevant to the dissertation, ensuring consistent coding style and formatting, and thoroughly testing all functionalities. The resulting codebase should be clean, well-commented, and easy to understand.

3. **Provide Detailed Code Modules:** Comprehensive documentation for each module will be provided within the codebase. This documentation should clearly describe the purpose of each module, its inputs and outputs, key algorithms and data structures used, and any dependencies on other modules. The provided API or execution instructions will clarify how these modules interact to achieve the overall system functionality. This modular approach promotes understanding, maintainability, and potential reuse of the code.

## Codebase Finalization (Facilitator)

This phase prepares the codebase for inclusion in the dissertation, ensuring its clarity, organization, and adherence to the defined architectural principles. The Facilitator plays a crucial role in coordinating these final steps.

This involves ensuring the codebase is functional, documented, and organized according to the four functional pillars (Continuity, Enforcement, Facilitation, and Specialization). Specifically, the Facilitator is responsible for:

1. **Preparing the Codebase for Submission:** This includes packaging the code (e.g., creating a designated repository or archive) and managing its transfer for inclusion with the dissertation.

2. **Finalizing the Code:** This entails ensuring all code is functional, documented, and free of any unnecessary or test code. Any outstanding bugs or issues should be resolved before submission. This also includes confirming adherence to the established architectural principles.

3. **Documenting the Modules:** Each module must have comprehensive documentation explaining its purpose, inputs, outputs, dependencies, and its role within the overall architecture. This documentation should align with the chosen function naming convention (either suffixing all function names with their type or deriving function types from their class category).

Crucially, the finalized codebase must reflect the core architectural principles:

- **No Direct Specialist Interactions with External Systems:** Specialists (modules responsible for specific tasks like model training or data processing) should not interact directly with external systems, including the dissertation submission process. The Facilitator manages all external interactions.

- **Enforcer Manages Data Storage:** The Enforcer modules handle all data storage and retrieval related to the finalized codebase. The Facilitator coordinates with the Enforcer to maintain consistent data management practices.

- **Consistent Function Naming:** A consistent function naming convention must be applied throughout the project to enhance readability and maintainability.

- **Clear Functional Pillar Definitions:** The code organization should clearly reflect the four functional pillars (Continuity, Enforcement, Facilitation, and Specialization), clarifying the architectural principles guiding its development.

Furthermore, the following finalized and fully documented specialist services are essential components of the submitted codebase:

- **`NormalizeWindow`**: Normalizes raw numerical array data based on a provided configuration. It must not depend on `google-cloud-storage`, `google-cloud-firestore`, or `requests`. The documentation should specify the normalization method and input/output formats.

- **`ComputePrincipalComponents`**: Performs principal component analysis (PCA) on normalized data using only the `numpy` library and returns the top two principal component vectors. The documentation should detail the PCA algorithm used.

- **`ProjectToPlane`**: Projects data onto a 2D plane defined by provided basis vectors. The documentation must specify the projection method and the format of input data and basis vectors.

- **`TrainOneEpoch`**: Executes a single training epoch using provided model artifact bytes, training data tensors, and configuration. The documentation should specify the outputs and any relevant details regarding the training process.

By adhering to these guidelines, the final codebase will be a valuable addition to the dissertation, demonstrating sound software engineering practices and enabling future research based on this work.

### B. Codebase Finalization

This section outlines the final steps for preparing the codebase for submission alongside the dissertation. The goal is to provide a clean, well-documented, and easily reproducible codebase that effectively supports the dissertation's findings. While the original checklist mentioned various service types (Facilitator, Enforcer, and Continuity), this finalization process focuses on preparing the code itself, regardless of its underlying architecture.

1. **Providing the Codebase:** Deliver a complete and functional codebase, including all necessary scripts, configuration files, and data preprocessing steps. Package the codebase as a compressed archive (e.g., .zip, .tar.gz) or within a clearly defined Git repository for easy access and review. Include a README file with clear instructions on how to execute the code, outlining dependencies, required libraries, and steps to reproduce any experiments or results presented in the dissertation.

2. **Finalizing the Code:** Thoroughly review, clean, and comment the entire codebase. Remove any debugging code, temporary files, or unused dependencies. Ensure consistent formatting and adherence to coding best practices. Critically, verify that the code runs without errors and produces the expected results, ensuring reproducibility.

3. **Providing Detailed Documentation:** Provide comprehensive documentation for each module within the codebase. This documentation should clearly describe the module's purpose, functionality, inputs, outputs, and any dependencies. Use docstrings within the code to explain individual functions and classes. Consider using external documentation (e.g., a separate document or a wiki) to provide a higher-level overview of the architecture and design choices. For complex algorithms or processes, include pseudocode to enhance clarity. This thorough documentation will enable reviewers to understand the code's role within the larger project context of the dissertation.
   Codebase Finalization

This section outlines the process of preparing the codebase for inclusion with the dissertation. The focus is on clarity, reproducibility, and comprehensive documentation to ensure the code effectively supports the research presented.

To achieve this, the following steps are recommended:

1. **Prepare the Codebase:** This involves creating a clean, well-organized version of the codebase specifically for the dissertation. Remove any extraneous files or experimental code not directly relevant to the core findings. Create a compressed archive (e.g., .zip) or use a version control repository (e.g., GitHub) for submission. Include clear instructions on how to access and run the code, specifying dependencies, necessary data, and expected outputs.

2. **Finalize Functionality and Documentation:** Before submitting the dissertation, ensure the codebase is in its final, polished state. This includes:

   - Removing any unnecessary files or debugging code.
   - Ensuring consistent formatting and style.
   - Thoroughly testing and debugging all code modules.
   - Addressing any performance bottlenecks.
   - Adding clear and concise comments to explain the functionality of different code sections.

3. **Create Detailed Code Modules:** Structure the codebase into well-defined, documented modules. Each module should have a clear purpose and accompanying documentation outlining:
   - The module's functionality.
   - Input parameters and expected outputs.
   - Any dependencies or special requirements.
   - Examples of how to use the module.

For larger projects, consider structuring the code documentation similarly to the sections proposed for the dissertation itself (e.g., "Architectural Philosophy," "Functional System Design"). This modular approach improves code readability and facilitates understanding and potential reuse. While advanced tooling like distributed tracing may be relevant for production systems, prioritize clear, concise documentation within the code itself for the dissertation. This process ensures the codebase is a well-structured, understandable component of the research, facilitating review and reproducibility.

## B. Codebase Finalization (Facilitator)

This phase prepares the codebase for submission alongside the dissertation, ensuring reproducibility, maintainability, and a clear demonstration of the system's architecture and performance. This involves essential tasks for submission and optional enhancements to further strengthen the project.

**Essential Tasks:**

1. **Codebase Submission:** Provide the complete and functional codebase as supplementary material, including clear instructions (a "code prompt") on how to execute it. This should ideally be through a shared repository or a packaged archive.

2. **Finalization:** Ensure the codebase is finalized, stable, and runnable before submission. This includes thorough testing, debugging, consistent formatting, and removal of any unnecessary files.

3. **Modular Documentation:** Organize the code into well-documented modules showcasing key system components. Use in-code documentation (comments and docstrings) to explain the functionality of each module and its components.

**Optional Enhancements:**

While not required for dissertation submission, the following enhancements can significantly improve the project's analytical depth and operational robustness:

- **Advanced Error Signal:** Implement a comprehensive error signal incorporating Vector Deviation Error and Frame Shift Error for a more nuanced understanding of model health.

- **Performance-Based Healing:** Implement adaptive system "healing" (model recalibration or other corrective actions) triggered by prediction accuracy, enabling more efficient resource allocation.

- **Multi-Scale Periodicity:** Incorporate intraday, daily, and weekly data to enhance the model's understanding of cyclical market patterns and improve long-term trend prediction.

- **"Rally Time" Prediction:** Predict the expected timeframe (in candlesticks) for market movements, providing valuable insight for trade timing and risk management.

- **Distributed Tracing:** Integrate system-wide distributed tracing using OpenTelemetry to visualize data flows and performance bottlenecks, facilitating deeper analysis and optimization.

**Integrating Key Concepts into the Codebase:**

The codebase should reflect the core principles of the project, such as modularity, explainability, and robust backtesting. This includes:

- **Modularity and Documentation:** Implement a modular structure reflecting the distinct services of the system (e.g., data acquisition, preprocessing, model training, backtesting, portfolio construction, and risk management). Thoroughly document each module's purpose, inputs, outputs, and dependencies.

- **Backtesting Integration:** Integrate the backtesting framework into the codebase, clearly demonstrating how to execute backtests with various parameters and configurations.

- **Adaptive Seesaw Blending Integration:** Integrate the Meta-Model implementation for Adaptive Seesaw Blending, clearly documenting its interaction with the "Flow" and "Shock" prediction engines.

- **Explainability:** Emphasize explainability through clear and concise code comments and documentation. Include a section explaining the model's decision-making process, using examples and visualizations where possible.

By addressing these points, the finalized codebase will not only fulfill the submission requirements but also effectively communicate the project's complexity and technical contributions, enabling reviewers and future researchers to understand and appreciate the work fully.
Codebase Finalization

This section details the final steps to prepare the codebase for inclusion with the dissertation. The focus is on organization, documentation, and ensuring the code is readily accessible for review and, if necessary, reproduction. This does _not_ include the architectural enhancements for paper trading (e.g., `Paper_Brokerage_Simulator`, `Live_Execution_Enforcer` toggling, UI changes, and dedicated Firestore storage) discussed elsewhere and considered future development.

The finalized codebase should include:

1. **Complete and Functional Codebase with Clear Instructions:** A clean, well-documented version of the complete codebase, suitable for submission alongside the dissertation, must be provided. This might involve creating a dedicated branch or repository. Clear instructions (a "code prompt") explaining how to navigate, execute, and understand the codebase are crucial. This prompt should be tailored to the dissertation's audience, guiding them through the key components and functionality.

2. **Robust and Reproducible Code:** Before dissertation submission, ensure the code is thoroughly tested, free of bugs and temporary/experimental code, and aligns with the dissertation's descriptions and findings. Dependency management should be clearly documented to ensure reproducibility. Consistent code style and formatting further enhance clarity.

3. **Well-Documented Modules:** The code must be organized into well-defined and documented modules. Each module should have clear explanations of its purpose, inputs, outputs, and any relevant algorithms or techniques used. For example, the `Narrative_Generation_Service` module should be fully documented, including details on its integration with the Feature Store, implementation of attribution methods (LIME, SHAP, and attention maps), use of the Feature Store for reporting and compliance, and any optional integration of "explanation AI." This modular documentation facilitates understanding, evaluation, and potential future modifications. Furthermore, services such as the `CalculateOrderBookImbalance` module, which calculates the Order Book Imbalance (OBI) from the Zerodha market depth data, should also be thoroughly documented, addressing any limitations in data precision (e.g., Zerodha's increments of ₹0.05).
   Codebase Finalization

This section outlines the final steps for preparing the codebase for inclusion with the dissertation. This involves providing the complete, documented, and tested codebase, along with clear instructions for execution and reproduction of results. Before finalization, the following enhancements and additions, critical for reflecting the latest research findings, should be integrated and thoroughly tested:

- **Model Profitability and Tracking Enhancement:** Implement price improvement calculations to enhance model profitability tracking and ensure a more robust performance evaluation.

- **Develop `CalculatePriceImprovementRate` Specialist Service:** Implement and document the `CalculatePriceImprovementRate` service. This service calculates the rolling average of price improvement, providing the "Price Improvement Rate" feature used as a context token within the Vision Transformer model.

- **Integrate Price Improvement Rate as Feature:** Integrate the "Price Improvement Rate" as a context token within the Vision Transformer predictive models, thoroughly documenting the implementation details.

- **Enhance `ComputeOrderBookState` Service:** Update the `ComputeOrderBookState` service to generate a feature vector including both the Order Book Imbalance (OBI) and the "Book Resilience Score." Document the implementation and usage of this enhanced feature vector. Note that for OBI calculations on small order books (e.g., spread < 0.25), spread calculations are discarded, focusing instead on order quantities and counts at the five fixed bid/ask levels. The OBI output is a normalized float between -1.0 and +1.0, representing selling and buying pressure, respectively. This serves as a fourth dimension (alongside Time, Price, and Volume) for the `DynamicPlaneGenerator`. Furthermore, the `GenerateDepthQuantityHeatmap` service generates a heatmap visualization of market depth changes over time, using order quantities at the 5 bid and 5 ask levels over N time steps. This heatmap, input to the `MarketDepthAnomalyDetector` CNN, aids in detecting unusual patterns.

- **Implement Execution Quality Feedback Loop:** Implement and document the feedback loop within the Self-Correction & Healing Controller. This loop monitors execution quality and adjusts the `CorrectionFactor` as needed.

Following the integration and testing of these enhancements, the finalized codebase should be prepared for submission. This includes:

1. **Provide Codebase:** Submit the complete codebase (including scripts, modules, and data processing pipelines) as supporting material. Include a clear README file with the directory structure, dependencies, and execution instructions (a code prompt) to facilitate reproduction of results. Consider providing a concise example execution command to demonstrate key functionalities.

2. **Finalize Codebase:** Ensure all code is finalized, cleaned of temporary or debugging code, and thoroughly tested before submission. Add comprehensive comments and documentation throughout.

3. **Provide Detailed Code Modules:** Organize the code into well-defined, documented modules. For each module, provide descriptions of its purpose, inputs, outputs, dependencies, and any relevant design decisions. Adhere to coding best practices for readability and maintainability. This modular structure and clear documentation will aid in understanding the overall architecture and functionality of the system.
   Codebase Finalization

This section outlines the final steps for preparing the SCoVA (Snapshot Computer Vision Algorithm) codebase for inclusion with the dissertation. This involves delivering the code, ensuring its organization and documentation are suitable for review, and confirming alignment with the core algorithmic design decisions.

Before finalizing the codebase, the following architectural and design aspects must be addressed:

- **Computer Vision Integration:** Clearly document SCoVA's use of computer vision as its primary modality throughout the codebase. This key technical specification should be prominently highlighted.

- **Non-Hierarchical Asymmetric Design:** Consider integrating "Non-Hierarchical Asymmetric" into the algorithm's name (e.g., ANHASCoVA) and explore its implications for the algorithm's architecture. This should be finalized _before_ the codebase is submitted.

- **Graph-based Perceptual Model:** If a graph-based perceptual model using a Graph Neural Network (GNN) with dynamic influence learning between timeframes replaces the previous model, ensure the codebase reflects this new architecture and its functionality. This includes thorough documentation of the GNN implementation and the dynamic learning process.

Once the above considerations are implemented and documented, the finalization process entails the following steps:

1. **Provide Codebase for Dissertation:** Deliver the complete and functional SCoVA codebase. Specify the delivery method (e.g., repository link, archived file) and provide clear instructions for executing the code and reproducing the dissertation's results.

2. **Finalize Codebase:** Ensure the codebase is in its final, polished state before dissertation submission. This includes:

   - Incorporating all revisions, optimizations, and bug fixes.
   - Removing any unused code or experimental components not directly related to the SCoVA project.
   - Ensuring consistent code style and formatting.

3. **Provide Detailed Code Modules:** Organize the code into well-defined, documented modules. For each module, provide comprehensive documentation that explains:
   - Its purpose and functionality.
   - Inputs and outputs.
   - Any relevant implementation details.
   - The "snapshot" aspect of SCoVA, emphasizing its use of discrete, dynamically generated visual snapshots of market data rather than continuous time-series data.
   - The finalized SCoVA architecture.

This meticulous approach to codebase finalization will enhance the understandability, reproducibility, and potential future extension of the SCoVA algorithm.

## Codebase Finalization for Dissertation

This phase focuses on preparing the codebase for inclusion with the dissertation. This involves ensuring clarity, organization, thorough documentation, and easy reproducibility of key model enhancements and features. It also includes preparing the codebase for submission and ensuring its maintainability.

**Documentation and Submission:**

- **Code Submission:** A clean and concise version of the codebase will be prepared for submission alongside the dissertation. Specific instructions regarding the submission method (e.g., packaging, repository upload) will be provided. A README file summarizing the project, structure, and execution instructions will be included.
- **Comprehensive Documentation:** The code will be thoroughly documented using comments and docstrings to explain the functionality of modules, functions, classes, their parameters, and return values. This will facilitate understanding and potential reproduction of the results.

**Key Model Enhancements and Features:**

- **Asymmetric Loss Function:** The implemented asymmetric loss function penalizes underestimation of losses more heavily than overestimation, reflecting a risk-averse approach. The documentation details how different penalty factors are applied when predicted returns are better or worse than actual returns, specifically highlighting the increased penalty for larger-than-predicted losses.
- **Vision Transformer Integration with Regime ID:** The Vision Transformer (ViT) architecture now accepts the Regime ID as a contextual token, replacing the previous use of raw asymmetric features. The code and documentation clearly demonstrate this integration and the Regime ID's usage within the model.
- **State-Dependent Attention Mechanism:** This mechanism dynamically adjusts its focus based on the market state. The documentation explains how market indicators (e.g., volatility, threat level) influence the attention weights.

**Final Codebase Structure and Integration:**

- **Asymmetric Feature Integration:** The `AsymmetricFeatureEngine` is fully integrated into the workflow. The `Workflow_Broker` interacts with the `AsymmetricFeatureEngine` to retrieve the calculated feature vector, serving as a context token input to the `Model_Inference_Service` alongside the Dynamic Plane image tensor. The ViT architecture is adapted to accommodate this combined input.
- **Simplified Vector Input for ViT:** A computed vector containing relevant market asymmetry information is used as direct input to the ViT. This approach prioritizes simplicity and future extensibility.
- **Asymmetry and Volatility Enhancements (Exploration):** While the simplified vector input is prioritized, potential enhancements to improve the model's understanding of market asymmetry and volatility will be carefully explored. These explorations will be balanced against the risk of over-engineering, prioritizing long-term maintainability and adaptability without requiring significant pipeline restructuring.

**Specific Code Modules Requiring Detailed Documentation:**

- **Asymmetric Prediction Models (`Bull_Flow_Engine` and `Bear_Flow_Engine`):** The code for these distinct models, trained on bull and bear market data respectively, is clearly delineated. The regime-detection model's logic for engine selection is also documented.
- **Asymmetric Risk Management (`Portfolio_Risk_Manager`):** The documentation explains the implementation of different risk parameters for short and long positions, highlighting the rationale behind this asymmetry.
- **Asymmetric Self-Correction (`HealingController`):** The code demonstrates how the system reacts differently to losses versus gains, reflecting the principles of prospect theory.
- **Asymmetric Feature Engineering (`CalculateAsymmetricFeatures`):** The documentation details the calculation of Upside and Downside Volatility and other asymmetric features, including their usage as a context token for the ViT.
- **Upside/Downside Volatility Calculation:** The calculation of these measures (standard deviation of positive and negative returns, respectively) within the input data window is clearly defined.

### B. Codebase Finalization

This section details the final steps for preparing the codebase for inclusion with the dissertation. It outlines the required enhancements related to Dual-Token Context Injection and the subsequent steps for code delivery and finalization.

**Pre-Finalization Enhancements (Dual-Token Context Injection):**

The following enhancements are crucial for implementing the Dual-Token Context Injection and must be completed before finalizing the codebase:

1. **Asymmetric Feature Vector Generation:** Modify the `AsymmetricFeatureEngine` to calculate and output a raw vector of asymmetric features. These features should capture relevant market asymmetries, such as upside/downside volatility and skewness. This vector will serve as input for the Dual-Token Context Injection mechanism, providing granular context.

2. **Asymmetric Regime Identification:** Enhance the `IdentifyAsymmetricRegime` module to generate a discrete Regime ID based on the computed asymmetric feature vector. This ID represents the current market regime and will also be used as input for the Dual-Token Context Injection, offering a high-level contextual representation.

3. **Vision Transformer Adaptation:** Modify the Vision Transformer (ViT) architecture to accommodate the Dual-Token Context Injection. This involves adding two new input tokens: the Regime ID token (categorical) and the Asymmetric Vector token (continuous). The ViT's self-attention mechanism should be adapted to effectively incorporate these new tokens alongside the existing dynamic plane image input. The code should clearly demonstrate how the ViT processes these distinct token types within its pipeline. Furthermore, the implementation should demonstrate the conditional use of the feature vector based on its explanatory power, bypassing its computation if it doesn't add value beyond the regime classifier to avoid performance bottlenecks.

**Codebase Delivery and Finalization:**

After implementing the Dual-Token Context Injection and ensuring all other codebase components are functional and well-documented, complete the following steps for finalization and delivery:

1. **Codebase Delivery:** Deliver the complete and functional codebase, accompanied by clear instructions for execution and result reproduction. Include a README file with detailed execution instructions, or provide a dedicated script to automate the process.

2. **Codebase Refinement:** Ensure the codebase is thoroughly tested, well-commented, and clean. All dependencies should be clearly documented and easily installable.

3. **Module Documentation:** Provide comprehensive documentation for all code modules, particularly those related to the model's core functionalities and the Dual-Token Context Injection. This documentation should explain the purpose, input, output, and internal workings of each module, facilitating understanding and potential future extensions. Specifically, the documentation should highlight how the Dual-Token Context Injection provides flexibility, maintains data integrity, and enables both high-level regime-based explanations and granular details when necessary.
